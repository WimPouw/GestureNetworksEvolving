---
title             : "Towards a gesture phonetics: A multiscale kinematic analysis of evolving manual languages in the lab"
shorttitle        : "Towards a gesture phonetics"

author: 
  - name          : "Wim Pouw"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Montessorilaan 3, 6525 HR Nijmegen"
    email         : "w.pouw@psych.ru.nl"
  - name          : "Mark Dingemanse"
    affiliation   : "1,3"
  - name          : "Yasamin Motamedi"
    affiliation   : "4"
  - name          : "Asli Ozyurek"
    affiliation   : "1,2,3"

affiliation:
  - id            : "1"
    institution   : "Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen"
  - id            : "2"
    institution   : "Institute for Psycholinguistics, Max Planck Nijmegen"
  - id            : "3"
    institution   : "Center for Language Studies, Radboud University Nijmegen "
  - id            : "4"
    institution   : "Language and Cognition Lab, University College London"

authornote: |
  This work is supported by a Donders Fellowship awarded to Wim Pouw and Asli Ozyurek and is supported by the Language in Interaction consortium project 'Communicative Alignment in Brain & Behavior' (CABB).

abstract: |
  Reverse engineering how language emerged is a daunting interdisciplinary project. Experimental cognitive science has contributed to this effort by invoking in the lab constraints likely playing a role for language emergence; constraints such as iterated transmission of communicative tokens between agents. Since such constraints played out over long phylogenetic time and involved vast populations, a crucial challenge for iterated language learning paradigms is to extend its limits. In the current approach we perform a multi-scale quantification of kinematic changes of an evolving silent gesture system. Silent gestures consist of complex multi-articulatory movement that have so far proven elusive to quantify in a structural and reproducable way, and is primarily studied through human coders meticulously interpreting the referential content of gestures. Here we reanalyzed video data from a silent gesture iterated learning experiment (Motamedi et al. 2019, experiment 1), which originally showed increases in systematicity of gestural form over language transmissions. We applied a signal-based approach, first utilizing computer vision techniques to quantify kinematics from videodata. Then we performed a multi-scale kinematic analysis showing that over generations of language users, silent gestures became more efficient and less complex in their kinematics, and we detect systematicity of the communicative tokens's interrelations which proved itself as a proxy of systematicity obtained via human observation data. Thus we demonstrate the potential for a signal-based approach of language evolution in complex multi-articulatory gestures. 
  
keywords          : "language evolution, silent gesture, kinematics, systematicity"
wordcount         : "X"

bibliography      : ["r-references.bib"]

fig_caption       : no
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man, noextraspace"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
library("papaja")     #papaja::apa6_pdf papaja::apa6_word
library(ggplot2)      #plotting
library(gridExtra)    #plotting (constructing multipanel plots)
library(ggExtra)      #plotting (adding distributions)
library(RColorBrewer) #plotting (color schemes)
library(dtw)          #dynamic time warping functions
library(effsize)      #effectsizes calculations
library(igraph)       #network graphing and analysis (network entropy)
library(cluster)      #cluster analysis (agglomerative clusterting coefficient)
library(nlme)         #mixed linear regression
library(signal)       #for butterworth filter
library(TDAstats)     #for persistent homology clusteriness analusis
library(pracma)       #for peak finding
library(r2glmm)       #mixed regression R^2
library(DescTools)    #Entropy calculation of the original motamedi data
library(EMAtools)     #cohen's d for mixed regression predictors
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r main_data_load, include = FALSE}
#prepare the data
  #time series data (ts): This file contains all the gesture time series and relevant info about the gesture (chain etc)
  #matrices data: This is the folder where all the gesture network matrices are stored and are created in the code chunk below
  #meta: This contains the original data from Motamedi where e.g., gesture codes, unique information units are given for each gesture

#SET FOLDERS: !!!PLEASE CHANGE TO OWN FOLDER STRUCTURE FOR CODE TO RUN APPROPRIATELY!!!
basefolder <- "D:/Research_Projects/GESTURE DATABASES FOR FDM/OPEN DATABASES/EmergingLanguageKirby/"
ts <- read.csv(paste0(basefolder, "/Experiment1/Data/ts_exp1.csv"))
matrices_data <- paste0(basefolder, "/Experiment1/Data/DistanceMatrices/")
plotfolder <- paste0(basefolder, "/Experiment1/Plots/")

  #load in original METAdata Motamedi into time series
  meta <- read.csv(paste0(basefolder, "/experiment1/experiment1/Data/ex1.csv"))
    #change the naming in the meta file and time so that it overlaps with the naming in the time series file
  meta$target <- gsub(" ", "_", meta$target)
  ts$object <- as.character(ts$object)
  ts$object[ts$object == "singer"] <- "sing"
  meta$target[meta$target == "to_take_a_photo"] <- "take_photo"
  
  #loop through object names in the time series file, match with meta naming, and change to time series naming 
  for(i in unique(ts$object))
  {    
    meta$target[grep(i, meta$target)] <- i
  }
  
  #recompute entropy Motamedi
  meta$uniq_codestring <- NA 
  #first only keep unique human-coded information elements in each gesture (this is where entropy is calculated on)
  for(r in 1:nrow(meta))
    {
    strr <- c(strsplit(as.character(meta$code_string[r]), split = ",")[[1]]) # get a vector with all elements
    if(length(strr) > 0) #if there is more than one element
    {
    meta$uniq_codestring[r] <- paste0(strr[!duplicated(strr)], collapse = ',') #remove duplicates and combine them in a single string again, then save into unique code string 
    }
  }
  
  #compute entropy on the concatenated list fo gesture information units per participant
  meta$entropy <- NA #initalize variabel which will contain ppn-level entropy, it will be saved in the metadataset
    for(ppn in unique(meta$participant))
    {
    gstring     <- paste0(as.character(meta$uniq_codestring[meta$participant == ppn]),collapse = ',')
    gstring     <- as.data.frame(strsplit(gstring, split = ","))
    gstring[,1] <- as.numeric(factor(gstring[,1]))
    meta$entropy[meta$participant==ppn]     <- Entropy(gstring[,1]) #add entropy measure to Motamedi meta file
    }

#load in time series object for each gesture the video_length, verb (no, yes), number of reps, guess time, code length, code_string(entropy calc)
  
  unique_videos <- unique( paste0(ts$ppn, ts$object)) #make an object where we can identify each unique gesture in the dataset
  #make new variables in in the time series files
  ts$ann_guesstime <- ts$ann_verb <- ts$ann_gcode <- ts$ann_reps <-  ts$ann_inf_units <-ts$segments <- ts$ann_entropy  <- NA
  #loop through all videos and load in relevant data from the meta files so that Time series data have this info too
  for(v in unique_videos)
  {
    condts <- (paste0(ts$ppn, ts$object)==v) #set condition for selection time series
    condmet <- (paste0(meta$participant, meta$target)==v) #set condition for selection time series
    
    #load in the meta data into time series object so that this information can be easily retrieved
    #when using these objects for analysis
    if(TRUE %in% condmet)
    {
    ts$ann_guesstime[condts]  <- unique(meta$guess_time[condmet])
    ts$ann_verb[condts]       <- as.character(unique(meta$verb[condmet]))
    ts$ann_gcode[condts]      <- as.character(unique(meta$code_string[condmet]))
    ts$ann_reps[condts]       <- unique(meta$num_reps[condmet])
    ts$ann_inf_units[condts]  <- unique(meta$code_len[condmet])
    ts$segments[condts]       <- unique(meta$num_reps[condmet])+unique(meta$code_len[condmet])
    ts$ann_entropy[condts]    <- unique(meta$entropy[condmet])
    }
  }
#hand correct police-officer, which should be not ("N") a verb
ts$verb[ts$object == "police_officer"] <- "N"
```

```{r repeated_functions, echo = FALSE, warning = FALSE}
#FUNCTION extractR.traces: we often need to extract the relevant movement traces from the dataset for input for the Multivariate DTW
  #the relevant z-scaled and centered x,y, traces are left and right hand movement, head movement
  #this function extracts these traces to be used as input for DTW
extractR.traces <- function(dat)
{
  dat <- data.frame(dat)
  ts1 <- cbind( as.vector(scale(dat$x_index_left, center = TRUE)),
                as.vector(scale(dat$y_index_left, center = TRUE)),
                as.vector(scale(dat$x_index_right, center = TRUE)),
                as.vector(scale(dat$y_index_right, center = TRUE)),
                as.vector(scale(dat$x_wrist_left, center = TRUE)),
                as.vector(scale(dat$y_wrist_left, center = TRUE)),
                as.vector(scale(dat$x_wrist_right, center = TRUE)),
                as.vector(scale(dat$y_wrist_right, center = TRUE)),
                as.vector(scale(dat$x_nose, center = TRUE)),
                as.vector(scale(dat$y_nose, center = TRUE)))
} 


#FUNCTION DTW.compare:  This function performs the multidimensional dynamic time warping (D) score
  #It takes two multivariable time series (see extractR.traces) as argument, and it takes as argument whether only hands should be compared 
DTW.compare <- function(TS1, TS2, manualonly)
{
    #make sure that if there is nothing detected than set to 0
    TS1 <- ifelse(is.nan(TS1), 0, TS1)
    TS2 <- ifelse(is.nan(TS2), 0, TS2)
    
    #perform the dynamic time warping, extract the distance, and then sum the score
    distancedtw <-  dtw(  TS1[,1:2],   TS2[,1:2])$normalizedDistance +
                  dtw(  TS1[,3:4],   TS2[,3:4])$normalizedDistance +
                  dtw(  TS1[,5:6],   TS2[,5:6])$normalizedDistance + 
                  dtw(  TS1[,7:8],   TS2[,7:8])$normalizedDistance +
                  dtw(  TS1[,9:10],   TS2[,9:10])$normalizedDistance
    
    #do the same procedure but only for the index and wrist traces (will be use to compare performance of different body   points)
    if(manualonly == "manual_only")
    {
    distancedtw <-  dtw(  TS1[,1:2],   TS2[,1:2])$normalizedDistance +
                    dtw(  TS1[,3:4],   TS2[,3:4])$normalizedDistance +
                    dtw(  TS1[,5:6],   TS2[,5:6])$normalizedDistance + 
                  dtw(  TS1[,7:8],   TS2[,7:8])$normalizedDistance
    }
    return(distancedtw)
}

#dimensionless smoothness measure
smooth.get <- function(velocity) #Hogan & Sternad formula
{
  if(!all(velocity ==0))
  {
  velocity <- as.vector(scale(velocity))
  acceleration <- butter.it(diff(velocity))
  jerk         <- butter.it(diff(acceleration))
  integrated_squared_jerk <- sum(jerk^2)
  max_squaredvelocity <- max(velocity^2)
  D3 <- (length(velocity))^3
  jerk_dimensionless <-  integrated_squared_jerk*(D3/max_squaredvelocity)
  smoothness <- jerk_dimensionless
  }
  if(all(velocity ==0)) #if all zero, this
  {
  smoothness <- NA
  }
  return(smoothness)
}

#butterworth filter
butter.it <- function(x)
{bf <- butter(1, 1/33, type="low")
x <- as.numeric(signal::filter(bf, x))}

#kinematic feature extraction:
  #THIS FUNCTION EXTRACTS FOR ALL keypoints the: submovements, intermittency (smoothness), rhythm, 
  #temporal variability (rhythmicity), gesture space
kin.get <- function(MT)
  {
    MT <- data.frame(MT)
  #perform submovement analysis(using findpeaks function), and also compute rhythm and temporal variability from it
    #extract peaks from velocity time series
    peaksnose       <- findpeaks(as.vector(scale(MT$velocity_nose)), minpeakdistance = 8, minpeakheight = -1, threshold=0.1)
      rhythmnose    <- abs(diff(MT$time_ms[peaksnose[,2]]))/1000 #compute interval in time between peaks
    #extract peaks from velocity time series
    peaksleft_w     <- findpeaks(as.vector(scale(MT$velocity_left_w)), minpeakdistance = 8, minpeakheight = -1, threshold=0.1)
      rhythmleft_w    <- abs(diff(MT$time_ms[peaksleft_w[,2]]))/1000 #compute interval in time between peaks
    #extract peaks from velocity time series
     peaksright_w    <- findpeaks(as.vector(scale(MT$velocity_right_w)), minpeakdistance = 8, minpeakheight = -1,threshold=0.1)
      rhythmright_w    <- abs(diff(MT$time_ms[peaksright_w[,2]]))/1000 #compute interval in time between peaks
    #extract peaks from velocity time series
    peaksleft_i     <- findpeaks(as.vector(scale(MT$velocity_left)), minpeakdistance = 8, minpeakheight = -1,threshold=0.1)
      rhythmleft_i    <- abs(diff(MT$time_ms[peaksleft_i[,2]]))/1000 #compute interval in time between peaks
    #extract peaks from velocity time series
    peaksright_i    <- findpeaks(as.vector(scale(MT$velocity_right)), minpeakdistance = 8, minpeakheight = -1,threshold=0.1)
      rhythmright_i    <- abs(diff(MT$time_ms[peaksright_i[,2]]))/1000 #compute interval in time between peaks

    #extract temporal variability from rhythm intervals
    rhythmicity <- NA
    rhythmicity <- c(sd(rhythmnose, na.rm=TRUE), sd(rhythmleft_w, na.rm = TRUE), sd(rhythmright_w, na.rm= TRUE), sd(rhythmleft_i, na.rm =TRUE), sd(rhythmright_i, na.rm = TRUE))
    rhythmicity <- mean(rhythmicity, na.rm= TRUE)
    if(is.nan(rhythmicity)){rhythmicity <- NA} #if there are no intervals to extract rhythm for set at NA (rather than nan)
    
    #compute average rhythm tempo
    rhythm <- NA
    rhythm <- c(rhythmnose, rhythmleft_w, rhythmright_w, rhythmleft_i,rhythmright_i)
    rhythm <- mean(rhythm, na.rm= TRUE)
    if(is.nan(rhythm)){rhythm <- NA} #if there are no intervals to extract rhythm for set at NA
    
    #compute total submovements from all keypoints
    submovements <- sum(c(nrow(peaksnose), nrow(peaksleft_w), nrow(peaksright_w), nrow(peaksleft_i),                  nrow(peaksright_i), na.rm=TRUE))
    
    #compute average intermittency (referred to here as smoothness) of all keypoints (use function smooth.get given above)
    smoothness <- mean(c(smooth.get(MT$velocity_nose), smooth.get(MT$velocity_left_w), smooth.get(MT$velocity_left_w),  smooth.get(MT$velocity_left), smooth.get(MT$velocity_right)))

    #compute average gesture space used by the gesture
    gspace <- mean(c( (max(MT$x_nose)-min(MT$x_nose))*(max(MT$y_nose)-min(MT$y_nose)),
                (max(MT$x_index_left)-min(MT$x_index_left))*(max(MT$y_index_left)-min(MT$y_index_left)),
                (max(MT$x_index_right)-min(MT$x_index_right))*(max(MT$y_index_right)-min(MT$y_index_right)),
                (max(MT$x_wrist_left)-min(MT$x_wrist_left))*(max(MT$y_wrist_left)-min(MT$y_wrist_left)),
                (max(MT$x_wrist_right)-min(MT$x_wrist_right))*(max(MT$y_wrist_right)-min(MT$y_wrist_right))))
    gspace <- gspace/1000
    
    #bind everything into a single output object containing all the relevent kinematic features
    features <- cbind(submovements, smoothness, gspace, rhythmicity,rhythm)
    return(features)
  }
```


  There is an ongoing scientific effort to unveil the historical and/or necessary constraints that allow(ed) for the emergence of human language [e.g., @bickertonAdamTongue2009; @deutscherUnfoldingLanguageEvolutionary2005; @mcneilageOriginSpeech2008; @tomaselloOriginsHumanCommunication2008]. An important approach within this enterprise comes from experimental cognitive science [@scott-phillipsLanguageEvolutionLaboratory2010]. In this approach interactive communication processes likely to have contributed to language emergence are simulated in the lab with human [e.g., @kirbyCumulativeCulturalEvolution2008] and sometimes non-human primate subjects [e.g., @claidiereCulturalEvolutionSystematically2014].  
  In such experiments, agents learn a novel set of signals which is iteratively transmitted to later generations (iterated learning) or also used in communication by later generations (iterated learning + communication). Over many cycles of learning and use, the signals are affected by various transmission biases [e.g., @christiansenNoworNeverBottleneckFundamental2016; @enfieldNaturalCausesLanguage2016]. Processes of iterated learning and communication can serve as a ‘Petri dish’ for how structural properties such as systematicitity, learnability, and compositionality evolve from more simpler communication systems — a process that must have occurred in human language evolution too  [@bickertonAdamTongue2009]. Furthermore, the ‘germs’ of the Petri dish (items undergoing cultural evolution) abide by population dynamic constraints such as historicity (the system is constrained by past contingencies) and adaptivity (the system is able to tweak itself in service of its informative goals). Such population dynamics must have played out over long temporal and vast population scales, but through these iterated learning paradigms such processes are to some limited degree brought under experimental control. A current challenge is to extend the limits of such paradigms and study how the same constraints can give rise to novel emergent structure at larger scales of interaction [e.g., @lou-magnusonSocialNetworkLimits2018; @lupyanLanguageStructurePartly2010; @ravivLargerCommunitiesCreate2019].  
  The current report showcases a signal-based approach for the study of kinematic communication systems — in this case silent gestures, i.e., manual communicative movements produced in the absence of speech. As we review below, silent gestures are a promising locus for studying the cultural evolution of signs and signalling. But they are also challenging to study given their continuous and complex (multi-)articulatory nature. Here we build on data from a recent iterated learning paradigm with silent gestures, wherein users reproduced communicative gestures within chains of 5 iterated generations [@motamediEvolvingArtificialSign2019]. With computer vision [@caoRealtimeMultiPerson2D2017] we obtained motion traces of manual- and head gestures. Subsequently we performed 'gesture network analysis' [@pouwGestureNetworksIntroducing2019], which is a procedure that combines bivariate time series analysis (Dynamic Time Warping) with network analysis and visualization. Next to reporting kinematic changes indicative of communicative efficiency, we show through gesture network analysis that there is an emergence of systematicity at the network level, which approximates systematicity obtained from the human-coded gesture content. 
  
## Language evolution and silent gesture
  Some scholars of language evolution hold that human language must have started in the manual or whole-body modality [@corballisHandMouthOrigins2002;@donaldOriginsModernMind1991;  @tomaselloOriginsHumanCommunication2008] while others have suggested that the manual modality and vocal systems have co-evolved [e.g., @levinsonOriginHumanMultimodal2014; @kendonReflectionsGesturefirstHypothesis2017]. Such opposing views are united by their conviction that human language is firmly rooted in manual communication, as evidenced by the pervasiveness of co-speech gesture and the ease of for humans to instantiate language in the manual modality.  
 Given the relative scarcity of people who master a sign language, silent gesture is especially interesting tool for studying language evolution de novo. It has for example been shown that syntactic conventions in a spoken language are not necessarily reproduced cross-modally in silent gestures by hearing participants  [@schouwstraTemporalStructureEmerging2017; @goldin-meadowNaturalOrderEvents2008]. Of course, no second language is learned anew, but such research does suggest that silent gesturing is to some degree *authentically* produced and it allows researchers to tap into biases that shape communication while reducing the influence of existing linguistic knowledge.  
 Gestures naturally afford visual-motor mappings to referents, that is they tend towards iconic presentation [e.g., @ortegaHearingNonsignersUse2019]. The manual modality is of course not unique in this, as spoken languages show plenty of iconicity [@dingemanseArbitrarinessIconicitySystematicity2015b], but hand movements are unique in the flexible way they can present iconic mappings and the degree to which they do so. It has been reported that in some sign languages communicative load can be carried to much greater extent by iconic means, which would otherwise need to be carried by other linguistic innovations such as combinatorial phonology [@aronoffRootsLinguisticOrganization2008; @slonimskaRoleIconicitySimultaneity2020a]. Indeed, gesture-first theories emphasize that there is a natural grounding of gestures in routine behaviors such as manual action with the environment, allowing perceivers to more easily recognize these as communicatively relevant movements, and enabling the development of communicative conventions.  
  While gestures have their natural tendencies of expression, they have been found to flexibly adapt to the social context. For example, in dyadic social interaction, repeated gestural referrals to an object or a picture will lead to those gestures becoming more reduced in size [@namboodiripadMeasuringConventionalizationManual2016; @gerwingLinguisticInfluencesGesture2004]. This is comparable to research in ‘pictionary’ paradigms where a concept is drawn out and to be interpreted by another player. After repeated trials of drawing, a reduction of the drawings’ complexity is observed, with smaller-sized and less iconic drawings as a result, while communicative accuracy increases over time [@fayInteractiveEvolutionHuman2010; @garrodFoundationsRepresentationWhere2007]. Though, drifts from less or more iconic/complexity are not fixed processes. When interaction between people is opened up, a whole new suit of social affordances arise. For example, while gestures may reduce in size and iconicity when some common ground is established, at any moment an interlocutor may request a clarification, soliciting large and iconic gestures per implicitly requested [@bavelasGesturingTelephoneIndependent2008; @hollerExperimentalInvestigationHow2011]. In such moments of interactional repair, common ground is calibrated and re-established. Indeed, establishing common ground is not something that is a linear phenomenon, but an interactive and dialectic process. These and many other interactive affordances turn out to be of central importance for smooth everyday language use—e.g., repairs are estimated to be requested every other minute or so [@dingemanseUniversalPrinciplesRepair2015].  
  According to cultural evolutionary accounts of language, local-scale processes of interaction and transmission between communicators are crucial for the emergence of any linguistic system [e.g., @enfieldNaturalCausesLanguage2016; @kirbyIteratedLearningEvolution2014; @kirbyLanguageLearningLanguage2003; @ravivLargerCommunitiesCreate2019]. Communicative items and systems only exist by virtue of social transmission, and they continue to propagate only if they meet communicative needs and are learnable. A key question that drives cultural evolution research is which constraints produce particular pressures for a certain communication system to adapt in one way or another, and how effective solutions are negotiated at the possible expense of other communicatively efficient solutions [@dingemanseArbitrarinessIconicitySystematicity2015b].  
  In a recent iterated learning study with silent gesture [@motamediEvolvingArtificialSign2019] two such possible constraints were studied simultaneously as well as separately. Learning occurred with a set of silent gesture-concept mappings (i.e., communicative tokens) communicated through five iterations of vertical transmissions, where gestures were transmitted from one participant to-be-reproduced by the next participant. Or, tokens would be communicated through five horizontal interactions in a director-matcher type task. These constraints - interaction and transmission - were first studied in combination in experiment 1, which is the focus for the current paper. An important aspect of the study was that every concept was characterisable along two dimensions: theme (e.g., food, religion) and function (e.g., person, location) (see Figure 1).  
  Figure 1. Concepts to be conveyed in gesture in Motamedi et al. 2019  
```{r plotconcepts, echo = FALSE, warning = FALSE}
library(raster)
#load in the finally edited time series example
mypng <- stack(paste0(plotfolder, "/Concepts/concepts.png"))
plotRGB(mypng,maxpixels=1e500000)

```
  
  These dimensions provided possible axes for compressibility of the communicative tokens. After all, by combining 10 unique gestures one can pick out any referent (e.g., “to make an arrest”) from the 24 token meaning space, one gesture marking the functional category (e.g., “action”) and another gesture for the theme category (“justice”).  
  Motamedi and colleagues (2019) showed with qualitative analysis that in early iterations of learning, large-sized iconic enactments were the most common way of gesturally depicting the referents. However, novel “grammatical” gestures emerged over generations, which represented meaning components reused across gestures. This kind of grammatical marking mainly targeted the thematic and functional categories, making the emerging system more compressible and systematic over the generations.  
  With meticulous hand coding of the different referential components of each silent gesture, it could further be quantitatively tested whether there was indeed systematicity emerging. Based on the full sequence of the referential components that were uniquely expressed in each gesture, entropy was computed, which expresses the amount of information that is needed to compress a signal. When a lot of components recur between gestures at a higher chance, the system has a more simple structure (requires less information to be compressed), indicating systematicity [e.g., @gibsonHowEfficiencyShapes2019]. Dovetailing with the qualitative observations and other studies in this field [e.g., @verhoefIconicityEmergenceCombinatorial2016], it was found that gesture-component entropy decreased over the generations. Furthermore, the gestures were coded for the amount of grammatical marking for the functional category, and this showed that such gestures occurred more often at later generations. Finally, gesture duration - as a measure of communicative efficiency - did not reliably change over the generations, which ran counter to predictions that more mature communication systems tend towards maximal efficiency.  
  These results obtained in the lab resonate with findings from homesign [e.g., @havilandEmergingGrammarNouns2013] and emerging sign languages [@senghasChildrenCreatingCore2004]. For example, it has been shown that in the expression of motion events first generation signers of Nicaraguan sign language performed more holistic presentations of path and manner, while in following generations manner and path were segmented. Such segmentations affords novel combinatoriality and therefore increases generativity of a language; it increases the meaning space with fewer means similar to how participants studied by @motamediEvolvingArtificialSign2019 started  to develop ways to express grammatical status of the referents.
  
## Current stage-1 study
  So far research on linguistic properties of manual or whole-body gesture has been based on human coding. Often this is theoretically well justified because the kinematic signal — similar to acoustics in speech — does not specify the semiotic content of the signal. That is, although gesture can be objectively rendered by its kinematics — i.e., rendered as a bodily posture in movement through space — a gesture;s meaning is not contained in the kinematics as such. A communicative context and a community of language users is needed to decide on such meanings, with the human coder acting as the representative. However, form-level systematicities can be revealing of linguistic structure, and the emergence of such structure has been found in many different kind communicative signals, such as whistling signals controlled by a slider  [@verhoefIconicityEmergenceCombinatorial2016], drumming sequences [@ravignaniMusicalEvolutionLab2016], letter sequences [@cornishSequenceMemoryConstraints2017], and a wide range of animal vocalizations [@engesserCombinatorialityVocalSystems2019]. A pressing challenge for applying a similar approach to silent gesture is how to quantify systemic changes from continuous and complex multi-articulatory movements. While there has been progress in quantifying form similarity between silent gestures [e.g., @namboodiripadMeasuringConventionalizationManual2016;@satoAllAspectsLearning2020], a standing challenge is how to understand such kinematic events at higher levels of description, which involves the study of communicative tokens in the context of the larger system they may be part of.  
  Here we address this challenge of relating dynamic multi-segmented kinematics with the possible systematicity emerging between gesture events. To this end, we first applied computer vision techniques [@caoRealtimeMultiperson2D2017] to extract human movement traces from video data, and submitted these multidimensional time series to gesture network analysis [@pouwGestureNetworksIntroducing2019]. This approach allows for a quantification of the interrelationships between communicative tokens [@verhoefIconicityEmergenceCombinatorial2016; @satoAllAspectsLearning2020]. We further report which kinematic properties changed as the communicative system evolved, and how this relates to changes at the system level. The study of @motamediEvolvingArtificialSign2019 is an ideal study to provide a ground-truth for the current signal-based approach, as gesture form and its information units has been extensively documented in a transparent way. As such, the current data provides a platform to launch a new approach for going beyond discrete detection of gesture by coders, to a continuous analysis of language in movement.

\pagebreak

# Method
Figure 2 shows the general overview of the gesture network analysis procedure for this experiment. We will discuss each step in this procedure in the following sections. Then we will discuss our main network and  kinematic outcome measures. We invest extra space for providing quantitative checks to motivate our particular measurement choices against possible alternative choices.  

```{r construct_matrices_and_save, results = 'hide', cache= TRUE, eval = FALSE}
#NOTE this is the code that constructs individual level level gesture networks
#This code takes a little time (about 10 min) to run, and its output are all the distance matrices in the distance_matrices folder
#The code chuck constructs:
  #Individual level matrices: Which are 5chainsx5generationx2participants of size 24x24 (576 cells) (networktype = 1)
  #seed level matrices:        Which are 5chains x with 24x24 matrices (networktype = 0)
for(ch in c(c("chain1", "chain2", "chain3", "chain4", "chain5"))) #goes through all the chains
{
  for(gen in c(1:5))                                              #goes through all the generations
  {
   ts_sub <- subset(ts, chain == as.character(ch) & generation ==  as.character(gen)) #subset data for the curren generation and chain
   #add relevant seeds
   seed_sub <- subset(seeds,  seedsetnum %in% unique(ts_sub$seedsetnum))      #also collect for this chain the relevant seed videos used
   ####################CONSTRUCT INDIVIDUAL LEVEL NETWORKS
   for(p in unique(ts_sub$ppn))                                               #loop through participant
   {
     print(paste0("working on individual level network:", p))                 #print a progress statement to the console
     tsp <- subset(ts_sub, ppn == p)                                          #select only the data for this participant
     network_p <- matrix(nrow = length(unique(tsp$object)), ncol = length(unique(tsp$object))) # make a new matrix

       for(g1 in unique(tsp$object))
       {
          #get index information for this gesture 1
          indexa <- as.numeric(which(unique(tsp$object)==g1))

          for(g2 in unique(tsp$object))
          {
          #get index information for this gesture 2
            indexb <- as.numeric(which(unique(tsp$object)==g2))

            if(is.na(network_p[indexa, indexb])) #this statements makes sure that no computations are made unnessecarily
            {
            ts1 <- subset(tsp, object == g1)
            ts2 <- subset(tsp, object == g2)
            ts1 <- extractR.traces(ts1)
            ts2 <- extractR.traces(ts2)
            dist <- DTW.compare(ts1, ts2, NA)
            
            #fill network
            network_p[indexa, indexb] <- dist
            network_p[indexb, indexa] <- dist #the matrix is symmetric so you can fill two cells Mij and Mji
            }
          }
       }
     #WRITE PARTICIPANT LEVEL NETWORK (priority)
          #FILENAMe: NETWORKTYPE_CHAIN_GEN_PPN
      colnames(network_p) <- as.character(unique(tsp$object))
      write.csv(network_p, paste0(matrices_data, "1_", ch, gen,p, ".csv"), row.names = FALSE)
    }
  }
}
```
```{r main method figure, results = 'hide', cache= TRUE, eval = FALSE}

exm <- ts[ts$generation=="s" & ts$seedsetnum == "arrest1",]

si = 3
ggplot(exm, aes(x=time_ms)) + geom_line(aes(y = x_nose),size= si, color = "red") + 
                              geom_line(aes(y = y_nose),size= si, color = "red") +
                              geom_line(aes(y = x_index_left),size= si, color = "purple")+
                              geom_line(aes(y = y_index_left),size= si, color = "purple")+
                              geom_line(aes(y = x_index_right),size= si, color = "green")+
                              geom_line(aes(y = y_index_right),size= si, color = "green")+
                              geom_line(aes(y = x_wrist_left),size= si, color = "blue")+
                              geom_line(aes(y = y_wrist_left),size= si, color = "blue")+
                              geom_line(aes(y = x_wrist_right),size= si, color = "cyan")+
                              geom_line(aes(y = y_wrist_right),size= si, color = "cyan")+theme_bw()


``` 
\pagebreak
Figure 2. General method gesture network analysis 
```{r plotmainmethodfigure, echo = FALSE, warning = FALSE, fig.height=8}
library(raster)
#load in the finally edited time series example
mypng <- stack(paste0(plotfolder, "/MethodPlot/main_method_v2.png"))
plotRGB(mypng,maxpixels=1e500000)

```
\small *Note Figure 2*. The general procedure is shown for the current gesture network analysis. A) shows the original experiment setup, where a seed set of 24 gestures was randomly selected for each chain containing five generations. Seed gestures were used to train the first generation of each chain; subsequently, gestures from the previous generation were used as training data. Participants then communicated gesturally about the same concepts. B) For our analysis we first performed video-based motion tracking with OpenPose [@caoRealtimeMultiPerson2D2017] to extract relevant 2D movement traces ($T_{i}$) of the nose, the wrists and index fingers. C) For each gesture comparison within a gesture set, the time series were then submitted in C) to a Dynamic Time Warping procedure where we computed for each body part a multivariate normalized distance measure, repeated for all body parts and summed, resulting in one overall distance measure D for each gesture comparison. D) All distance measures were saved into a matrix **D** containing all gesture comparisons $D_{i,j}$ within the comparison set, resulting in a 24x24 distance matrix. The distance matrix can be visualized as a fully connected weighted graph through multidimensional scaling, such that nodes indicate gesture utterances and the distance between gesture nodes the 'D' measure, indicating dissimilarity. \normalsize 

### Participant, design, & procedure of the original study (experiment 1)  
  Here we discuss the setup of the experiment which generated the data we reanalyzed (for more detailed information see Motamedi et al., 2019).  
 A seed gesture set was created with 48 pre-study participants who each depicted 1 out of 24 concepts. Thus for each concept there were two seed gestures performed by unique pre-study participants. Given that pre-study participants only produced one gesture, they were isolated from the other concepts that comprised the meaning space.  
  For the main experiment (exp. 1) 50 right-handed English-speaking non-signing participants were recruited. They were allocated pairwise to one of 5 iteration chains. Participants were first shown a balanced subset of 24 unique seed gestures. These chain-specific seed gesture sets will be referred to as generation 0, which were followed by generations 1 through 5. In the training phase, gestures were presented in random order and participants were asked to identify the meaning of the gesture from the 24-item meaning spaces, followed by feedback about their performance. They were then asked to self-record their own copy the gesture. Participants trained with a subset of 18 items (out of 24), and completed two rounds of training.   
  In the testing phase, participants took turns as director and matcher to gesturally communicate (withou using speech) and interpret items in the meaning space, with feedback following each trial. This director-matcher routine was repeated until both participants communicated all 24 meanings. Subsequent generations were initiated with new dyads whose training set was the gestures from one randomly selected participant from the prior generation.  
  The recorded videos of the seed gestures and the gestures participants produced in the testing phases are the data we use here. This means that we have 50 x 24 = 1200 gesture videos belonging to generation 1-5, and 48 seed gesture videos.

### Motion tracking
  Motion tracking was performed on each video recording with a sampling rate of 30Hz. To extract movement traces we used OpenPose [@caoRealtimeMultiPerson2D2017], which is a pre-trained deep neural network approach for estimating human poses from video data [for a tutorial see @pouwMaterialsTutorialGespin20192019]. We selected keypoints that were most likely to cover the gross variability in gestural utterances: positional x (horizontal) and y (vertical) movement traces belonging to left- and right index fingers, wrists, as well as the nose. For all position traces and its derivatives, we applied 1st order 30Hz low-pass Butterworth filter to smooth out high-frequency jitters having to do with sampling noise. We z-normalized and mean-centered position traces for each video to ensure that differences between subjects (e.g., body size) and within-subject differences in camera position at the start of the recording were inconsequential for our measurements.
  
### Dynamic Time Warping (DTW)
  DTW is a common signal processing algorithm to quantify similarity between temporally ordered signals [@giorginoComputingVisualizingDynamic2009;  @mueenExtractingOptimalPerformance2016a; @mullerInformationRetrievalMusic2007]. The algorithm performs a matching procedure between two time series by maximally realigning (warping) nearest values in time while preserving order, and comparing their relative distances after this non-linear alignment procedure. The degree that the two time series need to be stretched and warped indicates how dissimilar they are. This dissimilarity is expressed with the DTW distance measure, with a higher distance score for more dissimilar time series and a lower score for more similar time series.  
  The time series in the current instance are multivariate, as we have a horizontal (x) and vertical (y) positional time-series data. However, DTW is easily generalizable to multivariate data, and can compute its distances in a multidimensional space if required, yielding a multivariate dependent variant of DTW. We opt for a dependent DTW procedure here as x an y positional data are part of a single position coordinate in space. Additionally, we have 6 of these 2-dimensional time series for each body keypoint. To compute a single distance measure between gestures, we computed for each gesture comparison a multivariate dependent DTW Distance measure per keypoint, which was then summed for all keypoint comparisons to obtain a single Distance measure D (illustrated in Figure 2C). The D measure thus reflects a general dissimilarity (higher D) or similarity (lower D) of the whole manual+head movement utterance versus another utterance.  
  We used the R package ‘DTW’ [@giorginoComputingVisualizingDynamic2009] to produce the multivariate distances per keypoint. The DTW distance measure was normalized for both time series’ length, such that average distances are expressed per unit time, rather than summing distances over time which would yield higher (and biased) distance estimates for longer time series (i.e., longer gesture videos). For further conceptual overview and methodological considerations of our DTW procedure see [@pouwGestureNetworksIntroducing2019].  
```{r compute_measure_check, echo = FALSE, results = 'hide', message = FALSE, warning = FALSE}
sub_ts <- subset(ts, as.character(generation) == "1")   #keep 1 generation gestures
seeds  <- subset(ts, as.character(generation) ==  "s")  #keep only seed gestures

chainnum <- D <- Dran <- Dman <- Dranman<- vector() #initialize vectors to be filled and combined into data set for statistical analysis
for(chs in unique(sub_ts$chain)) #loop through all the chains (note we are going to make smaller and smaller data sets to loop faster)
{
        print(paste0("working on chain: ", chs)) #print something to see progress

    sub_ts_temp <-  subset(sub_ts, chain == chs) #make a smaller data.frame for this chain
    for(pp in unique(sub_ts_temp$ppn))           #for this chain data loop through participants
    {
       sub_ts_temp2 <- subset(sub_ts_temp, as.character(ppn) == pp) #make a smaller data.frame for this chain, participant
       
        for(obj in unique(sub_ts_temp2$object))  #make a smaller data.frame for this chain, particpant, object
        {
        #true pair (make a comparison pair to DTW, with one current reference and the origin seed)
        tt1 <- subset(sub_ts_temp2, as.character(ppn) == pp & as.character(object) == obj) #reference data
        sobject <- as.character(unique(tt1$seedsetnum))                              #which seed video is was the origin of the current?
        tt2 <- subset(seeds, as.character(seedsetnum) ==  sobject)     #get the exact seed based on seed list and object
        
        #random paired (make a comparison pair to DTW, with one current reference and a random [and unrelated] origin seed)
        listotherobjects <- unique(sub_ts_temp2$object[sub_ts_temp2$object != obj & 
                                                        sub_ts_temp2$theme !=  unique(tt1$theme) &
                                                        sub_ts_temp2$functional != unique(tt1$functional)]) #subset objects which are not the same reference,theme,                                                                                      and function 
        pickranobject    <- sample(     listotherobjects , 1)                 #pick randomly an object from that list
        tt2r <- subset(seeds, as.character(object) == pickranobject)          #extract random test data
          
      #Extract relevant traces to be inputted for DTW
        MT1   <- extractR.traces(tt1)
        MT2   <- extractR.traces(tt2)
        MT2r  <- extractR.traces(tt2r)
        
      #perform DTW distance calc for actual and random pair
       DTWpair    <-  DTW.compare(MT1, MT2, "NA")
       DTWranpair <-  DTW.compare(MT1, MT2r, "NA")
          #also perform same comparisons for only manual movements (excluding head movements)
       DTWpair_man    <-  DTW.compare(MT1, MT2, "manual_only")
       DTWranpair_man <-  DTW.compare(MT1, MT2r, "manual_only")
  
      #collect into dataset for statistical analysis
      chainnum <- c(chainnum, chs)
      D        <- c(D, DTWpair) 
      Dran     <- c(Dran, DTWranpair)
      Dman     <- c(Dman,   DTWpair_man)
      Dranman  <- c(Dranman, DTWranpair_man)
    }
  }
}

#bind into a dataset for statistical analysis of measure accuracy
t <- cbind.data.frame(chainnum,  D)
t2 <- cbind.data.frame(chainnum,  Dran)
tm1 <- cbind.data.frame(chainnum,  Dman)
tm2 <- cbind.data.frame(chainnum,  Dranman)
t$pairtype <- tm1$pairtype <- "true pair"
t2$pairtype  <- tm2$pairtype <- "random pair"
colnames(t) <- colnames(t2) <- colnames(tm1) <-colnames(tm2) <- c("chain", "DTWdistance", "pair")

#compare Distances random-true pairs
mcheck <- rbind.data.frame(t,t2)
test <- t.test(mcheck$DTWdistance~mcheck$pair)
D  <- cohen.d(mcheck$DTWdistance, mcheck$pair)$estimate
diff1 <- mcheck$DTWdistance[mcheck$pair == "random pair"]-mcheck$DTWdistance[mcheck$pair == "true pair"]
#compare Distances random-true pairs for only the manual
mcheckman <- rbind.data.frame(tm1,tm2)
Dman  <- cohen.d(mcheckman$DTWdistance, mcheckman$pair)$estimate
diff2 <- mcheckman$DTWdistance[mcheck$pair == "random pair"]-mcheckman$DTWdistance[mcheck$pair == "true pair"]

#make a dataset that compares differences in head included or head excluded DTW distances
comb <- as.data.frame(c(diff1,diff2))
comb$inc <- c(rep("head included" , length(diff1)), rep("head excluded", length(diff2)))
test_maninc <- t.test(comb[,1]~comb$inc)

```
  As a demonstration that our D measure reflects actual differences in kinematics, we computed for each individual in each chain the difference between a gesture seed and the gesture that the individual produced to copy it, for generation 1. These “true pairs” must be maximally similar (lower D) as the individual produced their copied gesture short after first exposure in the training phase, which should lead to high faithfulness in reproduction. We contrast this with a false or random comparison of the same gesture in generation 1 with a gesture seed that was neither in the same functional nor thematic category. These false random pairs must be more dissimilar, and should produce higher DTW distances.
  Figure 3 shows the distributions of the distances observed. DTW distance distributions were reliably different, *t* (`r printnum(round(test$parameter, 2))`) = `r printnum(round(test$statistic, 2))`, *p* = `r printnum(ifelse(test$p.value < .001, "< .001", test$p.value))`, Cohen's *d* =  `r printnum(round(D, 2))`, for the true pair, *M* =`r printnum(mean(mcheck$DTWdistance[mcheck$pair =="true pair"]))`(*SD* = `r printnum(sd(mcheck$DTWdistance[mcheck$pair =="true pair"]))`), as compared to the random pair, *M* = `r printnum(mean(mcheck$DTWdistance[mcheck$pair =="random pair"]))`(*SD* = `r printnum(sd(mcheck$DTWdistance[mcheck$pair =="random pair"]))`).  
  Importantly, we also find that adding head movement trajectory to our D calculation significantly increases false-real pair discriminability as compared when we compute our D measure on only manual keypoints (left/right wrist and index fingers), change in Cohen's *d* = `r printnum(round(D-Dman, 2))`, change D real vs. false = `r printnum(mean(comb[,1][comb$inc == "head included"])-mean(comb[,1][comb$inc == "head excluded"]))`, *p* = `r printnum(ifelse(test_maninc$p.value < .001, "< .001", test_maninc$p.value))`. Therefore we conclude that in the current experiment the gesture utterances are also crucially defined by head movements as well. This is an interesting finding in and of itself, and demonstrates the multi-articulatory nature of silent gestures.

Figure 3. Density distributions of D for true pairs and random pairs  
```{r plot_distributioncheck, echo = FALSE, message = FALSE, warning = FALSE, fig.width=6, fig.height=3}
tm <- mean(mcheck$DTWdistance[mcheck$pair =="true pair"])
fm <- mean(mcheck$DTWdistance[mcheck$pair =="random pair"])

colors <- brewer.pal(n = 2, name = "Set1")
a <- ggplot(mcheck, aes(x = DTWdistance, color = pair, fill= pair)) + geom_density(size = 2, alpha= 0.2) + geom_vline(xintercept = fm, linetype = "dashed", color = colors[1], size = 2) + geom_vline(xintercept = tm, linetype = "dashed", color = colors[2], size = 2) +
  scale_colour_manual(values=colors)+
  theme_bw() +  theme(panel.grid.major = element_blank()) + xlab("DTW distance")
a
```

*Note Figure 3*. Density distributions of D are shown for the random versus real pairs. With D based on head-, wrist- and finger movement there is good discriminability between real versus falsely paired gestures, confirming that our approach is tracking gesture similarity well.

### Gesture networks
  We constructed for each participant (nested in generation and chain), as well as each seed gesture set (seed set belonging to that chain), a distance matrix **D**, containing the continuous D comparisons for each gesture $D_{i,j}$ produced by that participant with each other gesture produced by that participant, yielding a 24x24 distance matrix **D**. The diagonal contains zeros for gesture comparisons that are identical ($D_{i,j} = 0 | i=j$). These characteristics make **D** a weighted symmetric distance matrix.  
  For each distance matrix we can construct a visual representation of its topology by projecting the distance of gesture tokens on a 2d plane using multidimensional scaling. These networks are fully connected graphs with distances between gesture nodes reflecting our D measure. Such 2D representations are imperfect approximations of the underlying multidimensional data and are only used as visual aids. The uncompressed distance matrices are used to calculate the topological properties, i.e., interrelationships of communicative tokens. We refer to these matrix properties as ‘network properties’ as these measures are intuitively understood in network terms. For multidimensional scaling, network visualization, and calculations of network entropy we use the R package ‘igraph’ [@csardiPackageIgraphNetwork2019].
  
## Gesture Network Properties
### Combinatorial structure: Network Entropy
  The network entropy measure is almost identical to a classic Shannon entropy calculation, where $Entropy\; H(X) = -\sum p(X)\log  p(X)$. The only difference is that our measure is computed on the weights of the networks’ edges for each node relative to the shortest path to the other nodes (ie., connections), and then normalized by the number of connections.  
  Entropy is a measure that quantifies the compressibility of data structures, and has been used to gauge the combinatorial structure of communicative tokens in the field of language evolution [e.g., @verhoefIconicityEmergenceCombinatorial2016; for theoretical grounding see @gibsonHowEfficiencyShapes2019]. In the original experiment, Motamedi and colleagues (2019) computed entropy from the gesture content codings, which captured recurrent information units between gestures. In our case, entropy quantifies the degree to which there are similar or more diverse edge lengths (i.e., similar/diverse levels of dissimilarity 'D'). If they are more similar, this means lower entropy reflecting that communicative tokens relate in more structural ways to each other.  
  To explain entropy with some simple examples: if we have a network where the chance of having an edge length of D = x is 1, then the network connections are fully compressible and we yield an entropy of 0 ($Entropy = -1 * 1 * log(1) = 0$). If there are different edge lengths (increasing the complexity of our network) such that we have a 0.5 chance that $D = x$ and 0.5 chance that $D = y$, then entropy goes up,  $Entropy = (-1 * 0.5 * log(0.5)) + (-1 * 0.5 * log(0.5)) = 0.68$ (remember that the log of a fraction becomes a negative number, that is why the result is multiplied by -1 at the start of the formula). Note further that when the system is so diverse that there is a almost zero chance that any connection is recurring, entropy will approach infinity (the system is incompressible). To generalize this for our case, when entropy goes up, it means that communicative tokens interrelate in a more random way (i.e., the system is more complex; i.e., has less compressible structure), while if entropy goes down, it means that communicative tokens show more structural interrelations.  

### Clustering
```{r network_manip_check, echo =FALSE, message = FALSE, warning = FALSE}
cluster_persistence <-   clustercof <- NA #initialize two vectors that will contain cluster peristence and AggCC's
matrices <- list.files(matrices_data)     #list all files in the matrices data
for(i in matrices) 
{
    mat <- read.csv(paste0(matrices_data, i)) #read in a matrix
    #agglomerative cluster coefficient
    cl <- agnes(mat, method = "average")  #compute the agglomerative clustering coefficient using average linkage
    clustercof <- c(clustercof, cl$ac)
    #average component persistence (TDA)
    phom.dat <-   as.data.frame(calculate_homology(as.dist(mat), format = "distmat")) #calculate persistent homology from matrix
    phom.dat <-    phom.dat[phom.dat$dimension==0,]     #only keep persistence data for 0-cycles (components/clusters)
    phom.dat$persistence <-   phom.dat[,3]-phom.dat[,2] #compute persistence from birth and death values
    tresh <- id_significant(phom.dat, dim = 0, cutoff = 0.975) #check for threshold for which something counts as reliable component
    persistence <- mean(phom.dat$persistence[phom.dat$persistence > tresh],na.rm=TRUE) #average persistence of reliable components
    cluster_persistence <- c( cluster_persistence, persistence) #bind into vector (one observation per matrix)
}
nedcheck <- cbind.data.frame(cluster_persistence, clustercof)       #bind all information
cors <- cor.test(nedcheck$cluster_persistence, nedcheck$clustercof) #cor.test different clustering measures
```
 While entropy is a system-wide property, we can also study more other relations between communicative tokens by assessing the degree to which they cluster or differentiate from each other. Clustering would indicate that there are multiple gestures that have similar features, which may indicate lack of differentiability and an increase in possible associations with other tokens. Indeed, we might expect that communicative tokens within a theme are likely to be ambiguous at beginning generations (e.g., the ambiguous reuse of the handcuffing gesture for ‘to make an arrest’ and ‘police officer’) and such gestures would cluster with edge weights of low D.  
 For the clustering measure we use a technique from topological data analysis [e.g., @sizemoreImportanceWholeTopological2018] called persistent homology analysis [@bendichPersistentHomologyAnalysis2016; @otterRoadmapComputationPersistent2017], which can assess how stable (i.e., persistent) network components are through a continuous quantification.  
 Consider that the distance matrices contain coordinates for each gesture in a multidimensional space relative to all other gestures. Persistent homology measures the degree to which gestures cling together in a relatively stable fashion. Its measure can be visualized as involving gradually expanding circles around every gesture token (Figure 4). When circles touch, they merge to form a new cluster component. At the start of this process every single gesture is in its own 'cluster'. Soon enough circles begin to touch, forming new clusters of multiple gestures. Some such clusters will merge when their circles touch, others are so distant that they exist on their own for a longer while. When all circles have grown maximally, all nodes are connected and only a single overall cluster remains. Throughout this process, every cluster has its own lifetime (from emergence to assimilation). The average lifetime of reliable clusters is a measure of the amount of clustering in the network.   
  To compute cluster persistence we used R-package 'TDAstats' [@wadhwaTDAstatsPipelineTopological2019]. We averaged persistence for the statistically significant components only, whereby we uses the 'TDAstats' own bootstrapping method (set at chance level of 0.975). The selection of statistically reliable components was applied because many detected components are of very short persistence and reflect noise/chance level occurrences of components. We computed the average persistence of components (0-cycles) for each distance matrix (i.e., each individual’s gesture network).  

Figure 4. Network property example  
```{r networkprop_plot, echo = FALSE, message = FALSE, waning = FALSE, fig.height = 3}
library(raster)
mypng <- stack(paste0(plotfolder, "/MethodPlot/network_plot_v1.png"))
plotRGB(mypng,maxpixels=1e500000)
```
\small *Note Figure 4.* A visual example of the persistent homology procedure in Topological Data Analysis. Each token has a certain distance to all other tokens. Persistent homology analysis (PH) assesses the stability of components in this spatial organization by gradually increasing a spatial threshold (the radius) at which nodes get connected, indicated here by red growing radii. At 1, all tokens are unconnected. At 2, two distinct clusters x and y emerge. At 3, these two clusters merge into a single cluster z. The longer clusters survive at gradually increasing radii, the stabler they are.\normalsize    
  Persistent homology is useful for multidimensional data structures like the weighted fully connected distance matrices we are working with. This is because it allows for a continuous quantification of cluster stability at multiple scales (clusters of clusters), in contrast to a binary assignment of nodes to particular clusters. Since Topological Data Analysis is relatively new analysis toolkit in cognitive science [@lumExtractingInsightsShape2013; @zhangTopologicalPortraitsMultiscale2020], we also made a comparison with another classic clustering measure: hierarchical clustering analysis with “average” linkage. For each matrix we computed the agglomerative clustering coefficient with R package ‘cluster’, where a low clustering coefficient indicates more clustering in the data while a larger value indicates less clustering. When cluster persistence according to persistent homology is high, the clustering coefficient is structurally lower (Figure 5), indicating that both measures converge on their estimate of ‘clusteriness’ of the data, *r* = `r printnum(round(cors$estimate, 2))`, *p* = `r printnum(ifelse(cors$p.value < .001, "< .001", test_maninc$p.value))`. Hereafter we only report cluster persistence as a measure of clusteriness.

Figure 5. Cluster measure comparison  
```{r network_manip_check2, echo =FALSE, message = FALSE, warning = FALSE, fig.height=3, fig.width=3}
a <- ggplot(nedcheck, aes(x = clustercof, y = cluster_persistence)) + geom_point(size =2) + geom_smooth(method = "lm", color = "darkgrey", linetype= "dashed") 
a <- a + theme_bw() + ylab("cluster persistence") + xlab("agglomerative cluster coefficient")
a
```

*Note Figure 5.* Higher cluster persistence as measured by persistent homology is related to a lower agglomerative cluster coefficient in hierarchical cluster analysis, indicating that both measures are tracking some clustering property in the matrices.

## Kinematic Properties
```{r kinematic_calcs, echo = FALSE, message = FALSE, warning = FALSE}
#plot for checking algorithm
tstemp <- ts              #create a temporary copy of the time series data
tstemp$identifier <- paste0(tstemp$generation, tstemp$ppn, tstemp$chain, tstemp$object)  #make an identifier for each video

dimjerk <- peaks <- vector()
feats <- data.frame()
for(i in unique(tstemp$identifier)) #go through all time series and extract the kinematic features using the custom function
{ 
  cc <- tstemp[tstemp$identifier == i,] #also get the data from human codings (repetitions, infnormation units, and segments)
  get <- cbind(kin.get(cc), cc$ann_reps[1], cc$ann_inf_units[1], cc$segments[1])
  feats <- rbind.data.frame(feats, get)
}
colnames(feats) <- c(colnames(feats[1:5]), "repetitions", "inf_units", "segments")

feats$smoothness <- log(feats$smoothness)     #this measure tends to explode at high values, so we log scale them
feats$submovements <- log(feats$submovements) #thus measure tends to explode at high values, so we log scale them

#correlations to report (intermittency and rhythm measure)
cx <- cor.test(feats$smoothness, feats$rhythm)
cxt <- c(round(cx$estimate, 2),ifelse(cx$p.value < .001, "< .001", round(cx$p.value, 3)))

```

  Gesture network analysis aims to target structural properties existing on the system level, studying the relations between tokens rather than the form or content of those tokens. However, it is equally important to understand what specific changes occur in the kinematics of the gestures, as such changes might predict changes on the system level.  
  We first selected five potential measures representative of kinematic quality of the movements in terms of segmentation, salience and temporality, namely submovements, intermittency, gesture space, rhythm, and temporal variability (or rhythmicity). See Figure 6 for two example time series from which most measures can be computed. All measures were computed for each keypoint’ time series seperately and then averaged so as to get an overall score for the multimodal utterance as a whole. Based on these exploratory measures we eventually selected three measures tracking gesture segmentation (intermittency score), gesture salience (gesture space), gesture’s temporality (temporal variability). Correlations and distributions are shown in Figure 7. 

### Gesture salience
  As a measure for gesture salience or reduction, we computed a gesture space measure. This was determined by extracting the maximum vertical amplitude of a keypoint multiplied by the maximum horizontal amplitude, i.e., the area in pixels that has been maximally covered by the movement.

### Gesture segmentation
  We first computed a submovement measurement similarly implemented by @trujilloMarkerlessAutomaticAnalysis2019. Submovements are computed with a basic peak finding function which identifies and counts maxima peaks in the movement speed time series. We set the minimum interpeak distance at 8 frames, and minimum height = -1 (z-scaled; 1 std.), minimum rise = 0.1 (z-scaled).  We logtransformed the submovement measure due to a skewed distribution.  
  A property of the submovement measure is that it discretizes continuous information and uses arbitrary thresholds for what counts as a submovement, thereby risking information loss about subtle intermittencies in the movement. To have a more continuous measure of intermittency (the opposite of smoothness) of the movement we computed a dimensionless jerk measure [@hoganSensitivitySmoothnessMeasures2009]. This measure is dimensionless in the sense that it is scaled by the maximum observed movement speed and duration of the movement. Dimensionless jerk is computed using the following formula $\int_{t2}^{t1} x''' (t)^{2}dt)* \frac{D^{3}}{max(v^{2})}$, where $x'''$ is jerk, which is squared and integrated over time and multiplied by duration $D$ cubed over the maximum squared velocity $max(v^{2})$. As figure 6 shows, this measure correlates very highly with submovements, thus we chose to only use intermittency for further analysis. We logtransformed our smoothness measure due to a skewed distribution. Note that a *higher* intemittency score indicates more intermittent (less smooth) movement.
  
### Gesture temporality
  From the submovement measure we computed the average interval between each submovement (in Hz), which is a measure of rhythm tempo. This measure was, as expected, highly correlated with intermittency score, as tempo goes up when more segmented movements are performed in the same time window, *r* = `r printnum(cxt[1])`, *p* = `r printnum(cxt[2])`, which led us to drop this measure for our analysis. Instead, we use another temporal measure that is more orthogonal to intermittency and gesture space, and which captures the stability of the rhythm, i.e., the temporal variability (the opposite of isochrony) of the movements. This measure is simply the standard deviation of the temporal interval between submovements (given in Hz): a higher score indicates more temporal variability and a lower score indicates more isochronous rhythm. Note, this measure cannot be calculated when there are less than 3 submovements (i.e., when there no intervals to detect the temporal variability of).

Figure 6. Overview kinematic measures
```{r plotjerk, echo = FALSE, warning= FALSE, fig.height=2.90, fig.width = 7}
#for method jerk plots
  #example 1
exm <- ts[ts$time_ms > 0 & ts$generation=="s" & ts$seedsetnum == "arrest2",] #extract sample from the data
exm$velocity_right_w <- as.vector(scale(exm$velocity_right_w)) #z-scale the speed vector

#extract peaks
peaksexm <- findpeaks(exm$velocity_right_w,minpeakdistance = 8,  minpeakheight = -1, threshold=0.1) #apply peakfinder function to the time series, with the same thresholds as our kin.get function given at the custom function section
exm$peak <- ifelse(exm$time_ms %in% exm$time_ms[peaksexm[,2]], exm$velocity_right_w, NA) #save the peak height into the time series by matching with the time
exm$peaktime <- ifelse(exm$time_ms %in% exm$time_ms[peaksexm[,2]], exm$time_ms, NA) #save the peak time into the time series by matching with the time

smoothness <- smooth.get(exm$velocity_right_w) #apply the custom function calculating smoothness
rhythm <- mean(abs(diff(exm$time_ms[peaksexm[,2]]))/1000) #extract time interval between peaks, and divide by 1000 ms to get Hz
rhythmicity <- sd(abs(diff(exm$time_ms[peaksexm[,2]]))/1000) #compute the st. dev. time interval between peaks, and divide by 1000 ms to get Hz


#left plot
a <- ggplot(exm) + geom_line(aes(x=time_ms, y = velocity_right_w)) +
  geom_point(aes(x = peaktime, y = peak), color = "red", size = 2) +
    annotate("text", label= paste0("submovements = ", nrow(peaksexm)), x=1750, y=2.15)+
  annotate("text", label = paste0("intermittency = ", round(log(smoothness)), round = 2), x=1750, y=1.80) +
  annotate("text", label = paste0("rhythm (Hz) = ", round(rhythm,2)), x=1750, y=1.60) +
    annotate("text", label = paste0("temporal var. = ", round(rhythmicity,2)), x=1750, y=1.35) +
  xlab("time (ms)") +
  ylab("speed right wrist (z-scaled)")+
  theme_bw()
 
#right plot (NOTE, this repeats what is done above)
exm5 <- ts[ts$time_ms > 0 & ts$generation=="5" & ts$chain == "chain5" & ts$seedsetnum == "arrest2" & ts$ppn == "full50",]
exm5$velocity_right_w <- as.vector(scale(exm5$velocity_right_w))

peaksexm5 <- findpeaks(as.vector(scale(exm5$velocity_right_w)),minpeakdistance = 8,  minpeakheight = -1, threshold=0.1)
exm5$peak <- ifelse(exm5$time_ms %in% exm5$time_ms[peaksexm5[,2]], exm5$velocity_right_w, NA)
exm5$peaktime <- ifelse(exm5$time_ms %in% exm5$time_ms[peaksexm5[,2]], exm5$time_ms, NA)

smoothness <- smooth.get(exm5$velocity_right_w)
rhythm <- mean(abs(diff(exm5$time_ms[peaksexm5[,2]]))/1000)
rhythmicity <- sd(abs(diff(exm5$time_ms[peaksexm5[,2]]))/1000)

b <- ggplot(exm5) + geom_line(aes(x=time_ms, y = velocity_right_w)) +
  geom_point(aes(x = peaktime, y = peak), color = "red", size = 2) +
    annotate("text", label= paste0("submovements = ", nrow(peaksexm5)), x=3000, y=2.15)+
  annotate("text", label = paste0("intermittency = ", round(log(smoothness)), round = 2), x=3000, y=1.80) +
  annotate("text", label = paste0("rhythm (Hz) = ", round(rhythm,2)), x=3000, y=1.60) +
    annotate("text", label = paste0("temporal var. = ", round(rhythmicity,2)), x=3000, y=1.35) +
  xlab("time (ms)") +
  ylab("speed right wrist (z-scaled)")+
  theme_bw()

grid.arrange(a,b, nrow=1)
```  
*Note Figure 6*. \small Two timeseries (belonging to two unique trials) are shown for right-hand wrist speed. From these time series, as well as the time series for other body parts, we computed  measures tracking segmentation, namely, submovements (number of observed peaks in red) and intermittency. We further computed measures concerning temporality, namely the average time between submovements, i.e., rhythm in Hertz. We also computed temporal variability, which is the standard deviation of the rhythm in Hertz. Gesture space was calculated from the x,y position traces and is not shown here.  \normalsize

\pagebreak  
Figure 7. Correlations and distributions for kinematic measures per trial
```{r summ_kin, echo = FALSE, message = FALSE, warning = FALSE, fig.height =6, fig.width = 6}
#AUTO MEASURES CORRELATIONS
ca <- cor.test(feats$submovements, feats$smoothness)
cat <- paste0("r = ", round(ca$estimate, 2),  ", p = ", ifelse(ca$p.value < .001, "< .001", round(ca$p.value, 3)))
a <- ggplot( feats, aes(x = submovements, y = smoothness)) + geom_point(size = 0.1) + geom_smooth(color = "orange") + theme_bw()+xlab("log submovements" )+ ylab("log intermittency") +
      annotate("text", label= cat, x=1.5, y=13)


#
cb <- cor.test(feats$smoothness, feats$rhythmicity)
cbt <- paste0("r = ", round(cb$estimate, 2),  ", p = ", ifelse(cb$p.value < .001, "< .001", round(cb$p.value, 3)))
b <- ggplot( feats, aes(x = smoothness, y = rhythmicity)) + geom_point(size = 0.1) + geom_smooth(color = "orange") + theme_bw()+xlim(4, 12) + xlab("log intermittency") +ylab("temporal variability")+
   annotate("text", label= cbt, x=6.5, y=4.75)

cc <- cor.test(feats$smoothness, feats$gspace)
cct <- paste0("r = ", round(cc$estimate, 2),  ", p = ", ifelse(cc$p.value < .001, "< .001", round(cc$p.value, 3)))

c <- ggplot( feats, aes(x = smoothness, y = gspace)) + geom_point(size = 0.1) + geom_smooth(color = "orange") + 
  theme_bw() + xlab("log intermittency") + 
     annotate("text", label= cct, x=2.5, y=200)

cd <- cor.test(feats$rhythmicity, feats$gspace)
cdt <- paste0("r = ", round(cd$estimate, 2),  ", p = ", ifelse(cd$p.value < .001, "< .001", round(cd$p.value, 3)))
d <- ggplot( feats, aes(x = rhythmicity, y = gspace)) + geom_point(size = 0.1) + geom_smooth(color = "orange") + 
  theme_bw() +xlab("temporal variability")+
       annotate("text", label= cdt, x=2.75, y=4.75)

grid.arrange(ggMarginal(a),ggMarginal(b),ggMarginal(c), ggMarginal(d), nrow= 2)

```
*Note Figure 7*. Left upper panel, correlations and distributions are shown for intermittency and submovement. Given their high correlation we will use intermittency score for our final analysis. Other correlations are shown for the selected measures, rhythmiticy, gesture space and intermittency.  

### Human coding and kinematic measures
  It would be helpful to know how these automated kinematic measures approximate hand-coded data from Motamedi and colleagues (2019). The hand-coded data consisted of the amount of unique information units of the gesture utterance, the number of repetitions in the utterance, as well as the number of segments (information units + repetitions). We should predict that our kinematic intermittency score should correlate with the number of segments, repetitions and information units as the kinematics will have to carry those information units by contrasts in the trajectories. Figure 8. shows the correlations for our kinematic measures and the human-coded gesture information. It shows that the amount of information units (unique, repeated or total) in the gesture as interpreted by a human coder are reliably correlating with kinematic intermittency (more intermittent more information), gesture space (larger space more information) and temporal variability (more stable rhythm more segments).

Figure 8. Correlations of kinematic measures with human-coded gesture information  
```{r r summ_kin2, echo = FALSE, message = FALSE, warning = FALSE, , fig.height =7, fig.width = 6}

#AUTO MEASURES CORRELATIONS with Human Annotations
sizetext <- 3
tcol <- "red"
  #segments vs. smoothness
ca <- cor.test(feats$segments, feats$smoothness)
cat <- paste0("r = ", round(ca$estimate, 2),  ", p = ", ifelse(ca$p.value < .001, "< .001", round(ca$p.value, 3)))
a <- ggplot( feats, aes(x = segments, y = smoothness)) + geom_point(size = 0.1) + geom_smooth(color = "red") + theme_bw()+xlab("segments \n (human coder)")+ ylab("intermittency") +
      annotate("text", label= cat, x=8, y=13, size = sizetext, color = tcol)

  #information units vs smoothness
cb <- cor.test(feats$inf_units, feats$smoothness)
cbt <- paste0("r = ", round(cb$estimate, 2),  ", p = ", ifelse(cb$p.value < .001, "< .001", round(cb$p.value, 3)))
b <- ggplot( feats, aes(x = inf_units, y = smoothness)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlim(4, 12) + xlab("information units \n (human coder)") +ylab("intermittency")+
   annotate("text", label= cbt, x=7, y=13, size = sizetext, color = tcol)

  #repetitions vs. smoothness
cc <- cor.test(feats$repetitions, feats$smoothness)
cct <- paste0("r = ", round(cc$estimate, 2),  ", p = ", ifelse(cc$p.value < .001, "< .001", round(cc$p.value, 3)))
c <- ggplot( feats, aes(x = repetitions, y = smoothness)) + geom_point(size = 0.1) + geom_smooth(method = "lm",color = "red") + 
  theme_bw() + xlab("repetitions \n (human coder)") + ylab("intermittency")+
     annotate("text", label= cct, x=4, y=11.5, size = sizetext, color = tcol)

  #segments vs. gspace
cd <- cor.test(feats$segments, feats$gspace)
cdt <- paste0("r = ", round(cd$estimate, 2),  ", p = ", ifelse(cd$p.value < .001, "< .001", round(cd$p.value, 3)))
d <- ggplot( feats, aes(x = segments, y = gspace)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlab("segments \n (human coder)" )+ ylab("gesture space") +
      annotate("text", label= cdt, x=8, y=13, size = sizetext, color = tcol)

  #information units vs. gspace
ce <- cor.test(feats$inf_units, feats$gspace)
cet <- paste0("r = ", round(ce$estimate, 2),  ", p = ", ifelse(cd$p.value < .001, "< .001", round(cd$p.value, 3)))
e <- ggplot( feats, aes(x = inf_units, y = gspace)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlim(4, 12) + xlab("information units \n (human coder)") +ylab("gesture space")+
   annotate("text", label= cet, x=7, y=4.75, size = sizetext, color = tcol)

  #repetitions vs. gspace
cf <- cor.test(feats$repetitions, feats$gspace)
cft <- paste0("r = ", round(cf$estimate, 2),  ", p = ", ifelse(cf$p.value < .001, "< .001", round(cf$p.value, 3)))
f <- ggplot( feats, aes(x = repetitions, y = gspace)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw() + xlab("repetitions \n (human coder))") +ylab("gesture space")+
   annotate("text", label= cft, x=4, y=4.75, size = sizetext, color = tcol)

  #segments vs. rhythmicity
cg <- cor.test(feats$segments, feats$rhythmicity)
cgt <- paste0("r = ", round(cg$estimate, 2),  ", p = ", ifelse(cg$p.value < .001, "< .001", round(cg$p.value, 3)))
g <- ggplot( feats, aes(x = segments, y = rhythmicity)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlab("segments \n (human coder)" )+ ylab("temporal  var.") +
      annotate("text", label= cgt, x=8, y=4.75, size = sizetext, color = tcol)

  #information units vs. rhythmicity
ch <- cor.test(feats$inf_units, feats$rhythmicity)
cht <- paste0("r = ", round(ch$estimate, 2),  ", p = ", ifelse(ch$p.value < .001, "< .001", round(ch$p.value, 3)))
h <- ggplot( feats, aes(x = inf_units, y = rhythmicity)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlim(4, 12) + xlab("information units \n (human coder)") +ylab("temporal  var.")+
   annotate("text", label= cht, x=7, y=4.75, size = sizetext, color = tcol)

  #repetitions vs. rhythmicity
ci <- cor.test(feats$repetitions, feats$rhythmicity)
cit <- paste0("r = ", round(ci$estimate, 2),  ", p = ", ifelse(ci$p.value < .001, "< .001", round(ci$p.value, 3)))
i <- ggplot( feats, aes(x = repetitions, y = rhythmicity)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw() + xlab("repetitions \n (human coder)") +ylab("temporal  var.")+
   annotate("text", label= cit, x=4, y=4.75, size = sizetext, color = tcol)

grid.arrange(a,b,c,
             d,e,f,
             g,h,i,nrow= 3)



```
*Note figure 8.* On the horizontal axes the human-coded number of gesture segments, unique information units, and the number repetitions (of information units) are shown. On the vertical axes our automatic kinematic measures are shown: intermittency, gesture space, and temporal variability. The findings show that our measures are a proxy for human judgments, such that more intermittent kinematics reflect more information units (repeater and/or unique). Larger gesture space is related to more information units. Lower temporal variability in kinematics is further associated with more information units.

# Main Results
We will first report findings on how relations between communicative tokens changed over the generations, as indicated by our network measures. We then validate whether network entropy approximates systematicity as observed by human coders. Subsequently, we will assess whether network changes occurred between particular tokens, namely the function vs. theme grouping. Finally, we will report on whether structural kinematic changes occurred over the generations for verb and non-verb gestures, and how such kinematic changes related to changes on the network level.  
  
## Network changes over generations
```{r network_evolution_main_findings, echo =FALSE, message = FALSE, warning = FALSE}
#initialize variables we want to collect from the matrix data
entropy1 <- chain <- generation <- kinch <- cluster_persistence <- p <- 
  submovements <- smoothness <- gspace <-   entropyWfunctions  <-entropyWthemes <- rhythmicity <- 
  ann_entropy <- vector()
for(ch in c("chain1", "chain2", "chain3", "chain4", "chain5"))
{
  for(gen in c(0:5))
  {
    #'participants' to loop through?
    if(gen == 0)
    {
    participants <- ""
    type = "0_"
    }
    if(gen != 0)
    {
    type = "1_"
    participants <- as.character(unique(ts$ppn[ts$chain == ch & ts$generation == gen]))
    }
    #DO network property collection
    for(ppn in participants) #loop through 'participants'
    {
    mat <- read.csv(paste0(matrices_data, type, ch, gen, ppn, ".csv"))      #fetch matrix
    
    #function/theme specific network properties to be collected
    funtypes <-  unique(ts$functional) #collect functional categories from time series data
    themtypes <-  unique(ts$theme)     #collect theme categories from time series data
    entropyWfun  <-entropyWtheme  <- vector()  #make some variable to be renewed after each iteration in the 'participant' loop
    for(at in 1:4) #there are 4 themes and function types and we want to have specific network properties within each category
    {
      indexfun <- which(colnames(mat)%in%unique(ts$object[ts$functional==funtypes[at] ]) ) #collect indices in the matrix that match category
      fun_mat     <- mat[indexfun,  indexfun] #make a function specific submatrix
      indextheme <- which(colnames(mat)%in%unique(ts$object[ts$theme==themtypes[at] ]) ) #do the same for theme
      them_mat     <- mat[indextheme, indextheme] 
    
      #within theme/function entropy/distance
          tempfunnet <- graph.adjacency(as.matrix(fun_mat), mode="undirected", weighted=TRUE, diag = FALSE)
          entropyWfun <- c(entropyWfun, mean(diversity(tempfunnet)))

          tempthemnet <- graph.adjacency(as.matrix(them_mat), mode="undirected", weighted=TRUE, diag = FALSE)
          entropyWtheme <- c(entropyWtheme, mean(diversity(tempthemnet)))
      }
    entropyWfunctions   <- c(entropyWfunctions, mean(entropyWfun))
    entropyWthemes <- c(entropyWthemes, mean(entropyWtheme))
    
    #######################GLOBAL network properties to be collected
    #compute entropy
    tempnet <- graph.adjacency(as.matrix(mat), mode="undirected", weighted=TRUE, diag = FALSE)
    entropy1 <- c(entropy1, mean(diversity(tempnet)))
    chain <- c(chain, ch)
    generation <- c(generation, gen)
    #average component persistence (TDA)
    phom.dat <-   as.data.frame(calculate_homology(as.dist(mat), format = "distmat")) #perform PH
    phom.dat <-    phom.dat[phom.dat$dimension==0,] #keep only 0-cycles (components)
    phom.dat$persistence <-   phom.dat[,3]-phom.dat[,2] #compute persistence from time death minus time birth
    tresh <- id_significant(phom.dat, dim = 0, cutoff = 0.975) #calculate the threshold for components to be outliving noise
    persistence <- mean(phom.dat$persistence[phom.dat$persistence > tresh],na.rm=TRUE) #what is the average persistence of persistences that reached above noise persistence?
    cluster_persistence <- c(cluster_persistence, persistence) #save persistence result
    
    p <- c(p, ppn) #record participant number and save in variable p
    
    #ADD kinematic analysis to network property dataset, to be compared to network properties later
    #get time series to regress for kinematic analysis
    if(gen != 0) #compute kinematics
    {
    kinematics<- data.frame() 
    kin <- ts[ts$ppn == ppn,]
      for(o in unique(kin$object)) #collect for each object/video network properties and save them in kinematics
      {
      subkin <- kin[kin$object == o,]
      kinematics <- rbind(kinematics, kin.get(subkin))
      }
    }
    if(gen == 0) #also do this for seed level networks (which are chain specific)
    {
      kinematics<- data.frame()
      kin <- subset(seeds, seedsetnum %in% ts$seedsetnum[ts$chain == ch])
      for(o in unique(kin$object))
      {
      subkin <- kin[kin$object == o,]
      kinematics <- rbind(kinematics, kin.get(subkin))
      }
      
    }
    #save network averaged kinematic info
    submovements <- c(submovements, mean(kinematics[,1], na.rm=TRUE))
    smoothness   <- c(smoothness, mean(kinematics[,2], na.rm=TRUE))
    gspace       <- c(gspace, mean(kinematics[,3], na.rm=TRUE))
    rhythmicity  <- c(rhythmicity, mean(kinematics[,4], na.rm = TRUE))
      #add Motamedi human coded entropy values to be compared later to global network entropy
    at <- unique(kin$ann_entropy)
    at <- at[!is.na(at)]
    ann_entropy <- c(ann_entropy, ifelse(!is.null(at), at, NA) )
    }
  }
}
#bind variables in one big data.frame 'ned'
ned <- cbind.data.frame(chain, generation, entropy1, cluster_persistence, p, submovements, gspace, smoothness, rhythmicity,  entropyWthemes, entropyWfunctions, ann_entropy)
```
  Figure 9 shows that for the gesture networks, that entropy was generally decreasing as a function of generation, indicating lower complexity of gesture interrelations as the system matures. Furthermore we find that clustering persistence was less pronounced at later generations. 

\pagebreak
```{r plot_results_individual_networks, warning = FALSE, message = FALSE, results = 'hide',fig.show='hide'}
#figure 9 plot code
library(cowplot)
library(scales)
#highlight two networks for early and late generations
ned$col <- ifelse(ned$generation==0 & ned$chain == "chain4", "example A", "Other") 
ned$col <- ifelse(ned$generation==5 & ned$chain == "chain4" & ned$p == "full40", "example B", ned$col) 

#main plot
a <- ggplot(ned, aes(x= generation, y = entropy1, color = col)) + geom_point(size = 3, alpha = 0.8) + facet_grid(.~chain) + geom_smooth(method= "lm", color = "red", alpha = 0.3) + theme_bw()+ ggtitle("entropy") + ylab("entropy")+ scale_color_brewer(palette="Set1")+ theme(strip.background =element_rect(fill="white"),legend.title = element_blank(), axis.title.x=element_blank())+ theme(legend.position = "none")
#agglomerative clustering coefficient

c <- ggplot(ned, aes(x= generation, y = cluster_persistence, color = col)) + geom_point(size = 3, alpha = 0.8) + facet_grid(.~chain) + geom_smooth(method= "lm", color= "orange", alpha = 0.3) + ggtitle("cluster persistence") + theme_bw() + ylab("cluster persistence")+ theme(legend.position = "none")+
  scale_color_brewer(palette="Set1")+ theme(strip.background =element_rect(fill="white"),legend.title = element_blank())


#plot two networks with multidimensional scaling
mat1 <-as.matrix(read.csv(paste0(matrices_data, "0_", "chain4", 0, ".csv")))
mat2 <- as.matrix(read.csv(paste0(matrices_data, "1_", "chain4", 5, "full40", ".csv")))
  #multipanel
par(mfrow=c(1,2),mar=c(0.75,0,0.75,0))

  #generation 1
  g_s <- graph.adjacency(mat1, mode="undirected", weighted=TRUE, diag = FALSE)
  l <- layout_with_mds(g_s, dist =mat1) #multidimensional scaling (so that layout reflects distance)
  l1 <- l
  #generation 5
  g_s2 <- graph.adjacency(mat2, mode="undirected", weighted=TRUE, diag = FALSE)
  l2 <- layout_with_mds(g_s2, dist =mat2 ) #multidimensional scaling (so that layout reflects distance)

  #rescale so that networks reflect real differences in distance
  l1[,1] <- rescale(l[,1], to = c(-0.6, 0.6), from = range(l2[,1]))
  l1[,2] <- rescale(l[,2], to = c(-0.6, 0.6), from = range(l2[,2]))
  l2[,1] <- rescale(l2[,1], to = c(-0.6, 0.6), from = range(l2[,1]))
  l2[,2] <- rescale(l2[,2], to = c(-0.6, 0.6), from = range(l2[,2]))
  
  
    plot(g_s, layout=l1, vertex.label=colnames(mat1),rescale=F, vertex.label.cex = 0.4, vertex.size = 10, vertex.color="mediumvioletred", edge.color = "indianred1",
       main = "Example A")         

  plot(g_s2, layout=l2, vertex.label=colnames(mat1), rescale=F,vertex.label.cex = 0.4, vertex.size = 10, vertex.color="mediumvioletred", edge.color = "lightblue",
       main = "Example B")
B <- recordPlot()


```
Figure 9. Changes in networks measures over generations within chains 
```{r plotresults, echo = FALSE, fig.height=7, fig.width=5, fig.align= 'center'}
plot_grid(a,c, B, nrow=3)
```
 *Note Figure 9.* \small For each chain the changes over generations in entropy and cluster persistence is shown, with generation 0 indicating the seed gesture set. For each generation > 0 there are two data points as there are two participants in each generation. Two example data points (red, and blue) are shown with their corresponding red and blue network representation (lower panel). In general cluster persistence decreased, indicating less differentiability between tokens. This may be seen in example A where there are relatively large cavities between tokens, while in example B the token organization is more homegenously tesselated. Indeed, entropy tends to decline over the generations, indicating that relationships between tokens became less diverse, possibly indicating systematicity in the way nodes are connected.  \normalsize

```{r test differences, echo = FALSE, message = FALSE, warning = FALSE}
   basem1 <- lme(entropy1~1, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   model1 <- lme(entropy1~generation, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   h0Entr <- anova(basem1, model1)
   h0EntrP <- summary(model1)
   h0EntrR <- r2beta(model1, method='sgv')
   h0EntrD <- lme.dscore(model1,ned, type = "nlme")
   
   basem1 <- lme(cluster_persistence~1, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   model1 <- lme(cluster_persistence~generation, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   h0cp <- anova(basem1, model1)
   h0cpP <- summary(model1)
   h0cpR <- r2beta(model1, method='sgv')
   h0cpD <- lme.dscore(model1,ned, type = "nlme")
```
  We tested these trends separately for each network property with mixed linear regression models, with chain as random intercept (random slopes did not converge for these models) and generation as independent predictor (0-5 generations, with generation 0 being the seed gesture network).  
  Generation was a reliable predictor for entropy as compared to a basemodel predicting the overall mean, chi-squared change (1) = `r printnum(round(h0Entr$L.Ratio[2],3) )`, *p* = `r printnum( round(h0Entr$'p-value'[2],3) )`, model *R*-squared =  `r printnum( round(h0EntrR$Rsq[2],2) )`. Model estimates showed that with increased generation the entropy decreased, *b* estimate =  `r printnum( h0EntrP$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( h0EntrP$tTable[2,3] )` ) = `r printnum( round(h0EntrP$tTable[2,4], 2) )`, *p* = `r printnum( round(h0EntrP$tTable[2,5], 3) )`, Cohen's *d* = `r printnum( round( h0EntrD$d, 3) )`).  
  Cluster persistence was predicted by generation as compared to a basemodel, chi-squared change (1) = `r printnum( round(h0cp$L.Ratio[2],3) )`, *p* `r printnum( ifelse(h0cp$'p-value'[2] <.001, "<.001", h0cp$'p-value'[2]) )`, model R-squared =  `r printnum( round(h0cpR$Rsq[2],2) )`. Model estimates showed that with increased generation the cluster persistence decreased, *b* estimate =  `r printnum(round(h0cpP$coefficients$fixed[2],5))`, *t* (`r printnum( h0cpP$tTable[2,3] )`) = `r printnum( round(h0cpP$tTable[2,4], 2) )`, *p* `r printnum(ifelse(h0cpP$tTable[2,5] < 0.001, "< .001", h0cpP$tTable[2,5]))`, Cohen's *d* = `r printnum( round( h0cpD$d, 3) )`). Note that the effect size of generation on cluster persistence is about twice as strong as compared to entropy.  
  We can further ask whether it is the case whether our network entropy measure is approximating the entropy of hand-coded gestures. Figure 10 confirms that this is indeed the case, such that entropy increase based on human-coded information units is related to increase in entropy based on gesture network entropy.

Figure 10. Gesture network entropy versus human-coded entropy
```{r entropycheck, echo = FALSE, message = FALSE, warning = FALSE, fig.align='center', fig.width = 4, fig.height=5}
ci <- cor.test(ned$ann_entropy[ned$generation!=0], ned$entropy1[ned$generation!=0])
cit <- paste0("r = ", round(ci$estimate, 2),  ", p = ", ifelse(ci$p.value < .001, "< .001", round(ci$p.value, 3)))

a <- ggplot(ned[ned$generation!=0,], aes(x = ann_entropy, y = entropy1, color = chain)) + geom_point(size = 1.5) + geom_smooth(method = "lm", color = "black", alpha = 0.2) + 
  ylab("network entropy") + xlab("entropy \n (based on human coding)" ) + theme_bw() +scale_colour_brewer(palette = "Set1")+
   annotate("text", label= cit, x=5.6, y=0.998, size = 5, color = "black")

a <- ggplot(ned[ned$generation!=0,], aes(x = ann_entropy, y = entropy1, color = chain)) + geom_point(size = 1.5) + geom_smooth(method = "lm", color = "black", alpha = 0.2) + 
  ylab("network entropy") + xlab("entropy \n (based on human coding)" ) + theme_bw() +scale_colour_brewer(palette = "Set1")+
   annotate("text", label= cit, x=5.6, y=0.998, size = 5, color = "black")

ci <- cor.test(ned$cluster_persistence[ned$generation!=0], ned$entropy1[ned$generation!=0])
cit <- paste0("r = ", round(ci$estimate, 2),  ", p = ", ifelse(ci$p.value < .001, "< .001", round(ci$p.value, 3)))
b <- ggplot(ned[ned$generation!=0,], aes(x = ann_entropy, y = cluster_persistence, color = chain)) + geom_point(size = 1.5) + geom_smooth(method = "lm", color = "black", alpha = 0.2) + 
  ylab("cluster persistence") + xlab("entropy \n (based on human coding)" ) + theme_bw() +scale_colour_brewer(palette = "Set1")+
   annotate("text", label= cit, x=5.6, y=2.5, size = 5, color = "black")

grid.arrange(a, b)

```
*Note figure 10.* \small The upper panel shows that there there is a strong relationship between the gesture network entropy with that of entropy computed on human-coded information units. That network entropy is uniquely related to systematicity is further corroborated by the finding that cluster persistence is not reliably correlated with entropy based on human coding. It does seems that gesture network analysis is a form-based proxy for systematicity in silent gesture.\normalsize

## Changes within them versus changes within function
  We can further localize where systematicity is most likely to increase (i.e., decrease in entropy) by subsetting the communicative tokens based on theme and function groupings. Thus for each participant we selected a sub-network grouped by function category gesture or theme category gesture and then computed network entropy for each of those subnetworks. This was done for all category tokens (e.g., "action", "agent", etc.) and averaged for function and theme separately, to yield an average entropy for each category. See figure 11 for the main results of these subset networks.
  
Figure 11. Change in entropy in theme-level networks versus function-grouped networks
```{r furthereentropy, message = FALSE, warning = FALSE, echo = FALSE}

a <- ggplot(ned, aes(x= generation, y = entropyWfunctions, color = chain)) + geom_point() + geom_smooth(method= "lm", size=1, alpha=0.02)+ geom_smooth(method= "lm", color = "red") + theme_bw()+ ggtitle("function-level entropy ") + theme(legend.position = "none")+ scale_color_brewer(palette = "Dark2")+ ylab("entropy")
b <- ggplot(ned, aes(x= generation, y = entropyWthemes, color = chain)) + geom_point()+ geom_smooth(method= "lm", size=1, alpha=0.02) + geom_smooth(method= "lm", color = "red") + ggtitle("theme-level entropy") +theme_bw()+ theme(legend.position = "none") + scale_color_brewer(palette = "Dark2")+ ylab("entropy")

grid.arrange(a,b, nrow =1)


   #model functon category (mixed regression modeling)
   basem1F <- lme(entropyWfunctions~1, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   model1F <- lme(entropyWfunctions~generation, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   comp1F <- anova(basem1F, model1F)
   sum1F <- summary(model1F)
   RSQ1F <-  r2beta(model1F, method='sgv')
   model1FD <- lme.dscore(model1F,ned, type = "nlme")
   
   #model theme-level (mixed regression modeling)
   basem1T <- lme(entropyWthemes~1, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   model1T <- lme(entropyWthemes~generation, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   comp1T <- anova(basem1T, model1T)
   sum1T <- summary(model1T)
   RSQ1T <-  r2beta(model1T, method='sgv')
   model1TD <- lme.dscore(model1T,ned, type = "nlme")

```
*Note Figure 11*. \small On the left panel, the average network entropy for the function-grouped gestures are plotted over the generations with red line showing the trend averaged over chain (other-colored lines). On the right panel this is shown for the gestures grouped by theme category. It can be seen that only the function-grouped gesture networks showed increased systematicity (lower entropy) over the generations.\normalsize

 We find that only functionally grouped tokens were minimizing entropy over the generations. Including generations for predicting function-level network entropy increased predictability as compared to a base model (random intercept chain, random slopes did not converge), chi-squared change (1) = `r printnum(round(comp1F$L.Ratio[2],3) )`, *p* = `r printnum( round(comp1F$'p-value'[2],3) )`, model *R*-squared =  `r printnum( round(RSQ1F$Rsq[2],2) )`, with generation relating to lower entropy  *b* estimate =  `r printnum( sum1F$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( sum1F$tTable[2,3] )` ) = `r printnum( round(sum1F$tTable[2,4], 2) )`, *p* = `r printnum( ifelse(sum1F$tTable[2,5]<.001 ,"p < .001",round(sum1F$tTable[2,5], 4) ))`, Cohen's *d* = `r printnum( round( model1FD$d, 3) )`).  
 There was however no reliable decrease in entropy for the theme-level networks, chi-squared change (1) = `r printnum(round(comp1T$L.Ratio[2],3) )`, *p* = `r printnum( round(comp1T$'p-value'[2],3) )`, model *R*-squared =  `r printnum( round(RSQ1T$Rsq[2],2) )`.

## Kinematic features
  Next we performed mixed regression analysis for assessing potential kinematic changes as a function of generation, with random intercept for objects nested within chains (random slopes did not converge). See figure 12 for main results.
  
Figure 12. Change in kinematic properties over generations  
```{r kinematicresults, message = FALSE, warning = FALSE, echo = FALSE,fig.align='center', fig.height=4, fig.width =5}
#retrieve and plot kinematic properties over generations
tstemp <- ts #copy temporary time series data
tstemp$identifier <- paste0(tstemp$generation, tstemp$ppn, tstemp$chain, tstemp$object)  #make unique identifiers for each video
seedsTS <- tstemp[tstemp$generation=="s",] #extract a separate seed time series

#initialize variables to be collected, and data.frame to collect variables in
chains <- generations <-objects <- verb <- vector()
featsdata <- data.frame()
for(ch in c("chain1", "chain2", "chain3", "chain4","chain5"))
{ 
  cc <- as.data.frame(tstemp[tstemp$chain == ch,]) #collect time series of chain 'ch'
  for(id in unique(cc$identifier)) #go through all gestures in this chain and get relevant kinematic info
  {
  ccsub <- cc[cc$identifier==id,]
  featsdata <- rbind.data.frame(featsdata, kin.get(ccsub)) #extract kinematic features for this gesture
  objects <- c(objects, as.character(ccsub$object[1]))     #save object of depiction 
  verb <- c(verb, ccsub$ann_verb[1])                     #verb info is it a verb yes no?
  generations <- c(generations, ccsub$generation[1])    
  chains <- c(chains,ch)
  }
  
  #add for this the chain the relevant seed gesture information
  seed_sub <- unique(cc$seedsetnum) #also collect for this chain the relevant seed videos used
  #add seeds
  for(seed_g in seed_sub)
  {
    ccsub <- seedsTS[seedsTS$seedsetnum==seed_g,]
    featsdata <- rbind.data.frame(featsdata, kin.get(ccsub))
    objects <- c(objects, as.character(ccsub$object[1]))
    verb <- c(verb, ccsub$ann_verb[1])
    generations <- c(generations, 0)
    chains <- c(chains, ch )
  }
}

kin_data <- cbind.data.frame(featsdata, chains, objects,verb, generations)
kin_data$verb <- as.character(kin_data$verb)
kin_data$verb <- ifelse(kin_data$verb == "N", "no verb", "verb")

 #smoothness (mixed regression modeling)
   basem1 <- lme(log(smoothness)~1, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   model1 <- lme(log(smoothness)~generations, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   sm_comp <- anova(basem1, model1)
   sm_sum <- summary(model1)
   sm_r <- r2beta(model1, method='sgv')
    #add verb (mixed regression modeling)
      model1v <- lme(log(smoothness)~generations+verb, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
     sm_compv <- anova(model1, model1v)
     sm_sumv <- summary(model1v)
     sm_rv <- r2beta(model1v, method='sgv')
     sm_Dv <-  lme.dscore(model1v,kin_data, type = "nlme")

  #rhythmicity (mixed regression modeling)
   basem1 <- lme(rhythmicity~1, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   model1 <- lme(rhythmicity~generations, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   rh_comp <- anova(basem1, model1)
   rh_sum <- summary(model1)
   rh_r <- r2beta(model1, method='sgv')
   rh_D <-  lme.dscore(model1,kin_data, type = "nlme")

    #add verb, not reliable
     #model1v <- lme(rhythmicity~generations*verb, data = kin_data, random = ~1|chains/objects, method = "ML", na.action =            #na.exclude)
     #rh_compv <- anova(model1, model1v)

#average gesture space (mixed regression modeling)
   basem1 <- lme(gspace~1, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   model1 <- lme(gspace~generations, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   gs_comp <- anova(basem1, model1)
   gs_sum <- summary(model1)
   gs_r <- r2beta(model1, method='sgv')
   gs_D <-  lme.dscore(model1,kin_data, type = "nlme")

#main plot
library(ggbeeswarm)
a <- ggplot(kin_data, aes(x= generations, y = log(smoothness), color = verb)) + geom_quasirandom(aes(group = generations),size=0.5, alpha= 0.75) + geom_smooth(method= "lm", alpha=0.1, size = 2) + theme_bw() + xlab("generation") + scale_color_brewer(palette = "Dark2")+ theme(legend.position = "none")+ylab("intermittency")+ggtitle("intermittency")+
  scale_x_continuous(limits=c(0, 5)) 

b <- ggplot(kin_data, aes(x= generations, y = rhythmicity, color = verb))+ geom_quasirandom(aes(group = generations),size=0.5, alpha= 0.75)+ geom_smooth(method= "lm", alpha=0.1, size = 2)+ theme_bw() + xlab("generation")+ylab("temporal var.") + ggtitle("temporal var.")+ theme(legend.position = "none")+ scale_color_brewer(palette = "Dark2")+
   scale_x_continuous(limits=c(0, 5))

c <- ggplot(kin_data, aes(x= generations, y = gspace, color = verb)) + geom_quasirandom(aes(group = generations),size=0.75, alpha= 0.5)+ geom_smooth(method= "lm", alpha=0.1, size= 2) + theme_bw() + xlab("generation") +ylab("gesture space")+ ggtitle("gesture space")+ theme(legend.position = "bottom")+ scale_color_brewer(palette = "Dark2")+
   scale_x_continuous(limits=c(0, 5))

grid.arrange(a,b,c, nrow= 1)

```
*Note Figure 12*. \small Generation trends per chain are shown for intermittency, temporal variability and gesture space. Each observation indicates a communicative token, and these are spatially organized per their density distribution and colored by verb (green) or no verb (orange). We can see that over the generations, movements become more smooth (lower intermittency score), with a more stable rhythm (lower temporal variability), and more minimized movements (smaller gesture space). Note, that temporal variability has lower data points as often the movement did not consist of more than 2 submovements. Thus, temporal variability indicates that *when there is a multi-segmented movement*, then such movements were more rhythmic.  \normalsize

  Generations reliably predicted intemittency of the movements relative to a basemodel, chi-squared change (1) = `r printnum( round(sm_comp$L.Ratio[2],3) )`, *p* `r printnum( ifelse(sm_comp$'p-value'[2] <.001, " <.001"  ,round(sm_comp$'p-value'[2],3) <.005) )`, model *R*-squared =  `r printnum( round(sm_r$Rsq[2],2) )`. When adding verb as another predictor, this improved model fit for intermittency, chi-squared change (1) = `r printnum( round(sm_compv$L.Ratio[2],3) )`, *p* = `r printnum( ifelse(sm_compv$'p-value'[2] <.001, " <.001", round(sm_compv$'p-value'[2],3)) )`, model *R*-squared =  `r printnum( round(sm_rv$Rsq[2],2) )`. In this final model generation predicted lower intermittency score, *b* estimate =  `r printnum( sm_sumv$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( sm_sumv$tTable[2,3] )` ) = `r printnum( round(sm_sumv$tTable[2,4], 2) )`, *p* `r printnum( ifelse(sm_sumv$tTable[2,5]<.001 ," < .001", round(sm_sumv$tTable[2,5], 4) ))`, Cohen's *d* = `r printnum( round(sm_Dv$d[1], 2) )`). Silent gestures conveying verbs showed lower intermittency in general, *b* estimate =  `r printnum( sm_sumv$coefficients$fixed[3], digits =4 ) `, *t* ( `r printnum( sm_sumv$tTable[3,3] )` ) = `r printnum( round(sm_sumv$tTable[3,4], 2) )`, *p* `r printnum( ifelse(sm_sumv$tTable[3,5]<.001 ," < .001", round(sm_sumv$tTable[3,5], 4) ))`, Cohen's *d* = `r printnum( round(sm_Dv$d[2], 2) )`). There were no interaction effects of generation and verb.  
  We also observe lower temporal variability as a function of generations, chi-squared change (1) = `r printnum( round(rh_comp$L.Ratio[2],3) )`, *p* `r printnum( ifelse(rh_comp$'p-value'[2] <.001, " <.001"  ,round(rh_comp$'p-value'[2],3) <.005) )`, model *R*-squared =  `r printnum( round(rh_r$Rsq[2],2) )`, indicating more stable rhythmic movements at later generations, *b* estimate =  `r printnum( rh_sum$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( rh_sum$tTable[2,3] )` ) = `r printnum( round(rh_sum$tTable[2,4], 2) )`, *p* `r printnum( ifelse(rh_sum$tTable[2,5]<.001 ," < .001", round(rh_sum$tTable[2,5], 4) ))`, Cohen's *d* = `r printnum( round(rh_D$d[1], 2) )`)). Adding verb or verb x generation to model temporal variability did not improve model fit.
  Finally, over the generations gesture space decreased, chi-squared change (1) = `r printnum( round(gs_comp$L.Ratio[2],3) )`, *p* `r printnum( ifelse(gs_comp$'p-value'[2] <.001, " <.001"  ,round(gs_comp$'p-value'[2],3) <.005) )`, model *R*-squared =  `r printnum( round(gs_r$Rsq[2],2) )`. Model estimated gesture space was less for later generations, *b* estimate =  `r printnum( gs_sum$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( gs_sum$tTable[2,3] )` ) = `r printnum( round(gs_sum$tTable[2,4], 2) )`, *p* `r printnum( ifelse(gs_sum$tTable[2,5]<.001 ," < .001", round(gs_sum$tTable[2,5], 4) ))`, Cohen's *d* = `r printnum( round(gs_D$d[1], 2) )`). Adding verb or verb x generation to model gesture space did not improve model fit. 
  In conclusion, our kinematic results show all the hallmarks of increased communicative efficiency. Namely, gestures were on average smaller, less temporally variable, and less intermittent as the communicative system matured. Silent gestures that conveyed a verb were generally less intermittent, suggesting that they consist of smoother movement patterns.

## Kinematic and network properties
  Figure 11 contains the correlations of the relationships of kinematic properties (average per participant) and the network measures cluster persistence and entropy. Network entropy goes down as the average gesture space decreases, and the movement becomes less intermittent. This also comes at a trade-off, such that this simplification of kinematics also reduces differientiability of communicative tokens as shown by less stable clustering when gesture become smaller, less temporally variable, and less intermittent. Thus on the kinematic level there seems to be a general decrease of complexity which is further reflected on the level of the system as a whole as utterances become less *kinematically* differientable (less clustering) and more structured in their relations (lower entropy).

\pagebreak
Figure 13. Relation between kinematic properties and network measures
```{r test_network_kinematics, echo = FALSE, message = FALSE, warning = FALSE, fig.height =5, fig.width=6}
  #individual level
sh <- cor.test(log(ned$smoothness), ned$entropy1)
sht <- paste0("r = ", round(sh$estimate, 2),  ", p = ", ifelse(sh$p.value < .001, "< .001", round(sh$p.value, 3)))
a <- ggplot(ned, aes(x = log(smoothness), y=entropy1)) + geom_point() + geom_smooth(method = "lm", color = "indianred1")+ theme_bw()+ggtitle("intermittency")+xlab("intermittency")+ylab("entropy")+
   annotate("text", label= sht, x=7, y=1.0, size = sizetext)

rh <- cor.test(ned$rhythmicity, ned$entropy1)
rht <- paste0("r = ", round(rh$estimate, 2),  ", p = ", ifelse(rh$p.value < .001, "< .001", round(rh$p.value, 3)))
b <- ggplot(ned, aes(x = rhythmicity, y=entropy1)) + geom_point() + geom_smooth(method = "lm", color= "black") + theme_bw()+ ggtitle("temporal var.")+xlab("temporal var.") +ylab("entropy")+
   annotate("text", label= rht, x=0.75, y=1.0, size = sizetext)

gh <- cor.test(ned$gspace, ned$entropy1)
ght <- paste0("r = ", round(gh$estimate, 2),  ", p = ", ifelse(gh$p.value < .001, "< .001", round(gh$p.value, 3)))
c <- ggplot(ned, aes(x = gspace, y=entropy1)) + geom_point() + geom_smooth(method = "lm", color ="orange")+ theme_bw()+ggtitle("gesture space")+xlab("gesture space")+ylab("entropy")+
   annotate("text", label= ght, x=115, y=1.0, size = sizetext)

#cluster persistence
scp <- cor.test(log(ned$smoothness), ned$cluster_persistence)
scpt <- paste0("r = ", round(scp$estimate, 2),  ", p = ", ifelse(scp$p.value < .001, "< .001", round(scp$p.value, 3)))
d <- ggplot(ned, aes(x = log(smoothness), y=cluster_persistence)) + geom_point() + geom_smooth(method = "lm", color = "indianred1")+ theme_bw()+xlab("intermittency") +ylab("cluster persistence")+
   annotate("text", label= scpt, x=7, y=2.70, size = sizetext)

rhcp <- cor.test(ned$rhythmicity, ned$cluster_persistence)
rhcpt <- paste0("r = ", round(rhcp$estimate, 2),  ", p = ", ifelse(rhcp$p.value < .001, "< .001", round(rhcp$p.value, 3)))
e <- ggplot(ned, aes(x = rhythmicity, y=cluster_persistence)) + geom_point() + geom_smooth(method = "lm", color = "black")+ theme_bw()+xlab("temporal var.")+ylab("cluster persistence")+
   annotate("text", label= rhcpt, x=0.75, y=2.70, size = sizetext)

gscp <- cor.test(ned$gspace, ned$cluster_persistence)
gscpt <- paste0("r = ", round(gscp$estimate, 2),  ", p = ", ifelse(gscp$p.value < .001, "< .001", round(gscp$p.value, 3)))
f <- ggplot(ned, aes(x = gspace, y=cluster_persistence)) + geom_point() + geom_smooth(method = "lm", color ="orange")+ theme_bw()+xlab("gesture space")+ylab("cluster persistence")+
   annotate("text", label= gscpt, x=115, y=2.70, size = sizetext)


grid.arrange(a,b,c,d,e,f,nrow = 2)
```
*Note Figure 13*. \small Correlations are shown for each kinematic property averaged over all utterances and the concomittant network measure result. It can be seen that less intermittency, lower temporal variability,  and smaller gesture spaces, relate to lower entropy and lower cluster persistence. This indicates that complexity in movement is cashed out in terms of systematicity and more homegeneous interrelationships (lower clustering) on the network level. \normalsize

\pagebreak
# Discussion
  Based on signal processing alone we have detected systematic changes reflective of a linguistically maturing communication system from continuous multi-articulatory kinematics of silent gestures. We applied computer vision techniques to extract kinematics from video data, and then applied an analysis procedure to detect structural relations between gestural utterances [@pouwGestureNetworksIntroducing2019]. We found that communicative tokens showed higher systematicity at later generations, conceptually replicating results that were based on human coding of the gesture’s content [@motamediEvolvingArtificialSign2019]. Indeed, gesture network entropy turned out to be a good approximation of entropy based on human coding of the gesture content.  
  We further find that tokens were less stably differentiable on the form level as tokens have lower cluster persistence over the generations. Moreover, we found a decrease in entropy for the functional rather than the thematic dimension. This is consonant with the manual coding findings of the original study and the regular encoding of functional categories such as object versus action-distinctions in sign languages [@paddenPatternedIconicitySign2013], suggesting further applications for the automated methods introduced here. While in the original study no increase in efficiency was found based on measuring gesture information units, we did detect signs of communicative efficiency for gesture kinematics. Over generations, gestures became less segmented (more smoother), more rhythmic (if comprised of more than 3 submovements), and smaller. We also show that verbs have a different kinematic quality to non-verbs, being more smooth (less intermittent) in their execution, replicating previous research showing that actions often occur of as a single segment [@ortegaTypesIconicityCombinatorial2020]. Finally, we show that the decrease in kinematic complexity on the token level, predicts system-level changes of decreased entropy and decrease in clustering.  
  A decrease in cluster persistence over generations here is likely to reflect a differentiability of communicative tokens, which as originally reported often showed iconic gestures at early stages in the iterations that were sometimes ambiguous in the theme category, and maximally differentiated from the other-themed gestures. For example, “arrest” and “police officer” could both contain a gesture that enacts the appliance of hand cuffs. Thus within themes there was clustering, but across themes there is differentiation. When gestures are disambiguated over the generations this will result in increased distances among the gestures within this category on the network level, i.e., leads to less clustering. While clusters became more unstable over the generations, the diversity of the interrelationships of the communicative tokens decreased (i.e., entropy decreased), and this is especially on the functional level. This suggests that there is a more consistent and thus homogeneous way in which the communicative system is organized, and the reorganization is caused by a reuse of gesture *across themes* and *within function*. That this increase in consistency is indeed a form of systematicity is fully supported by the detection of entropy decrease over the generations for communicative tokens grouped on the functional dimension (e.g., agent, action, location), but not the thematic dimension (e.g., justice, cooking).  
  The kinematic findings suggest that the manual utterances simplify, in the way of reducing in size, in the reduction of submovements and the decrease in temporal variability. This simplification seem to be a reduction in articulatory effort, as making a minimal amount of smaller rhythmic movements reduces the degrees of freedom for articulation [@bernsteinCoordinationRegulationsMovements1967; @kelsoFunctionallySpecificArticulatory1984]. Moreoever, this increased rhythmicity could also increase learnability and comprehensibility of the gesture, as we know from speech perception in noisy conditions that it is optimally perceived when speech is more rhythmic [@wangSpeakingRhythmicallyImproves2018].  
  Interestingly, this reduction of degrees of freedom of the pronunciation, is precisely what one finds for novice learners of ASL. ASL learners have been found to spatially reduce their signs as they become more fluent [@wilburExperimentalInvestigationStressed1990; @luptonMotorLearningSign1990]. Moreover, a reduction in duration between the compounds of the signs have been observed during learning progression, where multicomponent component signs are increasingly performed as a single sign. In the present paradigm, there is a similar evolution of pronunciation, such that gestural multi-articulatory utterances acquire stable functional organizations across generations. Suboptimal organization of sub-movements will be filtered out as it were over the generations, and the temporally extended movement sequence becomes likely more coordinated whereby degrees of freedom are reduced by functioning as a single multimodal coordinative structure [@bernsteinCoordinationRegulationsMovements1967; @kelsoConvergingEvidenceSupport1984; @kelsoDynamicPatternPerspective1983], affecting for example gesture's temporal variability and intermittency. That head movements improved differientation of real vs. falsely paired gestures in our analysis, further emphasize that multiple articulators coordinated in the production of meaning in the current task. This finding resonates with the known grammatic, phonetic, and prosodic functions that head movements have in sign languages such as ASL [@tyronePhoneticsHeadBody2016].  
  Note that our method allowed to account for the multiarticulatory nature of communication without formalized additional coding of the head movements, and we were able to quantify the unique communicative contributions of head and upper limb movements in the current paradigm. In this way, the current method is a bottom-up approach that will invite further investigation when needed. Our bottom-up approach further showed that gesture network entropy decreases alongside the entropy obtained from human coded content segmentation of the gestural utterance [@motamediEvolvingArtificialSign2019], suggesting that systematicity in form can be detected without the need for an apriori coding scheme. But our method as exposed here goes one step further. If gesture network analysis is complemented with kinematic feature analysis, it can be further assessed *what* is driving systematicity, providing insights on the evolution of the morphology of the silent gesture system. Coding schemes are notoriously difficult to formalize as any gesture researcher will confirm, and the current buttom-up method provides a formalized procedure for the detection of gesture evolution. As it is formalizated, the method is waiting to be applied to large datasets that are impractical to humanly annotate and code. Thus an exciting avenue of further research is how different morphological evolutions can yield similar or different levels of systematicity at the gesture network level depending on different communicative constraints in vast populations.  
  There are two important caveats however to the analyses presented here. First, in general, it is the case kinematic analysis cannot say anything about the precise semiotic content that might evolve, and this is especially the case with incereasing 'drifts towards the arbitrary' [@tomaselloOriginsHumanCommunication2008]. Although such drift might be detected via our network analysis, recognizing its possible semiotic uses still requires human interpretation.  
  Second, there is a limitation to dynamic time warping. If we appreciate that combinatorics increases as a communicative system matures, holistic gestures become segmented and the order of presentation of such segments might be (meaningfully) varied. However, the dynamic time warping algorithm is sensitive to ordering and would judge two gestures containing identical segments in different orders as very different, while for a human coder the similarity is transparent. Thus our analysis may at times judge sequences of gestures highly dissimilar when in fact they are merely ordered differently. There are ways to circumvent this (Pouw & Dixon 2019), but such analysis goes beyond the current approach.  
  Both of these caveats mean that our approach to kinematics, like all quantitative analyses of human behaviour, requires some degree of human oversight (for meaningful implementation) and human insight (for judicious interpretation). When these requirements are met, we believe that our fully reproducible and automatable methods can make important contributions: It will reduce the amount of manual coding which is currently consuming many researchers' time. It provides a much needed *multiscale* approach to how gestures evolve as communicative systems. Finally, the current method can scale up the study of language evolution across modalities, as the kinematic analysis shown here functions much like an acoustic analysis in speech. In this vein we hope with the current approach to break ground towards a gesture phonetics.

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
