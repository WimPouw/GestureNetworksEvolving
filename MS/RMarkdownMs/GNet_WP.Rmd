---
title             : "A systematic investigation of gesture kinematics in evolving manual languages in the lab"
shorttitle        : "Systematic investigtion fo evolving manual languages"

author: 
  - name          : "Wim Pouw"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Montessorilaan 3, 6525 HR Nijmegen"
    email         : "w.pouw@psych.ru.nl"
  - name          : "Mark Dingemanse"
    affiliation   : "1,3"
  - name          : "Yasamin Motamedi"
    affiliation   : "4"
  - name          : "Asli Ozyurek"
    affiliation   : "1,2,3"

affiliation:
  - id            : "1"
    institution   : "Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen"
  - id            : "2"
    institution   : "Institute for Psycholinguistics, Max Planck Nijmegen"
  - id            : "3"
    institution   : "Center for Language Studies, Radboud University Nijmegen "
  - id            : "4"
    institution   : "Center for Language Evolution, University College London"

authornote: |
  This work is supported by a Donders Fellowship awarded to Wim Pouw and Asli Ozyurek and is supported by the Language in Interaction consortium project 'Communicative Alignment in Brain & Behavior' (CABB).

abstract: |
  In the current approach we perform a systematic quantification of kinematic changes of an evolving silent gesture system. Silent gestures consist of complex multi-articulatory movement that have so far proven elusive to quantify in a structural and reproducable way, and is primarily studied through human coders meticulously interpreting the referential content of gestures. Here we demonstrate that the signal’s objective form (kinematics of gesture) is informative about the emergence of linguistic constraints. Here we reanalyzed the video data from a gesture evolution experiment (Motamedi et al. 2019), which originally showed increases in systematicity in gesture’s content over time. We applied a signal-based approach, utilizing computer vision techniques to quantify kinematics. Then we performed a kinematic analysis, showing that over generations of language users, silent gestures became more efficient and less complex in their kinematics. We further detect systematicity of the gesture form on the level of their interrelations, which proved itself as a proxy of systematicity obtained via human observation data. Finally, we demonstrate that unique gesture kinematic cultures emerged over generations, as isolated chains of participants gradually diverged over iterations from other chains. We thereby show how gestures can come to embody the linguistic system at the level of interrelationships between communicative tokens, paving a way towards a reproducible analysis of linguistic constraints in continuous multidimensional signals.
  
keywords          : "language evolution, silent gesture, kinematics, systematicity"
wordcount         : "X"

bibliography      : ["r-references.bib"]

fig_caption       : no
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man, noextraspace"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
library("papaja")     #papaja::apa6_pdf papaja::apa6_word
library(ggplot2)      #plotting
library(gridExtra)    #plotting (constructing multipanel plots)
library(ggExtra)      #plotting (adding distributions)
library(RColorBrewer) #plotting (color schemes)
library(dtw)          #dynamic time warping functions
library(effsize)      #effectsizes calculations
library(igraph)       #network graphing and analysis (network entropy)
library(cluster)      #cluster analysis (agglomerative clusterting coefficient)
library(nlme)         #mixed linear regression
library(signal)       #for butterworth filter
library(TDAstats)     #for persistent homology clusteriness analusis
library(pracma)       #for peak finding
library(r2glmm)       #mixed regression R^2
library(DescTools)    #Entropy calculation of the original motamedi data
library(EMAtools)     #cohen's d for mixed regression predictors
library(tsne)         #visualization for networks
library(cowplot)      #plotting aesthetics
library(scales)       #functions rescaling variables
library(stringr)          #string manipulation
library(parallelsugar)    #for parallel computing
library(clValid)
library(entropy)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r main_data_load, warning = FALSE, include = FALSE}
#prepare the data
  #time series data (ts): This file contains all the gesture time series and relevant info about the gesture (chain etc)
  #matrices data: This is the folder where all the gesture network matrices are stored and are created in the code chunk below
  #meta: This contains the original data from Motamedi where e.g., gesture codes, unique information units are given for each gesture

#SET FOLDERS: !!!PLEASE CHANGE TO OWN FOLDER STRUCTURE FOR CODE TO RUN APPROPRIATELY!!!
basefolder <- "E:/EmergingLanguageKirby/REVISION_COGSCI"
ts <- read.csv(paste0(basefolder, "/Experiment1/Data/ts_exp1.csv"))
matrices_data <- paste0(basefolder, "/Experiment1/Data/DistanceMatrices/")
plotfolder <- paste0(basefolder, "/Experiment1/Plots")

  #load in original METAdata Motamedi into time series
  meta <- read.csv(paste0(basefolder, "/experiment1/Data/ex1.csv"))
    #change the naming in the meta file and time so that it overlaps with the naming in the time series file
  meta$target <- gsub(" ", "_", meta$target)
  ts$object <- as.character(ts$object)
  meta$target[meta$target == "to_take_a_photo"] <- "take_photo"
  
  #loop through object names in the time series file, match with meta naming, and change to time series naming 
  for(i in unique(ts$object))
  {    
    meta$target[grep(i, meta$target)] <- i
  }
  
  #recompute entropy Motamedi
  meta$uniq_codestring <- NA 
  #first only keep unique human-coded information elements in each gesture (this is where entropy is calculated on)
  for(r in 1:nrow(meta))
    {
    strr <- c(strsplit(as.character(meta$code_string[r]), split = ",")[[1]]) # get a vector with all elements
    if(length(strr) > 0) #if there is more than one element
    {
    meta$uniq_codestring[r] <- paste0(strr[!duplicated(strr)], collapse = ',') #remove duplicates and combine them in a single string again, then save into unique code string 
    }
  }
  
  #compute entropy on the concatenated list fo gesture information units per participant
  meta$entropy <- NA #initalize variabel which will contain ppn-level entropy, it will be saved in the metadataset
    for(ppn in unique(meta$participant))
    {
    gstring     <- paste0(as.character(meta$uniq_codestring[meta$participant == ppn]),collapse = ',')
    gstring     <- as.data.frame(strsplit(gstring, split = ","))
    gstring[,1] <- as.numeric(factor(gstring[,1]))
    meta$entropy[meta$participant==ppn]     <- Entropy(gstring[,1]) #add entropy measure to Motamedi meta file
    }

#load in time series object for each gesture the video_length, verb (no, yes), number of reps, guess time, code length, code_string(entropy calc)
  
  unique_videos <- unique( paste0(ts$ppn, ts$object)) #make an object where we can identify each unique gesture in the dataset
  #make new variables in in the time series files
  ts$ann_guesstime <- ts$ann_verb <- ts$ann_gcode <- ts$ann_reps <-  ts$ann_inf_units <-ts$segments <- ts$ann_entropy  <- NA
  #loop through all videos and load in relevant data from the meta files so that Time series data have this info too
  for(v in unique_videos)
  {
    condts <- (paste0(ts$ppn, ts$object)==v) #set condition for selection time series
    condmet <- (paste0(meta$participant, meta$target)==v) #set condition for selection time series
    
    #load in the meta data into time series object so that this information can be easily retrieved
    #when using these objects for analysis
    if(TRUE %in% condmet)
    {
    ts$ann_guesstime[condts]  <- unique(meta$guess_time[condmet])
    ts$ann_verb[condts]       <- as.character(unique(meta$verb[condmet]))
    ts$ann_gcode[condts]      <- as.character(unique(meta$code_string[condmet]))
    ts$ann_reps[condts]       <- unique(meta$num_reps[condmet])
    ts$ann_inf_units[condts]  <- unique(meta$code_len[condmet])
    ts$segments[condts]       <- unique(meta$num_reps[condmet])+unique(meta$code_len[condmet])
    ts$ann_entropy[condts]    <- unique(meta$entropy[condmet])
    }
  }
#hand correct police-officer, which should be not ("N") a verb
ts$ann_verb[ts$object == "police_officer"] <- "N"
ts$ann_verb[ts$object == "singer"] <- "N"





```

```{r repeated_functions, echo = FALSE, warning = FALSE}
#FUNCTION extractR.traces: we often need to extract the relevant movement traces from the dataset for input for the Multivariate DTW
  #the relevant z-scaled and centered x,y, traces are left and right hand movement, head movement
  #this function extracts these traces to be used as input for DTW
extractR.traces <- function(dat)
{
  dat <- data.frame(dat)
  ts1 <- cbind( as.vector(scale(dat$x_index_left, center = TRUE)),
                as.vector(scale(dat$y_index_left, center = TRUE)),
                as.vector(scale(dat$x_index_right, center = TRUE)),
                as.vector(scale(dat$y_index_right, center = TRUE)),
                as.vector(scale(dat$x_wrist_left, center = TRUE)),
                as.vector(scale(dat$y_wrist_left, center = TRUE)),
                as.vector(scale(dat$x_wrist_right, center = TRUE)),
                as.vector(scale(dat$y_wrist_right, center = TRUE)),
                as.vector(scale(dat$x_nose, center = TRUE)),
                as.vector(scale(dat$y_nose, center = TRUE)))
} 


#FUNCTION DTW.compare:  This function performs the multidimensional dynamic time warping (D) score
  #It takes two multivariable time series (see extractR.traces) as argument, and it takes as argument whether only hands should be compared 
DTW.compare <- function(TS1, TS2, manualonly)
{
    #make sure that if there is nothing detected than set to 0
    TS1 <- ifelse(is.nan(TS1), 0, TS1)
    TS2 <- ifelse(is.nan(TS2), 0, TS2)
    
    #perform the dynamic time warping, extract the distance, and then sum the score
    distancedtw <-  dtw(  TS1[,1:2],   TS2[,1:2])$normalizedDistance +
                  dtw(  TS1[,3:4],   TS2[,3:4])$normalizedDistance +
                  dtw(  TS1[,5:6],   TS2[,5:6])$normalizedDistance + 
                  dtw(  TS1[,7:8],   TS2[,7:8])$normalizedDistance +
                  dtw(  TS1[,9:10],   TS2[,9:10])$normalizedDistance
    
    #do the same procedure but only for the index and wrist traces (will be use to compare performance of different body points)
    if(manualonly == "manual_only")
    {
    distancedtw <-  dtw(  TS1[,1:2],   TS2[,1:2])$normalizedDistance +
                    dtw(  TS1[,3:4],   TS2[,3:4])$normalizedDistance +
                    dtw(  TS1[,5:6],   TS2[,5:6])$normalizedDistance + 
                  dtw(  TS1[,7:8],   TS2[,7:8])$normalizedDistance
    }
    return(distancedtw)
}

#dimensionless smoothness measure
smooth.get <- function(velocity) #Hogan & Sternad formula
{
  if(!all(velocity ==0))
  {
  velocity <- as.vector(scale(velocity))
  acceleration <- butter.it(diff(velocity))
  jerk         <- butter.it(diff(acceleration))
  integrated_squared_jerk <- sum(jerk^2)
  max_squaredvelocity <- max(velocity^2)
  D3 <- (length(velocity))^3
  jerk_dimensionless <-  integrated_squared_jerk*(D3/max_squaredvelocity)
  smoothness <- jerk_dimensionless
  }
  if(all(velocity ==0)) #if all zero, this
  {
  smoothness <- NA
  }
  return(smoothness)
}

#butterworth filter
butter.it <- function(x)
{bf <- butter(1, 1/33, type="low")
x <- as.numeric(signal::filter(bf, x))}

#kinematic feature extraction:
  #THIS FUNCTION EXTRACTS FOR ALL keypoints the: submovements, intermittency (smoothness), rhythm, 
  #temporal variability (rhythmicity), gesture space
kin.get <- function(MT)
  {
    MT <- data.frame(MT)
  #perform submovement analysis(using findpeaks function), and also compute rhythm and temporal variability from it
    #extract peaks from velocity time series
    peaksnose       <- findpeaks(as.vector(scale(MT$velocity_nose)), minpeakdistance = 8, minpeakheight = -1, threshold=0.1)
      rhythmnose    <- abs(diff(MT$time_ms[peaksnose[,2]]))/1000 #compute interval in time between peaks
    #extract peaks from velocity time series
    peaksleft_w     <- findpeaks(as.vector(scale(MT$velocity_left_w)), minpeakdistance = 8, minpeakheight = -1, threshold=0.1)
      rhythmleft_w    <- abs(diff(MT$time_ms[peaksleft_w[,2]]))/1000 #compute interval in time between peaks
    #extract peaks from velocity time series
     peaksright_w    <- findpeaks(as.vector(scale(MT$velocity_right_w)), minpeakdistance = 8, minpeakheight = -1,threshold=0.1)
      rhythmright_w    <- abs(diff(MT$time_ms[peaksright_w[,2]]))/1000 #compute interval in time between peaks
    #extract peaks from velocity time series
    peaksleft_i     <- findpeaks(as.vector(scale(MT$velocity_left)), minpeakdistance = 8, minpeakheight = -1,threshold=0.1)
      rhythmleft_i    <- abs(diff(MT$time_ms[peaksleft_i[,2]]))/1000 #compute interval in time between peaks
    #extract peaks from velocity time series
    peaksright_i    <- findpeaks(as.vector(scale(MT$velocity_right)), minpeakdistance = 8, minpeakheight = -1,threshold=0.1)
      rhythmright_i    <- abs(diff(MT$time_ms[peaksright_i[,2]]))/1000 #compute interval in time between peaks

    #extract temporal variability from rhythm intervals
    rhythmicity <- NA
    rhythmicity <- c(sd(rhythmnose, na.rm=TRUE), sd(rhythmleft_w, na.rm = TRUE), sd(rhythmright_w, na.rm= TRUE), sd(rhythmleft_i, na.rm =TRUE), sd(rhythmright_i, na.rm = TRUE))
    rhythmicity <- mean(rhythmicity, na.rm= TRUE)
    if(is.nan(rhythmicity)){rhythmicity <- NA} #if there are no intervals to extract rhythm for set at NA (rather than nan)
    
    #compute average rhythm tempo
    rhythm <- NA
    rhythm <- c(rhythmnose, rhythmleft_w, rhythmright_w, rhythmleft_i,rhythmright_i)
    rhythm <- mean(rhythm, na.rm= TRUE)
    if(is.nan(rhythm)){rhythm <- NA} #if there are no intervals to extract rhythm for set at NA
    
    #compute total submovements from all keypoints
    submovements <- sum(c(nrow(peaksnose), nrow(peaksleft_w), nrow(peaksright_w), nrow(peaksleft_i),                  nrow(peaksright_i), na.rm=TRUE))
    
    #compute average intermittency (referred to here as smoothness) of all keypoints (use function smooth.get given above)
    smoothness <- mean(c(smooth.get(MT$velocity_nose), smooth.get(MT$velocity_left_w), smooth.get(MT$velocity_left_w),  smooth.get(MT$velocity_left), smooth.get(MT$velocity_right)))

    #compute average gesture space used by the gesture
    gspace <- mean(c( (max(MT$x_nose)-min(MT$x_nose))*(max(MT$y_nose)-min(MT$y_nose)),
                (max(MT$x_index_left)-min(MT$x_index_left))*(max(MT$y_index_left)-min(MT$y_index_left)),
                (max(MT$x_index_right)-min(MT$x_index_right))*(max(MT$y_index_right)-min(MT$y_index_right)),
                (max(MT$x_wrist_left)-min(MT$x_wrist_left))*(max(MT$y_wrist_left)-min(MT$y_wrist_left)),
                (max(MT$x_wrist_right)-min(MT$x_wrist_right))*(max(MT$y_wrist_right)-min(MT$y_wrist_right))))
    gspace <- gspace/1000
    
    #bind everything into a single output object containing all the relevent kinematic features
    features <- cbind(submovements, smoothness, gspace, rhythmicity,rhythm)
    return(features)
}

#network entropy mean
ent.N <- function(matri)
{ 
  entropym<- graph.adjacency(as.matrix(matri), mode="undirected", weighted=TRUE, diag = FALSE)
  entropy <- mean(igraph::diversity(entropym))
  return(entropy)
  }


#k-means equal size (credits to https://github.com/jmonlong/Hippocamplus/blob/master/content/post/2018-06-09-ClusterEqualSize.Rmd)
kmvar <- function(mat, clsize=10, method=c('random','maxd', 'mind', 'elki')){
  k = ceiling(nrow(mat)/clsize)
  km.o = kmeans(mat, k)
  labs = rep(NA, nrow(mat))
  centd = lapply(1:k, function(kk){
    euc = t(mat)-km.o$centers[kk,]
    sqrt(apply(euc, 2, function(x) sum(x^2)))
  })
  centd = matrix(unlist(centd), ncol=k)
  clsizes = rep(0, k)
  if(method[1]=='random'){
    ptord = sample.int(nrow(mat))
  } else if(method[1]=='elki'){
    ptord = order(apply(centd, 1, min) - apply(centd, 1, max))
  } else if(method[1]=='maxd'){
    ptord = order(-apply(centd, 1, max))
  } else if(method[1]=='mind'){
    ptord = order(apply(centd, 1, min))
  } else {
    stop('unknown method')
  }
  for(ii in ptord){
    bestcl = which.max(centd[ii,])
    labs[ii] = bestcl
    clsizes[bestcl] = clsizes[bestcl] + 1
    if(clsizes[bestcl] >= clsize){
      centd[,bestcl] = NA
    }
  }
  return(labs)
}

```

\begingroup
\setlength{\parindent}{0.5in}
\setlength{\leftskip}{0.5in}

  How do language scientists judge a communicative signal to be linguistic in nature? What properties must a communicative signal have? It can be contended that no *particular* property of a communicative signal is ever revealing about its systematic function, as the (linguistic) system as a whole needs to be taken into account to understand how this particular signal functions coherently. In this study we outline a systematic investigation of continuous communicative signals, and underline its value for the quantitative and objective linguistic study of such signals. Using communicative silent gesture as a test case, we show how continuous features of communicative signals (in this case manual kinematics) can be studied as systems at the level of the signals’ interrelationships.

Figure 1. Concepts to be conveyed in gesture in Motamedi et al. 2019  
```{r plotconcepts, echo = FALSE, warning = FALSE}
library(raster)
#load in the finally edited time series example
mypng <- stack(paste0(plotfolder, "/Concepts/concepts.png"))
plotRGB(mypng,maxpixels=1e500000)

```
  
  To understand the value of the current analysis, it helps to rehash what arbitrariness means in the linguistic sense [@dingemanseArbitrarinessIconicitySystematicity2015]. When a communicative signal is said to function arbitrarily it is not meant that the signal is randomly formed or unconstrained. Rather the signals' form is constrained by a higher order system. For example, it might be arbitrary whether verbs generally precede subjects or vice versa, but that these tokens stably enter into one or another relationship is important for understanding their linguistic function.  
  It is then tempting to conclude that the form of a communicative signal is not informative about linguistic constraints, and we may resort to the idea that a qualitatively different type of analysis that transcends form is needed for linguistic analysis. A paradigmatic example of this line of thought is the current mainstream to study manual gesture communication. Manual gestures are seldomly semiotically studied based on the signal’s measurable form, namely manual and whole-body postures in movement (i.e., the kinematic level). Instead gestures are studied as already categorizable expressions by researchers inferring meaning from their form. As such the kinematics are in one sense reduced (from continuous to discrete tokens) and another sense enriched (from movement to message) with meanings that are projected by human coders.  
  In the current approach we however show that we do not need to leave the domain of form to observe emergence of systematic properties. Our contribution therefore transcends methodological innovation, by calibrating theoretical intuitions about how form is informative about the linguistic content of communicative signals. We suggest that a signal's form, when appropriately studied at the level of interrelationships, is in potential as informative about the emergence of linguistic properties as content-based analysis. Form then directly embodies more abstract linguistic content, which emerges from a a sense-making process that shapes a signals form so as to relate in structural ways to other signals [@raczaszek-leonardiReconcilingSymbolicDynamic2008a; @daleHowHumansMake2018]. 
  This sense-making process is essentially simulated in iterated learning experiments [@kirbyIteratedLearningEvolution2014; @motamediEvolvingArtificialSign2019]. In such experiments, agents are tasked to learn a novel set of signals, iteratively transmitted to later generations and/or used in communication by later generations (iterated learning + communication). Over many cycles of learning and use, the signals are affected by various transmission biases [e.g., @christiansenNoworNeverBottleneckFundamental2016; @enfieldNaturalCausesLanguage2016]. Processes of iterated learning and communication can simulate how structural properties such as systematicity, learnability, and compositionality evolve from simpler communication systems — a process that must have occurred in human language evolution too (Bickerton, 2009). In such simulations communicative tokens undergo cultural evolution constrained by population dynamic properties such as historicity (the system is constrained by past contingencies) and adaptivity (the system is able to tweak itself in service of its informative goals). Such population dynamics must have played out over long temporal and vast population scales, but through these iterated learning paradigms such processes are to some limited degree brought under experimental control. These evolving or emerging communicative systems can be constituted by a variety of different signal media, from simple discrete symbol sequences to more challenging continuous signals (Cornish, Dale, Kirby, & Christiansen, 2017; Ravignani, Delgado, & Kirby, 2016; Verhoef, Kirby, & de Boer, 2016).
  
  
  
  
  
  These dimensions provided possible axes for compressibility of the communicative tokens. After all, by combining 10 unique gestures one can pick out any referent (e.g., “to make an arrest”) from the 24 token meaning space, one gesture marking the functional category (e.g., “action”) and another gesture for the theme category (“justice”). To exemplify further, once confronted with communicating 24 meanings one can invent 24 unique gesture utterances, such as in the following videos for "to sing" (https://osf.io/d8srx/) and "singer" (https://osf.io/974ke/), which is challenging to do since they are both very much related. However, one can also start differentiating by functional category such that "microphone" is preceded by a general object marking gesture ("https://osf.io/r3gcp/") and "singer" is preceded by a general person marking gesture ("https://osf.io/ex4tv/"), and then followed by the same thematic marking gesture conveying "music". Not only do these general functional markers aid the disambiguation of related meanings, this expressive invention also allows for a systematic reemployment of functional markings for the whole meaning space through compositionality. Once you invent 4 functional marker gestures, and 6 thematic marker gestures, you can systematically recombine these to convey 24 meanings. The communicative system then has compressed its information density from 24 information units to 10 information units.  
  Motamedi and colleagues (2019) indeed qualitatively observed such signs of compression of the meaning space as the system developed. In early iterations of learning, large-sized iconic enactments were the most common way of gesturally depicting the referents. However, the occurrences of functional markers increased over generations, which represented meaning components reused across gestures. This kind of functional marking mainly targeted the thematic and function categories.  
  With meticulous hand coding of the different referential components of each silent gesture, it could further be quantitatively tested whether there was indeed systematicity emerging. The gesture coding included information about form of a particular gesture segment, such as the number of manual articulators used (1 or 2 hands), as well as the referential target of the gesture (e.g., hat; pan; turn page). Based on the full sequences of the referential components that were uniquely expressed in each gesture, Shannon entropy was computed, which expresses compressibility of the content of a signal, i.e., the amount of information that is needed to compress the signal. When a lot of referential components in the gesture utterance recur between other gestures that the participant produced, the gesture system has a more compressible structure and indicates systematic reuse of gestural components [e.g., @gibsonHowEfficiencyShapes2019]. Dovetailing with the qualitative observations and other quantitative studies in this field [e.g., @verhoefIconicityEmergenceCombinatorial2016], it was found that gesture-component entropy decreased over the generations. Furthermore, the gestures were coded for the amount of marking for the functional category, and this showed that such gestures occurred more often at later generations. Finally, average gesture duration - as a measure of communicative efficiency - did not reliably change over the generations, which ran counter to predictions that more mature communication systems tend towards maximal efficiency [@gibsonHowEfficiencyShapes2019], and that gestures tend to simplify in form when repeatedly used [e.g., @gerwingLinguisticInfluencesGesture2004].  
  These results obtained in the lab resonate with findings from homesign [e.g., @havilandEmergingGrammarNouns2013] and emerging sign languages [@senghasChildrenCreatingCore2004]. For example, it has been shown that in the expression of motion events first generation signers of Nicaraguan sign language performed more holistic presentations of path and manner, while in following generations manner and path were segmented. Such segmentations affords novel combinatoriality and therefore increases generativity of a language. It expresses the meaning space with fewer means similar to how participants studied by @motamediEvolvingArtificialSign2019 started to compress the meaning space by developing ways mark functional status across referents (e.g., "agent", "action").  


# Current study
  Here we build on data from this recent iterated learning paradigm with silent gestures [@motamediEvolvingArtificialSign2019]. With computer vision [@caoRealtimeMultiPerson2D2017] we obtained motion traces of manual- and head gestures [see e.g., @lepicTakingMeaningHand2016; @ripperdaSpeedingDetectionNoniconic2020]. We then performed 'gesture network analysis' [@pouwGestureNetworksIntroducing2019], which is a procedure that combines bivariate time series analysis (Dynamic Time Warping) with network analysis and visualization. Through this gesture network analysis we show that the study of gesture's form can be revealing of linguistic constraints if we study the kinematic system as a whole, as proposed by the gesture network analysis approach.    
  
  We hypothesized the following. Over the iterations:  
  
1. gesture kinematics simplifies (token level).  
2. gesture kinematics relationships become more systematic, and this scales with (system level).  
3. changes in gesture kinematics are related to the systematicity of gesture (token-system level).  
4. idiosyncratic gesture cultures emerge as evidenced by drifts away from the community as a whole (inter-system level).  
  
Importantly, 1) is based on previous research showing how gestures simplify over repeated use [@namboodiripadMeasuringConventionalizationManual2016; @gerwingLinguisticInfluencesGesture2004], though at present we know of no quantitative kinematic analysis of the simplification of gesture systems as will be performed here. We will use three kinematic properties that revealing of simplication, gesture size, gesture's intermittency, and gesture rhythm. Whatever kinematic changes we observe, they are not in and of itself evidence for linguistic constraints. Only, 2) and 3) can be revealing of such constraints. Specifically, we will assess whether a Shannon-based Entropy measure computed on kinematic relationships show that the systems becomes more structured (i.e., compressible). By using a conceptually similar measure as has been applied on human coding [@motamediEvolvingArtificialSign2019], we can then also compare whether kinematics' entropy is related to the content's entropy. If so, we have a good evidence that linguistic constraints can be objectively studied from systematic changes in gesture form. If we further find that kinematic changes are related to order emerging on the system level, we show how the simplification of gesture kinematics is a co-constitutive (or an embodiment) of order on the system level. Finally, with 4), we show the true potential of a big data implementation to the gesture network approach, by assessing whether as a communicative gesture evolves within a chain of users, that community of users will drift away from other chains of users. In this way we can show how dialects emerge out of the unique historicity built within chains, leading chains to diverge on the whole. Order on the kinematic system level then, is order of an idiosyncratic kind.
  
\pagebreak

# Method
We will follow a bottom-up approach to the study of kinematics as communicative systems, by first showing what specific changes occur in the kinematics of the gestures. For the step 2, we asses possible systematic interrelationships in kinematic patterns of gestures through gesture network analysis [@pouwGestureNetworksIntroducing2019]. Figure 2 and 3 shows the general overview of the pre-processing steps for this experiment. We will discuss each step in this procedure in the following sections, and finally discuss our main gesture network measure (entropy). In the the supplemental information we reserve extra space for sanity checks and graphical descriptive results of the key measures.  

```{r construct_matrices_and_save, results = 'hide', cache= TRUE, eval = FALSE}
#NOTE this is the code that constructs individual level level gesture networks
#This code takes a little time (about 10 min) to run, and its output are all the distance matrices in the distance_matrices folder
#The code chuck constructs:
  #Individual level matrices: Which are 5chainsx5generationx2participants of size 24x24 (576 cells) (networktype = 1)
  #seed level matrices:        Which are 5chains x with 24x24 matrices (networktype = 0)
for(ch in c(c("chain1", "chain2", "chain3", "chain4", "chain5"))) #goes through all the chains
{
  for(gen in c(1:5))                                              #goes through all the generations
  {
   ts_sub <- subset(ts, chain == as.character(ch) & generation ==  as.character(gen)) #subset data for the curren generation and chain
   #add relevant seeds
   seed_sub <- subset(seeds,  seedsetnum %in% unique(ts_sub$seedsetnum))      #also collect for this chain the relevant seed videos used
   ####################CONSTRUCT INDIVIDUAL LEVEL NETWORKS
   for(p in unique(ts_sub$ppn))                                               #loop through participant
   {
     print(paste0("working on individual level network:", p))                 #print a progress statement to the console
     tsp <- subset(ts_sub, ppn == p)                                          #select only the data for this participant
     network_p <- matrix(nrow = length(unique(tsp$object)), ncol = length(unique(tsp$object))) # make a new matrix

       for(g1 in unique(tsp$object))
       {
          #get index information for this gesture 1
          indexa <- as.numeric(which(unique(tsp$object)==g1))

          for(g2 in unique(tsp$object))
          {
          #get index information for this gesture 2
            indexb <- as.numeric(which(unique(tsp$object)==g2))

            if(is.na(network_p[indexa, indexb])) #this statements makes sure that no computations are made unnessecarily
            {
            ts1 <- subset(tsp, object == g1)
            ts2 <- subset(tsp, object == g2)
            ts1 <- extractR.traces(ts1)
            ts2 <- extractR.traces(ts2)
            dist <- DTW.compare(ts1, ts2, "NA")
            
            #fill network
            network_p[indexa, indexb] <- dist
            network_p[indexb, indexa] <- dist #the matrix is symmetric so you can fill two cells Mij and Mji
            }
          }
       }
     #WRITE PARTICIPANT LEVEL NETWORK (priority)
          #FILENAMe: NETWORKTYPE_CHAIN_GEN_PPN
      colnames(network_p) <- as.character(unique(tsp$object))
      write.csv(network_p, paste0(matrices_data, "1_", ch, gen,p, ".csv"), row.names = FALSE)
    }
  }
}

#construct a generation level cross-chain matrix to see how drift towards the arbitrary emerges
  #these are a lot of computations, so we utilize pararallel processing for this script
numCores <- detectCores() #how many cores?

    #make a function which is the main operation for parallel computing
    dwarp.comb <- function(listcombs, traces, tsids)
    {
    listcombs <- str_split_fixed(listcombs, ",", 2)
    indexa <- tsids==listcombs[,1]
    indexb <- tsids==listcombs[,2]
    distances <- DTW.compare(traces[indexa,], traces[indexb,], "NA")
    return(distances)
    }

ts$id <- paste0(ts$object, ts$ppn)
for(gen in c(1:5))                                              #goes through all the generations
  {
  #get a list of all gestures
  traces <- extractR.traces(ts[ts$generation==gen,]) 
  tsids <- ts$id[ts$generation==gen]
  ids <-   unique(tsids)
  chains <- objects <- vector()
    for(i in ids)
    {
      chains <- c(chains, unique(ts$chain[ts$id==i]) )
      objects <- c(objects, unique(ts$object[ts$id==i]))
    }
  
  print(paste0("processing generation ", gen, " ---started at", Sys.time()))
        listcombs <- expand.grid(ids, ids) #make combination of all these gestures
        listcombs <- listcombs[1:(nrow(listcombs)/2),]
        listcombs <- paste(listcombs[,1],listcombs[,2], sep=",")
        distancest <- unlist(parallelsugar::mclapply(listcombs, traces, tsids, FUN = dwarp.comb, mc.cores = numCores))
  print(paste0("processed generation ", gen, " ---ended at", Sys.time()))
  
  
  fullmat <- matrix(ncol = length(ids), nrow = length(ids))
  fullmat[upper.tri(  fullmat, diag=FALSE)] <-  fullmat[lower.tri(  fullmat, diag=FALSE)] <- distancest[distancest!=0]
  colnames(fullmat) <- chains
  rownames(fullmat) <- objects
  
  write.csv(fullmat, paste0(matrices_data, "C_",gen,".csv"))
  }

```
```{r main method figure, results = 'hide', cache= TRUE, eval = FALSE}

exm <- ts[ts$generation=="s" & ts$seedsetnum == "arrest1",]

si = 3
ggplot(exm, aes(x=time_ms)) + geom_line(aes(y = x_nose),size= si, color = "red") + 
                              geom_line(aes(y = y_nose),size= si, color = "red") +
                              geom_line(aes(y = x_index_left),size= si, color = "purple")+
                              geom_line(aes(y = y_index_left),size= si, color = "purple")+
                              geom_line(aes(y = x_index_right),size= si, color = "green")+
                              geom_line(aes(y = y_index_right),size= si, color = "green")+
                              geom_line(aes(y = x_wrist_left),size= si, color = "blue")+
                              geom_line(aes(y = y_wrist_left),size= si, color = "blue")+
                              geom_line(aes(y = x_wrist_right),size= si, color = "cyan")+
                              geom_line(aes(y = y_wrist_right),size= si, color = "cyan")+theme_bw()



``` 
\pagebreak

### Participant, design, & procedure of the original study (experiment 1)  
  Here we discuss the setup of the experiment which generated the data we reanalyzed (for more detailed information see Motamedi et al., 2019).  
 A seed gesture set was created with 48 pre-study participants who each depicted 1 out of 24 concepts. Thus for each concept there were two seed gestures performed by unique pre-study participants. Given that pre-study participants only produced one gesture, they were isolated from the other concepts that comprised the meaning space.  
  For the main experiment (exp. 1) 50 right-handed English-speaking non-signing participants were recruited. They were allocated pairwise to one of 5 iteration chains. Participants were first shown a balanced subset of 24 unique seed gestures. These chain-specific seed gesture sets will be referred to as generation 0, which were followed by generations 1 through 5. In the training phase, gestures were presented in random order and participants were asked to identify the meaning of the gesture from the 24-item meaning spaces, followed by feedback about their performance. They were then asked to self-record their own copy the gesture. Participants trained with a subset of 18 items (out of 24), and completed two rounds of training.   
  In the testing phase, participants took turns as director and matcher to gesturally communicate (withou using speech) and interpret items in the meaning space, with feedback following each trial. This director-matcher routine was repeated until both participants communicated all 24 meanings. Subsequent generations were initiated with new dyads whose training set was the gestures from one randomly selected participant from the prior generation.  
  The recorded videos of the seed gestures and the gesture utterances participants produced in the testing phases are the data we use here. This means that we have 50 participants conveying 24 concepts = 1200 gesture videos belonging to generations 1-5, and 48 seed gesture videos with each concept conveyed by two unique seed participants.

### Motion tracking
  Motion tracking was performed on each video recording with a sampling rate of 30Hz. To extract movement traces, we used OpenPose [@caoRealtimeMultiPerson2D2017], which is a pre-trained deep neural network approach for estimating human poses from video data [for a tutorial see @pouwMaterialsTutorialGespin20192019]. We selected keypoints that were most likely to cover the gross variability in gestural utterances: positional x (horizontal) and y (vertical) movement traces belonging to left- and right index fingers, wrists, as well as the nose. For all position traces and its derivatives, we applied 1st order 30Hz low-pass Butterworth filter to smooth out high-frequency jitters having to do with sampling noise. We z-normalized and mean-centered position traces for each video to ensure that differences between subjects (e.g., body size) and within-subject differences in camera position at the start of the recording were inconsequential for our measurements.
  
Figure 2. Design experiment and openpose tracking
```{r plotmainmethodfigurea, echo = FALSE, warning = FALSE}
library(raster)
#load in the finally edited time series example
mypng <- stack(paste0(plotfolder, "/MethodPlot/main_method_v3a.png"))
plotRGB(mypng,maxpixels=1e500000)

```
\small *Note Figure 2*. The general procedure is shown for the current gesture network analysis. A) shows the original experiment setup (Motamedi et al., 2019), where a seed set of 24 gestures was randomly selected for each chain containing five generations. Seed gestures were used to train the first generation of each chain; subsequently, gestures from the previous generation were used as training data. Participants then communicated gesturally about the same concepts. B) For our analysis we first performed video-based motion tracking with OpenPose [@caoRealtimeMultiPerson2D2017] to extract relevant 2D movement traces ($T_{i}$) of the nose, the wrists and index fingers. After motion tracking the next steps were dynamic time warping and gesture network analysis (Figure 3).

## Kinematic Properties
```{r kinematic_calcs, echo = FALSE, message = FALSE, warning = FALSE}
#plot for checking algorithm
tstemp <- ts              #create a temporary copy of the time series data
tstemp$identifier <- paste0(tstemp$generation, tstemp$ppn, tstemp$chain, tstemp$object)  #make an identifier for each video

dimjerk <- peaks <- vector()
feats <- data.frame()
for(i in unique(tstemp$identifier)) #go through all time series and extract the kinematic features using the custom function
{ 
  cc <- tstemp[tstemp$identifier == i,] #also get the data from human codings (repetitions, infnormation units, and segments)
  get <- cbind(kin.get(cc), cc$ann_reps[1], cc$ann_inf_units[1], cc$segments[1])
  feats <- rbind.data.frame(feats, get)
}
colnames(feats) <- c(colnames(feats[1:5]), "repetitions", "inf_units", "segments")

feats$smoothness <- log(feats$smoothness)     #this measure tends to explode at high values, so we log scale them
feats$submovements <- log(feats$submovements) #thus measure tends to explode at high values, so we log scale them

#correlations to report (intermittency and rhythm measure)
cx <- cor.test(feats$smoothness, feats$rhythm)
cxt <- c(round(cx$estimate, 2),ifelse(cx$p.value < .001, "< .001", round(cx$p.value, 3)))

```

  We first selected five potential measures representative of kinematic quality of the movements in terms of segmentation, salience and temporality, namely submovements, intermittency, gesture space, rhythm, and temporal variability (or rhythmicity). See Figure 3 for two example time series from which most measures can be computed. All measures were computed for each keypoint’ time series seperately and then averaged so as to get an overall score for the multimodal utterance as a whole. Based on these exploratory measures we eventually selected three measures tracking gesture segmentation (intermittency score), gesture salience (gesture space), gesture’s temporality (temporal variability). Correlations and distributions are shown in Figure 7. 

### Gesture salience
  As a measure for gesture salience or reduction, we computed a gesture space measure. This was determined by extracting the maximum vertical amplitude of a keypoint multiplied by the maximum horizontal amplitude, i.e., the area in pixels that has been maximally covered by the movement.

### Gesture segmentation
  We first computed a submovement measurement similarly implemented by @trujilloMarkerlessAutomaticAnalysis2019. Submovements are computed with a basic peak finding function which identifies and counts maxima peaks in the movement speed time series. We set the minimum interpeak distance at 8 frames, and minimum height = -1 (z-scaled; 1 std.), minimum rise = 0.1 (z-scaled).  
  A property of the submovement measure is that it discretizes continuous information and uses arbitrary thresholds for what counts as a submovement, thereby risking information loss about subtle intermittencies in the movement. To have a more continuous measure of intermittency (the opposite of smoothness) of the movement we computed a dimensionless jerk measure [@hoganSensitivitySmoothnessMeasures2009]. This measure is dimensionless in the sense that it is scaled by the maximum observed movement speed and duration of the movement. Dimensionless jerk is computed using the following formula 
  
  $$\int_{t2}^{t1} x''' (t)^{2}dt)* \frac{D^{3}}{max(v^{2})}$$
  
  Here $x'''$ is jerk (second derivative of the speed), which is squared and integrated over time and multiplied by duration $D$ cubed over the maximum squared velocity $max(v^{2})$. As supplemental figure S2 shows, this measure correlates very highly with submovements, thus we chose to only use intermittency for further analysis. Note that a *higher* intermittency score indicates more intermittent (less smooth) movement. We logtransformed our smoothness measures due to skewed distributions.
  
### Gesture temporality
  From the submovement measure we computed the average interval between each submovement (in Hz), which is a measure of rhythm tempo. This measure was, as expected, highly correlated with intermittency score (see S2), as tempo goes up when more segmented movements are performed in the same time window, *r* = `r printnum(cxt[1])`, *p* = `r printnum(cxt[2])`, which led us to drop this measure for our analysis. Instead, we use another temporal measure that is more orthogonal to intermittency and gesture space, and which captures the stability of the rhythm, i.e., the temporal variability (the opposite of isochrony) of the movements. This measure is simply the standard deviation of the temporal interval between submovements (given in Hz): a higher score indicates more temporal variability and a lower score indicates more isochronous rhythm. Note, this measure cannot be calculated when there are less than 3 submovements (i.e., when there no intervals to detect the temporal variability of).


Figure 3. Overview kinematic measures
```{r plotjerk, echo = FALSE, warning= FALSE, fig.height=2.90, fig.width = 7}
#for method jerk plots
  #example 1
exm <- ts[ts$time_ms > 0 & ts$generation=="s" & ts$seedsetnum == "arrest2",] #extract sample from the data
exm$velocity_right_w <- as.vector(scale(exm$velocity_right_w)) #z-scale the speed vector

#extract peaks
peaksexm <- findpeaks(exm$velocity_right_w,minpeakdistance = 8,  minpeakheight = -1, threshold=0.1) #apply peakfinder function to the time series, with the same thresholds as our kin.get function given at the custom function section
exm$peak <- ifelse(exm$time_ms %in% exm$time_ms[peaksexm[,2]], exm$velocity_right_w, NA) #save the peak height into the time series by matching with the time
exm$peaktime <- ifelse(exm$time_ms %in% exm$time_ms[peaksexm[,2]], exm$time_ms, NA) #save the peak time into the time series by matching with the time

smoothness <- smooth.get(exm$velocity_right_w) #apply the custom function calculating smoothness
rhythm <- mean(abs(diff(exm$time_ms[peaksexm[,2]]))/1000) #extract time interval between peaks, and divide by 1000 ms to get Hz
rhythmicity <- sd(abs(diff(exm$time_ms[peaksexm[,2]]))/1000) #compute the st. dev. time interval between peaks, and divide by 1000 ms to get Hz


#left plot
a <- ggplot(exm) + geom_line(aes(x=time_ms, y = velocity_right_w)) +
  geom_point(aes(x = peaktime, y = peak), color = "red", size = 2) +
    annotate("text", label= paste0("submovements = ", nrow(peaksexm)), x=1750, y=2.15)+
  annotate("text", label = paste0("intermittency = ", round(log(smoothness)), round = 2), x=1750, y=1.80) +
  annotate("text", label = paste0("rhythm (Hz) = ", round(rhythm,2)), x=1750, y=1.60) +
    annotate("text", label = paste0("temporal var. = ", round(rhythmicity,2)), x=1750, y=1.35) +
  xlab("time (ms)") +
  ylab("speed right wrist (z-scaled)")+
  theme_bw()
 
#right plot (NOTE, this repeats what is done above)
exm5 <- ts[ts$time_ms > 0 & ts$generation=="5" & ts$chain == "chain5" & ts$seedsetnum == "arrest2" & ts$ppn == "full50",]
exm5$velocity_right_w <- as.vector(scale(exm5$velocity_right_w))

peaksexm5 <- findpeaks(as.vector(scale(exm5$velocity_right_w)),minpeakdistance = 8,  minpeakheight = -1, threshold=0.1)
exm5$peak <- ifelse(exm5$time_ms %in% exm5$time_ms[peaksexm5[,2]], exm5$velocity_right_w, NA)
exm5$peaktime <- ifelse(exm5$time_ms %in% exm5$time_ms[peaksexm5[,2]], exm5$time_ms, NA)

smoothness <- smooth.get(exm5$velocity_right_w)
rhythm <- mean(abs(diff(exm5$time_ms[peaksexm5[,2]]))/1000)
rhythmicity <- sd(abs(diff(exm5$time_ms[peaksexm5[,2]]))/1000)

b <- ggplot(exm5) + geom_line(aes(x=time_ms, y = velocity_right_w)) +
  geom_point(aes(x = peaktime, y = peak), color = "red", size = 2) +
    annotate("text", label= paste0("submovements = ", nrow(peaksexm5)), x=3000, y=2.15)+
  annotate("text", label = paste0("intermittency = ", round(log(smoothness)), round = 2), x=3000, y=1.80) +
  annotate("text", label = paste0("rhythm (Hz) = ", round(rhythm,2)), x=3000, y=1.60) +
    annotate("text", label = paste0("temporal var. = ", round(rhythmicity,2)), x=3000, y=1.35) +
  xlab("time (ms)") +
  ylab("speed right wrist (z-scaled)")+
  theme_bw()

grid.arrange(a,b, nrow=1)
```  
*Note Figure 3*. \small Two time series (belonging to two unique trials) are shown for right-hand wrist speed. From these time series, as well as the time series for other body parts, we computed  measures tracking segmentation, namely, submovements (number of observed peaks in red) and intermittency. We further computed measures concerning temporality, namely the average time between submovements, i.e., rhythm in Hertz. We also computed temporal variability, which is the standard deviation of the rhythm in Hertz. Gesture space was calculated from the x,y position traces and is not shown here.  \normalsize

\pagebreak  

#### Human coding and kinematic measures
  For information about how these automated kinematic measures approximate hand-coded data from Motamedi and colleagues (2019), see supplemental figure S2. The hand-coded data consisted of the amount of unique information units of the gesture utterance, the number of repetitions in the utterance, as well as the number of segments (information units + repetitions). We should predict that our kinematic intermittency score should correlate with the number of segments, repetitions and information units as the kinematics will have to carry those information units by contrasts in the trajectories. Supplemental Figure S2. shows the correlations for our kinematic measures and the human-coded gesture information. It shows that the amount of information units (unique, repeated or total) in the gesture as interpreted by a human coder are reliably correlating with kinematic intermittency (more intermittent more information), gesture space (larger space more information) and temporal variability (more stable rhythm more segments).

  
### Dynamic Time Warping (DTW)
  DTW is a common signal processing algorithm to quantify similarity between temporally ordered signals [@giorginoComputingVisualizingDynamic2009;  @mueenExtractingOptimalPerformance2016a; @mullerInformationRetrievalMusic2007]. The algorithm performs a matching procedure between two time series by maximally realigning (warping) nearest values in time while preserving order, and comparing their relative distances after this non-linear alignment procedure. The degree that the two time series need to be stretched and warped indicates how dissimilar they are. This dissimilarity is expressed with the DTW distance measure, with a higher distance score for more dissimilar time series and a lower score for more similar time series.  
  The time series in the current instance are multivariate, as we have a horizontal (x) and vertical (y) positional time-series data. However, DTW is easily generalizable to multivariate data, and can compute its distances in a multidimensional space if required, yielding a multivariate dependent variant of DTW. We opt for a dependent DTW procedure here as x an y positional data are part of a single position coordinate in space. Additionally, we have 6 of these 2-dimensional time series for each body keypoint. To compute a single distance measure between gestures, we computed for each gesture comparison a multivariate dependent DTW Distance measure per keypoint, which was then summed for all keypoint comparisons to obtain a single Distance measure D (illustrated in Figure 2C). The D measure thus reflects a general dissimilarity (higher D) or similarity (lower D) of the whole manual+head movement utterance versus another utterance.  
  We used the R package ‘DTW’ [@giorginoComputingVisualizingDynamic2009] to produce the multivariate distances per keypoint. The DTW distance measure was normalized for both time series’ length, such that average distances are expressed per unit time, rather than summing distances over time which would yield higher (and biased) distance estimates for longer time series (i.e., longer gesture videos). For further conceptual overview and methodological considerations of our DTW procedure see [@pouwGestureNetworksIntroducing2019].  
```{r compute_measure_check, echo = FALSE, results = 'hide', message = FALSE, warning = FALSE}
sub_ts <- subset(ts, as.character(generation) == "1")   #keep 1 generation gestures
seeds  <- subset(ts, as.character(generation) ==  "s")  #keep only seed gestures

chainnum <- D <- Dran <- Dman <- Dranman<- vector() #initialize vectors to be filled and combined into data set for statistical analysis
for(chs in unique(sub_ts$chain)) #loop through all the chains (note we are going to make smaller and smaller data sets to loop faster)
{
        print(paste0("working on chain: ", chs)) #print something to see progress

    sub_ts_temp <-  subset(sub_ts, chain == chs) #make a smaller data.frame for this chain
    for(pp in unique(sub_ts_temp$ppn))           #for this chain data loop through participants
    {
       sub_ts_temp2 <- subset(sub_ts_temp, as.character(ppn) == pp) #make a smaller data.frame for this chain, participant
       
        for(obj in unique(sub_ts_temp2$object))  #make a smaller data.frame for this chain, particpant, object
        {
        #true pair (make a comparison pair to DTW, with one current reference and the origin seed)
        tt1 <- subset(sub_ts_temp2, as.character(ppn) == pp & as.character(object) == obj) #reference data
        sobject <- as.character(unique(tt1$seedsetnum))                              #which seed video is was the origin of the current?
        tt2 <- subset(seeds, as.character(seedsetnum) ==  sobject)     #get the exact seed based on seed list and object
        
        #random paired (make a comparison pair to DTW, with one current reference and a random [and unrelated] origin seed)
        listotherobjects <- unique(sub_ts_temp2$object[sub_ts_temp2$object != obj & 
                                                        sub_ts_temp2$theme !=  unique(tt1$theme) &
                                                        sub_ts_temp2$functional != unique(tt1$functional)]) #subset objects which are not the same reference,theme,                                                                                      and function 
        pickranobject    <- sample(     listotherobjects , 1)                 #pick randomly an object from that list
        tt2r <- subset(seeds, as.character(object) == pickranobject)          #extract random test data
          
      #Extract relevant traces to be inputted for DTW
        MT1   <- extractR.traces(tt1)
        MT2   <- extractR.traces(tt2)
        MT2r  <- extractR.traces(tt2r)
        
      #perform DTW distance calc for actual and random pair
       DTWpair    <-  DTW.compare(MT1, MT2, "NA")
       DTWranpair <-  DTW.compare(MT1, MT2r, "NA")
          #also perform same comparisons for only manual movements (excluding head movements)
       DTWpair_man    <-  DTW.compare(MT1, MT2, "manual_only")
       DTWranpair_man <-  DTW.compare(MT1, MT2r, "manual_only")
  
      #collect into dataset for statistical analysis
      chainnum <- c(chainnum, chs)
      D        <- c(D, DTWpair) 
      Dran     <- c(Dran, DTWranpair)
      Dman     <- c(Dman,   DTWpair_man)
      Dranman  <- c(Dranman, DTWranpair_man)
    }
  }
}

#bind into a dataset for statistical analysis of measure accuracy
t <- cbind.data.frame(chainnum,  D)
t2 <- cbind.data.frame(chainnum,  Dran)
tm1 <- cbind.data.frame(chainnum,  Dman)
tm2 <- cbind.data.frame(chainnum,  Dranman)
t$pairtype <- tm1$pairtype <- "true pair"
t2$pairtype  <- tm2$pairtype <- "random pair"
colnames(t) <- colnames(t2) <- colnames(tm1) <-colnames(tm2) <- c("chain", "DTWdistance", "pair")

#compare Distances random-true pairs
mcheck <- rbind.data.frame(t,t2)
test <- t.test(mcheck$DTWdistance~mcheck$pair)
D  <- cohen.d(mcheck$DTWdistance, mcheck$pair)$estimate
diff1 <- mcheck$DTWdistance[mcheck$pair == "random pair"]-mcheck$DTWdistance[mcheck$pair == "true pair"]
#compare Distances random-true pairs for only the manual
mcheckman <- rbind.data.frame(tm1,tm2)
Dman  <- cohen.d(mcheckman$DTWdistance, mcheckman$pair)$estimate
diff2 <- mcheckman$DTWdistance[mcheck$pair == "random pair"]-mcheckman$DTWdistance[mcheck$pair == "true pair"]

#make a dataset that compares differences in head included or head excluded DTW distances
comb <- as.data.frame(c(diff1,diff2))
comb$inc <- c(rep("head included" , length(diff1)), rep("head excluded", length(diff2)))
test_maninc <- t.test(comb[,1]~comb$inc)

```
  As a demonstration that our D measure reflects actual differences in kinematics, we computed for each individual in each chain the difference between a gesture seed and the gesture that the individual produced to copy it, for generation 1. These “true pairs” must be maximally similar (lower D) as the individual produced their copied gesture short after first exposure in the training phase, which should lead to high faithfulness in reproduction. We contrast this with a false or random comparison of the same gesture in generation 1 with a gesture seed that was neither in the same functional nor thematic category. These false random pairs must be more dissimilar, and should produce higher DTW distances.
  Figure 3 shows the distributions of the distances observed. DTW distance distributions were reliably different, *t* (`r printnum(round(test$parameter, 2))`) = `r printnum(round(test$statistic, 2))`, *p* = `r printnum(ifelse(test$p.value < .001, "< .001", test$p.value))`, Cohen's *d* =  `r printnum(round(D, 2))`, for the true pair, *M* =`r printnum(mean(mcheck$DTWdistance[mcheck$pair =="true pair"]))`(*SD* = `r printnum(sd(mcheck$DTWdistance[mcheck$pair =="true pair"]))`), as compared to the random pair, *M* = `r printnum(mean(mcheck$DTWdistance[mcheck$pair =="random pair"]))`(*SD* = `r printnum(sd(mcheck$DTWdistance[mcheck$pair =="random pair"]))`).  
  Importantly, we also find that adding head movement trajectory to our D calculation significantly increases false-real pair discriminability as compared when we compute our D measure on only manual keypoints (left/right wrist and index fingers), change in Cohen's *d* = `r printnum(round(D-Dman, 2))`, change D real vs. false = `r printnum(mean(comb[,1][comb$inc == "head included"])-mean(comb[,1][comb$inc == "head excluded"]))`, *p* = `r printnum(ifelse(test_maninc$p.value < .001, "< .001", test_maninc$p.value))`. Therefore we conclude that in the current experiment the gesture utterances are also crucially defined by head movements as well. This is an interesting finding in and of itself, and demonstrates the multi-articulatory nature of silent gestures.

Figure 4. Density distributions of D for true pairs and random pairs  
```{r plot_distributioncheck, echo = FALSE, message = FALSE, warning = FALSE, fig.width=6, fig.height=3}
tm <- mean(mcheck$DTWdistance[mcheck$pair =="true pair"])
fm <- mean(mcheck$DTWdistance[mcheck$pair =="random pair"])

colors <- brewer.pal(n = 2, name = "Set1")
a <- ggplot(mcheck, aes(x = DTWdistance, color = pair, fill= pair)) + geom_density(size = 2, alpha= 0.2) + geom_vline(xintercept = fm, linetype = "dashed", color = colors[1], size = 2) + geom_vline(xintercept = tm, linetype = "dashed", color = colors[2], size = 2) +
  scale_colour_manual(values=colors)+
  theme_bw() +  theme(panel.grid.major = element_blank()) + xlab("DTW distance")
a
```

*Note Figure 4*. Density distributions of D are shown for the random versus real pairs. With D based on head-, wrist- and finger movement there is good discriminability between real versus falsely paired gestures, confirming that our approach is tracking gesture similarity well.

### Gesture networks
  We constructed for each participant (nested in generation and chain), as well as each seed gesture set (seed set belonging to that chain), a distance matrix **D**, containing the continuous D comparisons for each gesture $D_{i,j}$ produced by that participant with each other gesture produced by that participant, yielding a 24x24 distance matrix **D**. The diagonal contains zeros for gesture comparisons that are identical ($D_{i,j} = 0 | i=j$). These characteristics make **D** a weighted symmetric distance matrix.  
  For each distance matrix we can construct a visual representation of its topology by projecting the distance of gesture tokens on a 2d plane using a dimensionality reduction technique called "t-SNE", a variant of Stochastic Neighbor Embedding [@maatenVisualizingDataUsing2008]. These 2-d representations show locations of gesture nodes, with distances between gesture nodes approximating our D measure. Such 2D representations are imperfect approximations of the underlying multidimensional data and are only used as visual aids. The uncompressed distance matrices are used to calculate entropy and other measures. We refer to these measurements as ‘network properties’ as these measures are intuitively understood in network terms. For calculations of network entropy we use the R package ‘igraph’ [@csardiPackageIgraphNetwork2019], and for dimensionality reduction we use R package ‘tsne’ [@donaldsonTsneTDistributedStochastic2016].
  
  
Figure 5. General method gesture network analysis 
```{r plotmainmethodfigureb, echo = FALSE, warning = FALSE, fig.height=8}
library(raster)
#load in the finally edited time series example
mypng <- stack(paste0(plotfolder, "/MethodPlot/main_method_v3b.png"))
plotRGB(mypng,maxpixels=1e500000)

```
*Note* Figure 5. C) For each gesture comparison within a gesture set, the time series were then submitted in to a Dynamic Time Warping procedure where we computed for each body part a multivariate normalized distance measure, repeated for all body parts and summed, resulting in one overall distance measure D for each gesture comparison. D) All distance measures were saved into a matrix **D** containing all gesture comparisons $D_{i,j}$ within the comparison set, resulting in a 24x24 distance matrix. The distance matrix can be visualized as a fully connected weighted graph through multidimensional scaling, such that nodes indicate gesture utterances and the distance (or weight) between gesture nodes representing the 'D' measure, indicating dissimilarity. \normalsize 
  
## Gesture Network Properties
### Kinematic Network Entropy
 Entropy is a measure that quantifies the compressibility of data structures, and has been used to gauge the combinatorial structure of communicative tokens in the field of language evolution [e.g., @verhoefIconicityEmergenceCombinatorial2016; for theoretical grounding see @gibsonHowEfficiencyShapes2019]. In the original experiment, Motamedi and colleagues (2019) computed entropy from the gesture content codings, which captured recurrent information units between gestures. In our case, entropy quantifies the degree to which there are similar or more diverse edge lengths (i.e., similar/diverse levels of dissimilarity 'D'). If they are more similar, this means lower entropy reflecting that communicative tokens relate in more structural ways to each other. Thus it is important to emphasize here that network entropy gauges in our case how the kinematics interrelationships are compressible (more predictable), and this is conceptually similar as gauging the systematic recurrence of information units between the human judged gesture content.  
  The network entropy measure we used [see @eagleNetworkDiversityEconomic2010] is almost identical to a classic Shannon entropy calculation used in the original study to quantify the systematicity of the gesture's content [@motamediEvolvingArtificialSign2019], where $Entropy\; H(X) = -\sum p(X)\log  p(X)$. The only difference is that our measure is computed on the weights of the networks’ edges for each node relative to the shortest path to the other nodes (i.e., connections), and then normalized by the number of connections. In our case the measure quantifies the topological diversity of the gesture relationships, where a lower score indicates more similar relationships and a higher score indicates a more random set of relationships.
  Specifically, for each gesture node compute the diversity of kinematic distances to other gestures, using a scaled Shannon Entropy measure: 
  
   $$\; H(i) = -\sum_{j=1}^{k} p_{ij}\log  p_{ij}/log(k_i)$$
   
   Here, $k_i$ is the number of gesture connections for gesture $i$, and $p_{ij}$ is the proportial distance: 
   
   $$p_{ij} = D_{ij}/\sum_{j=1}^{k}D_{ij}$$
   
   Here $p_{ij}$ is the distance between gesture $i$ and gesture $j$ and the total distance involving gesture $i$.  
   Figure 4 shows a graphical example.
  
Figure 6. Example network entropy
```{r entropy_explanation, echo = FALSE, warning = FALSE, cache = TRUE, fig.width=3,  fig.length=6}
set.seed(12)
matrix_lowstruc <- matrixhigh_struc <- matrixhigh_function <- matrix(nrow = 24, ncol = 24)

plot.networktsne <- function(top, title,cluster1, ellipse)
{
  top <- cbind.data.frame(top, cluster1)
  colnames(top) <- c("Xpos", "Ypos", "grouping")
  pl <- ggplot(top, aes(x= Xpos, y = Ypos, color =grouping)) + geom_point(size= 2)
  if(ellipse == TRUE){pl <- pl + stat_ellipse(type = "t", level =0.60)}
  pl <- pl+  theme_void()  + theme(legend.position = "none")+ ggtitle(title) + scale_color_brewer(palette = "Set2")
  return(pl)
}

#fill some matrices with normally distributed high low distances and random

#low variance network
lowhidist <- matrix(runif(24*24, min = 10, max = 20), nrow = 24, ncol = 24)
lowhidist[1:4,1:4] <- rnorm(n = 16, mean = 5, sd = 1)
lowhidist[5:8, 5:8] <- rnorm(n = 16, mean = 5, sd = 1)
lowhidist[9:12, 9:12] <- rnorm(n = 16, mean = 5, sd = 1)
lowhidist[13:16,13:16] <- rnorm(n = 16, mean = 5, sd = 1)
lowhidist[17:20, 17:20] <- rnorm(n = 16, mean = 5, sd = 1)
lowhidist[21:24, 21:24] <- rnorm(n = 16, mean = 5, sd = 1)
cluster1 <- c(rep("theme 1", 4), rep("theme 2", 4), rep("theme 3", 4), rep("theme 4", 4), rep("theme 5", 4), rep("theme 6", 4))

#high variance network
SDhidist <- matrix(runif(24*24, min = 10, max = 20), nrow = 24, ncol = 24)
SDhidist[1:4,1:4] <- rnorm(n = 16, mean = 5, sd = 5)
SDhidist[5:8, 5:8] <- rnorm(n = 16, mean = 5, sd = 2)
SDhidist[9:12, 9:12] <- rnorm(n = 16, mean = 5, sd = 5)
SDhidist[13:16,13:16] <- rnorm(n = 16, mean = 5, sd = 2)
SDhidist[17:20, 17:20] <- rnorm(n = 16, mean = 5, sd = 5)
SDhidist[21:24, 21:24] <- rnorm(n = 16, mean = 5, sd = 2)


#random network
randist <- matrix(runif(24*24, min = 10, max = 20), nrow = 24, ncol = 24)

#get 2d projectsion using tsne
toplowhidist <- as.data.frame(tsne(as.dist(lowhidist), perplexity =12))
topmedhidist <- as.data.frame(tsne(as.dist(SDhidist), perplexity =12))
toprandist   <-  as.data.frame(tsne(as.dist(randist), perplexity =12))


#plot 2-d with entropy value
a <-  plot.networktsne(toplowhidist, paste0("lower entropy: ", round(ent.N(lowhidist),3)),cluster1, TRUE)
b <-  plot.networktsne(topmedhidist, paste0("medium entropy: ",  round(ent.N(SDhidist),3)), cluster1, TRUE)
c <-  plot.networktsne(toprandist, paste0("higher entropy: ",  round(ent.N(randist),3)),cluster1, TRUE)
  
grid.arrange(a, b,c, nrow = 3)


```
*Note*. 



# Main Results
We will first report findings on how relations between communicative tokens changed over the generations, as indicated by our network measures. We then validate whether network entropy approximates systematicity as observed by human coders. Subsequently, we will assess whether network changes occurred between particular tokens, namely the function vs. theme grouping. Finally, we will report on whether structural kinematic changes occurred over the generations for verb (action) and non-verb gestures (objects, persons, locations), and how such kinematic changes related to changes on the network level.  
  
## Network changes over generations
```{r network_evolution_main_findings, echo =FALSE, message = FALSE, warning = FALSE}
#initialize variables we want to collect from the matrix data
entropy1 <- chain <- generation <- kinch <- cluster_persistence <- p <- 
  submovements <- smoothness <- gspace <-   entropyWfunctions   <- entropyWthemes <- rhythmicity <- 
  ann_entropy <- vector()
for(ch in c("chain1", "chain2", "chain3", "chain4", "chain5"))
{
  for(gen in c(0:5))
  {
    #'participants' to loop through?
    if(gen == 0)
    {
    participants <- ""
    type = "0_"
    }
    if(gen != 0)
    {
    type = "1_"
    participants <- as.character(unique(ts$ppn[ts$chain == ch & ts$generation == gen]))
    }
    #DO network property collection
    for(ppn in participants) #loop through 'participants'
    {
    mat <- read.csv(paste0(matrices_data, type, ch, gen, ppn, ".csv"))      #fetch matrix
    
    #function/theme specific network properties to be collected
    funtypes <-  unique(ts$functional) #collect functional categories from time series data
    themtypes <-  unique(ts$theme)     #collect theme categories from time series data
    entropyWfun  <-entropyWtheme  <- vector()  #make some variable to be renewed after each iteration in the 'participant' loop
    
    
    theme_bindings <- unique(cbind(ts$object, ts$functional, ts$theme))
    concepts <- colnames(mat)
    entity <- themes <- vector(length = nrow(mat))
    for(i in 1:length(concepts))
    {
      entity[i] <- theme_bindings[theme_bindings[,1]==concepts[i],2] 
      themes[i] <- theme_bindings[theme_bindings[,1]==concepts[i],3] 
    }

    for(at in 1:4) #there are 4 themes and function types and we want to have specific network properties within each category
    {
      indexfun <- which(colnames(mat)%in%unique(ts$object[ts$functional==funtypes[at] ]) ) #collect indices in the matrix that match category
      fun_mat     <- mat[indexfun,  indexfun] #make a function specific submatrix
      indextheme <- which(colnames(mat)%in%unique(ts$object[ts$theme==themtypes[at] ]) ) #do the same for theme
      them_mat     <- mat[indextheme, indextheme] 
      
    #within theme/function entropy/distance
          #tempfunnet <- graph.adjacency(as.matrix(fun_mat), mode="undirected", weighted=TRUE, diag = FALSE)
          entropyWfun <- c(entropyWfun, ent.N(fun_mat)) #mean(igraph::diversity(tempfunnet)))

         #tempthemnet <- graph.adjacency(as.matrix(them_mat), mode="undirected", weighted=TRUE, diag = FALSE)
         entropyWtheme <- c(entropyWtheme, ent.N(fun_mat)) #mean(igraph::diversity(tempthemnet)))
     }
    entropyWfunctions   <- c(entropyWfunctions, mean(entropyWfun))
    entropyWthemes <- c(entropyWthemes, mean(entropyWtheme))
    
    #######################GLOBAL network properties to be collected
    #compute entropy
    #tempnet <- graph.adjacency(as.matrix(mat), mode="undirected", weighted=TRUE, diag = FALSE)
    entropy1 <- c(entropy1, ent.N(mat))
    chain <- c(chain, ch)
    generation <- c(generation, gen)
    #average component persistence (TDA)
    phom.dat <-   as.data.frame(calculate_homology(as.dist(mat), format = "distmat")) #perform PH
    phom.dat <-    phom.dat[phom.dat$dimension==0,] #keep only 0-cycles (components)
    phom.dat$persistence <-   phom.dat[,3]-phom.dat[,2] #compute persistence from time death minus time birth
    tresh <- id_significant(phom.dat, dim = 0, cutoff = 0.975) #calculate the threshold for components to be outliving noise
    persistence <- mean(phom.dat$persistence[phom.dat$persistence > tresh],na.rm=TRUE) #what is the average persistence of persistences that reached above noise persistence?
    cluster_persistence <- c(cluster_persistence, persistence) #save persistence result
    
    p <- c(p, ppn) #record participant number and save in variable p
    
    #ADD kinematic analysis to network property dataset, to be compared to network properties later
    #get time series to regress for kinematic analysis
    if(gen != 0) #compute kinematics
    {
    kinematics<- data.frame() 
    kin <- ts[ts$ppn == ppn,]
      for(o in unique(kin$object)) #collect for each object/video network properties and save them in kinematics
      {
      subkin <- kin[kin$object == o,]
      kinematics <- rbind(kinematics, kin.get(subkin))
      }
    }
    if(gen == 0) #also do this for seed level networks (which are chain specific)
    {
      kinematics<- data.frame()
      kin <- subset(seeds, seedsetnum %in% ts$seedsetnum[ts$chain == ch])
      for(o in unique(kin$object))
      {
      subkin <- kin[kin$object == o,]
      kinematics <- rbind(kinematics, kin.get(subkin))
      }
      
    }
    #save network averaged kinematic info
    submovements <- c(submovements, mean(kinematics[,1], na.rm=TRUE))
    smoothness   <- c(smoothness, mean(kinematics[,2], na.rm=TRUE))
    gspace       <- c(gspace, mean(kinematics[,3], na.rm=TRUE))
    rhythmicity  <- c(rhythmicity, mean(kinematics[,4], na.rm = TRUE))
      #add Motamedi human coded entropy values to be compared later to global network entropy
    at <- unique(kin$ann_entropy)
    at <- at[!is.na(at)]
    ann_entropy <- c(ann_entropy, ifelse(!is.null(at), at, NA) )
    }
  }
}
#bind variables in one big data.frame 'ned'
ned <- cbind.data.frame(chain, generation, entropy1, cluster_persistence, p, submovements, gspace, smoothness, rhythmicity,  entropyWthemes, entropyWfunctions, ann_entropy)
```
  Figure 9 shows that for the gesture networks, that entropy was generally decreasing as a function of generation, indicating lower complexity of gesture interrelations as the system matures. Furthermore, there was less clustering at later generations (lower cluster persistence), indicating that kinematic patterns became more differientiable.

\pagebreak
```{r plot_results_individual_networks, warning = FALSE, message = FALSE, results = 'hide',fig.show='hide'}
#figure 9 plot code

#main plot
a <- ggplot(ned, aes(x= generation, y = entropy1, color = chain)) + geom_point(size = 3, alpha = 0.8) + facet_grid(.~chain) + geom_smooth(method= "lm", color = "black", alpha = 0.3) + theme_bw()+ ggtitle("entropy") + ylab("entropy")+ scale_color_brewer(palette="Set1")+ theme(strip.background =element_rect(fill="white"),legend.title = element_blank(), axis.title.x=element_blank())+ theme(legend.position = "none")

b <- ggplot(ned, aes(x= generation, y = entropyWthemes, color = chain)) + geom_point(size = 3, alpha = 0.8) + facet_grid(.~chain) + geom_smooth(method= "lm", color = "black", alpha = 0.3) + theme_bw()+ ggtitle("local entropy themes") + ylab("local entropy")+ scale_color_brewer(palette="Set1")+ theme(strip.background =element_rect(fill="white"),legend.title = element_blank(), axis.title.x=element_blank())+ theme(legend.position = "none")

c <- ggplot(ned, aes(x= generation, y = entropyWfunctions, color = chain)) + geom_point(size = 3, alpha = 0.8) + facet_grid(.~chain) + geom_smooth(method= "lm", color = "black", alpha = 0.3) + theme_bw()+ ggtitle("local entropy functions") + ylab("local entropy")+ scale_color_brewer(palette="Set1")+ theme(strip.background =element_rect(fill="white"),legend.title = element_blank(), axis.title.x=element_blank())+ theme(legend.position = "none")




#agglomerative clustering coefficient

#c <- ggplot(ned, aes(x= generation, y = cluster_persistence, color = col)) + geom_point(size = 3, alpha = 0.8) + facet_grid(.~chain) + geom_smooth(method= "lm", color= "orange", alpha = 0.3) + ggtitle("cluster persistence") + theme_bw() + ylab("cluster persistence")+ theme(legend.position = "none")+
#  scale_color_brewer(palette="Set1")+ theme(strip.background =element_rect(fill="white"),legend.title = element_blank())


#plot two networks with multidimensional scaling
mat0 <-as.matrix(read.csv(paste0(matrices_data, "0_", "chain5", 0, ".csv")))
mat1 <-as.matrix(read.csv(paste0(matrices_data, "1_", "chain5", 1, "full41.csv")))
mat2 <-as.matrix(read.csv(paste0(matrices_data, "1_", "chain5", 2, "full43.csv")))
mat3 <-as.matrix(read.csv(paste0(matrices_data, "1_", "chain5", 3, "full46.csv")))
mat4 <- as.matrix(read.csv(paste0(matrices_data, "1_", "chain5", 4, "full48", ".csv")))
mat5 <- as.matrix(read.csv(paste0(matrices_data, "1_", "chain5", 5, "full50", ".csv")))


library(MASS)
library(ggnetwork)
library(network)
library(sna)

theme_bindings <- unique(cbind(ts$object, ts$functional, ts$theme))
concepts <- colnames(mat1)
entity <- themes <- vector(length = 24)
for(i in 1:24)
{
  entity[i] <- theme_bindings[theme_bindings[,1]==concepts[i],2] 
  themes[i] <- theme_bindings[theme_bindings[,1]==concepts[i],3] 
}

plot.networktsne <- function(matri)
{


  top <- tsne(as.dist(matri),  perplexity = 6) #isomap(as.dist(matri),k = 3, ndim=2)$points #tsne(as.dist(matri),  perplexity = 4, epoch=50) #
  
  
  df <- cbind.data.frame(concepts, top, entity, themes)
  colnames(df) <- c("concepts", "Xpos", "Ypos", "entity", "themes")
  pl <- ggplot(df, aes(x= Xpos, y = Ypos,xend = 0, yend = 0)) + geom_point(aes(shape = entity), size= 2) + geom_edges(aes(color = themes)) +  theme(legend.position = "none") + theme_void() + xlim(-1700, 1700) + ylim(-1700, 1700)
  return(pl)
}

  #l2 <- layout_with_mds(g_s2, dist =mat2 ) #multidimensional scaling (so that layout reflects distance)
   l0 <- plot.networktsne(mat0)
   l1 <- plot.networktsne(mat1)
   l2 <- plot.networktsne(mat2)
   l3 <- plot.networktsne(mat3)
   l4 <- plot.networktsne(mat4)
   l5 <- plot.networktsne(mat5)

  one <- grid.arrange( l0,  l1,  l2,  l3,  l4,  l5,  nrow = 3)

  
#check correlation entropy
ci <- cor.test(ned$ann_entropy[ned$generation!=0], ned$entropy1[ned$generation!=0])
cit <- paste0("r = ", round(ci$estimate, 2),  ", p = ", ifelse(ci$p.value < .001, "< .001", round(ci$p.value, 3)))

d <- ggplot(ned[ned$generation!=0,], aes(x = ann_entropy, y = entropy1, color = chain)) + geom_point(size = 1.5) + geom_smooth(method = "lm", color = "black", alpha = 0.2) + 
  ylab("kinematic network entropy") + xlab("discrete entropy \n (based on human coding)" ) + theme_bw() +scale_colour_brewer(palette = "Set1")+
   annotate("text", label= cit, x=5.6, y=0.998, size = 5, color = "black")


```
Figure 9. Changes in networks measures over generations within chains 
```{r plotresults, echo = FALSE, fig.align= 'center'}
  grid.arrange(a, b,c, d, ncol = 2)

```
 *Note Figure 9.* \small For each chain the changes over generations in entropy and cluster persistence is shown, with generation 0 indicating the seed gesture set. For each generation > 0 there are two data points as there are two participants in each generation. Two example data points (red, and blue) are shown with their corresponding red and blue network representation (lower panel). In general cluster persistence decreased, indicating less differentiability between tokens. This may be seen in example A where there are relatively large cavities between tokens, while in example B the token organization is more homegonously tessellated. Indeed, entropy tends to decline over the generations, indicating that relationships between tokens became less diverse, possibly indicating systematicity in the way nodes are connected. The right lower panel shows that there there is a strong relationship between the gesture network entropy with that of entropy computed on human-coded information units. That network entropy is uniquely related to systematicity is further corroborated by the finding that cluster persistence is not reliably correlated with entropy based on human coding. It does seem that gesture network analysis is a form-based proxy for systematicity in silent gesture. \normalsize

```{r test differences, echo = FALSE, message = FALSE, warning = FALSE}
   basem1 <- lme(entropy1~1, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   model1 <- lme(entropy1~generation, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   h0Entr <- anova(basem1, model1)
   h0EntrP <- summary(model1)
   h0EntrR <- r2beta(model1, method='sgv')
   h0EntrD <- lme.dscore(model1,ned, type = "nlme")
```
  We tested these trends separately for each network property with mixed linear regression models, with chain as random intercept (random slopes did not converge for these models) and generation as independent predictor (0-5 generations, with generation 0 being the seed gesture network).  
  Generation was a reliable predictor for entropy as compared to a basemodel predicting the overall mean, chi-squared change (1) = `r printnum(round(h0Entr$L.Ratio[2],3) )`, *p* = `r printnum( round(h0Entr$'p-value'[2],3) )`, model *R*-squared =  `r printnum( round(h0EntrR$Rsq[2],2) )`. Model estimates showed that with increased generation the entropy decreased, *b* estimate =  `r printnum( h0EntrP$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( h0EntrP$tTable[2,3] )` ) = `r printnum( round(h0EntrP$tTable[2,4], 2) )`, *p* = `r printnum( round(h0EntrP$tTable[2,5], 3) )`, Cohen's *d* = `r printnum( round( h0EntrD$d, 3) )`).  
  We can further ask whether it is the case whether our network entropy measure is approximating the entropy of hand-coded gestures. Figure 10 confirms that this is indeed the case, such that entropy increase based on human-coded information units is related to increase in entropy based on gesture network entropy.


Figure 11. Change in entropy in theme-level networks versus function-grouped networks
```{r furthereentropy, message = FALSE, warning = FALSE, echo = FALSE}

a <- ggplot(ned, aes(x= generation, y = entropyWfunctions, color = chain)) + geom_point() + geom_smooth(method= "lm", size=1, alpha=0.02)+ geom_smooth(method= "lm", color = "red") + theme_bw()+ ggtitle("function-level entropy ") + theme(legend.position = "none")+ scale_color_brewer(palette = "Dark2")+ ylab("entropy")
b <- ggplot(ned, aes(x= generation, y = entropyWthemes, color = chain)) + geom_point()+ geom_smooth(method= "lm", size=1, alpha=0.02) + geom_smooth(method= "lm", color = "red") + ggtitle("theme-level entropy") +theme_bw()+ theme(legend.position = "none") + scale_color_brewer(palette = "Dark2")+ ylab("entropy")

grid.arrange(a,b, nrow =1)


   #model functon category (mixed regression modeling)
   basem1F <- lme(entropyWfunctions~1, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   model1F <- lme(entropyWfunctions~generation, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   comp1F <- anova(basem1F, model1F)
   sum1F <- summary(model1F)
   RSQ1F <-  r2beta(model1F, method='sgv')
   model1FD <- lme.dscore(model1F,ned, type = "nlme")
   
   #model theme-level (mixed regression modeling)
   basem1T <- lme(entropyWthemes~1, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   model1T <- lme(entropyWthemes~generation, data = ned, random = ~1|chain, method = "ML", na.action = na.exclude)
   comp1T <- anova(basem1T, model1T)
   sum1T <- summary(model1T)
   RSQ1T <-  r2beta(model1T, method='sgv')
   model1TD <- lme.dscore(model1T,ned, type = "nlme")

```
*Note Figure 11*. \small On the left panel, the average network entropy for the function-grouped gestures are plotted over the generations with red line showing the trend averaged over chain (other-colored lines). On the right panel this is shown for the gestures grouped by theme category. It can be seen that only the function-grouped gesture networks showed increased systematicity (lower entropy) over the generations.\normalsize

 We find that only functionally grouped tokens were minimizing entropy over the generations. Including generations for predicting function-level network entropy increased predictability as compared to a base model (random intercept chain, random slopes did not converge), chi-squared change (1) = `r printnum(round(comp1F$L.Ratio[2],3) )`, *p* = `r printnum( round(comp1F$'p-value'[2],3) )`, model *R*-squared =  `r printnum( round(RSQ1F$Rsq[2],2) )`, with generation relating to lower entropy  *b* estimate =  `r printnum( sum1F$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( sum1F$tTable[2,3] )` ) = `r printnum( round(sum1F$tTable[2,4], 2) )`, *p* = `r printnum( ifelse(sum1F$tTable[2,5]<.001 ,"p < .001",round(sum1F$tTable[2,5], 4) ))`, Cohen's *d* = `r printnum( round( model1FD$d, 3) )`).  
 There was however no reliable decrease in entropy for the theme-level networks, chi-squared change (1) = `r printnum(round(comp1T$L.Ratio[2],3) )`, *p* = `r printnum( round(comp1T$'p-value'[2],3) )`, model *R*-squared =  `r printnum( round(RSQ1T$Rsq[2],2) )`.

## Kinematic features
  Next we performed mixed regression analysis for assessing potential kinematic changes as a function of generation, with random intercept for objects nested within chains (random slopes did not converge). See figure 12 for main results.
  
Figure 12. Change in kinematic properties over generations  
```{r kinematicresults, message = FALSE, warning = FALSE, echo = FALSE,fig.align='center', fig.height=4, fig.width =5}
#retrieve and plot kinematic properties over generations
tstemp <- ts #copy temporary time series data
tstemp$identifier <- paste0(tstemp$generation, tstemp$ppn, tstemp$chain, tstemp$object)  #make unique identifiers for each video
seedsTS <- tstemp[tstemp$generation=="s",] #extract a separate seed time series

#initialize variables to be collected, and data.frame to collect variables in
chains <- generations <-objects <- verb <- vector()
featsdata <- data.frame()
for(ch in c("chain1", "chain2", "chain3", "chain4","chain5"))
{ 
  cc <- as.data.frame(tstemp[tstemp$chain == ch,]) #collect time series of chain 'ch'
  for(id in unique(cc$identifier)) #go through all gestures in this chain and get relevant kinematic info
  {
  ccsub <- cc[cc$identifier==id,]
  featsdata <- rbind.data.frame(featsdata, kin.get(ccsub)) #extract kinematic features for this gesture
  objects <- c(objects, as.character(ccsub$object[1]))     #save object of depiction 
  verb <- c(verb, ccsub$ann_verb[1])                     #verb info is it a verb yes no?
  generations <- c(generations, ccsub$generation[1])    
  chains <- c(chains,ch)
  }
  
  #add for this the chain the relevant seed gesture information
  seed_sub <- unique(cc$seedsetnum) #also collect for this chain the relevant seed videos used
  #add seeds
  for(seed_g in seed_sub)
  {
    ccsub <- seedsTS[seedsTS$seedsetnum==seed_g,]
    featsdata <- rbind.data.frame(featsdata, kin.get(ccsub))
    objects <- c(objects, as.character(ccsub$object[1]))
    verb <- c(verb, ccsub$ann_verb[1])
    generations <- c(generations, 0)
    chains <- c(chains, ch )
  }
}

kin_data <- cbind.data.frame(featsdata, chains, objects,verb, generations)
kin_data$verb <- as.character(kin_data$verb)
kin_data$verb <- ifelse(kin_data$verb == "N", "no verb", "verb")

 #smoothness (mixed regression modeling)
  
  kin_data$generations <- as.numeric(kin_data$generations)
   basem1 <- lme(log(smoothness)~1, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   model1 <- lme(log(smoothness)~generations, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   sm_comp <- anova(basem1, model1)
   sm_sum <- summary(model1)
   sm_r <- r2beta(model1, method='sgv')
    #add verb (mixed regression modeling)
      model1v <- lme(log(smoothness)~generations+verb, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
     sm_compv <- anova(model1, model1v)
     sm_sumv <- summary(model1v)
     sm_rv <- r2beta(model1v, method='sgv')
     sm_Dv <-  lme.dscore(model1v,kin_data, type = "nlme")

  #rhythmicity (mixed regression modeling)
   basem1 <- lme(rhythmicity~1, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   model1 <- lme(rhythmicity~generations, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   rh_comp <- anova(basem1, model1)
   rh_sum <- summary(model1)
   rh_r <- r2beta(model1, method='sgv')
   rh_D <-  lme.dscore(model1,kin_data, type = "nlme")

    #add verb, not reliable
     #model1v <- lme(rhythmicity~generations*verb, data = kin_data, random = ~1|chains/objects, method = "ML", na.action =            #na.exclude)
     #rh_compv <- anova(model1, model1v)

#average gesture space (mixed regression modeling)
   basem1 <- lme(gspace~1, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   model1 <- lme(gspace~generations, data = kin_data, random = ~1|chains/objects, method = "ML", na.action = na.exclude)
   gs_comp <- anova(basem1, model1)
   gs_sum <- summary(model1)
   gs_r <- r2beta(model1, method='sgv')
   gs_D <-  lme.dscore(model1,kin_data, type = "nlme")

#main plot
library(ggbeeswarm)
a <- ggplot(kin_data, aes(x= generations, y = log(smoothness), color = verb)) + geom_quasirandom(aes(group = generations),size=0.5, alpha= 0.75) + geom_smooth(method= "lm", alpha=0.1, size = 2) + theme_bw() + xlab("generation") + scale_color_brewer(palette = "Dark2")+ theme(legend.position = "none")+ylab("intermittency")+ggtitle("intermittency")+
  scale_x_continuous(limits=c(0, 5)) 

b <- ggplot(kin_data, aes(x= generations, y = rhythmicity, color = verb))+ geom_quasirandom(aes(group = generations),size=0.5, alpha= 0.75)+ geom_smooth(method= "lm", alpha=0.1, size = 2)+ theme_bw() + xlab("generation")+ylab("temporal var.") + ggtitle("temporal var.")+ theme(legend.position = "none")+ scale_color_brewer(palette = "Dark2")+
   scale_x_continuous(limits=c(0, 5))

c <- ggplot(kin_data, aes(x= generations, y = gspace, color = verb)) + geom_quasirandom(aes(group = generations),size=0.75, alpha= 0.5)+ geom_smooth(method= "lm", alpha=0.1, size= 2) + theme_bw() + xlab("generation") +ylab("gesture space")+ ggtitle("gesture space")+ theme(legend.position = "bottom")+ scale_color_brewer(palette = "Dark2")+
   scale_x_continuous(limits=c(0, 5))

grid.arrange(a,b,c, nrow= 1)

```
*Note Figure 12*. \small Generation trends per chain are shown for intermittency, temporal variability and gesture space. Each observation indicates a communicative token, and these are spatially organized per their density distribution and colored by verb (green) or no verb (orange) (i.e., 'action' versus other function gestures). We can see that over the generations, movements become more smooth (lower intermittency score), with a more stable rhythm (lower temporal variability), and more minimized movements (smaller gesture space). Note, that temporal variability has lower data points as often the movement did not consist of more than 2 submovements. Thus, temporal variability indicates that *when there is a multi-segmented movement*, then such movements were more rhythmic.  \normalsize

  Generations reliably predicted intermittency of the movements relative to a basemodel, chi-squared change (1) = `r printnum( round(sm_comp$L.Ratio[2],3) )`, *p* `r printnum( ifelse(sm_comp$'p-value'[2] <.001, " <.001"  ,round(sm_comp$'p-value'[2],3) <.005) )`, model *R*-squared =  `r printnum( round(sm_r$Rsq[2],2) )`. When adding verb as another predictor, this improved model fit for intermittency, chi-squared change (1) = `r printnum( round(sm_compv$L.Ratio[2],3) )`, *p* = `r printnum( ifelse(sm_compv$'p-value'[2] <.001, " <.001", round(sm_compv$'p-value'[2],3)) )`, model *R*-squared =  `r printnum( round(sm_rv$Rsq[2],2) )`. In this final model generation predicted lower intermittency score, *b* estimate =  `r printnum( sm_sumv$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( sm_sumv$tTable[2,3] )` ) = `r printnum( round(sm_sumv$tTable[2,4], 2) )`, *p* `r printnum( ifelse(sm_sumv$tTable[2,5]<.001 ," < .001", round(sm_sumv$tTable[2,5], 4) ))`, Cohen's *d* = `r printnum( round(sm_Dv$d[1], 2) )`). Silent gestures conveying verbs showed lower intermittency in general, *b* estimate =  `r printnum( sm_sumv$coefficients$fixed[3], digits =4 ) `, *t* ( `r printnum( sm_sumv$tTable[3,3] )` ) = `r printnum( round(sm_sumv$tTable[3,4], 2) )`, *p* `r printnum( ifelse(sm_sumv$tTable[3,5]<.001 ," < .001", round(sm_sumv$tTable[3,5], 4) ))`, Cohen's *d* = `r printnum( round(sm_Dv$d[2], 2) )`). There were no interaction effects of generation and verb.  
  We also observe lower temporal variability as a function of generations, chi-squared change (1) = `r printnum( round(rh_comp$L.Ratio[2],3) )`, *p* `r printnum( ifelse(rh_comp$'p-value'[2] <.001, " <.001"  ,round(rh_comp$'p-value'[2],3) <.005) )`, model *R*-squared =  `r printnum( round(rh_r$Rsq[2],2) )`, indicating more stable rhythmic movements at later generations, *b* estimate =  `r printnum( rh_sum$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( rh_sum$tTable[2,3] )` ) = `r printnum( round(rh_sum$tTable[2,4], 2) )`, *p* `r printnum( ifelse(rh_sum$tTable[2,5]<.001 ," < .001", round(rh_sum$tTable[2,5], 4) ))`, Cohen's *d* = `r printnum( round(rh_D$d[1], 2) )`)). Adding verb or verb x generation to model temporal variability did not improve model fit.
  Finally, over the generations gesture space decreased, chi-squared change (1) = `r printnum( round(gs_comp$L.Ratio[2],3) )`, *p* `r printnum( ifelse(gs_comp$'p-value'[2] <.001, " <.001"  ,round(gs_comp$'p-value'[2],3) <.005) )`, model *R*-squared =  `r printnum( round(gs_r$Rsq[2],2) )`. Model estimated gesture space was less for later generations, *b* estimate =  `r printnum( gs_sum$coefficients$fixed[2], digits =4 ) `, *t* ( `r printnum( gs_sum$tTable[2,3] )` ) = `r printnum( round(gs_sum$tTable[2,4], 2) )`, *p* `r printnum( ifelse(gs_sum$tTable[2,5]<.001 ," < .001", round(gs_sum$tTable[2,5], 4) ))`, Cohen's *d* = `r printnum( round(gs_D$d[1], 2) )`). Adding verb or verb x generation to model gesture space did not improve model fit. 
  In conclusion, our kinematic results show all the hallmarks of increased communicative efficiency. Namely, gestures were on average smaller, less temporally variable, and less intermittent as the communicative system matured. Silent gestures that conveyed a verb were generally less intermittent, suggesting that they consist of smoother movement patterns.

## Relations between Kinematic and network properties
  Figure 11 contains the correlations of the relationships of kinematic properties (average per participant) and the network measures cluster persistence and entropy. Network entropy goes down as the average gesture space decreases, and the movement becomes less intermittent. This also comes at a trade-off, such that this simplification of kinematics also reduces differientiability of communicative tokens as shown by less stable clustering when gesture become smaller, less temporally variable, and less intermittent. Thus on the kinematic level there seems to be a general decrease of complexity which is further reflected on the level of the system as a whole as utterances become less *kinematically* differientable (less clustering) and more structured in their relations (lower entropy).

\pagebreak
Figure 13. Relation between kinematic properties and network measures
```{r test_network_kinematics, echo = FALSE, message = FALSE, warning = FALSE, fig.height =5, fig.width=6}
sizetext <-  3
#individual level
sh <- cor.test(log(ned$smoothness), ned$entropy1)
sht <- paste0("r = ", round(sh$estimate, 2),  ", p = ", ifelse(sh$p.value < .001, "< .001", round(sh$p.value, 3)))
a <- ggplot(ned, aes(x = log(smoothness), y=entropy1)) + geom_point() + geom_smooth(method = "lm", color = "indianred1")+ theme_bw()+ggtitle("intermittency")+xlab("intermittency")+ylab("entropy")+
   annotate("text", label= sht, x=7, y=1.0, size = sizetext)

rh <- cor.test(ned$rhythmicity, ned$entropy1)
rht <- paste0("r = ", round(rh$estimate, 2),  ", p = ", ifelse(rh$p.value < .001, "< .001", round(rh$p.value, 3)))
b <- ggplot(ned, aes(x = rhythmicity, y=entropy1)) + geom_point() + geom_smooth(method = "lm", color= "black") + theme_bw()+ ggtitle("temporal var.")+xlab("temporal var.") +ylab("entropy")+
   annotate("text", label= rht, x=0.75, y=1.0, size = sizetext)

gh <- cor.test(ned$gspace, ned$entropy1)
ght <- paste0("r = ", round(gh$estimate, 2),  ", p = ", ifelse(gh$p.value < .001, "< .001", round(gh$p.value, 3)))
c <- ggplot(ned, aes(x = gspace, y=entropy1)) + geom_point() + geom_smooth(method = "lm", color ="orange")+ theme_bw()+ggtitle("gesture space")+xlab("gesture space")+ylab("entropy")+
   annotate("text", label= ght, x=115, y=1.0, size = sizetext)

grid.arrange(a,b,c,nrow = 2)
```
*Note Figure 13*. \small Correlations are shown for each kinematic property averaged over all utterances and the concomittant network measure result. It can be seen that less intermittency, lower temporal variability,  and smaller gesture spaces, relate to lower entropy and lower cluster persistence. This indicates that complexity in movement is cashed out in terms of systematicity and more homegeneous interrelationships (lower clustering) on the network level. \normalsize


```{r, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}

#plot two networks with multidimensional scaling
D1 <-read.csv(paste0(matrices_data, "C_1.csv"), header= FALSE)
D2 <-read.csv(paste0(matrices_data, "C_2.csv"), header= FALSE)
D3 <-read.csv(paste0(matrices_data, "C_3.csv"), header= FALSE)
D4 <- read.csv(paste0(matrices_data, "C_4.csv"), header= FALSE)
D5 <- read.csv(paste0(matrices_data, "C_5.csv"), header= FALSE)



library(MASS)
library(ggnetwork)
library(network)
library(sna)
plot.networktsne <- function(D, gg)
{
  matri <- as.matrix(D[2:nrow(D), 2:ncol(D)])
  top <- tsne(as.dist(matri),  perplexity = 25, max_iter=2500)# 25 = good #isomap(as.dist(matri),k = 3, ndim=2)$points #tsne(as.dist(matri),  perplexity = 4, epoch=50) #
  chains <- as.vector(t(D[1,2:ncol(D)]))
  concepts <-  as.vector(D[2:nrow(D),1])
  df <- cbind.data.frame(top, concepts, chains)
  colnames(df) <- c( "Xpos", "Ypos","concepts", "chains")
  pl <- ggplot(df, aes(x= Xpos, y = Ypos,xend = 0, yend = 0, color =chains)) + geom_point(size= 1) +  theme_void() + scale_color_brewer(palette = "Set1")+ stat_ellipse(type = "t", level = 0.50) + theme(legend.position = "none") + ggtitle(paste0("gen: ",  gg))
  return(pl)
}

  #l2 <- layout_with_mds(g_s2, dist =mat2 ) #multidimensional scaling (so that layout reflects distance)
   l1 <- plot.networktsne(D1, 1)
   l2 <- plot.networktsne(D2, 2)
   l3 <- plot.networktsne(D3, 3)
   l4 <- plot.networktsne(D4, 4)
   l5 <- plot.networktsne(D5, 5)

 comb <-  grid.arrange( l1,  l2,  l3,  l4,  l5,  nrow =5)

#cluster performance per chain


dunn_index <- silwidth <- generation <- vector()
for(gen in 1:5)
{
  dat <- read.csv(paste0(matrices_data, "C_", gen,".csv"), header= FALSE)
  matri <- as.matrix(dat[2:nrow(dat), 2:ncol(dat)])
  clusters <-  as.numeric(as.factor(as.vector(t(dat[1,2:ncol(dat)]))))



  
  dunn_index <- c(dunn_index, dunn(as.dist(matri), clusters))
  silwidth <-  c(silwidth, mean(silhouette(clusters, as.dist(matri))[,3]))
  generation <- c(generation, gen)
  

}
cls <- cbind.data.frame(dunn_index, generation, silwidth)

a <- ggplot(cls, aes(x=generation, y=dunn_index)) + geom_point(size =3) + geom_smooth(method="lm", alpha =0, color = "black", linetype = "dashed") + theme_bw() + ylab("dunn index") + ylim(0.025, 0.09)
b <- ggplot(cls, aes(x=generation,y = silwidth)) + geom_point(size =3) + geom_smooth(method="lm", alpha=0, color = "black", linetype = "dashed")+ theme_bw() + ylab("sillouhette width")+ ylim(-0.16,-0.07)
comb2 <-grid.arrange(a,b, nrow=2)


comb2 <-grid.arrange(a,b, nrow=2)


grid.arrange(comb, comb2, nrow=1)

```



\pagebreak
# Discussion
  Based on signal processing alone we have detected systematic changes reflective of a linguistically maturing communication system from continuous multi-articulatory kinematics of silent gestures. We applied computer vision techniques to extract kinematics from video data, and then applied an analysis procedure to detect structural relations between gestural utterances [@pouwGestureNetworksIntroducing2019]. We found that communicative tokens showed higher systematicity at later generations, conceptually replicating results that were based on human coding of the gesture’s content [@motamediEvolvingArtificialSign2019]. Indeed, gesture network entropy turned out to be a good approximation of entropy based on human coding of the gesture content. We further find that tokens were less stably differentiable on the form level as tokens have lower cluster persistence over the generations. Moreover, we found a decrease in entropy for the functional rather than the thematic dimension. While in the original study no increase in efficiency was found based on measuring gesture information units, we did detect increases of communicative efficiency for gesture kinematics. Over generations, gestures became less segmented (smoother), more rhythmic (if comprised of more than 3 submovements), and smaller. We also show that action gestures have a different kinematic quality as compared to non-action gestures, being more smooth (less intermittent) in their execution. Finally, we show that the decrease in kinematic complexity on the token level, predicts system-level changes of decreased entropy and decrease in clustering.   
  That entropy decreased for gestures within the functional category at the level of kinematics, is consonant with the human coding findings of the original study and other related findings on sign languages showing regular employment of functional categories such as object- versus action distinctions [@paddenPatternedIconicitySign2013]. That gestures referring to actions are less intermittent as compared to non-action gestures in terms of their kinematics, conceptually replicates research based on human coding showing that action gestures often consist of a single segment [@ortegaTypesIconicityCombinatorial2020]. In sum, our findings indicate that kinematics are revealing of the functional nature of gesture references, showing unique trajectories of change during iterated learning.  
  A decrease in cluster persistence over generations here is likely to reflect the differentiability of communicative tokens, which as originally reported often showed iconic gestures at early stages in the iterations that were sometimes ambiguous in the theme category, and maximally differentiated from the other-themed gestures. For example, “arrest” and “police officer” could both contain a gesture that enacts the appliance of hand cuffs. Thus within themes there was clustering, but across themes there is differentiation. When gestures are disambiguated over the generations this will result in increased distances among the gestures within this category on the network level, i.e., leads to less clustering. While clusters became more unstable over the generations, the diversity of the interrelationships of the communicative tokens decreased (i.e., entropy decreased), and this is especially on the functional level. This suggests that there is a more consistent and thus homogeneous way in which the communicative system is organized, and the reorganization is caused by a reuse of gesture *across themes* and *within function*. That this increase in consistency is indeed a form of systematicity is fully supported by the detection of entropy decrease over the generations for communicative tokens grouped on the functional dimension (e.g., agent, action, location), but not the thematic dimension (e.g., justice, cooking).  
  The kinematic findings suggest that the manual utterances simplify, in the way of reducing in size, in the reduction of submovements, and the decrease in temporal variability of the utterance if it comprised of multiple submovements. This simplification seems to be a reduction in articulatory effort, as making a minimal amount of smaller rhythmic movements reduces the degrees of freedom for articulation [@bernsteinCoordinationRegulationsMovements1967; @kelsoFunctionallySpecificArticulatory1984]. Moreover, this increased rhythmicity could also increase learnability and comprehensibility of the gesture, as we know from speech perception in noisy conditions that it is optimally perceived when speech is more rhythmic [@wangSpeakingRhythmicallyImproves2018].  
  Interestingly, this reduction of degrees of freedom of the pronunciation, is precisely what one finds for novice learners of ASL. ASL learners have been found to spatially reduce their signs as they become more fluent [@wilburExperimentalInvestigationStressed1990; @luptonMotorLearningSign1990]. Moreover, a reduction in duration between the compounds of the signs have been observed during learning progression, where multicomponent component signs are increasingly performed as a single sign. In the present paradigm, there is a similar evolution of pronunciation, such that gestural multi-articulatory utterances acquire stable functional organizations across generations. Suboptimal organization of sub-movements will be filtered out as it were over the generations, and the temporally extended movement sequence becomes likely more coordinated whereby degrees of freedom are reduced by functioning as a single multi-aticulatory coordinative structure [@bernsteinCoordinationRegulationsMovements1967; @kelsoDynamicPatternPerspective1983], affecting for example gesture's temporal variability and intermittency. That head movements improved differientation of real vs. falsely paired gestures in our analysis, further emphasizes that multiple articulators coordinated in the production of meaning in the current task. This finding resonates with the known grammatic, phonetic, and prosodic functions that head movements have in sign languages such as ASL [@tyronePhoneticsHeadBody2016]. Indeed, as @sandlerBodyEvidenceNature2018 has argued for sign languages, the expressive power of the body lies in the combination of different articulators which can attain unique linguistic functions which can then be combined in parallel into a single linguistically complex utterance.  
  Note that our method allowed to account for the multiarticulatory nature of communication without formalized additional coding of the head movements, and we were able to quantify the unique communicative contributions of head and upper limb movements in the current paradigm. In this way, the current method is a bottom-up approach that will invite further investigation when needed. Our bottom-up approach further showed that gesture network entropy decreases alongside the entropy obtained from human coded content segmentation of the gestural utterance [@motamediEvolvingArtificialSign2019], suggesting that systematicity in form can be detected without the need for an apriori coding scheme. But our method as exposed here goes one step further. If gesture network analysis is complemented with kinematic feature analysis, it can be further assessed *what* is driving systematicity, providing insights on the evolution of the morphology of the silent gesture system. Coding schemes are notoriously difficult to formalize as any gesture researcher will confirm, and the current buttom-up method provides a formalized procedure for the detection of gesture evolution. As it is formalized and reproducible, the method is waiting to be applied to large datasets that are impractical to (completely) code by human annotators. An exciting avenue of further research is how different morphological evolutions can yield similar or different levels of systematicity at the gesture network level depending on different communicative constraints in vast populations. As such, we have shown that human-coded information units can be approximated from the kinematics (intermittency) and it can be assessed whether changes in such units are cashed out on the system level in the form of differientiability (clustering) and information compression (network entropy) - no human coding needed.  
  There are two important caveats to the analyses presented here. First, in general, it is the case that kinematic analysis cannot say anything about the precise semiotic content that might evolve, and this is especially the case with incereasing 'drifts towards the arbitrary' [@tomaselloOriginsHumanCommunication2008]. Although such drift might be detected via our network analysis, recognizing its possible semiotic content will always require extensive analysis [@sandlerBodyEvidenceNature2018]. However, since changes can be detected our analysis may invite human coding to be performed on a subset of the data which show promising changes over generations, inviting further necessary human interperation to understand what was driving such changes on the level of meaning.    
  A second caveat is that there is a limitation to dynamic time warping. If we appreciate that compositionality increases as a communicative system matures, holistic gestures become segmented and the order of presentation of such segments might be varied. However, the dynamic time warping algorithm is sensitive to ordering and would judge two gestures containing identical segments in different orders as very different, while for a human coder the similarity between differently ordered segments might be transparent. Thus our analysis may at times judge sequences of gestures highly dissimilar when in fact they are merely ordered differently. There are ways to circumvent this by only look for trajectory overlaps rather than ordering through time (Pouw & Dixon 2019), but such analysis goes beyond the current approach.  
  Both of these caveats mean that our approach to kinematics, like all quantitative analyses of human behavior, requires some degree of human oversight (for meaningful implementation) and human insight (for judicious interpretation). When these requirements are met, we believe that our fully reproducible and automatable methods can make important contributions: It will reduce the amount of manual coding which is currently consuming many researchers' time. It provides a much needed *multiscale* approach to how gestures evolve as communicative systems. The current method can further scale up the study of language evolution across modalities, as the kinematic analysis shown here functions much like an acoustic and articulatory analysis in speech. Beyond such promises, the current multiscale approach has shown that silent gesture kinematics evolve in structural ways during iterated learning.
  
# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```
<div id = "refs"></div>

# Supplemental

Figure S1. Correlations and distributions for kinematic measures per trial
```{r summ_kin, echo = FALSE, message = FALSE, warning = FALSE, fig.height =6, fig.width = 6}
#AUTO MEASURES CORRELATIONS
ca <- cor.test(feats$submovements, feats$smoothness)
cat <- paste0("r = ", round(ca$estimate, 2),  ", p = ", ifelse(ca$p.value < .001, "< .001", round(ca$p.value, 3)))
a <- ggplot( feats, aes(x = submovements, y = smoothness)) + geom_point(size = 0.1) + geom_smooth(color = "orange") + theme_bw()+xlab("log submovements" )+ ylab("log intermittency") +
      annotate("text", label= cat, x=1.5, y=13)


#
cb <- cor.test(feats$smoothness, feats$rhythmicity)
cbt <- paste0("r = ", round(cb$estimate, 2),  ", p = ", ifelse(cb$p.value < .001, "< .001", round(cb$p.value, 3)))
b <- ggplot( feats, aes(x = smoothness, y = rhythmicity)) + geom_point(size = 0.1) + geom_smooth(color = "orange") + theme_bw()+xlim(4, 12) + xlab("log intermittency") +ylab("temporal variability")+
   annotate("text", label= cbt, x=6.5, y=4.75)

cc <- cor.test(feats$smoothness, feats$gspace)
cct <- paste0("r = ", round(cc$estimate, 2),  ", p = ", ifelse(cc$p.value < .001, "< .001", round(cc$p.value, 3)))

c <- ggplot( feats, aes(x = smoothness, y = gspace)) + geom_point(size = 0.1) + geom_smooth(color = "orange") + 
  theme_bw() + xlab("log intermittency") + 
     annotate("text", label= cct, x=2.5, y=200)

cd <- cor.test(feats$rhythmicity, feats$gspace)
cdt <- paste0("r = ", round(cd$estimate, 2),  ", p = ", ifelse(cd$p.value < .001, "< .001", round(cd$p.value, 3)))
d <- ggplot( feats, aes(x = rhythmicity, y = gspace)) + geom_point(size = 0.1) + geom_smooth(color = "orange") + 
  theme_bw() +xlab("temporal variability")+
       annotate("text", label= cdt, x=2.75, y=4.75)

grid.arrange(ggMarginal(a),ggMarginal(b),ggMarginal(c), ggMarginal(d), nrow= 2)

```
*Note Figure S1*. Left upper panel, correlations and distributions are shown for intermittency and submovement. Given their high correlation we will use intermittency score for our final analysis. Other correlations are shown for the selected measures, rhythmiticy, gesture space and intermittency.  

Figure S2. Correlations of kinematic measures with human-coded gesture information  
```{r r summ_kin2, echo = FALSE, message = FALSE, warning = FALSE, , fig.height =7, fig.width = 6}

#AUTO MEASURES CORRELATIONS with Human Annotations
sizetext <- 3
tcol <- "red"
  #segments vs. smoothness
ca <- cor.test(feats$segments, feats$smoothness)
cat <- paste0("r = ", round(ca$estimate, 2),  ", p = ", ifelse(ca$p.value < .001, "< .001", round(ca$p.value, 3)))
a <- ggplot( feats, aes(x = segments, y = smoothness)) + geom_point(size = 0.1) + geom_smooth(color = "red") + theme_bw()+xlab("segments \n (human coder)")+ ylab("intermittency") +
      annotate("text", label= cat, x=8, y=13, size = sizetext, color = tcol)

  #information units vs smoothness
cb <- cor.test(feats$inf_units, feats$smoothness)
cbt <- paste0("r = ", round(cb$estimate, 2),  ", p = ", ifelse(cb$p.value < .001, "< .001", round(cb$p.value, 3)))
b <- ggplot( feats, aes(x = inf_units, y = smoothness)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlim(4, 12) + xlab("information units \n (human coder)") +ylab("intermittency")+
   annotate("text", label= cbt, x=7, y=13, size = sizetext, color = tcol)

  #repetitions vs. smoothness
cc <- cor.test(feats$repetitions, feats$smoothness)
cct <- paste0("r = ", round(cc$estimate, 2),  ", p = ", ifelse(cc$p.value < .001, "< .001", round(cc$p.value, 3)))
c <- ggplot( feats, aes(x = repetitions, y = smoothness)) + geom_point(size = 0.1) + geom_smooth(method = "lm",color = "red") + 
  theme_bw() + xlab("repetitions \n (human coder)") + ylab("intermittency")+
     annotate("text", label= cct, x=4, y=11.5, size = sizetext, color = tcol)

  #segments vs. gspace
cd <- cor.test(feats$segments, feats$gspace)
cdt <- paste0("r = ", round(cd$estimate, 2),  ", p = ", ifelse(cd$p.value < .001, "< .001", round(cd$p.value, 3)))
d <- ggplot( feats, aes(x = segments, y = gspace)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlab("segments \n (human coder)" )+ ylab("gesture space") +
      annotate("text", label= cdt, x=8, y=13, size = sizetext, color = tcol)

  #information units vs. gspace
ce <- cor.test(feats$inf_units, feats$gspace)
cet <- paste0("r = ", round(ce$estimate, 2),  ", p = ", ifelse(cd$p.value < .001, "< .001", round(cd$p.value, 3)))
e <- ggplot( feats, aes(x = inf_units, y = gspace)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlim(4, 12) + xlab("information units \n (human coder)") +ylab("gesture space")+
   annotate("text", label= cet, x=7, y=4.75, size = sizetext, color = tcol)

  #repetitions vs. gspace
cf <- cor.test(feats$repetitions, feats$gspace)
cft <- paste0("r = ", round(cf$estimate, 2),  ", p = ", ifelse(cf$p.value < .001, "< .001", round(cf$p.value, 3)))
f <- ggplot( feats, aes(x = repetitions, y = gspace)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw() + xlab("repetitions \n (human coder))") +ylab("gesture space")+
   annotate("text", label= cft, x=4, y=4.75, size = sizetext, color = tcol)

  #segments vs. rhythmicity
cg <- cor.test(feats$segments, feats$rhythmicity)
cgt <- paste0("r = ", round(cg$estimate, 2),  ", p = ", ifelse(cg$p.value < .001, "< .001", round(cg$p.value, 3)))
g <- ggplot( feats, aes(x = segments, y = rhythmicity)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlab("segments \n (human coder)" )+ ylab("temporal  var.") +
      annotate("text", label= cgt, x=8, y=4.75, size = sizetext, color = tcol)

  #information units vs. rhythmicity
ch <- cor.test(feats$inf_units, feats$rhythmicity)
cht <- paste0("r = ", round(ch$estimate, 2),  ", p = ", ifelse(ch$p.value < .001, "< .001", round(ch$p.value, 3)))
h <- ggplot( feats, aes(x = inf_units, y = rhythmicity)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw()+xlim(4, 12) + xlab("information units \n (human coder)") +ylab("temporal  var.")+
   annotate("text", label= cht, x=7, y=4.75, size = sizetext, color = tcol)

  #repetitions vs. rhythmicity
ci <- cor.test(feats$repetitions, feats$rhythmicity)
cit <- paste0("r = ", round(ci$estimate, 2),  ", p = ", ifelse(ci$p.value < .001, "< .001", round(ci$p.value, 3)))
i <- ggplot( feats, aes(x = repetitions, y = rhythmicity)) + geom_point(size = 0.1) + geom_smooth(method = "lm", color = "red") + theme_bw() + xlab("repetitions \n (human coder)") +ylab("temporal  var.")+
   annotate("text", label= cit, x=4, y=4.75, size = sizetext, color = tcol)

grid.arrange(a,b,c,
             d,e,f,
             g,h,i,nrow= 3)



```
*Note figure S2.* On the horizontal axes the human-coded number of gesture segments, unique information units, and the number repetitions (of information units) are shown. On the vertical axes our automatic kinematic measures are shown: intermittency, gesture space, and temporal variability. The findings show that our measures are a proxy for human judgments, such that more intermittent kinematics reflect more information units (repeater and/or unique). Larger gesture space is related to more information units. Lower temporal variability in kinematics is further associated with more information units.





\endgroup
