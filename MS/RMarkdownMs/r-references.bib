
@article{abneyComplexityMatchingDyadic2014,
  title = {Complexity Matching in Dyadic Conversation},
  author = {Abney, D. H. and Paxton, A. and Dale, R. and Kello, Christopher T.},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {143},
  pages = {2304--2315},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/xge0000021},
  abstract = {Recent studies of dyadic interaction have examined phenomena of synchronization, entrainment, alignment, and convergence. All these forms of behavioral matching have been hypothesized to play a supportive role in establishing coordination and common ground between interlocutors. In the present study, evidence is found for a new kind of coordination termed complexity matching. Temporal dynamics in conversational speech signals were analyzed through time series of acoustic onset events. Timing in periods of acoustic energy was found to exhibit behavioral matching that reflects complementary timing in turn-taking. In addition, acoustic onset times were found to exhibit power law clustering across a range of timescales, and these power law functions were found to exhibit complexity matching that is distinct from behavioral matching. Complexity matching is discussed in terms of interactive alignment and other theoretical principles that lead to new hypotheses about information exchange in dyadic conversation and interaction in general. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZG2VIXUT\\2014-41508-001.html},
  keywords = {Acoustics,Conversation,Dyads,Interpersonal Interaction,Interstimulus Interval,Stimulus Complexity},
  number = {6}
}

@article{abneyMovementDynamicsReflect2015,
  title = {Movement Dynamics Reflect a Functional Role for Weak Coupling and Role Structure in Dyadic Problem Solving},
  author = {Abney, D. H. and Paxton, A. and Dale, R. and Kello, C. T.},
  date = {2015},
  journaltitle = {Cognitive Processing},
  volume = {16},
  pages = {325--332},
  doi = {10.1007/s10339-015-0648-2},
  number = {4}
}

@article{allenAspectsRhythmASL1991,
  title = {Aspects of {{Rhythm}} in {{ASL}}},
  author = {Allen, George D. and Wilbur, Ronnie B. and Schick, Brenda B.},
  date = {1991},
  journaltitle = {Sign Language Studies},
  volume = {1072},
  pages = {297--320},
  issn = {1533-6263},
  doi = {10.1353/sls.1991.0020},
  url = {http://muse.jhu.edu/content/crossref/journals/sign_language_studies/v1072/72.allen.html},
  urldate = {2020-03-09},
  abstract = {The fluent production of American Sign Language (ASL), like speech involves highly skilled, complex motor activity. Thus, like all skilled motor acts, it is rhythmically structured. This paper presents the results of an experiment designed to determine whether the rhythm of ASL can be associated with rhythmic beats. Three groups of adult observer subjects, ASL-fluent deaf, ASL-fluent normally-hearing children of deaf parents, and sign-naive normally hearing, tapped a small metal stylus in time to the rhythm of five short ASL narratives 30 times repeated; temporal locations of the observers' taps were compared statistically for differences related to observer group membership and to various properties of the target signs. There was great overall agreement among subjects that repeated signs, signs with primary stress, and phrase-final signs played a major role in the rhythm of these ASL narratives. The ASL-fluent subjects tapped less often to signs with secondary or weak stress than did the ASL-naive subjects, however, confirming that knowledge of ASL isnecessary for full appreciation of the rhythm of ASL.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZW9NTRAD\\Allen et al. - 1991 - Aspects of Rhythm in ASL.pdf},
  langid = {english},
  number = {1}
}

@article{alviarComplexCommunicationDynamics2019,
  title = {Complex {{Communication Dynamics}}: {{Exploring}} the {{Structure}} of an {{Academic Talk}}},
  shorttitle = {Complex {{Communication Dynamics}}},
  author = {Alviar, Camila and Dale, Rick and Galati, Alexia},
  date = {2019},
  journaltitle = {Cognitive Science},
  volume = {43},
  pages = {e12718},
  issn = {1551-6709},
  doi = {10.1111/cogs.12718},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12718},
  urldate = {2020-05-25},
  abstract = {Communication is a multimodal phenomenon. The cognitive mechanisms supporting it are still understudied. We explored a natural dataset of academic lectures to determine how communication modalities are used and coordinated during the presentation of complex information. Using automated and semi-automated techniques, we extracted and analyzed, from the videos of 30 speakers, measures capturing the dynamics of their body movement, their slide change rate, and various aspects of their speech (speech rate, articulation rate, fundamental frequency, and intensity). There were consistent but statistically subtle patterns in the use of speech rate, articulation rate, intensity, and body motion across the presentation. Principal component analysis also revealed patterns of system-like covariation among modalities. These findings, although tentative, do suggest that the cognitive system is integrating body, slides, and speech in a coordinated manner during natural language use. Further research is needed to clarify the specific coordination patterns that occur between the different modalities.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LYFZYMDE\\Alviar et al. - 2019 - Complex Communication Dynamics Exploring the Stru.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8IAXEDEZ\\cogs.html},
  keywords = {Communication,Dynamic complex systems,Extended mind,Lecturing,Multimodality,Situated cognition},
  langid = {english},
  number = {3}
}

@article{amazeenCouplingBreathingMovement2001,
  title = {Coupling of Breathing and Movement during Manual Wheelchair Propulsion},
  author = {Amazeen, P. G. and Amazeen, E. L. and Beek, P. J.},
  date = {2001},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {27},
  pages = {1243--1259},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7VME6RK3\\Amazeen et al. - Coupling of Breathing and Movement During Manual W.pdf},
  langid = {english},
  number = {5}
}

@inproceedings{ambrazaitisWordProminenceRatings2020,
  title = {Word Prominence Ratings in {{Swedish}} Television News Readings – Effects of Pitch Accents and Head Movements},
  booktitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  author = {Ambrazaitis, Gilbert and Frid, Johan and House, David},
  date = {2020-05-25},
  pages = {314--318},
  publisher = {{ISCA}},
  doi = {10.21437/SpeechProsody.2020-64},
  url = {http://www.isca-speech.org/archive/SpeechProsody_2020/abstracts/216.html},
  urldate = {2020-05-26},
  abstract = {Prosodic prominence is a multimodal phenomenon where pitch accents are frequently aligned with visible movements by the hands, head, or eyebrows. However, little is known about how such movements function as visible prominence cues in multimodal speech perception with most previous studies being restricted to experimental settings. In this study, we are piloting the acquisition of multimodal prominence ratings for a corpus of natural speech (Swedish television news readings).},
  eventtitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\APE5P74D\\Ambrazaitis et al. - 2020 - Word prominence ratings in Swedish television news.pdf},
  langid = {english}
}

@article{anibleIconicityAmericanSign2020,
  title = {Iconicity in {{American Sign Language}}–{{English}} Translation Recognition},
  author = {Anible, Benjamin},
  date = {2020-03},
  journaltitle = {Language and Cognition},
  volume = {12},
  pages = {138--163},
  publisher = {{Cambridge University Press}},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2019.51},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/iconicity-in-american-sign-languageenglish-translation-recognition/0096C4538CD48E5AA546C453655E8F44},
  urldate = {2020-03-06},
  abstract = {Reaction times for a translation recognition study are reported where novice to expert English–ASL bilinguals rejected English translation distractors for ASL signs that were related to the correct translations through phonology, semantics, or both form and meaning (diagrammatic iconicity). Imageability ratings of concepts impacted performance in all conditions; when imageability was high, participants showed interference for phonologically related distractors, and when imageability was low participants showed interference for semantically related distractors, regardless of proficiency. For diagrammatically related distractors high imageability caused interference in experts, but low imageability caused interference in novices. These patterns suggest that imageability and diagrammaticity interact with proficiency – experts process diagrammatic related distractors phonologically, but novices process them semantically. This implies that motivated signs are dependent on the entrenchment of language systematicity; rather than decreasing their impact on language processing as proficiency grows, they build on the original benefit conferred by iconic mappings.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\I5TC9KTD\\Anible - 2020 - Iconicity in American Sign Language–English transl.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\KTNRNRG6\\0096C4538CD48E5AA546C453655E8F44.html},
  keywords = {bilingualism,cognitive linguistics,iconicity,signed language,translation recognition,usage-based},
  langid = {english},
  number = {1}
}

@inproceedings{arnoldUsingGeneralizedAdditive2013,
  title = {Using Generalized Additive Models and Random Forests to Model Prosodic Prominence in {{German}}},
  booktitle = {14th {{Annual Conference}} of the {{International Speech Communication Association}}},
  author = {Arnold, D. and Wagner, P. and Baayen, R. H.},
  date = {2013},
  pages = {5},
  location = {{Lyon, France}},
  abstract = {The perception of prosodic prominence is influenced by different sources like different acoustic cues, linguistic expectations and context. We use a generalized additive model and a random forest to model the perceived prominence on a corpus of spoken German. Both models are able to explain over 80\% of the variance. While the random forests give us some insights on the relative importance of the cues, the general additive model gives us insights on the interaction between different cues to prominence.},
  eventtitle = {{{INTERSPEECH}} 2013},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HFUMCEN4\\Arnold et al. - Using generalized additive models and random fores.pdf},
  langid = {english}
}

@article{aronoffRootsLinguisticOrganization2008,
  title = {The Roots of Linguistic Organization in a New Language},
  author = {Aronoff, Mark and Meir, Irit and Padden, Carol A. and Sandler, Wendy},
  date = {2008-01-01},
  journaltitle = {Interaction Studies},
  volume = {9},
  pages = {133--153},
  publisher = {{John Benjamins}},
  issn = {1572-0373, 1572-0381},
  doi = {10.1075/is.9.1.10aro},
  url = {https://www.jbe-platform.com/content/journals/10.1075/is.9.1.10aro},
  urldate = {2020-03-10},
  abstract = {It is possible for a language to emerge with no direct linguistic history or outside linguistic influence. Al-Sayyid Bedouin Sign Language (ABSL) arose about 70 years ago in a small, insular community with a high incidence of profound prelingual neurosensory deafness. In ABSL, we have been able to identify the beginnings of phonology, morphology, syntax, and prosody. The linguistic elements we find in ABSL are not exclusively holistic, nor are they all compositional, but a combination of both. We do not, however, find in ABSL certain features that have been posited as essential even for a proto-language. ABSL has a highly regular syntax as well as word-internal compounding, also highly regular but quite distinct from syntax in its patterns. ABSL, however, has no discernable word-internal structure of the kind observed in more mature sign languages: no spatially organized morphology and no evident duality of phonological ­patterning.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LGXC3LUA\\Aronoff et al. - 2008 - The roots of linguistic organization in a new lang.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\J3HZU43T\\is.9.1.html},
  langid = {english},
  number = {1}
}

@article{ascherslebenTemporalControlMovements2002,
  title = {Temporal {{Control}} of {{Movements}} in {{Sensorimotor Synchronization}}},
  author = {Aschersleben, Gisa},
  date = {2002-02-01},
  journaltitle = {Brain and Cognition},
  shortjournal = {Brain and Cognition},
  volume = {48},
  pages = {66--79},
  issn = {0278-2626},
  doi = {10.1006/brcg.2001.1304},
  url = {http://www.sciencedirect.com/science/article/pii/S0278262601913041},
  urldate = {2019-04-02},
  abstract = {Under conditions in which the temporal structure of events (e.g., a sequence of tones) is predictable, performing movements in synchrony with this sequence of events (e.g., dancing) is an easy task. A rather simplified version of this task is studied in the sensorimotor synchronization paradigm. Participants are instructed to synchronize their finger taps with an isochronous sequence of signals (e.g., clicks). Although this is an easy task, a systematic error is observed: Taps usually precede clicks by several tens of milliseconds. Different models have been proposed to account for this effect (“negative asynchrony” or “synchronization error”). One group of explanations is based on the idea that synchrony is established at the level of central representations (and not at the level of external events), and that the timing of an action is determined by the (anticipated) action effect. These assumptions are tested by manipulating the amount of sensory feedback available from the tap as well as its temporal characteristics. This article presents an overview of these representational models and the empirical evidence supporting them. It also discusses other accounts briefly in the light of further evidence.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MUE975DA\\Aschersleben - 2002 - Temporal Control of Movements in Sensorimotor Sync.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\CAAEWIZD\\S0278262601913041.html},
  number = {1}
}

@article{ascherslebenTimingMechanismsSensorimotor2002,
  title = {Timing Mechanisms in Sensorimotor Synchronization},
  author = {Aschersleben, Gisa and Stenneken, Prisca and Cole, J. D. and Prinz, Wolfgang},
  date = {2002},
  journaltitle = {Common mechanisms in perception and action},
  pages = {227--244},
  url = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_727550},
  urldate = {2019-04-02},
  abstract = {Author: Aschersleben, Gisa et al.; Genre: Book Chapter; Published in Print: 2002; Title: Timing mechanisms in sensorimotor synchronization},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LZMPZVPB\\ViewItemOverviewPage.html},
  langid = {english}
}

@article{ayRobustnessComplexityCoconstructed2007,
  title = {Robustness and Complexity Co-Constructed in Multimodal Signalling Networks},
  author = {Ay, Nihat and Flack, Jessica and Krakauer, David C},
  date = {2007-03-29},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {362},
  pages = {441--447},
  issn = {0962-8436},
  doi = {10.1098/rstb.2006.1971},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2323562/},
  urldate = {2019-08-15},
  abstract = {In animal communication, signals are frequently emitted using different channels (e.g. frequencies in a vocalization) and different modalities (e.g. gestures can accompany vocalizations). We explore two explanations that have been provided for multimodality: (i) selection for high information transfer through dedicated channels and (ii) increasing fault tolerance or robustness through multichannel signals. Robustness relates to an accurate decoding of a signal when parts of a signal are occluded. We show analytically in simple feed-forward neural networks that while a multichannel signal can solve the robustness problem, a multimodal signal does so more effectively because it can maximize the contribution made by each channel while minimizing the effects of exclusion. Multimodality refers to sets of channels where within each set information is highly correlated. We show that the robustness property ensures correlations among channels producing complex, associative networks as a by-product. We refer to this as the principle of robust overdesign. We discuss the biological implications of this for the evolution of combinatorial signalling systems; in particular, how robustness promotes enough redundancy to allow for a subsequent specialization of redundant components into novel signals.},
  eprint = {17255020},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\W59H23UJ\\Ay et al. - 2007 - Robustness and complexity co-constructed in multim.pdf},
  number = {1479},
  pmcid = {PMC2323562}
}

@article{azarLanguageContactDoes2019,
  title = {Language Contact Does Not Drive Gesture Transfer: {{Heritage}} Speakers Maintain Language Specific Gesture Patterns in Each Language},
  shorttitle = {Language Contact Does Not Drive Gesture Transfer},
  author = {Azar, Zeynep and Backus, Ad and Özyürek, Aslı},
  date = {2019-04-30},
  journaltitle = {Bilingualism: Language and Cognition},
  pages = {1--15},
  issn = {1366-7289, 1469-1841},
  doi = {10.1017/S136672891900018X},
  url = {https://www.cambridge.org/core/product/identifier/S136672891900018X/type/journal_article},
  urldate = {2019-08-30},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SIHT5VTL\\Azar et al. - 2019 - Language contact does not drive gesture transfer .pdf},
  langid = {english}
}

@article{baerReflexActivationLaryngeal1979,
  title = {Reflex Activation of Laryngeal Muscles by Sudden Induced Subglottal Pressure Changes},
  author = {Baer, T.},
  date = {1979-05},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {65},
  pages = {1271--1275},
  issn = {0001-4966},
  doi = {10.1121/1.382795},
  abstract = {In measuring the effect of subglottal pressure changes on fundamental frequency (Fo) of phonation, the effects of changing laryngeal muscle activity must be eliminated. Several investigators have used a strategy in which pulsatile increases of subglottal pressure are induced by pushing on the chest or abdomen of a phonating subject. Fundamental frequency is then correlated with subglottal pressure changes during an interval before laryngeal response is assumed to occur. The present study was undertaken to repeat such an experiment while monitoring electromyographic (EMG) activity of some laryngeal muscles, to discover empirically the latency of the laryngeal response. The results showed a consistent response to each push, with a latency of about 30 ms. Despite this response, analyses of fundamental frequency versus subglottal pressure changes during the interval of constant EMG activity were in general agreement with previously published values. With respect to the nature of the electromyographic response itself, its timing was found to be within the range of latencies appropriate for peripheral feedback, and was also similar to that for an acoustically--or tactually--elicited startle reflex.},
  eprint = {458049},
  eprinttype = {pmid},
  keywords = {Electromyography,Glottis,Humans,Laryngeal Muscles,Muscles,Phonation,Pressure,Reflex,Voice},
  langid = {english},
  number = {5}
}

@article{bakersusane.VentilationSpeechCharacteristics2008,
  title = {Ventilation and {{Speech Characteristics During Submaximal Aerobic Exercise}}},
  author = {{Baker Susan E.} and {Hipp Jenny} and {Alessio Helaine}},
  date = {2008-10-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {51},
  pages = {1203--1214},
  doi = {10.1044/1092-4388(2008/06-0223)},
  url = {https://pubs.asha.org/doi/10.1044/1092-4388%282008/06-0223%29},
  urldate = {2019-10-17},
  abstract = {Purpose
      This study examined alterations in ventilation and speech characteristics as well
         as perceived dyspnea during submaximal aerobic exercise tasks.
      
      
      Method
      Twelve healthy participants completed aerobic exercise-only and simultaneous speaking
         and aerobic exercise tasks at 50\% and 75\% of their maximum oxygen consumption (VO2 max). Measures of ventilation, oxygen consumption, heart rate, perceived dyspnea,
         syllables per phrase, articulation rate, and inappropriate linguistic pause placements
         were obtained at baseline and throughout the experimental tasks.
      
      
      Results
      Ventilation was significantly lower during the speaking tasks compared with the nonspeaking
         tasks. Oxygen consumption, however, did not significantly differ between speaking
         and nonspeaking tasks. The perception of dyspnea was significantly higher during the
         speaking tasks compared with the nonspeaking tasks. All speech parameters were significantly
         altered over time at both task intensities.
      
      
      Conclusions
      It is speculated that decreased ventilation without a reduction in oxygen consumption
         implies that utilization of oxygen by the working muscles was increased during the
         speaking tasks to meet the metabolic needs. A greater ability to utilize oxygen from
         inspired air is found in individuals who are at higher fitness levels, and therefore
         these findings may have implications for individuals who must complete simultaneous
         speech and exercise for occupational purposes (e.g., fitness/military drill instructors,
         singers performing choreography).},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5Y2ALGCH\\06-0223).html},
  number = {5}
}

@article{balasubramanianAnalysisMovementSmoothness2015,
  title = {On the Analysis of Movement Smoothness},
  author = {Balasubramanian, Sivakumar and Melendez-Calderon, Alejandro and Roby-Brami, Agnes and Burdet, Etienne},
  date = {2015-12},
  journaltitle = {Journal of NeuroEngineering and Rehabilitation},
  volume = {12},
  issn = {1743-0003},
  doi = {10.1186/s12984-015-0090-9},
  url = {http://www.jneuroengrehab.com/content/12/1/112},
  urldate = {2020-01-31},
  abstract = {Quantitative measures of smoothness play an important role in the assessment of sensorimotor impairment and motor learning. Traditionally, movement smoothness has been computed mainly for discrete movements, in particular arm, reaching and circle drawing, using kinematic data. There are currently very few studies investigating smoothness of rhythmic movements, and there is no systematic way of analysing the smoothness of such movements. There is also very little work on the smoothness of other movement related variables such as force, impedance etc. In this context, this paper presents the first step towards a unified framework for the analysis of smoothness of arbitrary movements and using various data. It starts with a systematic definition of movement smoothness and the different factors that influence smoothness, followed by a review of existing methods for quantifying the smoothness of discrete movements. A method is then introduced to analyse the smoothness of rhythmic movements by generalising the techniques developed for discrete movements. We finally propose recommendations for analysing smoothness of any general sensorimotor behaviour.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\A64EVWVB\\Balasubramanian et al. - 2015 - On the analysis of movement smoothness.pdf},
  langid = {english},
  number = {1}
}

@article{baldisseraAnticipatoryPosturalAdjustments2008,
  title = {Anticipatory Postural Adjustments in Arm Muscles Associated with Movements of the Contralateral Limb and Their Possible Role in Interlimb Coordination},
  author = {Baldissera, Fausto and Rota, Viviana and Esposti, Roberto},
  date = {2008-02},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {185},
  pages = {63--74},
  issn = {1432-1106},
  doi = {10.1007/s00221-007-1131-9},
  abstract = {While sitting on a turnable stool, with both shoulders flexed at 90 degrees or, alternatively, with arms parallel to the trunk and the elbows flexed at 90 degrees--the hands being semisupine--subjects performed unidirectional and cyclic movements on the horizontal plane of the right arm (adduction-abduction) or hand (flexion-extension). The left arm was still, in a position symmetrical to that of the right limb and with the hand contacting a fixed support by the palmar or dorsal surface. During both unidirectional and cyclic arm or hand movements, activation of the prime mover muscles (right Pectoralis Major for arm adduction and Infraspinatus for abduction; right Flexor Carpi Radialis and Extensor Carpi Radialis for the hand movements) was accompanied by activation of the homologous muscles of the contralateral arm and inhibition of antagonists. The contralateral activities (1) regularly preceded the burst in the movement prime movers and (2) were organised in fixation chains that, exerting forces on the hand fixed support, will counterbalance the rotatory action exerted on the trunk by the primary movement. Based on these features, these activities may be classified as anticipatory postural adjustments (APAs). The observed APAs distribution is such as to favour the preferential (mirror symmetrical) coupling of upper limb movements on the horizontal plane. The possible role of these APAs in determining the different constraints experienced when performing mirror symmetrical versus isodirectional coupling is discussed.},
  eprint = {17912507},
  eprinttype = {pmid},
  keywords = {Adult,Arm,Electromyography,Female,Humans,Male,Middle Aged,Movement,Muscle; Skeletal,Postural Balance,Posture,Psychomotor Performance},
  langid = {english},
  number = {1}
}

@article{banseAcousticProfilesVocal,
  title = {Acoustic {{Profiles}} in {{Vocal Emotion Expression}}},
  author = {Banse, Rainer and Scherer, Klaus R},
  journaltitle = {Journal of Personality and Social Psychology},
  volume = {70},
  pages = {614--636},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\X8RRIRXT\\Banse and Scherer - Acoustic Profiles in Vocal Emotion Expression.pdf},
  langid = {english}
}

@article{bardRoleAfferentInformation1992,
  title = {Role of Afferent Information in the Timing of Motor Commands: A Comparative Study with a Deafferented Patient},
  shorttitle = {Role of Afferent Information in the Timing of Motor Commands},
  author = {Bard, C. and Paillard, J. and Lajoie, Y. and Fleury, M. and Teasdale, N. and Forget, R. and Lamarre, Y.},
  date = {1992-02},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  volume = {30},
  pages = {201--206},
  issn = {0028-3932},
  abstract = {The accuracy of the motor system in synchronizing simultaneous movements initiations was tested in two conditions: (1) when the motor commands were triggered by an external signal (reactive condition), and (2) when subjects self-paced their movement onsets (self-paced condition). The task consisted of initiating simultaneously ipsilateral finger extension and heel raising. Eight normal subjects and a deafferented patient were tested. In the reactive condition, both normal subjects and the deafferented patient exhibited a precession of finger initiation over heel raising. This delay corresponds to the difference observed in the reaction time of the two limbs when measured independently. It reflects the difference in conduction times of the efferent pathways, as if the two motor commands were released simultaneously through a common triggering signal in the motor cortex. In contrast, in the self-paced condition normal subjects showed precession of heel over finger onsets, suggesting that synchrony is based upon the evaluation of afferent information. Unlike normal subjects, the patient showed no heel precession in the self-paced condition. These findings suggest that reactive and self-paced responses are produced through two different control modes and that afferent information contributes to the timing of motor commands in the self-paced mode.},
  eprint = {1560897},
  eprinttype = {pmid},
  keywords = {Adult,Afferent Pathways,Demyelinating Diseases,Electromyography,Female,Humans,Male,Mental Processes,Movement,Reaction Time,Time Factors},
  langid = {english},
  number = {2}
}

@article{barkerTensileTransmissionLumbar2004,
  title = {Tensile {{Transmission Across}} the {{Lumbar Fasciae}} in {{Unembalmed Cadavers}}: {{Effects}} of {{Tension}} to {{Various Muscular Attachments}}},
  shorttitle = {Tensile {{Transmission Across}} the {{Lumbar Fasciae}} in {{Unembalmed Cadavers}}},
  author = {Barker, Priscilla J. and Briggs, Christopher A. and Bogeski, Goce},
  date = {2004-01-15},
  journaltitle = {Spine},
  volume = {29},
  pages = {129--138},
  issn = {0362-2436},
  doi = {10.1097/01.BRS.0000107005.62513.32},
  url = {https://journals.lww.com/spinejournal/Fulltext/2004/01150/The_Relation_Between_the_Transversus_Abdominis.5.aspx?casa_token=ma0ldjmjKIoAAAAA:bfObNe6Lmxf1A2InwLNv0sx9uFRZBB_XDwztOJj8GSZg1aUE-hmZxP0jR5Eeh2ziAa2WJCnGEBSQrpUMhF4uqGPz},
  urldate = {2020-04-03},
  abstract = {Study Design. ~Traction was applied to muscles attaching to the posterior and middle layers of lumbar fascia (PLF, MLF). Effects on fasciae were determined via tensile force measures and movement of markers.
        Objectives. ~To document tensile transmission to the PLF and MLF when traction was applied to latissimus dorsi (LD), gluteus maximus (GM), external and internal oblique (EO, IO), and transversus abdominis (TrA) in unembalmed cadavers.
        Summary of Background Data. ~A previous study on embalmed cadavers applied traction to muscle attachments while monitoring fascial movement but did not test TrA or the MLF.
        Methods. ~The PLF and MLF were dissected then marked on eight unembalmed cadavers. A strain gauge was inserted through fascia at L3; 10N traction was applied to each muscle attachment while photographs and tension measures were taken. Movement of fascial markers was detected photographically. Fascial widths were also measured.
        Results. ~Tension was clearly transmitted to fascial vertebral attachments. Tensile forces and fascial areas affected were highest for traction on LD and TrA in the PLF and for TrA in the MLF. Movement of PLF markers from tension on LD and TrA occurred bilaterally between T12 and S1. Effects from other muscles were variably bilateral, with those from GM and IO occurring below L3 and those from EO occurring above L3. Tensile forces were relatively high in the MLF and its width was less than half that of the PLF.
        Conclusions. ~Low levels of tension are effectively transmitted between TrA and the MLF or PLF. Via them, TrA may influence intersegmental movement.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\GSE3CXK3\\The_Relation_Between_the_Transversus_Abdominis.5.html},
  langid = {american},
  number = {2}
}

@article{baronchelliNetworksCognitiveScience2013,
  title = {Networks in {{Cognitive Science}}},
  author = {Baronchelli, Andrea and Ferrer-i-Cancho, Ramon and Pastor-Satorras, Romualdo and Chater, Nick and Christiansen, Morten H.},
  date = {2013-07},
  journaltitle = {Trends in Cognitive Sciences},
  volume = {17},
  pages = {348--360},
  issn = {13646613},
  doi = {10.1016/j.tics.2013.04.010},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S136466131300096X},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\RFFKLKMX\\Baronchelli et al. - 2013 - Networks in Cognitive Science.pdf},
  langid = {english},
  number = {7}
}

@incollection{barthSlightestWhiffAir2014,
  title = {The {{Slightest Whiff}} of {{Air}}: {{Airflow Sensing}} in {{Arthropods}}},
  shorttitle = {The {{Slightest Whiff}} of {{Air}}},
  booktitle = {Flow {{Sensing}} in {{Air}} and {{Water}}: {{Behavioral}}, {{Neural}} and {{Engineering Principles}} of {{Operation}}},
  author = {Barth, Friedrich G.},
  editor = {Bleckmann, Horst and Mogdans, Joachim and Coombs, Sheryl L.},
  date = {2014},
  pages = {169--196},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-41446-6_7},
  url = {https://doi.org/10.1007/978-3-642-41446-6_7},
  urldate = {2019-05-07},
  abstract = {The perception of medium flows has received ever increasing attention during the last two decades and has increasingly been recognized as a sensory capacity of its own. A combination of experimental work and physical–mathematical modeling has deepened our understanding of the workings of airflow sensors, mainly represented by insect filiform hairs and arachnid trichobothria, both as individual sensors and sensor arrays. This chapter points to the diversity of arthropod airflow sensors and stresses the importance of comparative studies. These should include animal groups so far largely neglected by sensory biology and neuroethology. Another need is to analyze biologically relevant flow patterns and to relate these to the functional properties of the various patterns of sensor arrangement found in different animal taxa. Finally, the capture of a freely flying fly by a wandering spider is taken to illustrate the challenges and promises of studies that aim to reveal the relation between a particular airflow pattern and a specific behavior.},
  isbn = {978-3-642-41446-6},
  keywords = {Digital Particle Image Velocimetry,Hair Shaft,High Frequency Component,Medium Flow,Oribatid Mite},
  langid = {english}
}

@article{bausWhenDoesIconicity2013,
  title = {When Does {{Iconicity}} in {{Sign Language Matter}}?},
  author = {Baus, Cristina and Carreiras, Manuel and Emmorey, Karen},
  date = {2013-03-01},
  journaltitle = {Language and Cognitive Processes},
  shortjournal = {Lang Cogn Process},
  volume = {28},
  pages = {261--271},
  issn = {0169-0965},
  doi = {10.1080/01690965.2011.620374},
  abstract = {We examined whether iconicity in American Sign Language (ASL) enhances translation performance for new learners and proficient signers. Fifteen hearing nonsigners and 15 proficient ASL-English bilinguals performed a translation recognition task and a production translation task. Nonsigners were taught 28 ASL verbs (14 iconic; 14 non-iconic) prior to performing these tasks. Only new learners benefited from sign iconicity, recognizing iconic translations faster and more accurately and exhibiting faster forward (English-ASL) and backward (ASL-English) translation times for iconic signs. In contrast, proficient ASL-English bilinguals exhibited slower recognition and translation times for iconic signs. We suggest iconicity aids memorization in the early stages of adult sign language learning, but for fluent L2 signers, iconicity interacts with other variables that slow translation (specifically, the iconic signs had more translation equivalents than the non-iconic signs). Iconicity may also have slowed translation performance by forcing conceptual mediation for iconic signs, which is slower than translating via direct lexical links.},
  eprint = {23543899},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Z8ELZT78\\Baus et al. - 2013 - When does Iconicity in Sign Language Matter.pdf},
  keywords = {American Sign Language,bilingualism,iconicity,translation},
  langid = {english},
  number = {3},
  pmcid = {PMC3608132}
}

@article{bavelasGesturingTelephoneIndependent2008,
  title = {Gesturing on the Telephone: {{Independent}} Effects of Dialogue and Visibility},
  shorttitle = {Gesturing on the Telephone},
  author = {Bavelas, Janet and Gerwing, Jennifer and Sutton, Chantelle and Prevost, Danielle},
  date = {2008-02-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {58},
  pages = {495--520},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2007.02.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0749596X07000289},
  urldate = {2019-10-14},
  abstract = {Speakers often gesture in telephone conversations, even though they are not visible to their addressees. To test whether this effect is due to being in a dialogue, we separated visibility and dialogue with three conditions: face-to-face dialogue (10 dyads), telephone dialogue (10 dyads), and monologue to a tape recorder (10 individuals). For the rate of gesturing, both dialogue and visibility had significant, independent effects, with the telephone condition consistently higher than the tape recorder. Also, as predicted, visibility alone significantly affected how speakers gestured: face-to-face speakers were more likely to make life-size gestures, to put information in their gestures that was not in their words, to make verbal reference to their gestures, and to use more gestures referring to the interaction itself. We speculate that demonstration, as a modality, may underlie these findings and may be intimately tied to dialogue while being suppressed in monologue.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FFGN2YU7\\S0749596X07000289.html},
  keywords = {Demonstration,Face-to-face dialogue,Gestures,Telephone,Visibility},
  number = {2}
}

@article{beckageSmallWorldsSemantic2011,
  title = {Small {{Worlds}} and {{Semantic Network Growth}} in {{Typical}} and {{Late Talkers}}},
  author = {Beckage, Nicole and Smith, Linda and Hills, Thomas},
  editor = {Perc, Matjaz},
  date = {2011-05-11},
  journaltitle = {PLoS ONE},
  volume = {6},
  pages = {e19348},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0019348},
  url = {http://dx.plos.org/10.1371/journal.pone.0019348},
  urldate = {2020-01-21},
  abstract = {Network analysis has demonstrated that systems ranging from social networks to electric power grids often involve a small world structure-with local clustering but global ac cess. Critically, small world structure has also been shown to characterize adult human semantic networks. Moreover, the connectivity pattern of these mature networks is consistent with lexical growth processes in which children add new words to their vocabulary based on the structure of the language-learning environment. However, thus far, there is no direct evidence that a child’s individual semantic network structure is associated with their early language learning. Here we show that, while typically developing children’s early networks show small world structure as early as 15 months and with as few as 55 words, children with language delay (late talkers) have this structure to a smaller degree. This implicates a maladaptive bias in word acquisition for late talkers, potentially indicating a preference for ‘‘oddball’’ words. The findings provide the first evidence of a link between small-world connectivity and lexical development in individual children.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BCG8ME59\\Beckage et al. - 2011 - Small Worlds and Semantic Network Growth in Typica.pdf},
  langid = {english},
  number = {5}
}

@article{bendichPersistentHomologyAnalysis2016,
  title = {Persistent {{Homology Analysis}} of {{Brain Artery Trees}}},
  author = {Bendich, Paul and Marron, J. S. and Miller, Ezra and Pieloch, Alex and Skwerer, Sean},
  date = {2016},
  journaltitle = {The annals of applied statistics},
  shortjournal = {Ann Appl Stat},
  volume = {10},
  pages = {198--218},
  issn = {1932-6157},
  doi = {10.1214/15-AOAS886},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5026243/},
  urldate = {2020-03-11},
  abstract = {New representations of tree-structured data objects, using ideas from topological data analysis, enable improved statistical analyses of a population of brain artery trees. A number of representations of each data tree arise from persistence diagrams that quantify branching and looping of vessels at multiple scales. Novel approaches to the statistical analysis, through various summaries of the persistence diagrams, lead to heightened correlations with covariates such as age and sex, relative to earlier analyses of this data set. The correlation with age continues to be significant even after controlling for correlations from earlier significant summaries.},
  eprint = {27642379},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\GI5CR3WI\\Bendich et al. - 2016 - Persistent Homology Analysis of Brain Artery Trees.pdf},
  number = {1},
  pmcid = {PMC5026243}
}

@article{benusPragmaticAspectsTemporal2011,
  title = {Pragmatic Aspects of Temporal Accommodation in Turn-Taking},
  author = {Beňuš, Štefan and Gravano, Agustín and Hirschberg, Julia},
  date = {2011-09},
  journaltitle = {Journal of Pragmatics},
  volume = {43},
  pages = {3001--3027},
  issn = {03782166},
  doi = {10.1016/j.pragma.2011.05.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378216611001469},
  urldate = {2020-05-28},
  abstract = {This study investigates the relationship between the variability in the temporal alignment of turn initiations and the pragmatics of interpersonal communication. The data come from spontaneous, task-oriented dialogues in Standard American English. In addition to analyzing the temporal aspects of turn-taking behavior in general, we focus on the timing of turn-initial single word grounding responses such as mmhm, okay, or yeah, and conversational fillers such as um or uh. Based on qualitative and quantitative analyses of temporal and rhythmic alignment patterns, we propose that these patterns are linked to the achievement of pragmatic goals by interlocutors. More specifically, we examine the role of timing in establishing common ground, and test the hypothesis that the degree of accommodation to temporal and metrical characteristics of an interlocutor’s speech is one aspect of turn-taking behavior that signals asymmetrical dominance relationships between interlocutors. Our results show that dominance relationships linked to floorcontrol, as well as mutual common ground, are pragmatically constructed in part through the accommodation patterns in timing of turn-initial single word utterances.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\RY23RBIA\\Beňuš et al. - 2011 - Pragmatic aspects of temporal accommodation in tur.pdf},
  langid = {english},
  number = {12}
}

@article{bergesonAbsolutePitchTempo2002,
  title = {Absolute Pitch and Tempo in Mothers' Songs to Infants},
  author = {Bergeson, Tonya R. and Trehub, Sandra E.},
  date = {2002-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {13},
  pages = {72--75},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00413},
  abstract = {We examined the relative stability of pitch, tempo, and rhythm in maternal speech and singing to prelinguistic infants. Mothers were recorded speaking and singing to their infants on two occasions separated by 1 week or more. The pitch level and tempo of identical utterances were highly variable across the 1-week period, but these features were virtually unchanged in song repetitions. Rhythmic patterning was largely maintained in speech, as in song. Mothers' accurate reproduction of their sung performances can be considered a form of absolute pitch and absolute tempo.},
  eprint = {11892783},
  eprinttype = {pmid},
  keywords = {Communication,Humans,Infant,Mother-Child Relations,Mothers,Periodicity,Speech,Verbal Behavior},
  langid = {english},
  number = {1}
}

@article{bernardisSpeechGestureShare2006,
  title = {Speech and Gesture Share the Same Communication System},
  author = {Bernardis, Paolo and Gentilucci, Maurizio},
  date = {2006-01},
  journaltitle = {Neuropsychologia},
  volume = {44},
  pages = {178--190},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2005.05.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0028393205001892},
  urldate = {2020-03-17},
  abstract = {Humans speak and produce symbolic gestures. Do these two forms of communication interact, and how? First, we tested whether the two communication signals influenced each other when emitted simultaneously. Participants either pronounced words, or executed symbolic gestures, or emitted the two communication signals simultaneously. Relative to the unimodal conditions, multimodal voice spectra were enhanced by gestures, whereas multimodal gesture parameters were reduced by words. In other words, gesture reinforced word, whereas word inhibited gesture. In contrast, aimless arm movements and pseudo-words had no comparable effects. Next, we tested whether observing word pronunciation during gesture execution affected verbal responses in the same way as emitting the two signals. Participants responded verbally to either spoken words, or to gestures, or to the simultaneous presentation of the two signals. We observed the same reinforcement in the voice spectra as during simultaneous emission. These results suggest that spoken word and symbolic gesture are coded as single signal by a unique communication system. This signal represents the intention to engage a closer interaction with a hypothetical interlocutor and it may have a meaning different from when word and gesture are encoded singly.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HXN2VM6I\\Bernardis and Gentilucci - 2006 - Speech and gesture share the same communication sy.pdf},
  langid = {english},
  number = {2}
}

@book{bernsteinCoordinationRegulationsMovements1967,
  title = {The {{Co}}-Ordination and {{Regulations}} of {{Movements}}},
  author = {Bernstein, N.},
  date = {1967},
  edition = {[1st English ed.] edition},
  publisher = {{Pergamon Press}},
  langid = {english}
}

@article{bertranProsodicTypologyDichotomy1999,
  title = {Prosodic {{Typology}}: {{On}} the {{Dichotomy}} between {{Stress}}-{{Timed}} and {{Syllable}}-{{Timed Languages}}},
  author = {Bertran, A. P.},
  date = {1999},
  journaltitle = {Language Design},
  volume = {2},
  pages = {103--130}
}

@article{beugherSemiautomaticAnnotationTool2018,
  title = {A Semi-Automatic Annotation Tool for Unobtrusive Gesture Analysis},
  author = {Beugher, Stijn De and Brône, Geert and Goedemé, Toon},
  date = {2018-06},
  journaltitle = {Language Resources and Evaluation},
  volume = {52},
  pages = {433--460},
  issn = {1574-020X, 1574-0218},
  doi = {10.1007/s10579-017-9404-9},
  url = {http://link.springer.com/10.1007/s10579-017-9404-9},
  urldate = {2019-06-21},
  abstract = {In a variety of research fields, including linguistics, human–computer interaction research, psychology, sociology and behavioral studies, there is a growing interest in the role of gestural behavior related to speech and other modalities. The analysis of multimodal communication requires high-quality video data and detailed annotation of the different semiotic resources under scrutiny. In the majority of cases, the annotation of hand position, hand motion, gesture type, etc. is done manually, which is a time-consuming enterprise requiring multiple annotators and substantial resources. In this paper we present a semi-automatic alternative, in which the focus lies on minimizing the manual workload while guaranteeing highly accurate annotations. First, we discuss our approach, which consists of several processing steps such as identifying the hands in images, calculating motion of the hands, segmenting the recording in gesture and non-gesture events, etc. Second, we validate our approach against existing corpora in terms of accuracy and usefulness. The proposed approach is designed to provide annotations according to the McNeill (Hand and mind: what gestures reveal about thought, University of Chicago Press, Chicago, 1992) gesture space and the output is compatible with annotation tools such as ELAN or ANVIL.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\75PKN23Z\\Beugher et al. - 2018 - A semi-automatic annotation tool for unobtrusive g.pdf},
  langid = {english},
  number = {2}
}

@book{bickertonAdamTongue2009,
  title = {Adam's {{Tongue}}},
  author = {Bickerton, D.},
  date = {2009},
  publisher = {{Hill \& Wang}},
  location = {{New York}}
}

@article{bickhardLanguageInteractionSystem2007,
  title = {Language as an Interaction System},
  author = {Bickhard, Mark H.},
  date = {2007-08},
  journaltitle = {New Ideas in Psychology},
  volume = {25},
  pages = {171--187},
  issn = {0732118X},
  doi = {10.1016/j.newideapsych.2007.02.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0732118X07000244},
  urldate = {2020-02-25},
  abstract = {In this article I present a programmatic outline of a new kind of model of language, and offer some criticisms of standard approaches. The discussion begins with issues concerning representation because, so I argue, problems with standard approaches to representation are at the heart of notions of and problems with language.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZHWBLIFB\\Bickhard - 2007 - Language as an interaction system.pdf},
  langid = {english},
  number = {2}
}

@article{billonRoleSensoryInformation1996,
  title = {The Role of Sensory Information in the Production of Periodic Finger-Tapping Sequences},
  author = {Billon, M. and Semjen, A. and Cole, J. and Gauthier, G.},
  date = {1996-06-01},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {110},
  pages = {117--130},
  issn = {1432-1106},
  doi = {10.1007/BF00241381},
  url = {https://doi.org/10.1007/BF00241381},
  urldate = {2019-04-02},
  abstract = {A subject lacking proprioceptive and tactile sensibility below the neck and a group of control subjects performed sequences of periodic finger taps involving a pattern of accentuation. The required intertap interval was 700 ms. In some situations, the taps were synchronized with the clicks of a metronome. Feedback conditions were manipulated by either allowing or not allowing the subjects to hear the taps and see their finger movements. We recorded the trajectory of the subjects' finger displacement in the vertical plane, and the force and moment of contact of the finger with the response key. The control subjects achieved precise timing of the finger taps by trading off downstroke onset for movement duration, e.g., they initiated shorter-duration tapping movements with a delay. This strategy did not vary depending on task demands (e.g., synchronization) or feedback conditions. The deafferented patient produced intertap intervals on average close to the required value. However, his tap timing was characterized by increased variability and severe distortion (lengthening) after the accentuated tap, regardless of feedback conditions. He did not manifest the compensatory strategy whereby, in control subjects, movement onset was adjusted to movement duration. Thus, such a strategy in controls seems to depend on intact proprioceptive and/or tactile information from the moving limb. Upon withdrawal of visual and acoustic feedback, the deafferented subject increased the force of the taps and the amplitude of tapping movements; his mean synchronization error with the metronome also increased. However, he did not lose correct phasing between the taps and the clicks of the metronome. These findings suggest that, under normal circumstances, sequential movements are timed by an internal timekeeper which paces sensory consequences relating to the occurrence of behaviorally important events (e.g., finger taps), and not the onset of the movements eliciting those events. In the synchronization task, the timekeeper may be phase locked to the periodic acoustic stimuli by direct entrainment. Feedback information may be needed, however, for keeping any synchronization error as small as possible.},
  keywords = {Internal clock,Movement timing,Sequence timing,Synchronization},
  langid = {english},
  number = {1}
}

@article{billonSimultaneityTwoEffectors1996,
  title = {Simultaneity of Two Effectors in Synchronization with a Periodic External Signal},
  author = {Billon, M. and Bard, C. and Fleury, M. and Blouin, J. and Teasdale, N.},
  date = {1996-02-01},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {15},
  pages = {25--38},
  issn = {0167-9457},
  doi = {10.1016/0167-9457(95)00037-2},
  url = {http://www.sciencedirect.com/science/article/pii/0167945795000372},
  urldate = {2019-04-02},
  abstract = {Several studies have shown that the control of simultaneous movements differ according to the execution context. For instance, when subjects raise simultaneously their index finger and heel as fast as possible after an auditory signal, the simultaneity is controlled by sending synchronously the motor commands to both effectors. On the other hand, when subjects self-pace their movements, the simultaneity is controlled by processing the delay between afferent signals from both movements at the central level (Paillard, 1948). It has been hypothesized that a mode of control similar to the self-paced condition is also used when subjects produce simultaneous and repetitive movements in synchronization with a metronome (Fraisse, 1980). We examined this hypothesis by asking subjects to move simultaneously the index finger and heel in synchronization with metronome sounds. Results showed that the events chronology (i.e., heel movement first, finger movement second and metronome sound third) was a function of the relative distance of the effectors and auditory organ from the central comparator. We deduced that the synchronization and simultaneity was evaluated by computing the time elapsed between the arrival of the sensory feedback of the movement and auditory signal. The second goal of the study was to assess whether, in such a task, each effector is synchronized separately to the metronome sound or together as an unit. A strong positive correlation was found between finger and heel synchronization errors. This supports the hypothesis that finger and heel movements are synchronized as an unit to the metronome rather than independently. In conclusion, simultaneity between effectors and synchronization between effectors and an external signal, although they share similar processes based on afferent information, are likely to be controlled separately.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TQNZ46K7\\0167945795000372.html},
  keywords = {Movement sequence,Sensorimotor process,Tapping,Temporal coordination,Timing},
  number = {1}
}

@article{blasiHumanSoundSystems2019,
  title = {Human Sound Systems Are Shaped by Post-{{Neolithic}} Changes in Bite Configuration},
  author = {Blasi, D. E. and Moran, S. and Moisik, S. R. and Widmer, P. and Dediu, D. and Bickel, B.},
  date = {2019-03-15},
  journaltitle = {Science},
  volume = {363},
  pages = {eaav3218},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aav3218},
  url = {https://science.sciencemag.org/content/363/6432/eaav3218},
  urldate = {2019-05-04},
  abstract = {The first fricatives
In 1985, the linguist Charles Hockett proposed that the use of teeth and jaws as tools in hunter-gatherer populations makes consonants produced with lower lip and upper teeth (“f” and “v” sounds) hard to produce. He thus conjectured that these sounds were a recent innovation in human language. Blasi et al. combined paleoanthropology, speech sciences, historical linguistics, and methods from evolutionary biology to provide evidence for a Neolithic global change in the sound systems of the world's languages. Spoken languages have thus been shaped by changes in the human bite configuration owing to changes in dietary and behavioral practices since the Neolithic.
Science, this issue p. eaav3218
Structured Abstract
INTRODUCTIONHuman speech manifests itself in spectacular diversity, ranging from ubiquitous sounds such as “m” and “a” to the rare click consonants in some languages of southern Africa. This range is generally thought to have been fixed by biological constraints since at least the emergence of Homo sapiens. At the same time, the abundance of each sound in the languages of the world is commonly taken to depend on how easy the sound is to produce, perceive, and learn. This dependency is also regarded as fixed at the species level.
RATIONALEGiven this dependency, we expect that any change in the human apparatus for production, perception, or learning affects the probability—or even the range—of the sounds that languages have. Paleoanthropological evidence suggests that the production apparatus has undergone a fundamental change of just this kind since the Neolithic. Although humans generally start out with vertical and horizontal overlap in their bite configuration (overbite and overjet, respectively), masticatory exertion in the Paleolithic gave rise to an edge-to-edge bite after adolescence. Preservation of overbite and overjet began to persist long into adulthood only with the softer diets that started to become prevalent in the wake of agriculture and intensified food processing. We hypothesize that this post-Neolithic decline of edge-to-edge bite enabled the innovation and spread of a new class of speech sounds that is now present in nearly half of the world’s languages: labiodentals, produced by positioning the lower lip against the upper teeth, such as in “f” or “v.”
RESULTSBiomechanical models of the speech apparatus show that labiodentals incur about 30\% less muscular effort in the overbite and overjet configuration than in the edge-to-edge bite configuration. This difference is not present in similar articulations that place the upper lip, instead of the teeth, against the lower lip (as in bilabial “m,” “w,” or “p”). Our models also show that the overbite and overjet configuration reduces the incidental tooth/lip distance in bilabial articulations to 24 to 70\% of their original values, inviting accidental production of labiodentals. The joint effect of a decrease in muscular effort and an increase in accidental production predicts a higher probability of labiodentals in the language of populations where overbite and overjet persist into adulthood. When the persistence of overbite and overjet in a population is approximated by the prevalence of agriculturally produced food, we find that societies described as hunter-gatherers indeed have, on average, only about one-fourth the number of labiodentals exhibited by food-producing societies, after controlling for spatial and phylogenetic correlation. When the persistence is approximated by the increase in food-processing technology over the history of one well-researched language family, Indo-European, we likewise observe a steady increase of the reconstructed probability of labiodental sounds, from a median estimate of about 3\% in the proto-language (6000 to 8000 years ago) to a presence of 76\% in extant languages.
CONCLUSIONOur findings reveal that the transition from prehistoric foragers to contemporary societies has had an impact on the human speech apparatus, and therefore on our species’ main mode of communication and social differentiation: spoken language. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/363/6432/eaav3218/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Labiodentals depend on bite configuration.Biomechanical modeling shows that labiodental sounds like “f” are easier to produce (and to accidentally arise) under overbite and overjet (A) than under the edge-to-edge bite (B) that prevailed before the Neolithic (C). Overbite and overjet persisted only when exposed to the softer diets that became characteristic with food production (D versus E) and more recently with intensified food processing (F). Both developments led to a spread of labiodental sounds.
Linguistic diversity, now and in the past, is widely regarded to be independent of biological changes that took place after the emergence of Homo sapiens. We show converging evidence from paleoanthropology, speech biomechanics, ethnography, and historical linguistics that labiodental sounds (such as “f” and “v”) were innovated after the Neolithic. Changes in diet attributable to food-processing technologies modified the human bite from an edge-to-edge configuration to one that preserves adolescent overbite and overjet into adulthood. This change favored the emergence and maintenance of labiodentals. Our findings suggest that language is shaped not only by the contingencies of its history, but also by culturally induced changes in human biology.
Diet-induced changes in the human bite over recent millennia led to the spread of new speech sounds, including “f” and “v.”
Diet-induced changes in the human bite over recent millennia led to the spread of new speech sounds, including “f” and “v.”},
  eprint = {30872490},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SNSELIZ9\\eaav3218.html},
  langid = {english},
  number = {6432}
}

@article{boerAcousticModelsOrangutan2015,
  title = {Acoustic Models of Orangutan Hand-Assisted Alarm Calls},
  author = {de Boer, Bart and Wich, Serge A. and Hardus, Madeleine E. and Lameira, Adriano R.},
  date = {2015-03-15},
  journaltitle = {Journal of Experimental Biology},
  volume = {218},
  pages = {907--914},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.110577},
  url = {https://jeb.biologists.org/content/218/6/907},
  urldate = {2020-04-23},
  abstract = {Skip to Next Section
Orangutans produce alarm calls called kiss-squeaks, which they sometimes modify by putting a hand in front of their mouth. Through theoretical models and observational evidence, we show that using the hand when making a kiss-squeak alters the acoustics of the production in such a way that more formants per kilohertz are produced. Our theoretical models suggest that cylindrical wave propagation is created with the use of the hand and face as they act as a cylindrical extension of the lips. The use of cylindrical wave propagation in animal calls appears to be extremely rare, but is an effective way to lengthen the acoustic system; it causes the number of resonances per kilohertz to increase. This increase is associated with larger animals, and thus using the hand in kiss-squeak production may be effective in exaggerating the size of the producer. Using the hand appears to be a culturally learned behavior, and therefore orangutans may be able to associate the acoustic effect of using the hand with potentially more effective deterrence of predators.},
  eprint = {25788727},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\6ZLCTNCG\\Boer et al. - 2015 - Acoustic models of orangutan hand-assisted alarm c.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\WVTNFPWB\\907.html},
  langid = {english},
  number = {6}
}

@software{boersmaPraatDoingPhonetics2019,
  title = {Praat: Doing Phonetics by Computer [{{Computer}} Program]},
  author = {Boersma, P. and Weenink, D.},
  date = {2019},
  url = {http://www.praat.org/},
  version = {6.1.03}
}

@article{boeWhichWayDawn2019,
  title = {Which Way to the Dawn of Speech?: {{Reanalyzing}} Half a Century of Debates and Data in Light of Speech Science},
  shorttitle = {Which Way to the Dawn of Speech?},
  author = {Boë, Louis-Jean and Sawallis, Thomas R. and Fagot, Joël and Badin, Pierre and Barbier, Guillaume and Captier, Guillaume and Ménard, Lucie and Heim, Jean-Louis and Schwartz, Jean-Luc},
  date = {2019-12-01},
  journaltitle = {Science Advances},
  volume = {5},
  pages = {eaaw3916},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aaw3916},
  url = {https://advances.sciencemag.org/content/5/12/eaaw3916},
  urldate = {2020-01-03},
  abstract = {Recent articles on primate articulatory abilities are revolutionary regarding speech emergence, a crucial aspect of language evolution, by revealing a human-like system of proto-vowels in nonhuman primates and implicitly throughout our hominid ancestry. This article presents both a schematic history and the state of the art in primate vocalization research and its importance for speech emergence. Recent speech research advances allow more incisive comparison of phylogeny and ontogeny and also an illuminating reinterpretation of vintage primate vocalization data. This review produces three major findings. First, even among primates, laryngeal descent is not uniquely human. Second, laryngeal descent is not required to produce contrasting formant patterns in vocalizations. Third, living nonhuman primates produce vocalizations with contrasting formant patterns. Thus, evidence now overwhelmingly refutes the long-standing laryngeal descent theory, which pushes back “the dawn of speech” beyond \textasciitilde 200 ka ago to over \textasciitilde 20 Ma ago, a difference of two orders of magnitude.
Fresh analysis of primate calls shows that speech dawned in monkeys some 100 times earlier than the appearance of modern humans.
Fresh analysis of primate calls shows that speech dawned in monkeys some 100 times earlier than the appearance of modern humans.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\G3TPISG8\\Boë et al. - 2019 - Which way to the dawn of speech Reanalyzing half.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AXFU8KRA\\eaaw3916.html},
  langid = {english},
  number = {12}
}

@article{bohnYoungChildrenSpontaneously2019,
  title = {Young Children Spontaneously Recreate Core Properties of Language in a New Modality},
  author = {Bohn, Manuel and Kachel, Gregor and Tomasello, Michael},
  date = {2019-11-27},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1904871116},
  url = {https://www.pnas.org/content/early/2019/11/26/1904871116},
  urldate = {2019-12-03},
  abstract = {How the world’s 6,000+ natural languages have arisen is mostly unknown. Yet, new sign languages have emerged recently among deaf people brought together in a community, offering insights into the dynamics of language evolution. However, documenting the emergence of these languages has mostly consisted of studying the end product; the process by which ad hoc signs are transformed into a structured communication system has not been directly observed. Here we show how young children create new communication systems that exhibit core features of natural languages in less than 30 min. In a controlled setting, we blocked the possibility of using spoken language. In order to communicate novel messages, including abstract concepts, dyads of children spontaneously created novel gestural signs. Over usage, these signs became increasingly arbitrary and conventionalized. When confronted with the need to communicate more complex meanings, children began to grammatically structure their gestures. Together with previous work, these results suggest that children have the basic skills necessary, not only to acquire a natural language, but also to spontaneously create a new one. The speed with which children create these structured systems has profound implications for theorizing about language evolution, a process which is generally thought to span across many generations, if not millennia.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\B342IPFB\\Bohn et al. - 2019 - Young children spontaneously recreate core propert.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\MXCZ975A\\1904871116.html},
  keywords = {cognitive development,evolution,gesture,language},
  langid = {english}
}

@article{bordoniFascialBreath,
  title = {The {{Fascial Breath}}},
  author = {Bordoni, Bruno and Simonelli, Marta and Morabito, Bruno},
  journaltitle = {Cureus},
  shortjournal = {Cureus},
  volume = {11},
  issn = {2168-8184},
  doi = {10.7759/cureus.5208},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6758955/},
  urldate = {2020-04-03},
  abstract = {The word diaphragm comes from the Greek (διάϕραγμα), which meant something that divides, but also expressed a concept related to emotions and intellect. Breath is part of a concept of symmorphosis, that is the maximum ability to adapt to multiple functional questions in a defined biological context. The act of breathing determines and defines our holobiont: how we react and who we are. The article reviews the fascial structure that involves and forms the diaphragm muscle with the aim of changing the vision of this complex muscle: from an anatomical and mechanistic form to a fractal and asynchronous form. Another step forward for understanding the diaphragm muscle is that it is not only covered, penetrated and made up of connective tissue, but the contractile tissue itself is a fascial tissue~with the same embryological derivation. All the diaphragm muscle is fascia.},
  eprint = {31565613},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YYLN4BER\\Bordoni et al. - The Fascial Breath.pdf},
  number = {7},
  pmcid = {PMC6758955}
}

@article{bosbachInferringAnotherExpectation2005,
  title = {Inferring Another's Expectation from Action: The Role of Peripheral Sensation},
  shorttitle = {Inferring Another's Expectation from Action},
  author = {Bosbach, S. and Cole, J. D. and Prinz, W. and Knoblich, G.},
  date = {2005-10},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat. Neurosci.},
  volume = {8},
  pages = {1295--1297},
  issn = {1097-6256},
  doi = {10.1038/nn1535},
  abstract = {It is unclear how knowledge of one's actions and one's body contribute to the understanding of others' actions. Here we show that two subjects lacking cutaneous touch and sense of movement and position show a selective deficit in interpreting another person's anticipation of weight when seeing him lifting boxes. We suggest that this ability occurs through mental simulation of action dependent on internal motor representations, which require peripheral sensation for their maintenance.},
  eprint = {16136040},
  eprinttype = {pmid},
  keywords = {Female,Forecasting,Humans,Judgment,Kinesthesis,Male,Mental Processes,Sensation,Sensory Deprivation,Weight Lifting},
  langid = {english},
  number = {10}
}

@article{brambleRunningBreathingMammals1983,
  title = {Running and Breathing in Mammals},
  author = {Bramble, D. and Carrier, D.},
  date = {1983-01-21},
  journaltitle = {Science},
  volume = {219},
  pages = {251--256},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.6849136},
  url = {https://www.sciencemag.org/lookup/doi/10.1126/science.6849136},
  urldate = {2020-02-25},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XPHURPTJ\\Bramble and Carrier - 1983 - Running and breathing in mammals.pdf},
  langid = {english},
  number = {4582}
}

@article{brookshireVisualCortexEntrains2017,
  title = {Visual Cortex Entrains to Sign Language},
  author = {Brookshire, Geoffrey and Lu, Jenny and Nusbaum, Howard C. and Goldin-Meadow, Susan and Casasanto, Daniel},
  date = {2017-05-24},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  pages = {201620350},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1620350114},
  url = {https://www.pnas.org/content/early/2017/05/23/1620350114},
  urldate = {2019-08-22},
  abstract = {Despite immense variability across languages, people can learn to understand any human language, spoken or signed. What neural mechanisms allow people to comprehend language across sensory modalities? When people listen to speech, electrophysiological oscillations in auditory cortex entrain to slow ({$<<<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>\&$}lt;{$<$}/mml:mo{$><$}/mml:math{$>$}8 Hz) fluctuations in the acoustic envelope. Entrainment to the speech envelope may reflect mechanisms specialized for auditory perception. Alternatively, flexible entrainment may be a general-purpose cortical mechanism that optimizes sensitivity to rhythmic information regardless of modality. Here, we test these proposals by examining cortical coherence to visual information in sign language. First, we develop a metric to quantify visual change over time. We find quasiperiodic fluctuations in sign language, characterized by lower frequencies than fluctuations in speech. Next, we test for entrainment of neural oscillations to visual change in sign language, using electroencephalography (EEG) in fluent speakers of American Sign Language (ASL) as they watch videos in ASL. We find significant cortical entrainment to visual oscillations in sign language {$<$}5 Hz, peaking at ∼∼{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>$}∼{$<$}/mml:mo{$><$}/mml:math{$>$}1 Hz. Coherence to sign is strongest over occipital and parietal cortex, in contrast to speech, where coherence is strongest over the auditory cortex. Nonsigners also show coherence to sign language, but entrainment at frontal sites is reduced relative to fluent signers. These results demonstrate that flexible cortical entrainment to language does not depend on neural processes that are specific to auditory speech perception. Low-frequency oscillatory entrainment may reflect a general cortical mechanism that maximizes sensitivity to informational peaks in time-varying signals.},
  eprint = {28559320},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ECNYBPH7\\Brookshire et al. - 2017 - Visual cortex entrains to sign language.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\WUCDMU8F\\1620350114.html},
  keywords = {cortical entrainment,EEG,oscillations,sign language},
  langid = {english}
}

@article{brookshireVisualCortexEntrains2017a,
  title = {Visual Cortex Entrains to Sign Language},
  author = {Brookshire, Geoffrey and Lu, Jenny and Nusbaum, Howard C. and Goldin-Meadow, Susan and Casasanto, Daniel},
  date = {2017-06-13},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {114},
  pages = {6352--6357},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1620350114},
  url = {https://www.pnas.org/content/114/24/6352},
  urldate = {2020-03-09},
  abstract = {Despite immense variability across languages, people can learn to understand any human language, spoken or signed. What neural mechanisms allow people to comprehend language across sensory modalities? When people listen to speech, electrophysiological oscillations in auditory cortex entrain to slow ({$<<<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>\&$}lt;{$<$}/mml:mo{$><$}/mml:math{$>$}8 Hz) fluctuations in the acoustic envelope. Entrainment to the speech envelope may reflect mechanisms specialized for auditory perception. Alternatively, flexible entrainment may be a general-purpose cortical mechanism that optimizes sensitivity to rhythmic information regardless of modality. Here, we test these proposals by examining cortical coherence to visual information in sign language. First, we develop a metric to quantify visual change over time. We find quasiperiodic fluctuations in sign language, characterized by lower frequencies than fluctuations in speech. Next, we test for entrainment of neural oscillations to visual change in sign language, using electroencephalography (EEG) in fluent speakers of American Sign Language (ASL) as they watch videos in ASL. We find significant cortical entrainment to visual oscillations in sign language {$<$}5 Hz, peaking at ∼∼{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>$}∼{$<$}/mml:mo{$><$}/mml:math{$>$}1 Hz. Coherence to sign is strongest over occipital and parietal cortex, in contrast to speech, where coherence is strongest over the auditory cortex. Nonsigners also show coherence to sign language, but entrainment at frontal sites is reduced relative to fluent signers. These results demonstrate that flexible cortical entrainment to language does not depend on neural processes that are specific to auditory speech perception. Low-frequency oscillatory entrainment may reflect a general cortical mechanism that maximizes sensitivity to informational peaks in time-varying signals.},
  eprint = {28559320},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZW55PX63\\Brookshire et al. - 2017 - Visual cortex entrains to sign language.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\K3USRCKB\\6352.html},
  isbn = {9781620350119},
  keywords = {cortical entrainment,EEG,oscillations,sign language},
  langid = {english},
  number = {24}
}

@article{byrneGreatApeGestures2017,
  title = {Great Ape Gestures: Intentional Communication with a Rich Set of Innate Signals},
  shorttitle = {Great Ape Gestures},
  author = {Byrne, R. W. and Cartmill, E. and Genty, E. and Graham, K. E. and Hobaiter, C. and Tanner, J.},
  date = {2017},
  journaltitle = {Animal Cognition},
  shortjournal = {Anim Cogn},
  volume = {20},
  pages = {755--769},
  issn = {1435-9448},
  doi = {10.1007/s10071-017-1096-4},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5486474/},
  urldate = {2019-11-18},
  abstract = {Great apes give gestures deliberately and voluntarily, in order to influence particular target audiences, whose direction of attention they take into account when choosing which type of gesture to use. These facts make the study of ape gesture directly relevant to understanding the evolutionary precursors of human language; here we present an assessment of ape gesture from that perspective, focusing on the work of the “St Andrews Group” of researchers. Intended meanings of ape gestures are relatively few and simple. As with human words, ape gestures often have several distinct meanings, which are effectively disambiguated by behavioural context. Compared to the signalling of most other animals, great ape gestural repertoires are large. Because of this, and the relatively small number of intended meanings they achieve, ape gestures are redundant, with extensive overlaps in meaning. The great majority of gestures are innate, in the sense that the species’ biological inheritance includes the potential to develop each gestural form and use it for a specific range of purposes. Moreover, the phylogenetic origin of many gestures is relatively old, since gestures are extensively shared between different genera in the great ape family. Acquisition of an adult repertoire is a process of first exploring the innate species potential for many gestures and then gradual restriction to a final (active) repertoire that is much smaller. No evidence of syntactic structure has yet been detected.},
  eprint = {28502063},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BRYNHC2D\\Byrne et al. - 2017 - Great ape gestures intentional communication with.pdf},
  number = {4},
  pmcid = {PMC5486474}
}

@collection{cangelosiSimulatingEvolutionLanguage2002,
  title = {Simulating the {{Evolution}} of {{Language}}},
  editor = {Cangelosi, Angelo and Parisi, Domenico},
  date = {2002},
  publisher = {{Springer-Verlag}},
  location = {{London}},
  doi = {10.1007/978-1-4471-0663-0},
  url = {https://www.springer.com/gp/book/9781852334284},
  urldate = {2020-03-16},
  abstract = {This book is the first to provide a comprehensive survey of the computational models and methodologies used for studying the evolution and origin of language and communication. Comprising contributions from the most influential figures in the field, it presents and summarises the state-of-the-art in computational approaches to language evolution, and highlights new lines of development.Essential reading for researchers and students in the fields of evolutionary and adaptive systems, language evolution modelling and linguistics, it will also be of interest to researchers working on applications of neural networks to language problems. Furthermore, due to the fact that language evolution models use multi-agent methodologies, it will also be of great interest to computer scientists working on multi-agent systems, robotics and internet agents.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BK5ELUDL\\9781852334284.html},
  isbn = {978-1-85233-428-4},
  langid = {english}
}

@inproceedings{caoRealtimeMultiPerson2D2017,
  title = {Realtime {{Multi}}-{{Person 2D Pose Estimation Using Part Affinity Fields}}},
  author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2017},
  pages = {7291--7299},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html},
  urldate = {2019-04-17},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\F9E2SARJ\\Cao et al. - 2017 - Realtime Multi-Person 2D Pose Estimation Using Par.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\LC5IZ8SB\\Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html}
}

@inproceedings{caoRealtimeMultiperson2D2017,
  title = {Realtime {{Multi}}-Person {{2D Pose Estimation Using Part Affinity Fields}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2017-07},
  pages = {1302--1310},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.143},
  url = {http://ieeexplore.ieee.org/document/8099626/},
  urldate = {2019-12-11},
  abstract = {We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII MultiPerson benchmark, both in performance and efficiency.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\86MLSR4E\\Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf},
  isbn = {978-1-5386-0457-1},
  langid = {english}
}

@inproceedings{carelloEcologicalAcousticsAcoustic2001,
  title = {Ecological Acoustics 1 {{Acoustic Specification}} of {{Object Properties}}},
  author = {Carello, C. and Wagman, J. B. and Turvey, M. T.},
  date = {2001},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DNTEG4F7\\Carello et al. - 2001 - Ecological acoustics 1 Acoustic Specification of O.pdf},
  keywords = {Acoustics}
}

@article{carelloPerceptionObjectLength1998,
  title = {Perception of Object Length by Sound},
  author = {Carello, Claudia and Anderson, Krista L. and Kunkler-Peck, Andrew J.},
  date = {1998},
  journaltitle = {Psychological Science},
  volume = {9},
  pages = {211--214},
  issn = {1467-9280(Electronic),0956-7976(Print)},
  doi = {10.1111/1467-9280.00040},
  abstract = {The goal of the present research was to provide an empirical evaluation of the basic capability of size perception by sound (in particular, perception of the lengths of dropped wooden dowels) and to identify the physical properties of the objects that constrain that perception. In Exp 1, 8 Ss were told to listen to a rod dropped 5 times and move the adjustable surface out from the proximal edge of the desk to a position that could just be reached with the rod, so its length would fit between the desk and the surface. The location of the surface was recorded. The same procedure was followed in a 2nd experiment using 6 Ss. Results show the ordinal and metrical success of naive listeners was related to length but not to the simple acoustic variables (duration, amplitude, frequency) likely to be related to it. Additional analysis suggests the potential relevance of an object's inertia tensor in constraining perception of that object's length, analogous to the case that has been made for perceiving length by effortful touch. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CJAAPZYY\\Carello et al. - 1998 - Perception of object length by sound.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AXRWYI7M\\1998-02495-009.html},
  keywords = {Auditory Perception,Auditory Stimulation,Stimulus Parameters},
  number = {3}
}

@article{carrierEnergeticParadoxHuman1984,
  title = {The {{Energetic Paradox}} of {{Human Running}} and {{Hominid Evolution}} [and {{Comments}} and {{Reply}}]},
  author = {Carrier, David R. and Kapoor, A. K. and Kimura, Tasuku and Nickels, Martin K. and {Satwanti} and Scott, Eugenie C. and So, Joseph K. and Trinkaus, Erik},
  date = {1984},
  journaltitle = {Current Anthropology},
  volume = {25},
  pages = {483--495},
  issn = {0011-3204},
  url = {https://www.jstor.org/stable/2742907},
  urldate = {2020-02-25},
  abstract = {The energetic cost of running is relatively high in man. In spite of this, humans are adept endurance runners, capable of running down, for example, zebra and kangaroo. Distance running is made possible for man in part by an exceptional ability to dissipate exercise heat loads. Most mammals lose heat by panting, which is coupled to breathing and locomotor cycles during running. This interdependence may limit the effectiveness of panting as a means of heat dissipation. Because sweating is not dependent on respiration, it may be more compatible with running as a thermoregulatory mechanism. Furthermore, man's lack of body hair improves thermal conductance while running, as it facilitates convection at the skin surface. While horses, for example, have been shown to possess energetically optimal speeds in each gait, the energetic cost for a man to run a given distance does not change with speed. It is hypothesized that this is because bipedality allows breathing frequency to vary relative to stride frequency. Man's constant cost of transport may enable human hunters to pursue the prey animal at speeds that force it to run inefficiently, thereby expediting its eventual fatigue. Given what is known of heat dissipation in Old World Anthropoidea, the bipedality of early hominids, and human exercise physiology, one factor important in the origin of the Hominidae may have been the occupation of a new niche as a diurnal endurance predator.},
  number = {4}
}

@article{carrierEnergeticParadoxHuman1984a,
  title = {The {{Energetic Paradox}} of {{Human Running}} and {{Hominid Evolution}} [and {{Comments}} and {{Reply}}]},
  author = {Carrier, David R. and Kapoor, A. K. and Kimura, Tasuku and Nickels, Martin K. and {Satwanti} and Scott, Eugenie C. and So, Joseph K. and Trinkaus, Erik},
  date = {1984},
  journaltitle = {Current Anthropology},
  volume = {25},
  pages = {483--495},
  publisher = {{[University of Chicago Press, Wenner-Gren Foundation for Anthropological Research]}},
  issn = {0011-3204},
  url = {https://www.jstor.org/stable/2742907},
  urldate = {2020-04-21},
  abstract = {The energetic cost of running is relatively high in man. In spite of this, humans are adept endurance runners, capable of running down, for example, zebra and kangaroo. Distance running is made possible for man in part by an exceptional ability to dissipate exercise heat loads. Most mammals lose heat by panting, which is coupled to breathing and locomotor cycles during running. This interdependence may limit the effectiveness of panting as a means of heat dissipation. Because sweating is not dependent on respiration, it may be more compatible with running as a thermoregulatory mechanism. Furthermore, man's lack of body hair improves thermal conductance while running, as it facilitates convection at the skin surface. While horses, for example, have been shown to possess energetically optimal speeds in each gait, the energetic cost for a man to run a given distance does not change with speed. It is hypothesized that this is because bipedality allows breathing frequency to vary relative to stride frequency. Man's constant cost of transport may enable human hunters to pursue the prey animal at speeds that force it to run inefficiently, thereby expediting its eventual fatigue. Given what is known of heat dissipation in Old World Anthropoidea, the bipedality of early hominids, and human exercise physiology, one factor important in the origin of the Hominidae may have been the occupation of a new niche as a diurnal endurance predator.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\L52MCDN2\\Carrier et al. - 1984 - The Energetic Paradox of Human Running and Hominid.pdf},
  number = {4}
}

@article{chandrasekaranNaturalStatisticsAudiovisual2009,
  title = {The Natural Statistics of Audiovisual Speech},
  author = {Chandrasekaran, Chandramouli and Trubanova, Andrea and Stillittano, Sébastien and Caplier, Alice and Ghazanfar, Asif A.},
  editor = {Friston, Karl J.},
  date = {2009-07-17},
  journaltitle = {PLoS Computational Biology},
  volume = {5},
  pages = {e1000436},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000436},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1000436},
  urldate = {2019-07-11},
  abstract = {Humans, like other animals, are exposed to a continuous stream of signals, which are dynamic, multimodal, extended, and time varying in nature. This complex input space must be transduced and sampled by our sensory systems and transmitted to the brain where it can guide the selection of appropriate actions. To simplify this process, it’s been suggested that the brain exploits statistical regularities in the stimulus space. Tests of this idea have largely been confined to unimodal signals and natural scenes. One important class of multisensory signals for which a quantitative input space characterization is unavailable is human speech. We do not understand what signals our brain has to actively piece together from an audiovisual speech stream to arrive at a percept versus what is already embedded in the signal structure of the stream itself. In essence, we do not have a clear understanding of the natural statistics of audiovisual speech. In the present study, we identified the following major statistical features of audiovisual speech. First, we observed robust correlations and close temporal correspondence between the area of the mouth opening and the acoustic envelope. Second, we found the strongest correlation between the area of the mouth opening and vocal tract resonances. Third, we observed that both area of the mouth opening and the voice envelope are temporally modulated in the 2–7 Hz frequency range. Finally, we show that the timing of mouth movements relative to the onset of the voice is consistently between 100 and 300 ms. We interpret these data in the context of recent neural theories of speech which suggest that speech communication is a reciprocally coupled, multisensory event, whereby the outputs of the signaler are matched to the neural processes of the receiver.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QWRX4JBI\\Chandrasekaran et al. - 2009 - The Natural Statistics of Audiovisual Speech.pdf},
  langid = {english},
  number = {7}
}

@article{changMutualInteractionsSpeech1987,
  title = {Mutual Interactions between Speech and Finger Movements},
  author = {Chang, P. and Hammond, G. R.},
  date = {1987-06},
  journaltitle = {Journal of Motor Behavior},
  shortjournal = {J Mot Behav},
  volume = {19},
  pages = {265--274},
  issn = {0022-2895},
  doi = {10.1080/00222895.1987.10735411},
  abstract = {Speech output and finger movements were recorded as right-handed males repeated a syllable while making cyclical finger movements in three experimental conditions: (2) maintaining constant amplitude in both response systems; (b) alternating speech amplitude while attempting to maintain constant finger movement amplitude; and (c) alternating finger movement amplitude while attempting to maintain constant speech amplitude. Observations showed that output of the two response systems was coupled (one syllable was uttered with each finger movement) and entrained in amplitude (the amplitude pattern of the response that the subject attempted to keep constant followed that of the concurrently-active amplitude-modulated response). These interactions were bidirectional and were present with both left-handed and right-handed finger movements. The interactions are more extensive and subtle than mere interference wtih one response system by the other, and apparently do not depend on anatomical overlap of the responding neural systems.},
  eprint = {14988062},
  eprinttype = {pmid},
  langid = {english},
  number = {2}
}

@article{chanNetworkStructureInfluences,
  title = {Network {{Structure Influences Speech Production}}},
  author = {Chan, K. Y. and Vitevich, M. S.},
  journaltitle = {Cognitive Science},
  volume = {43},
  pages = {685--697},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01100.x},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9T6NVBGM\\j.1551-6709.2010.01100.html}
}

@article{charltonAreMenBetter2013,
  title = {Are Men Better than Women at Acoustic Size Judgements?},
  author = {Charlton, Benjamin D. and Taylor, Anna M. and Reby, David},
  date = {2013-08-23},
  journaltitle = {Biology Letters},
  shortjournal = {Biology Letters},
  volume = {9},
  pages = {20130270},
  doi = {10.1098/rsbl.2013.0270},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsbl.2013.0270},
  urldate = {2019-10-17},
  abstract = {Formants are important phonetic elements of human speech that are also used by humans and non-human mammals to assess the body size of potential mates and rivals. As a consequence, it has been suggested that formant perception, which is crucial for speech perception, may have evolved through sexual selection. Somewhat surprisingly, though, no previous studies have examined whether sexes differ in their ability to use formants for size evaluation. Here, we investigated whether men and women differ in their ability to use the formant frequency spacing of synthetic vocal stimuli to make auditory size judgements over a wide range of fundamental frequencies (the main determinant of vocal pitch). Our results reveal that men are significantly better than women at comparing the apparent size of stimuli, and that lower pitch improves the ability of both men and women to perform these acoustic size judgements. These findings constitute the first demonstration of a sex difference in formant perception, and lend support to the idea that acoustic size normalization, a crucial prerequisite for speech perception, may have been sexually selected through male competition. We also provide the first evidence that vocalizations with relatively low pitch improve the perception of size-related formant information.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\JMAPCEK8\\Charlton et al. - 2013 - Are men better than women at acoustic size judgeme.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ADS7BQVN\\rsbl.2013.html},
  number = {4}
}

@book{christiansenCreatingLanguageIntegrating2016,
  title = {Creating {{Language}}: {{Integrating Evolution}}, {{Acquisition}}, and {{Processing}}},
  author = {Christiansen, M. H. and Chater, Nick and Culicover, P.},
  date = {2016},
  publisher = {{MIT Press}},
  location = {{Massachusetts}}
}

@article{christiansenNoworNeverBottleneckFundamental2016,
  title = {The {{Now}}-or-{{Never}} Bottleneck: {{A}} Fundamental Constraint on Language},
  shorttitle = {The {{Now}}-or-{{Never}} Bottleneck},
  author = {Christiansen, M. H. and Chater, N.},
  date = {2016},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {39},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X1500031X},
  url = {https://www.cambridge.org/core/product/identifier/S0140525X1500031X/type/journal_article},
  urldate = {2020-03-16},
  abstract = {Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this “Now-or-Never” bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This observation has strong implications for the nature of language processing: (1) the language system must “eagerly” recode and compress linguistic input; (2) as the bottleneck recurs at each new representational level, the language system must build a multilevel linguistic representation; and (3) the language system must deploy all available information predictively to ensure that local linguistic ambiguities are dealt with “Right-First-Time”; once the original input is lost, there is no way for the language system to recover. This is “Chunk-and-Pass” processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Moreover, this perspective provides a cognitive foundation for grammaticalization and other aspects of language change. Chunk-and-Pass processing also helps explain a variety of core properties of language, including its multilevel representational structure and duality of patterning. This approach promises to create a direct relationship between psycholinguistics and linguistic theory. More generally, we outline a framework within which to integrate often disconnected inquiries into language processing, language acquisition, and language change and evolution.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\A86QWSVA\\Christiansen and Chater - 2016 - The Now-or-Never bottleneck A fundamental constra.pdf},
  langid = {english}
}

@article{chuSynchronizationSpeechGesture2014,
  title = {Synchronization of Speech and Gesture: {{Evidence}} for Interaction in Action},
  author = {Chu, M. and Hagoort, P.},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {143},
  doi = {10.1037/a0036281},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3V5X83P7\\Chu and Hagoort - Synchronization of Speech and Gesture Evidence fo.pdf},
  langid = {english},
  number = {3}
}

@article{claidiereCulturalEvolutionSystematically2014,
  title = {Cultural Evolution of Systematically Structured Behaviour in a Non-Human Primate},
  author = {Claidière, Nicolas and Smith, Kenny and Kirby, Simon and Fagot, Joël},
  date = {2014-12-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {281},
  pages = {20141541},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2014.1541},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2014.1541},
  urldate = {2020-03-16},
  abstract = {Culture pervades human life and is at the origin of the success of our species. A wide range of other animals have culture too, but often in a limited form that does not complexify through the gradual accumulation of innovations. We developed a new paradigm to study cultural evolution in primates in order to better evaluate our closest relatives' cultural capacities. Previous studies using transmission chain experimental paradigms, in which the behavioural output of one individual becomes the target behaviour for the next individual in the chain, show that cultural transmission can lead to the progressive emergence of systematically structured behaviours in humans. Inspired by this work, we combined a pattern reproduction task on touch screens with an iterated learning procedure to develop transmission chains of baboons (Papio papio). Using this procedure, we show that baboons can exhibit three fundamental aspects of human cultural evolution: a progressive increase in performance, the emergence of systematic structure and the presence of lineage specificity. Our results shed new light on human uniqueness: we share with our closest relatives essential capacities to produce human-like cultural evolution.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CAJK2GLH\\Claidière et al. - 2014 - Cultural evolution of systematically structured be.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ZFLGUFAV\\rspb.2014.html},
  number = {1797}
}

@software{clarkeGgbeeswarmCategoricalScatter2017,
  title = {Ggbeeswarm: {{Categorical Scatter}} ({{Violin Point}}) {{Plots}}},
  shorttitle = {Ggbeeswarm},
  author = {Clarke, Erik and Sherrill-Mix, Scott},
  date = {2017-08-07},
  url = {https://CRAN.R-project.org/package=ggbeeswarm},
  urldate = {2019-09-03},
  abstract = {Provides two methods of plotting categorical scatter plots such that the arrangement of points within a category reflects the density of data at that region, and avoids over-plotting.},
  version = {0.6.0}
}

@article{coleEvokedPotentialsMan1991,
  title = {Evoked Potentials in a Man with a Complete Large Myelinated Fibre Sensory Neuropathy below the Neck},
  author = {Cole, J. D. and Katifi, H. A.},
  date = {1991-03-01},
  journaltitle = {Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section},
  shortjournal = {Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section},
  volume = {80},
  pages = {103--107},
  issn = {0168-5597},
  doi = {10.1016/0168-5597(91)90147-P},
  url = {http://www.sciencedirect.com/science/article/pii/016855979190147P},
  urldate = {2019-04-02},
  abstract = {Cortical somatosensory evoked potentials (SEPs) were recorded from a man with a severe neuropathy without touch and proprioception below the neck. Peripheral neurophysiological tests showed a complete large myelinated fibre sensory neuropathy. Sensory threshold to electrical stimulation of the median nerve was 15 mA (normal 2–4 mA). With a stimulus of 39 mA, duration 400 μsec, applied at the wrist a cortical SEP was recorded with a latency of 84 msec, giving a propagation velocity of 11.9 m/sec. At stimulation rates of above 3.3 Hz the SEP was absent. It is concluded that the SEPs recorded were conducted along Aδ peripheral fibres.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TA76HKNA\\016855979190147P.html},
  keywords = {Aδ SEPs,Electrical stimuli,Sensory neuropathy},
  number = {2}
}

@article{coleEvokedPotentialsSubject1995,
  title = {Evoked Potentials in a Subject with a Large-Fibre Sensory Neuropathy below the Neck},
  author = {Cole, J. D. and Merton, W. L. and Barrett, G. and Katifi, H. A. and Treede, R.-D.},
  date = {1995-02-01},
  journaltitle = {Canadian Journal of Physiology and Pharmacology},
  shortjournal = {Can. J. Physiol. Pharmacol.},
  volume = {73},
  pages = {234--245},
  issn = {0008-4212},
  doi = {10.1139/y95-034},
  url = {https://www.nrcresearchpress.com/doi/abs/10.1139/y95-034},
  urldate = {2019-04-02},
  abstract = {The results from experiments in various modalities of evoked potentials are described in a subject with a complete large peripheral neuropathy below the neck. He has no tactile or position sensitivity below that level, but has retained fatigue, pain, and temperature sensation. Percutaneous electrical stimulation of peripheral nerves led to scalp recorded evoked potentials with thresholds and propagation velocities compatible with conduction along A-δ peripheral pathways. CO2 laser evoked potentials were similar to those seen in controls, further support for intact A-δ peripheral fibres. Movement-related cortical potentials (MRCPs) were recorded associated with active and passive movement of the middle finger. The former were normal, evidence that the termination of the MRCP is not dependent on peripheral feedback. By comparing passive MRCPs between controls and the subject it was possible to establish which parts of the potentials are visual and which are proprioceptive and to gain evidence of central reo..., On décrit les résultats d'expériences effectuées en utilisant divers protocoles de potentiels évoqués (PÉ) chez un sujet souffrant d'une neuropathie périphérique des fibres de grand diamètre de toute la partie du corps située au-dessous du cou. Ce sujet n'a ni sensibilité posturale ni sensibilité tactile sous ce niveau, mais ressent encore la douleur due à la fatigue et présente une sensibilité thermique. Une stimulation électrique percutanée des nerfs périphériques a permis d'enregistrer au niveau crânien des PÉ dont les seuils et les vitesses de propagation s'accordaient avec une conduction le long des voies périphériques A-δ. Les PÉ au laser CO2 ont été similaires à ceux observés chez les témoins, confirmant ainsi l'existence de fibres périphériques A-δ intactes. On a enregistré des potentiels corticaux liés au mouvement (PCLM) lors de mouvements actifs et passifs du médius. Les premiers étaient normaux, ce qui indique que l'interruption du PCLM ne dépend pas d'une boucle de rétroaction périphérique. L...},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4D4VU529\\y95-034.html},
  number = {2}
}

@article{coleGestureFollowingDeafferentation2002,
  title = {Gesture Following Deafferentation: A Phenomenologically Informed Experimental Study},
  shorttitle = {Gesture Following Deafferentation},
  author = {Cole, Jonathan and Gallagher, Shaun and McNeill, David},
  date = {2002-03-01},
  journaltitle = {Phenomenology and the Cognitive Sciences},
  shortjournal = {Phenomenology and the Cognitive Sciences},
  volume = {1},
  pages = {49--67},
  issn = {1572-8676},
  doi = {10.1023/A:1015572619184},
  url = {https://doi.org/10.1023/A:1015572619184},
  urldate = {2019-04-16},
  abstract = {Empirical studies of gesture in a subject who has lost proprioception and the sense of touch from the neck down show that specific aspects of gesture remain normal despite abnormal motor processes for instrumental movement. The experiments suggest that gesture, as a linguistic phenomenon, is not reducible to instrumental movement. They also support and extend claims made by Merleau-Ponty concerning the relationship between language and cognition. Gesture, as language, contributes to the accomplishment of thought.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\WMQEJIJ8\\Cole et al. - 2002 - Gesture following deafferentation a phenomenologi.pdf},
  keywords = {Artificial Intelligence,Empirical Study,Experimental Study,Motor Process,Specific Aspect},
  langid = {english},
  number = {1}
}

@book{coleLosingTouchMan2016,
  title = {Losing {{Touch}}: {{A}} Man without His Body},
  shorttitle = {Losing {{Touch}}},
  author = {Cole, J. D.},
  date = {2016-10-28},
  publisher = {{Oxford University Press}},
  abstract = {What is like to live without touch or movement/position sense (proprioception)? The only way to understand the importance of these senses, so familiar we cannot imagine their absence, is to ask someone in that position. Ian Waterman lost them below the neck over forty years ago, though pain and temperature perception and his peripheral movement nerves were unaffected. Without proprioceptive feedback and touch the movement brain was disabled. Completely unable to move, he felt disembodied and frightened. Then, slowly, he taught himself to dress, eat and walk by thinking about each movement and with visual supervision. In Losing Touch, the narrative moves between biography and scientific research, theatre, documentary and zero gravity. He has been married three times, and built up successful careers in disability access audit, using his impairment to his advantage, and in rare turkey breeding and journalism. The neuroscience has led to data on movement without feedback, the pleasantness of touch, gesture, pain and body orientation in space. The account shows how the science was actually done but also reveals Ian's journey from passive subject to informed critic of science and scientists and that the science has given him both more understanding but also greater confidence personally. His unique response to such a rare condition has also led to a BBC documentary, theatrical portrayals and a weightless flight with NASA. As a young man he sought triumph over his impairment; now, nearly 65, he has more mature reflections on living with such an extraordinary loss, the limits it has imposed and the opportunities it has enabled. He gives his views on scientists and on others he has met including Oliver Sacks and Peter Brook. In an Afterword those from science, the arts and philosophy give an appreciation of his contribution. The book is the result of nearly 30 years close collaboration between author and subject.},
  eprint = {V6akDAAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-19-108769-1},
  keywords = {Medical / Neurology,Philosophy / Mind & Body,Psychology / Cognitive Psychology & Cognition,Psychology / Physiological Psychology,Science / General,Science / Life Sciences / Neuroscience},
  langid = {english},
  pagetotal = {201}
}

@book{coleLosingTouchMan2016a,
  title = {Losing {{Touch}}: {{A}} Man without His Body},
  shorttitle = {Losing {{Touch}}},
  author = {Cole, Jonathan},
  date = {2016-09-01},
  edition = {1 edition},
  publisher = {{Oxford University Press}},
  location = {{Oxford, United Kingdom ; New York, NY, United States of America}},
  abstract = {What is like to live without touch or movement/position sense (proprioception)? The only way to understand the importance of these senses, so familiar we cannot imagine their absence, is to ask someone in that position. Ian Waterman lost them below the neck over forty years ago, though pain and temperature perception and his peripheral movement nerves were unaffected. Without proprioceptive feedback and touch the movement brain was disabled. Completely unable to move, he felt disembodied and frightened. Then, slowly, he taught himself to dress, eat and walk by thinking about each movement and with visual supervision. In Losing Touch, the narrative moves between biography and scientific research, theatre, documentary and zero gravity. He has been married three times, and built up successful careers in disability access audit, using his impairment to his advantage, and in rare turkey breeding and journalism. The neuroscience has led to data on movement without feedback, the pleasantness of touch, gesture, pain and body orientation in space. The account shows how the science was actually done but also reveals Ian's journey from passive subject to informed critic of science and scientists and that the science has given him both more understanding but also greater confidence personally. His unique response to such a rare condition has also led to a BBC documentary, theatrical portrayals and a weightless flight with NASA.As a young man he sought triumph over his impairment; now, nearly 65, he has more mature reflections on living with such an extraordinary loss, the limits it has imposed and the opportunities it has enabled. He gives his views on scientists and on others he has met including Oliver Sacks and Peter Brook. In an Afterword those from science, the arts and philosophy give an appreciation of his contribution. The book is the result of nearly 30 years close collaboration between author and subject.},
  isbn = {978-0-19-877887-5},
  langid = {english},
  pagetotal = {224}
}

@book{colePrideDailyMarathon1995,
  title = {Pride and a {{Daily Marathon}}},
  author = {Cole, Jonathan},
  date = {1995-07-11},
  edition = {New Ed edition},
  publisher = {{A Bradford Book}},
  location = {{Cambridge, Mass}},
  abstract = {At the age of 19, Ian Waterman was suddenly struck down at work by a rare neurological illness that deprived him of all sensation below the neck. He fell on the floor in a heap, unable to stand or control his limbs, having lost the sense of joint position and proprioception, of that "sixth sense" of his body in space, which we all take for granted. After months in a neurological ward he was judged incurable and condemned to a life of wheelchair dependence. This is the first U.S. publication of a remarkable book by his physician, Jonathan Cole. It tells the compelling story, including a clear clinical description of a rare condition, of how Waterman reclaimed a life of full mobility against all expectations, by mental effort and sheer courage. Cole describes how Waterman gradually adapted to his strange condition. As the doctors had predicted, there was no neurological recovery. He had to monitor every movement by sight to work out where his limbs were, since he had no feedback from his peripheral nerves. But with astonishing persistence Waterman developed elaborate tricks and strategies to control his movements, enabling him to cope not only with the day-to-day problems of living, but even with the challenges of work, love, and marriage.},
  isbn = {978-0-262-53136-8},
  langid = {english},
  pagetotal = {216}
}

@book{colombettiFeelingBodyAffective2014,
  title = {The Feeling Body: {{Affective}} Science Meets the Enactive Mind},
  author = {Colombetti, G.},
  date = {2014},
  publisher = {{MIT press}},
  location = {{Cambridge, MA}}
}

@article{comstockSensorimotorSynchronizationAuditory2018,
  title = {Sensorimotor {{Synchronization With Auditory}} and {{Visual Modalities}}: {{Behavioral}} and {{Neural Differences}}},
  shorttitle = {Sensorimotor {{Synchronization With Auditory}} and {{Visual Modalities}}},
  author = {Comstock, Daniel C. and Hove, Michael J. and Balasubramaniam, Ramesh},
  date = {2018-07-18},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front Comput Neurosci},
  volume = {12},
  issn = {1662-5188},
  doi = {10.3389/fncom.2018.00053},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6058047/},
  urldate = {2019-10-18},
  abstract = {It has long been known that the auditory system is better suited to guide temporally precise behaviors like sensorimotor synchronization (SMS) than the visual system. Although this phenomenon has been studied for many years, the underlying neural and computational mechanisms remain unclear. Growing consensus suggests the existence of multiple, interacting, context-dependent systems, and that reduced precision in visuo-motor timing might be due to the way experimental tasks have been conceived. Indeed, the appropriateness of the stimulus for a given task greatly influences timing performance. In this review, we examine timing differences for sensorimotor synchronization and error correction with auditory and visual sequences, to inspect the underlying neural mechanisms that contribute to modality differences in timing. The disparity between auditory and visual timing likely relates to differences in the processing specialization between auditory and visual modalities (temporal vs. spatial). We propose this difference could offer potential explanation for the differing temporal abilities between modalities. We also offer suggestions as to how these sensory systems interface with motor and timing systems.},
  eprint = {30072885},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SV43Z9DY\\Comstock et al. - 2018 - Sensorimotor Synchronization With Auditory and Vis.pdf},
  pmcid = {PMC6058047}
}

@online{ConversationalGesturesAutism,
  title = {Conversational Gestures in Autism Spectrum Disorders: {{Asynchrony}} but Not Decreased Frequency - de {{Marchena}} - 2010 - {{Autism Research}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/aur.159?casa_token=mR8s8HhLJ-wAAAAA:YDn6WZNz0NJB1KygloMRwkT5_5E2x9v3qvGSQSVfOOo4vfS7ED1CzE7a3HvHtetb5H8_vf8vRyZppYbY},
  urldate = {2019-08-31},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5J538ALY\\aur.html}
}

@article{cookEmbodiedCommunicationSpeakers2009,
  title = {Embodied Communication: Speakers' Gestures Affect Listeners' Actions},
  shorttitle = {Embodied Communication},
  author = {Cook, Susan Wagner and Tanenhaus, Michael K.},
  date = {2009-10},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {113},
  pages = {98--104},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2009.06.006},
  abstract = {We explored how speakers and listeners use hand gestures as a source of perceptual-motor information during naturalistic communication. After solving the Tower of Hanoi task either with real objects or on a computer, speakers explained the task to listeners. Speakers' hand gestures, but not their speech, reflected properties of the particular objects and the actions that they had previously used to solve the task. Speakers who solved the problem with real objects used more grasping handshapes and produced more curved trajectories during the explanation. Listeners who observed explanations from speakers who had previously solved the problem with real objects subsequently treated computer objects more like real objects; their mouse trajectories revealed that they lifted the objects in conjunction with moving them sideways, and this behavior was related to the particular gestures that were observed. These findings demonstrate that hand gestures are a reliable source of perceptual-motor information during human communication.},
  eprint = {19682672},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9AQ99SZN\\Cook and Tanenhaus - 2009 - Embodied communication speakers' gestures affect .pdf},
  keywords = {Acoustic Stimulation,Adult,Cognition,Gestures,Humans,Problem Solving,Psychomotor Performance,Speech,Speech Perception},
  langid = {english},
  number = {1},
  pmcid = {PMC2763957}
}

@article{cooperriderForegroundGestureBackground2019,
  title = {Foreground Gesture, Background Gesture},
  author = {Cooperrider, K.},
  date = {2019},
  journaltitle = {Gesture},
  volume = {16},
  pages = {176--202},
  doi = {10.1075/gest.16.2.02coo},
  url = {https://benjamins.com/catalog/gest.16.2.02coo},
  urldate = {2020-01-26},
  abstract = {Do speakers intend their gestures to communicate? Central as this question is to the study of gesture, researchers cannot seem to agree on the answer. According to one common framing, gestures are an “unwitting” window into the mind (McNeill, 1992); but, according to another common framing, they are designed along with speech to form “composite utterances” (Enfield, 2009). These two framings correspond to two cultures within gesture studies~– the first cognitive and the second interactive in orientation~– and they appear to make incompatible claims. In this article I attempt to bridge the cultures by developing a distinction between foreground gestures and background gestures. Foreground gestures are designed in their particulars to communicate a critical part of the speaker’s message; background gestures are not designed in this way. These are two fundamentally different kinds of gesture, not two different ways of framing the same monolithic behavior. Foreground gestures can often be identified by one or more of the following hallmarks: they are produced along with demonstratives; they are produced in the absence of speech; they are co-organized with speaker gaze; and they are produced with conspicuous effort. The distinction between foreground and background gestures helps dissolve the apparent tension between the two cultures: interactional researchers have focused on foreground gestures and elevated them to the status of a prototype, whereas cognitive researchers have done the same with background gestures. The distinction also generates a number of testable predictions about gesture production and understanding, and it opens up new lines of inquiry into gesture across child development and across cultures.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Q6GLFS9Y\\gest.16.2.html},
  langid = {english},
  number = {2}
}

@book{corballisHandMouthOrigins2002,
  title = {From Hand to Mouth: {{The}} Origins of Language},
  author = {Corballis, M. C.},
  date = {2002},
  publisher = {{Princeton University Press}},
  location = {{Princeton, NJ.}}
}

@article{cordoPropertiesPosturalAdjustments1982,
  title = {Properties of Postural Adjustments Associated with Rapid Arm Movements},
  author = {Cordo, P. J. and Nashner, L. M.},
  date = {1982-02},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {J. Neurophysiol.},
  volume = {47},
  pages = {287--302},
  issn = {0022-3077},
  doi = {10.1152/jn.1982.47.2.287},
  abstract = {1. We have examined rapid postural adjustments associated with a class of voluntary movements that disturb postural equilibrium. In the text that follows, these motor activities are termed associated postural adjustments and voluntary focal movements, respectively. Standing human subjects performed a variety of movement tasks on a hand-held manipulandum, resulting in disturbances to their postural equilibrium. The experimental use of movements that interact with the subject's environment in a relatively simple was permitted a more precise comparison of the postural adjustments with their associated focal movements. 2. Subjects either pulled or pushed on a stiff interface (the handle) or they responded in a predetermined way to handle perturbations. These activities were carried out with various degrees of steady-state postural stability. Prior to and during these movements, support surface and handle forces, electromyographic (EMG) signals, and body sway were monitored. 3. In addition to previously shown postural adjustments associated with reaction-time armed movements, we have demonstrated these postural activities occur in concept with segmental stretch reflexes and self-initiated (untriggered) movements. Postural adjustments were initiated shortly before all focal movements tested except the short-latency component of the biceps stretch reflex (25- to 30-ms latency). However, this reflex component was rarely elicited by handle perturbations in free-standing subjects; therefore, postural adjustments usually preceded any biceps activity under this condition. 4. By varying the degree of steady-state postural equilibrium, a reciprocal gain/threshold relationship between postural and focal components was documented, i.e., when stability was high, postural activity was reduced or absent and focal activity enhanced. Conversely, the biceps stretch reflex was difficult to elicit under any condition where the subjects was not fully supported in the direction of movement and reaction times of focal movements were prolonged. 5. Postural activities associated with focal movements were found to share a number of organizational properties with automatic postural adjustments to support surface movements. Specifically, the postural muscle synergies were equivalent in muscle composition, relative activation magnitudes, and relative temporal sequencing. Furthermore, both types of postural adjustments were highly specific in locus and magnitude to the quality of steady-state postural equilibrium (e.g., postural "set"). 6. A conceptual model is proposed that suggests one simple way in which the reciprocal influence of postural set on postural and focal movement components and their temporal sequencing might be accomplished. Furthermore, we propose in this model a common central organization of postural adjustments associated with focal movements and those elicited by support-surface movements.},
  eprint = {7062101},
  eprinttype = {pmid},
  keywords = {Afferent Pathways,Arm,Feedback,Humans,Models; Neurological,Motor Activity,Movement,Muscle Contraction,Muscles,Organ Specificity,Posture},
  langid = {english},
  number = {2}
}

@article{cornishSequenceMemoryConstraints2017,
  title = {Sequence {{Memory Constraints Give Rise}} to {{Language}}-{{Like Structure}} through {{Iterated Learning}}},
  author = {Cornish, Hannah and Dale, Rick and Kirby, Simon and Christiansen, Morten H.},
  editor = {Berwick, Robert C},
  date = {2017-01-24},
  journaltitle = {PLoS ONE},
  volume = {12},
  pages = {e0168532},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0168532},
  url = {https://dx.plos.org/10.1371/journal.pone.0168532},
  urldate = {2020-03-24},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\6B9XFGD9\\Cornish et al. - 2017 - Sequence Memory Constraints Give Rise to Language-.pdf},
  langid = {english},
  number = {1}
}

@article{corpsCoordinatingUtterancesTurnTaking2018,
  title = {Coordinating {{Utterances During Turn}}-{{Taking}}: {{The Role}} of {{Prediction}}, {{Response Preparation}}, and {{Articulation}}},
  shorttitle = {Coordinating {{Utterances During Turn}}-{{Taking}}},
  author = {Corps, Ruth E. and Gambi, Chiara and Pickering, Martin J.},
  date = {2018-02-17},
  journaltitle = {Discourse Processes},
  volume = {55},
  pages = {230--240},
  publisher = {{Routledge}},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2017.1330031},
  url = {https://doi.org/10.1080/0163853X.2017.1330031},
  urldate = {2020-05-28},
  abstract = {During conversation, interlocutors rapidly switch between speaker and listener roles and take turns at talk. How do they achieve such fine coordination? Most research has concentrated on the role of prediction, but listeners must also prepare a response in advance (assuming they wish to respond) and articulate this response at the appropriate moment. Such mechanisms may overlap with the processes of comprehending the speaker’s incoming turn and predicting its end. However, little is known about the stages of response preparation and production. We discuss three questions pertaining to such stages: (1) Do listeners prepare their own response in advance?, (2) Can listeners buffer their prepared response?, and (3) Does buffering lead to interference with concurrent comprehension? We argue that fine coordination requires more than just an accurate prediction of the interlocutor’s incoming turn: Listeners must also simultaneously prepare their own response.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CSDSJ97A\\Corps et al. - 2018 - Coordinating Utterances During Turn-Taking The Ro.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\MFUZWG2L\\0163853X.2017.html},
  number = {2}
}

@incollection{cox2016,
  booktitle = {Chromatic and {{Anisotropic Cross}}-{{Recurrence Quantification Analysis}} of {{Interpersonal Behavior}} | {{SpringerLink}}},
  author = {Cox, R. F. A. and van der Steen, S. and Guevara, M. and De Jonge-Hoekstra, L. and van Dijk, M.},
  date = {2016},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-29922-8_11},
  urldate = {2019-12-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VCUXJUG5\\978-3-319-29922-8_11.html},
  options = {useprefix=true},
  series = {Recurrence {{Plots}} and {{Their Quanitifactions}}}
}

@article{crasbornCombiningVideoNumeric,
  title = {Combining Video and Numeric Data in the Analysis of Sign Languages within the {{ELAN}} Annotation Software},
  author = {Crasborn, Onno and Sloetjes, Han and Auer, Eric and Wittenburg, Peter},
  pages = {7},
  abstract = {This paper describes hardware and software that can be used for the phonetic study of sign languages. The field of sign language phonetics is characterised, and the hardware that is currently in use is described. The paper focuses on the software that was developed to enable the recording of finger and hand movement data, and the additions to the ELAN annotation software that facilitate the further visualisation and analysis of the data.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QLUDTLMV\\Crasborn et al. - Combining video and numeric data in the analysis o.pdf},
  langid = {english}
}

@article{cravottaEffectsEncouragingUse2019,
  title = {Effects of {{Encouraging}} the {{Use}} of {{Gestures}} on {{Speech}}},
  author = {Cravotta, A. and Busà, M. G. and Prieto, P.},
  date = {2019},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  doi = {10.21437/SpeechProsody.2018-42}
}

@article{cravottaRestrainingEncouragingUse2018,
  title = {Restraining and Encouraging the Use of Hand Gestures: Effects on Speech},
  shorttitle = {Restraining and Encouraging the Use of Hand Gestures},
  author = {Cravotta, A. and Grazia, B. M. and Prieto, P.},
  date = {2018},
  issn = {2333-2042},
  doi = {http://dx.doi.org/10.21437/SpeechProsody.2018-42},
  url = {http://repositori.upf.edu/handle/10230/35125},
  urldate = {2019-05-04},
  abstract = {Previous studies have investigated the effects of the inability to make hand gestures on speakers’ fluency; however, the question of whether encouraging speakers to gesture affects their fluency has received little attention. This study investigates the effect of restraining (Experiment 1) and encouraging (Experiment 2) hand gestures on the following correlates of speech: speech discourse length (number of words and discourse length in seconds), disfluencies (filled pauses, self-corrections, repetitions, insertions, interruptions, silent pauses), and acoustic properties (speech rate, measures of intensity and pitch). In two experiments, 10 native speakers of
Italian took part in a narration task where they were asked to describe comic strips. Each experiment compared two conditions. In Experiment 1, subjects first received no instructions as to how to behave when narrating. Then they were told to sit on their hands while speaking. In Experiment 2, subjects first received no instructions and were then actively encouraged to use hand gestures. The results showed that restraining gestures leads to quieter and slower paced speech, while encouraging gestures triggers longer speech discourse, faster speech rate and more fluent and louder speech. Thus, both restraining and encouraging hand gestures seem to clearly affect prosodic properties of speech, particularly speech fluency.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YKLIAL3A\\Prieto Vives et al. - 2018 - Restraining and encouraging the use of hand gestur.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\64PB283C\\35125.html},
  langid = {english}
}

@article{crevecoeurOptimalIntegrationGravity2009,
  title = {Optimal Integration of Gravity in Trajectory Planning of Vertical Pointing Movements},
  author = {Crevecoeur, Frédéric and Thonnard, Jean-Louis and Lefèvre, Philippe},
  date = {2009-08},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {J. Neurophysiol.},
  volume = {102},
  pages = {786--796},
  issn = {0022-3077},
  doi = {10.1152/jn.00113.2009},
  abstract = {The planning and control of motor actions requires knowledge of the dynamics of the controlled limb to generate the appropriate muscular commands and achieve the desired goal. Such planning and control imply that the CNS must be able to deal with forces and constraints acting on the limb, such as the omnipresent force of gravity. The present study investigates the effect of hypergravity induced by parabolic flights on the trajectory of vertical pointing movements to test the hypothesis that motor commands are optimized with respect to the effect of gravity on the limb. Subjects performed vertical pointing movements in normal gravity and hypergravity. We use a model based on optimal control to identify the role played by gravity in the optimal arm trajectory with minimal motor costs. First, the simulations in normal gravity reproduce the asymmetry in the velocity profiles (the velocity reaches its maximum before half of the movement duration), which typically characterizes the vertical pointing movements performed on Earth, whereas the horizontal movements present symmetrical velocity profiles. Second, according to the simulations, the optimal trajectory in hypergravity should present an increase in the peak acceleration and peak velocity despite the increase in the arm weight. In agreement with these predictions, the subjects performed faster movements in hypergravity with significant increases in the peak acceleration and peak velocity, which were accompanied by a significant decrease in the movement duration. This suggests that movement kinematics change in response to an increase in gravity, which is consistent with the hypothesis that motor commands are optimized and the action of gravity on the limb is taken into account. The results provide evidence for an internal representation of gravity in the central planning process and further suggest that an adaptation to altered dynamics can be understood as a reoptimization process.},
  eprint = {19458149},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KRY3V7BI\\Crevecoeur et al. - 2009 - Optimal integration of gravity in trajectory plann.pdf},
  keywords = {Adult,Algorithms,Arm,Biomechanical Phenomena,Computer Simulation,Female,Gravitation,Humans,Hypergravity,Learning,Male,Mental Processes,Middle Aged,Models; Neurological,Motor Activity,Time Factors},
  langid = {english},
  number = {2}
}

@software{csardiPackageIgraphNetwork2019,
  title = {Package 'igraph' {{Network Analysis}} and {{Visualization}}},
  author = {Csárdi, G.},
  date = {2019},
  url = {http://bioconductor.statistik.tu-dortmund.de/cran/web/packages/igraph/igraph.pdf},
  version = {1.2.4.1}
}

@inproceedings{cumminsRhythmicCommonalitiesHand1992,
  title = {Rhythmic Commonalities between Hand Gestures and Speech},
  booktitle = {Proceedings of the 18th {{Meeting}} of {{Cognitiive Science Society}}},
  author = {Cummins, F. and Port, R. F.},
  date = {1992},
  pages = {415--419},
  publisher = {{Erlbbaum}},
  location = {{Hillsdale}}
}

@incollection{cumminsRhythmSpeech2015,
  title = {Rhythm and {{Speech}}},
  booktitle = {The {{Handbook}} of {{Speech Production}}},
  author = {Cummins, Fred},
  editor = {Redford, Melissa A.},
  date = {2015-04-24},
  pages = {158--177},
  publisher = {{John Wiley \& Sons, Inc}},
  location = {{Hoboken, NJ}},
  doi = {10.1002/9781118584156.ch8},
  url = {http://doi.wiley.com/10.1002/9781118584156.ch8},
  urldate = {2019-09-27},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5SS5Q277\\Cummins - 2015 - Rhythm and Speech.pdf},
  isbn = {978-1-118-58415-6 978-0-470-65993-9},
  langid = {english}
}

@inproceedings{cwiekIconicProsodyRooted2019,
  title = {Iconic {{Prosody}} Is Rooted in Sensori-Motor Properties: {{Fundamental}} Frequency and the Vertical Space},
  booktitle = {41st {{Annual Meeting}} of the {{Cognitive Science SocietyAt}}},
  author = {Cwiek, A. and Fuchs, S.},
  date = {2019},
  location = {{Montreal, Canada}},
  eventtitle = {{{CogSci}} 2019}
}

@article{daleHowHumansMake2018,
  title = {“{{How}} Do Humans Make Sense?” Multiscale Dynamics and Emergent Meaning},
  shorttitle = {“{{How}} Do Humans Make Sense?},
  author = {Dale, Rick and Kello, Christopher T.},
  date = {2018-08-01},
  journaltitle = {New Ideas in Psychology},
  shortjournal = {New Ideas in Psychology},
  volume = {50},
  pages = {61--72},
  issn = {0732-118X},
  doi = {10.1016/j.newideapsych.2017.09.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0732118X1730020X},
  urldate = {2019-08-15},
  abstract = {The challenges posed by the composite nature of sense-making encourage us to study how that composite is dynamically assembled. In this paper, we consider the computational underpinnings that drive the composite nature of interaction. We look to the dynamic properties of recurrent neural networks. What kind of dynamic system inherently integrates multiple signals across different levels and modalities? We argue below that three fundamental properties are needed: dynamic memory, timescale integration, and multimodal integration. We argue that a growing area of investigation in neural networks, reservoir computing, has all these properties (Jaeger, 2001). A simple version of this model is then created to demonstrate “emergent meaning,” using a simplified model communication system.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ANKZFS9Q\\S0732118X1730020X.html}
}

@article{dalziellDanceChoreographyCoordinated2013,
  title = {Dance {{Choreography Is Coordinated}} with {{Song Repertoire}} in a {{Complex Avian Display}}},
  author = {Dalziell, Anastasia H. and Peters, Richard A. and Cockburn, Andrew and Dorland, Alexandra D. and Maisey, Alex C. and Magrath, Robert D.},
  date = {2013-06-17},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {23},
  pages = {1132--1135},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2013.05.018},
  url = {https://www.cell.com/current-biology/abstract/S0960-9822(13)00581-2},
  urldate = {2019-10-17},
  eprint = {23746637},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EHU86PNY\\Dalziell et al. - 2013 - Dance Choreography Is Coordinated with Song Repert.pdf},
  langid = {english},
  number = {12}
}

@thesis{dannerEffectsSpeechContext2017,
  title = {Effects of {{Speech Context On Charactersitics}} of {{Manual Gesture}}},
  author = {Danner, S. G.},
  date = {2017},
  institution = {{University of Southern California}}
}

@article{dannerQuantitativeAnalysisMultimodal2018,
  title = {Quantitative Analysis of Multimodal Speech Data},
  author = {Danner, Samantha Gordon and Barbosa, Adriano Vilela and Goldstein, Louis},
  date = {2018-11-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {71},
  pages = {268--283},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2018.09.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447017302280},
  urldate = {2019-05-04},
  abstract = {This study presents techniques for quantitatively analyzing coordination and kinematics in multimodal speech using video, audio and electromagnetic articulography (EMA) data. Multimodal speech research has flourished due to recent improvements in technology, yet gesture detection/annotation strategies vary widely, leading to difficulty in generalizing across studies and in advancing this field of research. We describe how FlowAnalyzer software can be used to extract kinematic signals from basic video recordings; and we apply a technique, derived from speech kinematic research, to detect bodily gestures in these kinematic signals. We investigate whether kinematic characteristics of multimodal speech differ dependent on communicative context, and we find that these contexts can be distinguished quantitatively, suggesting a way to improve and standardize existing gesture identification/annotation strategy. We also discuss a method, Correlation Map Analysis (CMA), for quantifying the relationship between speech and bodily gesture kinematics over time. We describe potential applications of CMA to multimodal speech research, such as describing characteristics of speech-gesture coordination in different communicative contexts. The use of the techniques presented here can improve and advance multimodal speech and gesture research by applying quantitative methods in the detection and description of multimodal speech.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\U3QL5MLF\\Danner et al. - 2018 - Quantitative analysis of multimodal speech data.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\H4ZUTUED\\S0095447017302280.html},
  keywords = {Bodily gesture,Communicative context,Correlation Map Analysis,FlowAnalyzer,Multimodal speech,Time-varying coordination}
}

@article{debreslioskaDiscourseReferenceBimodal2019,
  title = {Discourse {{Reference Is Bimodal}}: {{How Information Status}} in {{Speech Interacts}} with {{Presence}} and {{Viewpoint}} of {{Gestures}}},
  shorttitle = {Discourse {{Reference Is Bimodal}}},
  author = {Debreslioska, Sandra and Gullberg, Marianne},
  date = {2019-01-02},
  journaltitle = {Discourse Processes},
  volume = {56},
  pages = {41--60},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2017.1351909},
  url = {https://doi.org/10.1080/0163853X.2017.1351909},
  urldate = {2019-09-20},
  abstract = {Speakers use speech and gestures to represent referents in discourse. Depending on referents’ information status, in speech speakers will vary richness of expression (e.g., lexical noun phrase [NP]/pronoun), nominal definiteness (indefinite/definite), and grammatical role (subject/object). This study tested whether these three linguistic markers of information status interact with presence of gestures and gestural viewpoint (observer/character). The results show that gestures are more frequent with less accessible referents expressed with richer spoken forms but that richness of expression does not interact with viewpoint. In contrast, nominal definiteness and grammatical role interact with both presence and viewpoint of gestures. Gestures occur mainly with indefinite lexical NPs and objects. Character viewpoint gestures occur mainly with indefinite lexical NPs and objects plus predicates. The results shed light on when and how speakers use gestures in connected discourse and specifically highlight the discursive function of gestural viewpoint.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZCX2ASCE\\0163853X.2017.html},
  keywords = {bimodal discourse,gestures,information status,mode of representation,referential expressions},
  number = {1}
}

@article{dejongCorrelationPcenterAdjustments1994,
  title = {The Correlation of {{P}}-Center Adjustments with Articulatory and Acoustic Events},
  author = {De Jong, Kenneth J.},
  date = {1994-07-01},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  volume = {56},
  pages = {447--460},
  issn = {1532-5962},
  doi = {10.3758/BF03206736},
  url = {https://doi.org/10.3758/BF03206736},
  urldate = {2019-09-28},
  abstract = {To evaluate articulatory models of perceptual center (P-center) location, listeners performed perceptual adjustments on stimuli which were extracted from a corpus of articulatory data. To avoid streaming effects, the stimuli were not edited to obtain temporal variation; instead, they varied in stress and segmental content. Adjustments were evaluated as to their simultaneity with acoustic and articulatory events. The first experiment yielded various articulatory and acoustic correlates of P-center location; the second yielded different articulatory predictors and no acoustic effective predictors. Multiple correlation analyses showed a variation from P-center locations predicted by the articulatory events that were associated with other predictors. Thus, P-center locations do not correspond to any particular kinematic articulatory event, but rather to a complex of events taken from throughout the stimuli. These results are discussed in terms of their relevance to a model of P-centers as indices of underlying gestural timing.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8JYH24TV\\De Jong - 1994 - The correlation of P-center adjustments with artic.pdf},
  keywords = {Acoustic Event,Tongue Dorsum,Voice Onset Time,Vowel Duration,Word Pair},
  langid = {english},
  number = {4}
}

@article{delacruz-paviaFindingPhrasesInterplay2019,
  title = {Finding {{Phrases}}: {{The Interplay}} of {{Word Frequency}}, {{Phrasal Prosody}} and {{Co}}-Speech {{Visual Information}} in {{Chunking Speech}} by {{Monolingual}} and {{Bilingual Adults}}},
  shorttitle = {Finding {{Phrases}}},
  author = {de la Cruz-Pavía, Irene and Werker, Janet F. and Vatikiotis-Bateson, Eric and Gervain, Judit},
  date = {2019-04-19},
  journaltitle = {Language and Speech},
  shortjournal = {Lang Speech},
  pages = {0023830919842353},
  issn = {0023-8309},
  doi = {10.1177/0023830919842353},
  url = {https://doi.org/10.1177/0023830919842353},
  urldate = {2020-01-06},
  abstract = {The audiovisual speech signal contains multimodal information to phrase boundaries. In three artificial language learning studies with 12 groups of adult participants we investigated whether English monolinguals and bilingual speakers of English and a language with opposite basic word order (i.e., in which objects precede verbs) can use word frequency, phrasal prosody and co-speech (facial) visual information, namely head nods, to parse unknown languages into phrase-like units. We showed that monolinguals and bilinguals used the auditory and visual sources of information to chunk “phrases” from the input. These results suggest that speech segmentation is a bimodal process, though the influence of co-speech facial gestures is rather limited and linked to the presence of auditory prosody. Importantly, a pragmatic factor, namely the language of the context, seems to determine the bilinguals’ segmentation, overriding the auditory and visual cues and revealing a factor that begs further exploration.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HUVB9ZII\\de la Cruz-Pavía et al. - 2019 - Finding Phrases The Interplay of Word Frequency, .pdf},
  keywords = {artificial grammar learning,bilingualism,co-speech visual information,frequency-based information,phrase segmentation,prosody},
  langid = {english},
  options = {useprefix=true}
}

@article{dennisPrivacyOpenScience2019,
  title = {Privacy versus Open Science},
  author = {Dennis, Simon and Garrett, Paul and Yim, Hyungwook and Hamm, Jihun and Osth, Adam F. and Sreekumar, Vishnu and Stone, Ben},
  date = {2019-08-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {51},
  pages = {1839--1848},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01259-5},
  url = {https://doi.org/10.3758/s13428-019-01259-5},
  urldate = {2019-11-30},
  abstract = {Pervasive internet and sensor technologies promise to revolutionize psychological science. However, the data collected using these technologies are often very personal—indeed, the value of the data is often directly related to how personal they are. At the same time, driven by the replication crisis, there is a sustained push to publish data to open repositories. These movements are in fundamental conflict. In this article, we propose a way to navigate this issue. We argue that there are significant advantages to be gained by ceding the ownership of data to the participants who generate the data. We then provide desiderata for a privacy-preserving platform. In particular, we suggest that researchers should use an interface to perform experiments and run analyses, rather than observing the stimuli themselves. We argue that this method not only improves privacy but will also encourage greater compliance with good research practices than is possible through open repositories.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\V4WH7AAN\\Dennis et al. - 2019 - Privacy versus open science.pdf},
  keywords = {Differential privacy,Open repositories,Open science,Privacy},
  langid = {english},
  number = {4}
}

@book{deutscherUnfoldingLanguageEvolutionary2005,
  title = {The {{Unfolding}} of {{Language}}: {{An Evolutionary}} of {{Mankind}}'s {{Greatest Invention}}},
  author = {Deutscher, G.},
  date = {2005},
  publisher = {{Metropolitan Books}},
  location = {{New York}}
}

@article{devosTurntimingSignedConversations2015,
  title = {Turn-Timing in Signed Conversations: Coordinating Stroke-to-Stroke Turn Boundaries},
  shorttitle = {Turn-Timing in Signed Conversations},
  author = {de Vos, Connie and Torreira, Francisco and Levinson, Stephen C.},
  date = {2015},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00268},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00268/full},
  urldate = {2020-05-19},
  abstract = {In spoken interactions, interlocutors carefully plan and time their utterances, minimising gaps and overlaps between consecutive turns. Cross-linguistic comparison has indicated that spoken languages vary only minimally in terms of turn-timing, and language acquisition research has shown pre-linguistic vocal turn-taking in the first half year of life. These observations suggest that the turn-taking system may provide a fundamental basis for our linguistic capacities. The question remains however to what extent our capacity for rapid turn-taking is determined by modality constraints. The avoidance of overlapping turns could be motivated by the difficulty of hearing and speaking at the same time. If so, turn-taking in sign might show greater toleration for overlap. Alternatively, signed conversations may show a similar distribution of turn-timing as spoken languages, thus avoiding both gaps and overlaps. To address this question we look at turn-timing in question-answer sequences in spontaneous conversations of Sign Language of the Netherlands. The findings indicate that although there is considerable overlap in two or more signers' articulators in conversation, when proper allowance is made for onset preparation, post-utterance retraction and the intentional holding of signs for response, turn-taking latencies in sign look remarkably like those reported for spoken language. This is consistent with the possibility that, at least with regard to responses to questions, speakers and signers follow similar time courses in planning and producing their utterances in on-going conversation. This suggests that turn-taking systems may well be a shared cognitive infrastructure underlying all modern human languages, both spoken and signed.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VJFH38W5\\de Vos et al. - 2015 - Turn-timing in signed conversations coordinating .pdf},
  keywords = {conversation analysis,sign language,sign phonetics,turn-taking,turn-timing,visual-gestural modality},
  langid = {english},
  options = {useprefix=true}
}

@online{DimensionsGuideDatabase,
  title = {Dimensions.{{Guide}} | {{Database}} of {{Dimensioned Drawings}}},
  url = {https://www.dimensions.guide},
  urldate = {2019-05-01},
  abstract = {A comprehensive reference database of dimensioned drawings documenting the standard measurements and sizes of the everyday objects and spaces that make up our world.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NZQXJZAC\\www.dimensions.guide.html},
  langid = {english}
}

@article{dingemanseArbitrarinessIconicitySystematicity2015b,
  title = {Arbitrariness, {{Iconicity}}, and {{Systematicity}} in {{Language}}},
  author = {Dingemanse, Mark and Blasi, Damián E. and Lupyan, Gary and Christiansen, Morten H. and Monaghan, Padraic},
  date = {2015-10-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {19},
  pages = {603--615},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2015.07.013},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661315001771},
  urldate = {2020-03-10},
  abstract = {The notion that the form of a word bears an arbitrary relation to its meaning accounts only partly for the attested relations between form and meaning in the languages of the world. Recent research suggests a more textured view of vocabulary structure, in which arbitrariness is complemented by iconicity (aspects of form resemble aspects of meaning) and systematicity (statistical regularities in forms predict function). Experimental evidence suggests these form-to-meaning correspondences serve different functions in language processing, development, and communication: systematicity facilitates category learning by means of phonological cues, iconicity facilitates word learning and communication by means of perceptuomotor analogies, and arbitrariness facilitates meaning individuation through distinctive forms. Processes of cultural evolution help to explain how these competing motivations shape vocabulary structure.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PHKPL4HL\\Dingemanse et al. - 2015 - Arbitrariness, Iconicity, and Systematicity in Lan.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\WAYCL3R5\\S1364661315001771.html},
  keywords = {arbitrariness,Iconicity,lexicon,sound-symbolism,systematicity,vocabulary},
  langid = {english},
  number = {10}
}

@article{dingemanseConstrualsIconicityExperimental2020,
  title = {Construals of Iconicity: Experimental Approaches to Form–Meaning Resemblances in Language},
  shorttitle = {Construals of Iconicity},
  author = {Dingemanse, Mark and Perlman, Marcus and Perniss, Pamela},
  date = {2020-03},
  journaltitle = {Language and Cognition},
  volume = {12},
  pages = {1--14},
  publisher = {{Cambridge University Press}},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2019.48},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/construals-of-iconicity-experimental-approaches-to-formmeaning-resemblances-in-language/1E2DF517E530A8D5C4B489A0AD76AFA7},
  urldate = {2020-03-09},
  abstract = {While speculations on form–meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well equipped to contribute to a comprehensive science of language.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\F3B3U98R\\Dingemanse et al. - 2020 - Construals of iconicity experimental approaches t.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\YVZNZJAS\\1E2DF517E530A8D5C4B489A0AD76AFA7.html},
  keywords = {conceptual foundations,experimental linguistics,iconicity,linguistic theory},
  langid = {english},
  number = {1}
}

@article{dingemanseUniversalPrinciplesRepair2015,
  title = {Universal {{Principles}} in the {{Repair}} of {{Communication Problems}}},
  author = {Dingemanse, Mark and Roberts, Seán G. and Baranova, Julija and Blythe, Joe and Drew, Paul and Floyd, Simeon and Gisladottir, Rosa S. and Kendrick, Kobin H. and Levinson, Stephen C. and Manrique, Elizabeth and Rossi, Giovanni and Enfield, N. J.},
  date = {2015-09-16},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {10},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0136100},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4573759/},
  urldate = {2020-03-18},
  abstract = {There would be little adaptive value in a complex communication system like human language if there were no ways to detect and correct problems. A systematic comparison of conversation in a broad sample of the world’s languages reveals a universal system for the real-time resolution of frequent breakdowns in communication. In a sample of 12 languages of 8 language families of varied typological profiles we find a system of ‘other-initiated repair’, where the recipient of an unclear message can signal trouble and the sender can repair the original message. We find that this system is frequently used (on average about once per 1.4 minutes in any language), and that it has detailed common properties, contrary to assumptions of radical cultural variation. Unrelated languages share the same three functionally distinct types of repair initiator for signalling problems and use them in the same kinds of contexts. People prefer to choose the type that is the most specific possible, a principle that minimizes cost both for the sender being asked to fix the problem and for the dyad as a social unit. Disruption to the conversation is kept to a minimum, with the two-utterance repair sequence being on average no longer that the single utterance which is being fixed. The findings, controlled for historical relationships, situation types and other dependencies, reveal the fundamentally cooperative nature of human communication and offer support for the pragmatic universals hypothesis: while languages may vary in the organization of grammar and meaning, key systems of language use may be largely similar across cultural groups. They also provide a fresh perspective on controversies about the core properties of language, by revealing a common infrastructure for social interaction which may be the universal bedrock upon which linguistic diversity rests.},
  eprint = {26375483},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\S4WAC66J\\Dingemanse et al. - 2015 - Universal Principles in the Repair of Communicatio.pdf},
  number = {9},
  pmcid = {PMC4573759}
}

@online{Doi101016,
  title = {Doi:10.1016/{{S1364}}-6613(03)00136-0 | {{Elsevier Enhanced Reader}}},
  url = {https://reader.elsevier.com/reader/sd/pii/S1364661303001360?token=641BF18A47FF7B99F9BFC808B67B13B78B765E129697D3B02D2B89C46823CF0850CD50831C636F4BD44E539F83A46F4B},
  urldate = {2020-02-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9G5YHC7F\\S1364661303001360.html}
}

@book{donaldOriginsModernMind1991,
  title = {Origins of the Modern Mind: {{Three}} Stages in the Evolution of Culture and Cognition},
  author = {Donald, M.},
  date = {1991},
  publisher = {{Boston: Harvard University Press}}
}

@article{drijversVisualContextEnhanced2017,
  title = {Visual {{Context Enhanced}}: {{The Joint Contribution}} of {{Iconic Gestures}} and {{Visible Speech}} to {{Degraded Speech Comprehension}}},
  shorttitle = {Visual {{Context Enhanced}}},
  author = {Drijvers, Linda and Özyürek, Asli},
  date = {2017-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  volume = {60},
  pages = {212--222},
  issn = {1092-4388, 1558-9102},
  doi = {10.1044/2016_JSLHR-H-16-0101},
  url = {http://pubs.asha.org/doi/10.1044/2016_JSLHR-H-16-0101},
  urldate = {2019-09-18},
  abstract = {Purpose: This study investigated whether and to what extent iconic co-speech gestures 7 8 contribute to information from visible speech to enhance degraded speech comprehension at 9 10 11 different levels of noise-vocoding. Previously, the contributions of these two visual 12 13 articulators to speech comprehension have only been studied separately. 14 15 16
Method: Twenty participants watched videos of an actress uttering an action verb and 17 18 completed a free-recall task. The videos were presented in three speech (2-band; 6-band For Peer Review 19 20 noise-vocoding; clear), three multimodal (Speech+Lips blurred; Speech+VisibleSpeech; 21 22 Speech+VisibleSpeech+Gesture) and two visual only conditions (VisibleSpeech; 23 24 25 VisibleSpeech+Gesture). 26 27 28
Results: Accuracy levels were higher when both visual articulators were present compared to 29 30 one or none. The enhancement effects of a) visible speech, b) gestural information on top of 31 32 visible speech and c) both visible speech and iconic gestures were larger in 6-band than 233 34 band noise-vocoding or visual only conditions. Gestural enhancement in 2-band noise35 36 37 vocoding did not differ from gestural enhancement in visual only conditions. 38 39 40
Conclusions: When perceiving degraded speech in a visual context, listeners benefit more 41 42},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\UTLHPE5D\\Drijvers and Özyürek - 2017 - Visual Context Enhanced The Joint Contribution of.pdf},
  langid = {english},
  number = {1}
}

@book{dunbarHumanEvolutionOur2016,
  title = {Human {{Evolution}}: {{Our Brains}} and {{Behavior}}},
  author = {Dunbar, R.},
  date = {2016},
  publisher = {{Oxford University Press}},
  location = {{Oxford}}
}

@online{DynamicsMultipleSignalling,
  title = {Dynamics of Multiple Signalling Systems: Animal Communication in a World in Flux | {{Elsevier Enhanced Reader}}},
  url = {https://reader.elsevier.com/reader/sd/pii/S0169534709003450?token=5025236735D1260CACE1396EEDD537C42FB865B2301A20CC868105583859347FDC86D5B96C5CE1B9DEFE2046778DC01A},
  urldate = {2019-11-15},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\28XCG4GC\\S0169534709003450.html}
}

@online{DynamicsMultipleSignallinga,
  title = {Dynamics of Multiple Signalling Systems: Animal Communication in a World in Flux | {{Elsevier Enhanced Reader}}},
  shorttitle = {Dynamics of Multiple Signalling Systems},
  doi = {10.1016/j.tree.2009.11.003},
  url = {https://reader.elsevier.com/reader/sd/pii/S0169534709003450?token=5025236735D1260CACE1396EEDD537C42FB865B2301A20CC868105583859347FDC86D5B96C5CE1B9DEFE2046778DC01A},
  urldate = {2019-11-15},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4HW3XZPQ\\S0169534709003450.html},
  langid = {english}
}

@incollection{edelsbrunnerPersistentHomologyTheory2013,
  title = {Persistent {{Homology}}: {{Theory}} and {{Practice}}},
  shorttitle = {Persistent {{Homology}}},
  booktitle = {European {{Congress}} of {{Mathematics Kraków}}, 2 – 7 {{July}}, 2012},
  author = {Edelsbrunner, Herbert and Morozov, Dmitriy},
  editor = {Latała, Rafał and Ruciński, Andrzej and Strzelecki, Paweł and Świątkowski, Jacek and Wrzosek, Dariusz and Zakrzewski, Piotr},
  date = {2013-11-30},
  pages = {31--50},
  publisher = {{European Mathematical Society Publishing House}},
  location = {{Zuerich, Switzerland}},
  doi = {10.4171/120-1/3},
  url = {http://www.ems-ph.org/doi/10.4171/120-1/3},
  urldate = {2020-03-11},
  abstract = {Persistent homology is a recent grandchild of homology that has found use in science and engineering as well as in mathematics. This paper surveys the method as well as the applications, neglecting completeness in favor of highlighting ideas and directions.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZW8JN79S\\Edelsbrunner and Morozov - 2013 - Persistent Homology Theory and Practice.pdf},
  isbn = {978-3-03719-120-0},
  langid = {english}
}

@article{edelsbrunnerTopologicalPersistenceSimplification2002,
  title = {Topological {{Persistence}} and {{Simplification}}},
  author = {{Edelsbrunner} and {Letscher} and {Zomorodian}},
  date = {2002-11},
  journaltitle = {Discrete \& Computational Geometry},
  volume = {28},
  pages = {511--533},
  issn = {0179-5376, 1432-0444},
  doi = {10.1007/s00454-002-2885-2},
  url = {http://link.springer.com/10.1007/s00454-002-2885-2},
  urldate = {2020-01-06},
  abstract = {We formalize a notion of topological simplification within the framework of a filtration, which is the history of a growing complex. We classify a topological change that happens during growth as either a feature or noise depending on its lifetime or persistence within the filtration. We give fast algorithms for computing persistence and experimental evidence for their speed and utility.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9Z9IY3DU\\Edelsbrunner et al. - 2002 - Topological Persistence and Simplification.pdf},
  langid = {english},
  number = {4}
}

@online{EffectSyllableArticulation,
  title = {Effect of {{Syllable Articulation}} on {{Precision}} and {{Power Grip Performance}}},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0053061},
  urldate = {2020-02-24},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\X3RY27NN\\article.html}
}

@article{ejiriCooccurencesPreverbalVocal2001,
  title = {Co-Occurences of Preverbal Vocal Behavior and Motor Action in Early Infancy},
  author = {Ejiri, Keiko and Masataka, Nobuo},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {40--48},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00147},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00147},
  urldate = {2020-01-26},
  abstract = {This study reports on co-occurrence of vocal behaviors and motor actions in infants in the prelinguistic stage. Four Japanese infants were studied longitudinally from the age of 6 months to 11 months. For all the infants, a 40 min sample was coded for each monthly period. The vocalizations produced by the infants co-occurred with their rhythmic actions with high frequency, particularly in the period preceding the onset of canonical babbling. Acoustical analysis was conducted on the vocalizations recorded before and after the period when co-occurrence took place most frequently. Among the vocalizations recorded in the period when co-occurrence appeared most frequently, those that co-occurred with rhythmic action had significantly shorter syllable duration and shorter formant-frequency transition duration compared with those that did not co-occur with rhythmic action. The rapid transitions and short syllables were similar to patterns of duration found in mature speech. The acoustic features remained even after co-occurrence disappeared. These findings suggest that co-occurrence of rhythmic action and vocal behavior may contribute to the infant’s acquisition of the ability to perform the rapid glottal and articulatory movements that are indispensable for spoken language acquisition.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7EB3ZUMA\\1467-7687.html},
  langid = {english},
  number = {1}
}

@article{ejiriRelationshipRhythmicBehavior1998,
  title = {Relationship between {{Rhythmic Behavior}} and {{Canonical Babbling}} in {{Infant Vocal Development}}},
  author = {Ejiri, Keiko},
  date = {1998},
  journaltitle = {Phonetica},
  volume = {55},
  pages = {226--237},
  issn = {1423-0321, 0031-8388},
  doi = {10.1159/000028434},
  url = {https://www.karger.com/Article/FullText/28434},
  urldate = {2020-01-26},
  abstract = {The onset of canonical babbling (CB) is a landmark event in infants’ vocal development for spoken language. Previous research has suggested that the onset of CB coincides with the peak period of rhythmic activities. To examine this phenomenon in detail, 28 Japanese infants (14 girls, 14 boys) were observed longitudinally from the age of 5 to 9 months. In the experimental sessions, an audible or an inaudible rattle was placed into a hand of each tested infant. Then the number of times that the infant shook the rattle was counted. In the observational sessions, infants’ spontaneous rhythmic activities under natural conditions were observed. The result shows that rhythmic activities reached their peak around the onset of CB. When the infants began to babble, they shook whichever rattle was in their hand, regardless of its audibility. After this period, they shook the audible rattles more frequently than the inaudible ones. These findings suggest that, around the onset of CB, infants learn to control their motor activities based on auditory feedback.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\N5V7PDX5\\Ejiri - 1998 - Relationship between Rhythmic Behavior and Canonic.pdf},
  langid = {english},
  number = {4}
}

@software{eklundBeeswarmBeeSwarm2016,
  title = {Beeswarm: {{The Bee Swarm Plot}}, an {{Alternative}} to {{Stripchart}}},
  shorttitle = {Beeswarm},
  author = {Eklund, Aron},
  date = {2016-04-25},
  url = {https://CRAN.R-project.org/package=beeswarm},
  urldate = {2019-04-23},
  abstract = {The bee swarm plot is a one-dimensional scatter plot like "stripchart", but with closely-packed, non-overlapping points.},
  version = {0.2.3}
}

@online{EmergenceCombinatorialStructure,
  title = {Emergence of Combinatorial Structure and Economy through Iterated Learning with Continuous Acoustic Signals | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.wocn.2014.02.005},
  url = {https://reader.elsevier.com/reader/sd/pii/S0095447014000205?token=269ED5275540ECD10962036E68A36E9B8E5FA4F5C3C613CD1A2C61CFF7FDE03A3AB6E9E71D87DDD3BFC3D7933ED769A6},
  urldate = {2020-03-09},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9BQHNLMM\\S0095447014000205.html},
  langid = {english}
}

@online{EmergenceDynamicalOrder,
  title = {Emergence of {{Dynamical Order}}: {{Synchronization Phenomena}} in {{Complex Systems}} - {{Susanna C}}. {{Manrubia}}, {{Alexander S}}. {{Mikhailov}}, {{Dam}}¡an {{H}}. {{Zannette}} - {{Google Books}}},
  url = {https://books.google.nl/books?id=w6RpDQAAQBAJ&pg=PA340&lpg=PA340&dq=pikovsky+emergence&source=bl&ots=uKWbVFXqRZ&sig=ACfU3U1zs5_xWCrVeyw3FXY_clGXvKTRCA&hl=en&sa=X&ved=2ahUKEwj5l6S516zjAhUrIMUKHW7QBCYQ6AEwAHoECAkQAQ#v=onepage&q=pikovsky%20emergence&f=false},
  urldate = {2019-07-11},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QRZ95AGX\\books.html}
}

@book{enfieldNaturalCausesLanguage2016,
  title = {Natural causes of language: Frames, biases, and cultural transmission},
  shorttitle = {Natural causes of language},
  author = {Enfield, N.J.},
  date = {2016-12-12},
  publisher = {{Language Science Press}},
  location = {{Berlin}},
  url = {https://langsci-press.org/catalog/book/48},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SW5ZP9CB\\48.html},
  langid = {en\_},
  series = {Conceptual Foundations of Language Science}
}

@article{engesserCombinatorialityVocalSystems2019,
  title = {Combinatoriality in the Vocal Systems of Nonhuman Animals},
  author = {Engesser, Sabrina and Townsend, Simon W.},
  date = {2019},
  journaltitle = {WIREs Cognitive Science},
  volume = {10},
  pages = {e1493},
  issn = {1939-5086},
  doi = {10.1002/wcs.1493},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1493},
  urldate = {2020-03-25},
  abstract = {A key challenge in the field of human language evolution is the identification of the selective conditions that gave rise to language's generative nature. Comparative data on nonhuman animals provides a powerful tool to investigate similarities and differences among nonhuman and human communication systems and to reveal convergent evolutionary mechanisms. In this article, we provide an overview of the current evidence for combinatorial structures found in the vocal system of diverse species. We show that considerable structural diversity exits across and within species in the forms of combinatorial structures used. Based on this we suggest that a fine-grained classification and differentiation of combinatoriality is a useful approach permitting systematic comparisons across animals. Specifically, this will help to identify factors that might promote the emergence of combinatoriality and, crucially, whether differences in combinatorial mechanisms might be driven by variations in social and ecological conditions or cognitive capacities. This article is categorized under: Cognitive Biology {$>$} Evolutionary Roots of Cognition Linguistics {$>$} Evolution of Language},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VH6GGANA\\Engesser and Townsend - 2019 - Combinatoriality in the vocal systems of nonhuman .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\7NH859KG\\wcs.html},
  keywords = {animal communication,combinatoriality,language evolution},
  langid = {english},
  number = {4}
}

@article{esteve-gibertProsodicStructureShapes2013,
  title = {Prosodic Structure Shapes the Temporal {{Realization}} of Intonation and Manual Gesture Movements},
  author = {Esteve-Gibert, N. and Prieto, P.},
  date = {2013-06-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {56},
  pages = {850--864},
  doi = {10.1044/1092-4388(2012/12-0049)},
  url = {https://jslhr.pubs.asha.org/doi/full/10.1044/1092-4388%282012/12-0049%29},
  urldate = {2019-04-16},
  abstract = {Purpose
      Previous work on the temporal coordination between gesture and speech found that the
         prominence in gesture coordinates with speech prominence. In this study, the authors
         investigated the anchoring regions in speech and pointing gesture that align with
         each other. The authors hypothesized that (a) in contrastive focus conditions, the
         gesture apex is anchored in the intonation peak and (b) the upcoming prosodic boundary
         influences the timing of gesture and intonation movements.
      
      
      Method
      Fifteen Catalan speakers pointed at a screen while pronouncing a target word with
         different metrical patterns in a contrastive focus condition and followed by a phrase
         boundary. A total of 702 co-speech deictic gestures were acoustically and gesturally
         analyzed.
      
      
      Results
      Intonation peaks and gesture apexes showed parallel behavior with respect to their
         position within the accented syllable: They occurred at the end of the accented syllable
         in non–phrase-final position, whereas they occurred well before the end of the accented
         syllable in phrase-final position. Crucially, the position of intonation peaks and
         gesture apexes was correlated and was bound by prosodic structure.
      
      
      Conclusions
      The results refine the phonological synchronization rule (McNeill, 1992), showing
         that gesture apexes are anchored in intonation peaks and that gesture and prosodic
         movements are bound by prosodic phrasing.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5K5WWP56\\Esteve-Gibert Núria and Prieto Pilar - 2013 - Prosodic Structure Shapes the Temporal Realization.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\QM79RK99\\12-0049).html},
  number = {3}
}

@article{esteve-gibertProsodyAuditoryVisual2018,
  title = {Prosody in the {{Auditory}} and {{Visual Domains}}: {{A Developmental Perspective}}},
  author = {Esteve-Gibert, N. and Guellai, Bahia},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2018.00338},
  url = {http://www.readcube.com/articles/10.3389/fpsyg.2018.00338},
  urldate = {2019-08-09},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\683BYBAJ\\fpsyg.2018.html}
}

@online{EvolutionaryDynamicsDispersal,
  title = {Evolutionary Dynamics in the Dispersal of Sign Languages | {{Royal Society Open Science}}},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.191100},
  urldate = {2020-02-20},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3CXKSNQZ\\rsos.html}
}

@report{fairhurstReciprocityAlignmentQuantifying2019,
  title = {Reciprocity and Alignment: Quantifying Coupling in Dynamic Interactions},
  shorttitle = {Reciprocity and Alignment},
  author = {Fairhurst, Merle Theresa and Dumas, Guillaume},
  date = {2019-03-25},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/nmg4x},
  url = {https://osf.io/nmg4x},
  urldate = {2020-04-10},
  abstract = {Recent accounts of social cognition focus on how we do things together suggesting that becoming aligned relies on a reciprocal exchange of information. The next step is to develop richer computational methods that quantify the degree of coupling and describe the nature of the information exchange. We put forward a definition of coupling comparing it to related terminology and detail available computational methods and the level of organisation to which they pertain, presenting them as a hierarchy from weakest to richest forms of coupling. The rationale is that a temporally coherent link between two dynamical systems at the lowest level of organisation sustains mutual adaptation and alignment at the highest level. Postulating that when we do things together, we do so dynamically over time, we argue that to determine and measure instances of true reciprocity in social exchanges is key. Along with this computationally rich definition of coupling, we present challenges for the field to be tackled by a diverse community working towards a dynamic account of social cognition.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IG85C3SH\\Fairhurst and Dumas - 2019 - Reciprocity and alignment quantifying coupling in.pdf},
  langid = {english},
  type = {preprint}
}

@article{falandaysInteractionismLanguageNeural2018,
  title = {Interactionism in Language: {{From}} Neural Networks to Bodies to Dyads},
  shorttitle = {Interactionism in Language},
  author = {Falandays, J. Benjamin and Batzloff, Brandon J. and Spevack, Samuel C. and Spivey, Michael J.},
  date = {2018},
  journaltitle = {Language, Cognition and Neuroscience},
  pages = {No Pagination Specified-No Pagination Specified},
  issn = {2327-3801(Electronic),2327-3798(Print)},
  doi = {10.1080/23273798.2018.1501501},
  abstract = {In a science of language, it can be useful to partition different formats of linguistic information into different categories, such as phonetics, phonology, semantics, and syntax. However, when the actual phenomena of language processing cross those boundaries and blur those lines, it can become difficult to understand how these different formats of linguistic information maintain their integrity while engaging in complex interactions with one another. For example, if the function of a cortical network that is known to process phonetics is immediately taking into account contextual influences from a cortical network that is known to process semantics, then it seems clear that this “phonetic cortical network” is doing more than just phonetics. In the neuroscience and cognitive science of language, the scope of analysis where different formats of linguistic information are seen to interact reveals a wide array of context effects in almost every possible direction. When one expands the scope of analysis to include nonlinguistic sensory modalities, such as vision and action, research is showing that even those lines are getting blurred. Visual perception and motor movement appear to influence various aspects of language processing in real time. As this scope of analysis expands further still, research is showing that two human brains and bodies exhibit various forms of synchrony or coordination with one another during natural conversation. Interactionism at all these levels of analysis poses a challenge to traditional frameworks that treat different components of language, perception, and action as operating via domain specific computations. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7GGTYQQG\\2018-36198-001.html}
}

@article{fayInteractiveEvolutionHuman2010,
  title = {The Interactive Evolution of Human Communication Systems},
  author = {Fay, Nicolas and Garrod, Simon and Roberts, Leo and Swoboda, Nik},
  date = {2010-04},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  volume = {34},
  pages = {351--386},
  issn = {1551-6709},
  doi = {10.1111/j.1551-6709.2009.01090.x},
  abstract = {This paper compares two explanations of the process by which human communication systems evolve: iterated learning and social collaboration. It then reports an experiment testing the social collaboration account. Participants engaged in a graphical communication task either as a member of a community, where they interacted with seven different partners drawn from the same pool, or as a member of an isolated pair, where they interacted with the same partner across the same number of games. Participants' horizontal, pair-wise interactions led "bottom up" to the creation of an effective and efficient shared sign system in the community condition. Furthermore, the community-evolved sign systems were as effective and efficient as the local sign systems developed by isolated pairs. Finally, and as predicted by a social collaboration account, and not by an iterated learning account, interaction was critical to the creation of shared sign systems, with different isolated pairs establishing different local sign systems and different communities establishing different global sign systems.},
  eprint = {21564217},
  eprinttype = {pmid},
  langid = {english},
  number = {3}
}

@article{ferrericanchoPatternsSyntacticDependency2004,
  title = {Patterns in Syntactic Dependency Networks},
  author = {Ferrer i Cancho, Ramon and Solé, Ricard V. and Köhler, Reinhard},
  date = {2004-05-26},
  journaltitle = {Physical Review E},
  volume = {69},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.69.051915},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.051915},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XC7Y9XLI\\Ferrer i Cancho et al. - 2004 - Patterns in syntactic dependency networks.pdf},
  langid = {english},
  number = {5}
}

@book{feyereisenCognitivePsychologySpeechRelated2017,
  title = {The {{Cognitive Psychology}} of {{Speech}}-{{Related Gesture}}},
  author = {Feyereisen, P.},
  date = {2017-07-28},
  publisher = {{Routledge}},
  location = {{New York}},
  abstract = {Why do we gesture when we speak? The Cognitive Psychology of Speech-Related Gesture offers answers to this question while introducing readers to the huge interdisciplinary field of gesture. Drawing on ideas from cognitive psychology, this book highlights key debates in gesture research alongside advocating new approaches to conventional thinking.  Beginning with the definition of the notion of communication, this book explores experimental approaches to gesture production and comprehension, the possible gestural origin of language and its implication for brain organization, and the development of gestural communication from infancy to childhood. Through these discussions the author presents the idea that speech-related gestures are not just peripheral phenomena, but rather a key function of the cognitive architecture, and should consequently be studied alongside traditional concepts in cognitive psychology.  The Cognitive Psychology of Speech Related Gesture offers a broad overview which will be essential reading for all students of gesture research and language, as well as speech therapists, teachers and communication practitioners. It will also be of interest to anybody who is curious about why we move our bodies when we talk.},
  eprint = {nJguDwAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-1-351-78827-4},
  keywords = {Psychology / Cognitive Psychology & Cognition,Psychology / General},
  langid = {english},
  pagetotal = {353}
}

@article{filippiTemporalModulationSpeech2019,
  title = {Temporal Modulation in Speech, Music, and Animal Vocal Communication: Evidence of Conserved Function},
  shorttitle = {Temporal Modulation in Speech, Music, and Animal Vocal Communication},
  author = {Filippi, Piera and Hoeschele, Marisa and Spierings, Michelle and Bowling, Daniel L.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  volume = {1453},
  pages = {99--113},
  issn = {1749-6632},
  doi = {10.1111/nyas.14228},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14228},
  urldate = {2019-10-22},
  abstract = {Speech is a distinctive feature of our species. It is the default channel for language and constitutes our primary mode of social communication. Determining the evolutionary origins of speech is a challenging prospect, in large part because it appears to be unique in the animal kingdom. However, direct comparisons between speech and other forms of acoustic communication, both in humans (music) and animals (vocalization), suggest that important components of speech are shared across domains and species. In this review, we focus on a single aspect of speech—temporal patterning—examining similarities and differences across speech, music, and animal vocalization. Additional structure is provided by focusing on three specific functions of temporal patterning across domains: (1) emotional expression, (2) social interaction, and (3) unit identification. We hypothesize an evolutionary trajectory wherein the ability to identify units within a continuous stream of vocal sounds derives from social vocal interaction, which, in turn, derives from vocal emotional communication. This hypothesis implies that unit identification has parallels in music and precursors in animal vocal communication. Accordingly, we demonstrate the potential of comparisons between fundamental domains of biological acoustic communication to provide insight into the evolution of language.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PWE2GTN9\\nyas.html},
  keywords = {emotion expression,language evolution,social interaction,temporal patterns,unit identification},
  langid = {english},
  number = {1}
}

@article{fitchMonkeyVocalTracts2016,
  title = {Monkey Vocal Tracts Are Speech-Ready},
  author = {Fitch, W. Tecumseh and de Boer, Bart and Mathur, Neil and Ghazanfar, Asif A.},
  date = {2016-12-01},
  journaltitle = {Science Advances},
  volume = {2},
  pages = {e1600723},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.1600723},
  url = {https://advances.sciencemag.org/content/2/12/e1600723},
  urldate = {2020-03-06},
  abstract = {For four decades, the inability of nonhuman primates to produce human speech sounds has been claimed to stem from limitations in their vocal tract anatomy, a conclusion based on plaster casts made from the vocal tract of a monkey cadaver. We used x-ray videos to quantify vocal tract dynamics in living macaques during vocalization, facial displays, and feeding. We demonstrate that the macaque vocal tract could easily produce an adequate range of speech sounds to support spoken language, showing that previous techniques based on postmortem samples drastically underestimated primate vocal capabilities. Our findings imply that the evolution of human speech capabilities required neural changes rather than modifications of vocal anatomy. Macaques have a speech-ready vocal tract but lack a speech-ready brain to control it.
X-ray analyses of macaque vocal tract movements show that monkeys’ inability to speak is not due to limitations of peripheral anatomy.
X-ray analyses of macaque vocal tract movements show that monkeys’ inability to speak is not due to limitations of peripheral anatomy.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9DPG5VVG\\Fitch et al. - 2016 - Monkey vocal tracts are speech-ready.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\SN5IVXBR\\e1600723.html},
  langid = {english},
  number = {12}
}

@incollection{fletcherProsodySpeechTiming2010,
  title = {The {{Prosody}} of {{Speech}}: {{Timing}} and {{Rhythm}}},
  shorttitle = {The {{Prosody}} of {{Speech}}},
  booktitle = {The {{Handbook}} of {{Phonetic Sciences}}},
  author = {Fletcher, Janet},
  date = {2010},
  pages = {521--602},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781444317251.ch15},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781444317251.ch15},
  urldate = {2020-03-17},
  abstract = {This chapter contains sections titled: Introduction Lengthenings and Shortenings: The Temporal Signatures of Prosody Speech Timing: A Rhythmic Dimension Tempo and Pausing Concluding Comments References},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FVIAETUK\\9781444317251.html},
  isbn = {978-1-4443-1725-1},
  keywords = {influential interactive segment duration model - rule system by Klatt,intermediate phrase,intonational phrase,isochrony - perceptual phenomenon in spoken English and “stress-timed” languages in general,lengthenings and shortenings - temporal signatures of prosody,measuring tempo - monitoring speaking tempo,prosody of speech - timing and rhythm,segmental and syllable timing patterns – prosodic word,speaking rate and articulation rate,speech - activity in unfolding time,speech rhythms - successions and alternations of events with specific temporal paradigm,syllable duration - factors contributing to syllable timing versus stress timing,syllable structure,utterance,vowel reduction - maximizing difference between stressed and unstressed syllables},
  langid = {english}
}

@article{fowlerEmbodiedEmbeddedLanguage2010,
  title = {Embodied, {{Embedded Language Use}}},
  author = {Fowler, Carol A.},
  date = {2010-10-01},
  journaltitle = {Ecological psychology : a publication of the International Society for Ecological Psychology},
  volume = {22},
  pages = {286},
  issn = {10.1080/10407413.2010.517115},
  doi = {10.1080/10407413.2010.517115},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3020794/},
  urldate = {2019-05-04},
  abstract = {Language use has a public face that is as important to study as the private faces under intensive psycholinguistic study. In the domain of phonology, public use of speech must meet an interpersonal “parity” constraint if it is to serve ...},
  eprint = {21243080},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IL765TVU\\Fowler - 2010 - Embodied, Embedded Language Use.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\WLRZ6HFX\\PMC3020794.html},
  langid = {english},
  number = {4}
}

@article{fowlerEmbodiedEmbeddedLanguage2010a,
  title = {Embodied, {{Embedded Language Use}}},
  author = {Fowler, Carol A.},
  date = {2010-10-01},
  journaltitle = {Ecological psychology : a publication of the International Society for Ecological Psychology},
  shortjournal = {Ecol Psychol},
  volume = {22},
  pages = {286--303},
  issn = {1040-7413},
  doi = {10.1080/10407413.2010.517115},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3020794/},
  urldate = {2020-03-08},
  abstract = {Language use has a public face that is as important to study as the private faces under intensive psycholinguistic study. In the domain of phonology, public use of speech must meet an interpersonal “parity” constraint if it is to serve to communicate. That is, spoken language forms must reliably be identified by listeners. To that end, language forms are embodied, at the lowest level of description, as phonetic gestures of the vocal tract that lawfully structure informational media such as air and light. Over time, under the parity constraint, sound inventories emerge over communicative exchanges that have the property of sufficient identifiability., Communicative activities involve more than vocal tract actions. Talkers gesture and use facial expressions and eye gaze to communicate. Listeners embody their language understandings, exhibiting dispositions to behave in ways related to language understanding. Moreover, linguistic interchanges are embedded in the larger context of language use. Talkers recruit the environment in their communicative activities, for example, in using deictic points. Moreover, in using language as a “coordination device,” interlocutors mutually entrain.},
  eprint = {21243080},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3PWVMW4Z\\Fowler - 2010 - Embodied, Embedded Language Use.pdf},
  number = {4},
  pmcid = {PMC3020794}
}

@article{fowlerEventApproachStudy1986,
  title = {An Event Approach to the Study of Speech Perception from a Direct-Realist Perspective},
  author = {Fowler, Carol A.},
  date = {1986},
  journaltitle = {Journal of Phonetics},
  volume = {14},
  pages = {3--28},
  issn = {1095-8576(Electronic),0095-4470(Print)},
  abstract = {Proposes an event approach to a theory of speech perception and speech production, focusing on the perception of speech events (i.e., a talker's phonetically structured articulations). In defining a speech event interchangeably from the perspectives of talkers and listeners, the author adopts a "direct realist" perspective: Perception is assumed to recover events in the real world. To do this, perception must be direct and unmediated by cognitive processes of inference or hypothesis testing. Barriers to the theory are outlined and evidence presented to refute them. Support for direct perception of local, short-term events and longer ones is discussed. Attention is also given to the way in which perception of a linguistic message guides a listener's behavior. Differences in the reliability of the information conveyed between direct and indirect perception and the implications for the relation between an utterance and what it signified are examined. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YUI62C42\\1987-23991-001.html},
  keywords = {Articulation (Speech),Phonetics,Speech Perception,Theories},
  number = {1}
}

@article{fowlerListeningEyeHand1991,
  title = {Listening with Eye and Hand: {{Cross}}-Modal Contributions to Speech Perception},
  shorttitle = {Listening with Eye and Hand},
  author = {Fowler, Carol A. and Dekle, Dawn J.},
  date = {1991},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {17},
  pages = {816--828},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.17.3.816},
  abstract = {Three experiments investigated the "McGurk effect" whereby optically specified syllables experienced synchronously with acoustically specified syllables integrate in perception to determine a listener's auditory perceptual experience. Experiments contrasted the cross-modal effect of orthographic on acoustic syllables presumed to be associated in experience and memory with that of haptically experienced and acoustic syllables presumed not to be associated. The latter pairing gave rise to cross-modal influences when Ss were informed that cross-modal syllables were paired independently. Mouthed syllables affected reports of simultaneously heard syllables (and vice versa). These effects were absent when syllables were simultaneously seen (spelled) and heard. The McGurk effect does not arise from association in memory but from conjoint near specification of the same casual source in the environment—in speech, the moving vocal tract producing phonetic gestures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NRS9DDFB\\1992-00274-001.html},
  keywords = {Auditory Perception,Auditory Stimulation,Orthography,Speech Perception},
  number = {3}
}

@article{frohlichMultimodalCommunicationLanguage2019,
  title = {Multimodal Communication and Language Origins: Integrating Gestures and Vocalizations},
  shorttitle = {Multimodal Communication and Language Origins},
  author = {Fröhlich, Marlen and Sievers, Christine and Townsend, Simon W. and Gruber, Thibaud and van Schaik, Carel P.},
  date = {2019},
  journaltitle = {Biological Reviews},
  volume = {94},
  pages = {1809--1829},
  issn = {1469-185X},
  doi = {10.1111/brv.12535},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12535},
  urldate = {2019-12-09},
  abstract = {The presence of divergent and independent research traditions in the gestural and vocal domains of primate communication has resulted in major discrepancies in the definition and operationalization of cognitive concepts. However, in recent years, accumulating evidence from behavioural and neurobiological research has shown that both human and non-human primate communication is inherently multimodal. It is therefore timely to integrate the study of gestural and vocal communication. Herein, we review evidence demonstrating that there is no clear difference between primate gestures and vocalizations in the extent to which they show evidence for the presence of key language properties: intentionality, reference, iconicity and turn-taking. We also find high overlap in the neurobiological mechanisms producing primate gestures and vocalizations, as well as in ontogenetic flexibility. These findings confirm that human language had multimodal origins. Nonetheless, we note that in great apes, gestures seem to fulfil a carrying (i.e. predominantly informative) role in close-range communication, whereas the opposite holds for face-to-face interactions of humans. This suggests an evolutionary shift in the carrying role from the gestural to the vocal stream, and we explore this transition in the carrying modality. Finally, we suggest that future studies should focus on the links between complex communication, sociality and cooperative tendency to strengthen the study of language origins.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QP3HIK3A\\Fröhlich et al. - 2019 - Multimodal communication and language origins int.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8H8XMILJ\\brv.html},
  keywords = {cognition,comparative approach,evolution of language,gestural origins,learning,multimodality,ontogeny,primates,vocal origins},
  langid = {english},
  number = {5}
}

@article{frohlichMustAllSignals2020,
  title = {Must All Signals Be Evolved? {{A}} Proposal for a New Classification of Communicative Acts},
  shorttitle = {Must All Signals Be Evolved?},
  author = {Fröhlich, Marlen and van Schaik, Carel P.},
  date = {2020-03-16},
  journaltitle = {WIREs Cognitive Science},
  issn = {1939-5078, 1939-5086},
  doi = {10.1002/wcs.1527},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1527},
  urldate = {2020-03-24},
  abstract = {While signals in evolutionary biology are usually defined as “acts or traits that have evolved because of their effect on others”, work on gestures and vocalizations in various animal taxa have revealed population- or even individual-specific meanings of social signals. These results strongly suggest that communicative acts that are like signals with regard to both form and function (meaning) can also be acquired ontogenetically, and we discuss direct evidence for such plasticity in captive settings with rich opportunities for repeated social interactions with the same individuals. Therefore, in addition to evolved signals, we can recognize invented signals that are acquired during ontogeny (either through ontogenetic ritualization or social transmission). Thus, both gestures and vocalizations can be inventions or innate adaptations. We therefore propose to introduce innate versus invented signals as major distinct categories, with invented signals subdivided into dyad-specific and cultural signals. We suggest that elements of some signals may have mixed origins, and propose criteria to recognize acquired features of signals. We also suggest that invented signals may be most common in species with intentional communication, consistent with their ubiquity in humans, and that the ability to produce them was a necessary condition for the evolution of language.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\V5THSFP7\\Fröhlich and van Schaik - 2020 - Must all signals be evolved A proposal for a new .pdf},
  langid = {english},
  options = {useprefix=true}
}

@article{fuchsAssessingRespiratoryContributions2015,
  title = {Assessing Respiratory Contributions to F0 Declination in {{German}} across Varying Speech Tasks and Respiratory Demands},
  author = {Fuchs, S. and Petrone, C. and Rochet-Capellan, A. and Reichel, W. D. and Koenig, L. L.},
  date = {2015},
  journaltitle = {Journal of Phonetics},
  volume = {52},
  pages = {35--45},
  doi = {10.1016/j.wocn.2015.04.002},
  url = {https://hal.archives-ouvertes.fr/hal-01164773},
  urldate = {2019-08-08},
  abstract = {Many past studies have sought to determine the factors that affect f0 declination, and the physiological underpinnings of the phenomenon.  This study assessed the relation between respiration and f0 declination by means of simultaneous acoustic and respiratory recordings from read and spontaneous speech from speakers of German. Within the respective Intonational Phrase unit, we analysed the effect of the number of syllables and voiceless obstruents. Both factors could influence the slope of either f0 declination or rib cage movement. If respiration and f0 declination are related physiologically, their relationship might also be modulated by either one or both factors. Our results show consistently for both speech tasks that the slope of the rib cage movement is not related with f0 declination when length and consonant content vary. Furthermore f0 slopes are generally shallower in spontaneous than in read speech. Finally, although a higher number of voiceless obstruents yielded a greater rib cage compression, it did not affect f0 declination. These results suggest that although f0 declination occurs in many languages, it might not have a purely physiological origin in breathing, but rather reflects cognitive processing which allows speakers to look ahead when planning their utterances.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\49ZD9E2Z\\Fuchs et al. - 2015 - Assessing respiratory contributions to f0 declinat.pdf},
  keywords = {f0 declination,number of syllables,reading,respiration,speech tasks,spontaneous speech,voiceless obstruents}
}

@article{fuchsExploringSourceShortterm2019,
  title = {Exploring the Source of Short-Term Variations in Respiratory Data},
  author = {Fuchs, Susanne and Koenig, Laura L. and Petrone, Caterina},
  date = {2019-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {145},
  pages = {EL66-EL71},
  issn = {0001-4966},
  doi = {10.1121/1.5087272},
  url = {https://asa.scitation.org/doi/10.1121/1.5087272},
  urldate = {2019-08-08},
  abstract = {This study explores short-term respiratory volume changes in German oral and nasal stops and discusses to what extent these changes may be explained by laryngeal-oral coordination. It is expected that respiratory volumes decrease more rapidly when the glottis and the vocal tract are open after the release of voiceless aspirated stops. Two experiments were performed using Inductance Plethysmography and acoustics, varying consonantal properties, loudness, and prosodic focus. Results show consistent differences in respiratory slopes between voiceless vs voiced and nasal stops, which are more extreme in a loud or focused position. Thus, respiratory changes can even occur at a local level.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FZBWF5AZ\\Fuchs et al. - 2019 - Exploring the source of short-term variations in r.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\7J4V352F\\1.html},
  number = {1}
}

@article{fusaroliDialogInterpersonalSynergy2014,
  title = {Dialog as Interpersonal Synergy},
  author = {Fusaroli, Riccardo and Rączaszek-Leonardi, Joanna and Tylén, Kristian},
  date = {2014-01-01},
  journaltitle = {New Ideas in Psychology},
  shortjournal = {New Ideas in Psychology},
  volume = {32},
  pages = {147--157},
  issn = {0732-118X},
  doi = {10.1016/j.newideapsych.2013.03.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0732118X13000342},
  urldate = {2020-05-14},
  abstract = {What is the proper unit of analysis in the psycholinguistics of dialog? While classical approaches are largely based on models of individual linguistic processing, recent advances stress the social coordinative nature of dialog. In the influential interactive alignment model, dialogue is thus approached as the progressive entrainment of interlocutors' linguistic behaviors toward the alignment of situation models. Still, the driving mechanisms are attributed to individual cognition in the form of automatic structural priming. Challenging these ideas, we outline a dynamical framework for studying dialog based on the notion of interpersonal synergy. Crucial to this synergetic model is the emphasis on dialog as an emergent, self-organizing, interpersonal system capable of functional coordination. A consequence of this model is that linguistic processes cannot be reduced to the workings of individual cognitive systems but must be approached also at the interpersonal level. From the synergy model follows a number of new predictions: beyond simple synchrony, good dialog affords complementary dynamics, constrained by contextual sensitivity and functional specificity. We substantiate our arguments by reference to recent empirical studies supporting the idea of dialog as interpersonal synergy.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\JAXFLHUQ\\Fusaroli et al. - 2014 - Dialog as interpersonal synergy.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\26XWHSYL\\S0732118X13000342.html},
  keywords = {Alignment,Complementarity,Interpersonal coordination,Linguistic coordination,Social interaction,Synergy},
  langid = {english}
}

@book{gallagherHowBodyShapes2005,
  title = {How the {{Body Shapes}} the {{Mind}}},
  author = {Gallagher, Shaun},
  date = {2005-01-27},
  publisher = {{Oxford University Press}},
  url = {http://www.oxfordscholarship.com/view/10.1093/0199271941.001.0001/acprof-9780199271948},
  urldate = {2019-04-16},
  abstract = {This book contributes to the idea that to have an understanding of the mind, consciousness, or cognition, a detailed scientific and phenomenological understanding of the body is essential. There is still a need to develop a common vocabulary that is capable of integrating discussions of brain mechanisms in neuroscience, behavioral expressions in psychology, design concerns in artificial intelligence and robotics, and debates about embodied experience in the phenomenology and philosophy of mind. This book helps to formulate this common vocabulary by developing a conceptual framework that avoids both the overly reductionistic approaches that explain everything in terms of bottom-up neuronal mechanisms, and the inflationistic approaches that explain everything in terms of Cartesian, top-down cognitive states. Through discussions of neonate imitation, the Molyneux problem, gesture, self-awareness, free will, social cognition and intersubjectivity, as well as pathologies such as deafferentation, unilateral neglect, phantom limb, autism and schizophrenia, the book proposes to remap the conceptual landscape by revitalizing the concepts of body image and body schema, proprioception, ecological experience, intermodal perception, and enactive concepts of ownership and agency for action. Informed by both philosophical theory and scientific evidence, it addresses two basic sets of questions that concern the structure of embodied experience. First, questions about the phenomenal aspects of that structure, specifically the relatively regular and constant phenomenal features found in the content of experience. Second, questions about aspects of the structure of consciousness that are more hidden, those that may be more difficult to get at because they happen before one knows it, and do not normally enter into the phenomenal content of experience in an explicit way.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Y5QC7YKC\\acprof-9780199271948.html},
  isbn = {978-0-19-160311-2},
  langid = {american}
}

@article{gardenforsDemonstrationPantomimeEvolution2017,
  title = {Demonstration and {{Pantomime}} in the {{Evolution}} of {{Teaching}}},
  author = {Gärdenfors, Peter},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {8},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00415},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00415/full},
  urldate = {2020-01-26},
  abstract = {Donald proposes that early Homo evolved mimesis as a new form of cognition. This article investigates the mimesis hypothesis in relation to the evolution of teaching. The capacities that distinguish hominin teaching from that of other animals are demonstration and pantomime. A conceptual analysis of the instructional and communicative functions of demonstration and pantomime is presented. Archaeological evidence that demonstration was used for transmitting the Oldowan technology is summarized. It is argued that pantomime develops out of demonstration so that the primary objective of pantomime is that the onlooker learns the motoric patterns shown in the pantomime. The communicative use of pantomime is judged to be secondary. This use of pantomime is also contrasted with other forms of gestures. A key feature of the analysis is that the meaning of a pantomime is characterized by the force patterns of the movements. These force patterns form the core of a model of the cognitive mechanism behind pantomime. Finally, the role of pantomime in the evolution of language is also discussed.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\R5WJ35IA\\Gärdenfors - 2017 - Demonstration and Pantomime in the Evolution of Te.pdf},
  keywords = {Demonstration,evolution of language,Gesture,mental simulation,Mimesis,pantomime,Teaching},
  langid = {english}
}

@article{garrodFoundationsRepresentationWhere2007,
  title = {Foundations of Representation: Where Might Graphical Symbol Systems Come From?},
  shorttitle = {Foundations of Representation},
  author = {Garrod, Simon and Fay, Nicolas and Lee, John and Oberlander, Jon and Macleod, Tracy},
  date = {2007-11-12},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  volume = {31},
  pages = {961--987},
  issn = {0364-0213},
  doi = {10.1080/03640210701703659},
  abstract = {It has been suggested that iconic graphical signs evolve into symbolic graphical signs through repeated usage. This article reports a series of interactive graphical communication experiments using a 'pictionary' task to establish the conditions under which the evolution might occur. Experiment 1 rules out a simple repetition based account in favor of an account that requires feedback and interaction between communicators. Experiment 2 shows how the degree of interaction affects the evolution of signs according to a process of grounding. Experiment 3 confirms the prediction that those not involved directly in the interaction have trouble interpreting the graphical signs produced in Experiment 1. On the basis of these results, this article argues that icons evolve into symbols as a consequence of the systematic shift in the locus of information from the sign to the users' memory of the sign's usage supported by an interactive grounding process.},
  eprint = {21635324},
  eprinttype = {pmid},
  langid = {english},
  number = {6}
}

@article{garrodJointActionInteractive2009,
  title = {Joint {{Action}}, {{Interactive Alignment}}, and {{Dialog}}},
  author = {Garrod, Simon and Pickering, Martin J.},
  date = {2009},
  journaltitle = {Topics in Cognitive Science},
  volume = {1},
  pages = {292--304},
  issn = {1756-8765},
  doi = {10.1111/j.1756-8765.2009.01020.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-8765.2009.01020.x},
  urldate = {2019-06-24},
  abstract = {Dialog is a joint action at different levels. At the highest level, the goal of interlocutors is to align their mental representations. This emerges from joint activity at lower levels, both concerned with linguistic decisions (e.g., choice of words) and nonlinguistic processes (e.g., alignment of posture or speech rate). Because of the high-level goal, the interlocutors are particularly concerned with close coupling at these lower levels. As we illustrate with examples, this means that imitation and entrainment are particularly pronounced during interactive communication. We then argue that the mechanisms underlying such processes involve covert imitation of interlocutors’ communicative behavior, leading to emulation of their expected behavior. In other words, communication provides a very good example of predictive emulation, in a way that leads to successful joint activity.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EQUU2PFQ\\Garrod and Pickering - 2009 - Joint Action, Interactive Alignment, and Dialog.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\UK5T6L9Y\\j.1756-8765.2009.01020.html},
  keywords = {Dialog,Emulation,Interactive alignment,Joint action,Prediction},
  langid = {english},
  number = {2}
}

@article{gaveauTemporalStructureVertical2011,
  title = {The {{Temporal Structure}} of {{Vertical Arm Movements}}},
  author = {Gaveau, Jérémie and Papaxanthis, Charalambos},
  date = {2011-07-12},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {6},
  pages = {e22045},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0022045},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0022045},
  urldate = {2020-04-29},
  abstract = {The present study investigates how the CNS deals with the omnipresent force of gravity during arm motor planning. Previous studies have reported direction-dependent kinematic differences in the vertical plane; notably, acceleration duration was greater during a downward than an upward arm movement. Although the analysis of acceleration and deceleration phases has permitted to explore the integration of gravity force, further investigation is necessary to conclude whether feedforward or feedback control processes are at the origin of this incorporation. We considered that a more detailed analysis of the temporal features of vertical arm movements could provide additional information about gravity force integration into the motor planning. Eight subjects performed single joint vertical arm movements (45° rotation around the shoulder joint) in two opposite directions (upwards and downwards) and at three different speeds (slow, natural and fast). We calculated different parameters of hand acceleration profiles: movement duration (MD), duration to peak acceleration (D PA), duration from peak acceleration to peak velocity (D PA-PV), duration from peak velocity to peak deceleration (D PV-PD), duration from peak deceleration to the movement end (D PD-End), acceleration duration (AD), deceleration duration (DD), peak acceleration (PA), peak velocity (PV), and peak deceleration (PD). While movement durations and amplitudes were similar for upward and downward movements, the temporal structure of acceleration profiles differed between the two directions. More specifically, subjects performed upward movements faster than downward movements; these direction-dependent asymmetries appeared early in the movement (i.e., before PA) and lasted until the moment of PD. Additionally, PA and PV were greater for upward than downward movements. Movement speed also changed the temporal structure of acceleration profiles. The effect of speed and direction on the form of acceleration profiles is consistent with the premise that the CNS optimises motor commands with respect to both gravitational and inertial constraints.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AHHDCHBD\\Gaveau and Papaxanthis - 2011 - The Temporal Structure of Vertical Arm Movements.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\P8QUXH2F\\article.html},
  keywords = {Acceleration,Arms,Deceleration,Gravitation,Kinematics,Musculoskeletal system,Shoulders,Velocity},
  langid = {english},
  number = {7}
}

@article{gerwingLinguisticInfluencesGesture2004,
  title = {Linguistic Influences on Gesture’s Form},
  author = {Gerwing, Jennifer and Bavelas, Janet},
  date = {2004-01-01},
  journaltitle = {Gesture},
  volume = {4},
  pages = {157--195},
  publisher = {{John Benjamins}},
  issn = {1568-1475, 1569-9773},
  doi = {10.1075/gest.4.2.04ger},
  url = {https://www.jbe-platform.com/content/journals/10.1075/gest.4.2.04ger},
  urldate = {2020-03-18},
  abstract = {Hand gestures in face-to-face dialogue are symbolic acts, integrated with speech. Little is known about the factors that determine the physical form of these gestures. When the gesture depicts a previous nonsymbolic action, it obviously resembles this action; however, such gestures are not only noticeably different from the original action but, when they occur in a series, are different from each other. This paper presents an experiment with two separate analyses (one quantitative, one qualitative) testing the hypothesis that the immediate communicative function is a determinant of the symbolic form of the gesture. First, we manipulated whether the speaker was describing the previous action to an addressee who had done the same actions and therefore shared common ground or to one who had done different actions and therefore did not share common ground. The common ground gestures were judged to be significantly less complex, precise, or informative than the latter, a finding similar to the effects of common ground on words. In the qualitative analysis, we used the given versus new principle to analyze a series of gestures about the same actions by the same speaker. The speaker emphasized the new information in each gesture by making it larger, clearer, etc. When this information became given, a gesture for the same action became smaller or less precise, which is similar to findings for given versus new information in words. Thus the immediate communicative function (e.g., to convey information that is common ground or that is new) played a major role in determining the physical form of the gestures.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HDMQG72F\\gest.4.2.html},
  langid = {english},
  number = {2}
}

@article{ghazanfarEvolutionHumanVocal2008,
  title = {Evolution of Human Vocal Production},
  author = {Ghazanfar, Asif A. and Rendall, Drew},
  date = {2008-06-03},
  journaltitle = {Current biology: CB},
  shortjournal = {Curr. Biol.},
  volume = {18},
  pages = {R457-460},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2008.03.030},
  eprint = {18522811},
  eprinttype = {pmid},
  keywords = {Animals,Biological Evolution,Humans,Larynx,Neocortex,Primates,Respiratory Mechanics,Speech,Speech Perception,Thorax,Vocalization; Animal},
  langid = {english},
  number = {11}
}

@article{ghazanfarMultisensoryVocalCommunication2013,
  title = {Multisensory Vocal Communication in Primates and the Evolution of Rhythmic Speech},
  author = {Ghazanfar, Asif A.},
  date = {2013-09-01},
  journaltitle = {Behavioral ecology and sociobiology},
  shortjournal = {Behav Ecol Sociobiol},
  volume = {67},
  issn = {0340-5443},
  doi = {10.1007/s00265-013-1491-z},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3821777/},
  urldate = {2019-10-13},
  abstract = {The integration of the visual and auditory modalities during human speech perception is the default mode of speech processing. That is, visual speech perception is not a capacity that is “piggybacked” on to auditory-only speech perception. Visual information from the mouth and other parts of the face is used by all perceivers to enhance auditory speech. This integration is ubiquitous and automatic and is similar across all individuals across all cultures. The two modalities seem to be integrated even at the earliest stages of human cognitive development. If multisensory speech is the default mode of perception, then this should be reflected in the evolution of vocal communication. The purpose of this review is to describe the data that reveal that human speech is not uniquely multisensory. In fact, the default mode of communication is multisensory in nonhuman primates as well but perhaps emerging with a different developmental trajectory. Speech production, however, exhibits a unique bimodal rhythmic structure in that both the acoustic output and the movements of the mouth are rhythmic and tightly correlated. This structure is absent in most monkey vocalizations. One hypothesis is that the bimodal speech rhythm may have evolved through the rhythmic facial expressions of ancestral primates, as indicated by mounting comparative evidence focusing on the lip-smacking gesture.},
  eprint = {24222931},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IN8HWBHH\\Ghazanfar - 2013 - Multisensory vocal communication in primates and t.pdf},
  number = {9},
  pmcid = {PMC3821777}
}

@article{ghazanfarVocaltractResonancesIndexical2007,
  title = {Vocal-Tract Resonances as Indexical Cues in Rhesus Monkeys},
  author = {Ghazanfar, Asif A. and Turesson, Hjalmar K. and Maier, Joost X. and van Dinther, Ralph and Patterson, Roy D. and Logothetis, Nikos K.},
  date = {2007-03-06},
  journaltitle = {Current Biology},
  shortjournal = {Curr Biol},
  volume = {17},
  pages = {425--430},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2007.01.029},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2361420/},
  urldate = {2019-10-17},
  abstract = {Vocal-tract resonances (or formants) are acoustic signatures in the voice and are related to the shape and length of the vocal tract. Formants play an important role in human communication, helping us not only to distinguish several different speech sounds , but also to extract important information related to the physical characteristics of the speaker, so-called indexical cues. How did formants come to play such an important role in human vocal communication? One hypothesis suggests that the ancestral role of formant perception—a role that might be present in extant nonhuman primates—was to provide indexical cues . Although formants are present in the acoustic structure of vowel-like calls of monkeys  and implicated in the discrimination of call types , it is not known whether they use this feature to extract indexical cues. Here, we investigate whether rhesus monkeys can use the formant structure in their “coo” calls to assess the age-related body size of conspecifics. Using a preferential-looking paradigm  and synthetic coo calls in which formant structure simulated an adult/large- or juvenile/small-sounding individual, we demonstrate that untrained monkeys attend to formant cues and link large-sounding coos to large faces and small-sounding coos to small faces—in essence, they can, like humans , use formants as indicators of age-related body size.},
  eprint = {17320389},
  eprinttype = {pmid},
  number = {5-2},
  options = {useprefix=true},
  pmcid = {PMC2361420}
}

@article{gibsonHowEfficiencyShapes2019,
  title = {How {{Efficiency Shapes Human Language}}},
  author = {Gibson, Edward and Futrell, Richard and Piantadosi, Steven P. and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
  date = {2019-05-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  pages = {389--407},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.02.003},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661319300580},
  urldate = {2020-03-12},
  abstract = {Cognitive science applies diverse tools and perspectives to study human language. Recently, an exciting body of work has examined linguistic phenomena through the lens of efficiency in usage: what otherwise puzzling features of language find explanation in formal accounts of how language might be optimized for communication and learning? Here, we review studies that deploy formal tools from probability and information theory to understand how and why language works the way that it does, focusing on phenomena ranging from the lexicon through syntax. These studies show how a pervasive pressure for efficiency guides the forms of natural language and indicate that a rich future for language research lies in connecting linguistics to cognitive psychology and mathematical theories of communication and inference.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HFCJ2V5L\\Gibson et al. - 2019 - How Efficiency Shapes Human Language.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\NXL7JLUW\\S1364661319300580.html},
  keywords = {communication,cross-linguistic universals,language complexity,language efficiency,language evolution,language learnability},
  langid = {english},
  number = {5}
}

@inproceedings{ginosarLearningIndividualStyles2019,
  title = {Learning Individual Styles of Conversational Gesture},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ginosar, S. and Bar, A. and Kohavi, G. and Chan, C. and Owens, A. and Malik, J.},
  date = {2019},
  pages = {3497--3506},
  url = {https://arxiv.org/abs/1906.04160},
  urldate = {2019-08-09},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LYKM7A5J\\1906.html}
}

@article{giorginoComputingVisualizingDynamic2009,
  title = {Computing and {{Visualizing Dynamic Time Warping Alignments}} in {{{\emph{R}}}} : {{The}} {\textbf{Dtw}} {{Package}}},
  shorttitle = {Computing and {{Visualizing Dynamic Time Warping Alignments}} in {{{\emph{R}}}}},
  author = {Giorgino, Toni},
  date = {2009},
  journaltitle = {Journal of Statistical Software},
  volume = {31},
  issn = {1548-7660},
  doi = {10.18637/jss.v031.i07},
  url = {http://www.jstatsoft.org/v31/i07/},
  urldate = {2019-08-07},
  abstract = {This introduction to the R package dtw is a (slightly) modified version of Giorgino (2009), published in the Journal of Statistical Software. Dynamic time warping is a popular technique for comparing time series, providing both a distance measure that is insensitive to local compression and stretches and the warping which optimally deforms one of the two input series onto the other. A variety of algorithms and constraints have been discussed in the literature. The dtw package provides an unification of them; it allows R users to compute time series alignments mixing freely a variety of continuity constraints, restriction windows, endpoints, local distance definitions, and so on. The package also provides functions for visualizing alignments and constraints using several classic diagram types.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HBYA9Z4B\\Giorgino - 2009 - Computing and Visualizing Dynamic Time Warping Ali.pdf},
  langid = {english},
  number = {7}
}

@article{giraudCorticalOscillationsSpeech2012,
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  shorttitle = {Cortical Oscillations and Speech Processing},
  author = {Giraud, Anne-Lise and Poeppel, David},
  date = {2012-04},
  journaltitle = {Nature Neuroscience},
  volume = {15},
  pages = {511--517},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3063},
  url = {http://www.nature.com/articles/nn.3063},
  urldate = {2019-08-30},
  abstract = {Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example, by segregating information and organizing spike timing. Recent data show that delta, theta and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. We argue that they are foundational in speech and language processing, ‘packaging’ incoming information into units of the appropriate temporal granularity. Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\B5CYUJLD\\Giraud and Poeppel - 2012 - Cortical oscillations and speech processing emerg.pdf},
  langid = {english},
  number = {4}
}

@article{goldin-meadowGestureSignLanguage2017,
  title = {Gesture, Sign, and Language: {{The}} Coming of Age of Sign Language and Gesture Studies},
  shorttitle = {Gesture, Sign, and Language},
  author = {Goldin-Meadow, Susan and Brentari, Diane},
  date = {2017-01},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  volume = {40},
  pages = {e46},
  issn = {1469-1825},
  doi = {10.1017/S0140525X15001247},
  abstract = {How does sign language compare with gesture, on the one hand, and spoken language on the other? Sign was once viewed as nothing more than a system of pictorial gestures without linguistic structure. More recently, researchers have argued that sign is no different from spoken language, with all of the same linguistic structures. The pendulum is currently swinging back toward the view that sign is gestural, or at least has gestural components. The goal of this review is to elucidate the relationships among sign language, gesture, and spoken language. We do so by taking a close look not only at how sign has been studied over the past 50 years, but also at how the spontaneous gestures that accompany speech have been studied. We conclude that signers gesture just as speakers do. Both produce imagistic gestures along with more categorical signs or words. Because at present it is difficult to tell where sign stops and gesture begins, we suggest that sign should not be compared with speech alone but should be compared with speech-plus-gesture. Although it might be easier (and, in some cases, preferable) to blur the distinction between sign and gesture, we argue that distinguishing between sign (or speech) and gesture is essential to predict certain types of learning and allows us to understand the conditions under which gesture takes on properties of sign, and speech takes on properties of gesture. We end by calling for new technology that may help us better calibrate the borders between sign and gesture.},
  eprint = {26434499},
  eprinttype = {pmid},
  keywords = {categorical,gesture-speech mismatch,gradient,homesign,imagistic,learning,morphology,phonology,syntax},
  langid = {english},
  pmcid = {PMC4821822}
}

@article{goldin-meadowNaturalOrderEvents2008,
  title = {The Natural Order of Events: {{How}} Speakers of Different Languages Represent Events Nonverbally},
  shorttitle = {The Natural Order of Events},
  author = {Goldin-Meadow, Susan and So, Wing Chee and Özyürek, Aslı and Mylander, Carolyn},
  date = {2008-07-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {105},
  pages = {9163--9168},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0710060105},
  url = {https://www.pnas.org/content/105/27/9163},
  urldate = {2019-08-26},
  abstract = {To test whether the language we speak influences our behavior even when we are not speaking, we asked speakers of four languages differing in their predominant word orders (English, Turkish, Spanish, and Chinese) to perform two nonverbal tasks: a communicative task (describing an event by using gesture without speech) and a noncommunicative task (reconstructing an event with pictures). We found that the word orders speakers used in their everyday speech did not influence their nonverbal behavior. Surprisingly, speakers of all four languages used the same order and on both nonverbal tasks. This order, actor–patient–act, is analogous to the subject–object–verb pattern found in many languages of the world and, importantly, in newly developing gestural languages. The findings provide evidence for a natural order that we impose on events when describing and reconstructing them nonverbally and exploit when constructing language anew.},
  eprint = {18599445},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\L9ZXNEJ5\\Goldin-Meadow et al. - 2008 - The natural order of events How speakers of diffe.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\54FTX72G\\9163.html},
  keywords = {gesture,language genesis,sign language,word order},
  langid = {english},
  number = {27}
}

@article{goldin-meadowNaturalOrderEvents2008a,
  title = {The Natural Order of Events: {{How}} Speakers of Different Languages Represent Events Nonverbally},
  shorttitle = {The Natural Order of Events},
  author = {Goldin-Meadow, S. and So, W. C. and Ozyurek, A. and Mylander, C.},
  date = {2008-07-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {105},
  pages = {9163--9168},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0710060105},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0710060105},
  urldate = {2020-03-24},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ECWLU2PY\\Goldin-Meadow et al. - 2008 - The natural order of events How speakers of diffe.pdf},
  langid = {english},
  number = {27}
}

@article{gordonMultimodalCommunicationWolf2011,
  title = {Multimodal Communication of Wolf Spiders on Different Substrates: Evidence for Behavioural Plasticity},
  shorttitle = {Multimodal Communication of Wolf Spiders on Different Substrates},
  author = {Gordon, Shira D. and Uetz, George W.},
  date = {2011-02},
  journaltitle = {Animal Behaviour},
  volume = {81},
  pages = {367--375},
  issn = {00033472},
  doi = {10.1016/j.anbehav.2010.11.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0003347210004409},
  urldate = {2019-09-17},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YUMBQQ6V\\Gordon and Uetz - 2011 - Multimodal communication of wolf spiders on differ.pdf},
  langid = {english},
  number = {2}
}

@article{grazianoHowReferentialGestures2020,
  title = {How {{Referential Gestures Align With Speech}}: {{Evidence From Monolingual}} and {{Bilingual Speakers}}},
  shorttitle = {How {{Referential Gestures Align With Speech}}},
  author = {Graziano, Maria and Nicoladis, Elena and Marentette, Paula},
  date = {2020},
  journaltitle = {Language Learning},
  volume = {70},
  pages = {266--304},
  issn = {1467-9922},
  doi = {10.1111/lang.12376},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12376},
  urldate = {2020-03-03},
  abstract = {When speaking, people often produce gestures that are closely timed with the speech with which they constitute a semantically coherent unit. Analyzing the temporal patterns between the two modalities may reveal insights about how speakers plan them. Using elicited narratives, we tested English/French monolinguals and bilinguals to check whether bilinguals, known to experience a higher degree of competition in lexical access, show a different pattern of gesture–speech alignment compared to that of monolinguals. Results revealed no difference in the temporal patterns between gestures and co-semantic speech for the two language groups. Synchronous gestures were significantly more frequent than asynchronous ones; asynchronous gestures both preceded and followed the correlated speech, yet preceding gestures tended to occur more often. A qualitative analysis conducted for asynchronous gestures revealed that they may serve a rhetoric function. We argue that the variability in gesture–speech timing results from speakers’ strategic use of gesture.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9WC9AXIF\\Graziano et al. - 2020 - How Referential Gestures Align With Speech Eviden.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\L5M33N6D\\lang.html},
  keywords = {bilinguals,gesture,gesture–speech alignment,monolinguals,speech production,temporal patterns},
  langid = {english},
  number = {1}
}

@article{guskiAcousticTauEasy1992,
  title = {Acoustic {{Tau}}: {{An Easy Analogue}} to {{Visual Tau}}?},
  shorttitle = {Acoustic {{Tau}}},
  author = {Guski, Rainer},
  date = {1992-09-01},
  journaltitle = {Ecological Psychology},
  volume = {4},
  pages = {189--197},
  issn = {1040-7413},
  doi = {10.1207/s15326969eco0403_4},
  url = {https://doi.org/10.1207/s15326969eco0403_4},
  urldate = {2019-05-04},
  abstract = {In an earlier article in this journal, B. Shaw, McGowan, and Turvey (1991) resented an acoustic variable they supposed would specify the time to contact with a sound-emitting source moving rectilinearly toward an observer at constant speed. Their formulation, t = 2l(dI/dt), follows Lee's (1976) main ideas in his derivation of optical tau for small visual angles. In this article I compare the functions of vision and hearing more generally and consider what information vision and hearing would use in normal circumstances. In this context, one may then ask a more specific question, Which kind of auditory information might be used about impending collisions? I make the following points: (a) The specification analysis of Shaw et al. (1991) is a substantial contribution to the growing field of ecological acoustics, (b) the idea that a more general attack on the acoustic guidance of action (Shaw et al., 1991, p. 254) starts with a proposal for an uncommon case without considering the general functions of seeing and hearing first seems inconsistent, (c) an auditory variable specifying time to turn or time to jump is needed, and (d) I do not believe that the auditory system can use the kind of information proposed by Shaw et al. (1991) in estimating time to contact.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TANWDUB2\\s15326969eco0403_4.html},
  number = {3}
}

@article{gustisonVocalLocomotorCoordination2019,
  title = {Vocal and Locomotor Coordination Develops in Association with the Autonomic Nervous System},
  author = {Gustison, Morgan L and Borjon, Jeremy I and Takahashi, Daniel Y and Ghazanfar, Asif A},
  editor = {Tchernichovski, Ofer and Calabrese, Ronald L and Goller, Franz},
  date = {2019-07-16},
  journaltitle = {eLife},
  volume = {8},
  pages = {e41853},
  issn = {2050-084X},
  doi = {10.7554/eLife.41853},
  url = {https://doi.org/10.7554/eLife.41853},
  urldate = {2019-10-17},
  abstract = {In adult animals, movement and vocalizations are coordinated, sometimes facilitating, and at other times inhibiting, each other. What is missing is how these different domains of motor control become coordinated over the course of development. We investigated how postural-locomotor behaviors may influence vocal development, and the role played by physiological arousal during their interactions. Using infant marmoset monkeys, we densely sampled vocal, postural and locomotor behaviors and estimated arousal fluctuations from electrocardiographic measures of heart rate. We found that vocalizations matured sooner than postural and locomotor skills, and that vocal-locomotor coordination improved with age and during elevated arousal levels. These results suggest that postural-locomotor maturity is not required for vocal development to occur, and that infants gradually improve coordination between vocalizations and body movement through a process that may be facilitated by arousal level changes.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9PA6LWLP\\Gustison et al. - 2019 - Vocal and locomotor coordination develops in assoc.pdf},
  keywords = {energetics,locomotion,marmoset monkey,motor development,vocal ontogeny}
}

@article{hagoortNeurobiologyLanguageSingleword2019,
  title = {The Neurobiology of Language beyond Single-Word Processing},
  author = {Hagoort, Peter},
  date = {2019-10-04},
  journaltitle = {Science},
  volume = {366},
  pages = {55--58},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax0289},
  url = {https://science.sciencemag.org/content/366/6461/55},
  urldate = {2019-10-14},
  abstract = {In this Review, I propose a multiple-network view for the neurobiological basis of distinctly human language skills. A much more complex picture of interacting brain areas emerges than in the classical neurobiological model of language. This is because using language is more than single-word processing, and much goes on beyond the information given in the acoustic or orthographic tokens that enter primary sensory cortices. This requires the involvement of multiple networks with functionally nonoverlapping contributions.},
  eprint = {31604301},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IGUFE222\\55.html},
  langid = {english},
  number = {6461}
}

@article{hansonEffectsObstruentConsonants2009,
  title = {Effects of Obstruent Consonants on Fundamental Frequency at Vowel Onset in {{English}}},
  author = {Hanson, Helen M.},
  date = {2009-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {125},
  pages = {425--441},
  issn = {1520-8524},
  doi = {10.1121/1.3021306},
  abstract = {When a vowel follows an obstruent, the fundamental frequency in the first few tens of milliseconds of the vowel is known to be influenced by the voicing characteristics of the consonant. This influence was re-examined in the study reported here. Stops, fricatives, and the nasal /m/ were paired with the vowels /i,a/ to form CVm syllables. Target syllables were embedded in carrier sentences, and intonation was varied to produce each syllable in either a high, low, or neutral pitch environment. In a high-pitch environment, F0 following voiceless obstruents is significantly increased relative to the baseline /m/, but following voiced obstruents it closely traces the baseline. In a low-pitch environment, F0 is very slightly increased following all obstruents, voiced and unvoiced. It is suggested that for certain pitch environments a conflict can occur between gestures corresponding to the segmental feature [stiff vocal folds] and intonational elements. The results are different acoustic manifestations of [stiff] in different pitch environments. The spreading of the vocal folds that occurs during unvoiced stops in certain contexts in English is an enhancing gesture, which aids the resolution of the gestural conflict by allowing the defining segmental gesture to be weakened without losing perceptual salience.},
  eprint = {19173428},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\WIKTF2GU\\Hanson - 2009 - Effects of obstruent consonants on fundamental fre.pdf},
  keywords = {Female,Humans,Linguistics,Male,Phonetics,Pitch Perception},
  langid = {english},
  number = {1},
  pmcid = {PMC2677272}
}

@article{hardusToolUseWild2009,
  title = {Tool Use in Wild Orang-Utans Modifies Sound Production: A Functionally Deceptive Innovation?},
  shorttitle = {Tool Use in Wild Orang-Utans Modifies Sound Production},
  author = {Hardus, M. E. and Lameira, A. R. and Schaik, C. S. and Wich, S. A.},
  date = {2009-10-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {276},
  pages = {3689--3694},
  doi = {10.1098/rspb.2009.1027},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2009.1027},
  urldate = {2019-05-04},
  abstract = {Culture has long been assumed to be uniquely human but recent studies, in particular on great apes, have suggested that cultures also occur in non-human primates. The most apparent cultural behaviours in great apes involve tools in the subsistence context where they are clearly functional to obtain valued food. On the other hand, tool-use to modify acoustic communication has been reported only once and its function has not been investigated. Thus, the question whether this is an adaptive behaviour remains open, even though evidence indicates that it is socially transmitted (i.e. cultural). Here we report on wild orang-utans using tools to modulate the maximum frequency of one of their sounds, the kiss squeak, emitted in distress. In this variant, orang-utans strip leaves off a twig and hold them to their mouth while producing a kiss squeak. Using leaves as a tool lowers the frequency of the call compared to a kiss squeak without leaves or with only a hand to the mouth. If the lowering of the maximum frequency functions in orang-utans as it does in other animals, two predictions follow: (i) kiss squeak frequency is related to body size and (ii) the use of leaves will occur in situations of most acute danger. Supporting these predictions, kiss squeaks without tools decreased with body size and kiss squeaks with leaves were only emitted by highly distressed individuals. Moreover, we found indications that the calls were under volitional control. This finding is significant for at least two reasons. First, although few animal species are known to deceptively lower the maximum frequency of their calls to exaggerate their perceived size to the listener (e.g. vocal tract elongation in male deer) it has never been reported that animals may use tools to achieve this, or that they are primates. Second, it shows that the orang-utan culture extends into the communicative domain, thus challenging the traditional assumption that primate calling behaviour is overall purely emotional.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AMGVVAKC\\Hardus Madeleine E. et al. - 2009 - Tool use in wild orang-utans modifies sound produc.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\PSELNL5N\\rspb.2009.html},
  number = {1673}
}

@article{harrisonHorsingSpontaneousFourLegged2009,
  title = {Horsing {{Around}}: {{Spontaneous Four}}-{{Legged Coordination}}},
  shorttitle = {Horsing {{Around}}},
  author = {Harrison, Steven J. and Richardson, Michael J.},
  date = {2009-11-06},
  journaltitle = {Journal of Motor Behavior},
  volume = {41},
  pages = {519--524},
  issn = {0022-2895},
  doi = {10.3200/35-08-014},
  url = {https://doi.org/10.3200/35-08-014},
  urldate = {2019-05-07},
  abstract = {Motivated by previous research suggesting that informational and mechanical interlimb coupling can stabilize rhythmic movement patterns, the authors show that stable 4-legged patterns between 2 individuals, either walking or running, can emerge unintentionally from simple forms of coupling. Specifically, they show that the leg movements of pairs of naive individuals become spontaneously phase locked when visually or mechanically coupled via a foam appendage. Analysis of each of the phase locked trials revealed distinct preferences for particular 4-legged patterns, with interpersonal in- and anti-phase coordination patterns (equitable with quadruped pace and trot, respectively) observed almost exclusively. Preference for either pattern depended on the strength of coupling. The authors discuss these findings in light of previous claims that the patterns of human and animal locomotion—as well as coordinated movements in general—can emerge from lawful coupling relations that exist between the subcomponents of perceptual-motor systems.},
  eprint = {19567365},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AIALRCKZ\\Harrison and Richardson - 2009 - Horsing Around Spontaneous Four-Legged Coordinati.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\NWAQA7S9\\35-08-014.html},
  keywords = {coordination dynamics,interpersonal coordination,mechanical coupling,quadruped gait,visual coupling},
  number = {6}
}

@article{hasselmanClassifyingAcousticSignals2015,
  title = {Classifying Acoustic Signals into Phoneme Categories: Average and Dyslexic Readers Make Use of Complex Dynamical Patterns and Multifractal Scaling Properties of the Speech Signal},
  shorttitle = {Classifying Acoustic Signals into Phoneme Categories},
  author = {Hasselman, Fred},
  date = {2015-03-26},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  volume = {3},
  pages = {e837},
  issn = {2167-8359},
  doi = {10.7717/peerj.837},
  url = {https://peerj.com/articles/837},
  urldate = {2019-04-05},
  abstract = {Several competing aetiologies of developmental dyslexia suggest that the problems with acquiring literacy skills are causally entailed by low-level auditory and/or speech perception processes. The purpose of this study is to evaluate the diverging claims about the specific deficient peceptual processes under conditions of strong inference. Theoretically relevant acoustic features were extracted from a set of artificial speech stimuli that lie on a /bAk/-/dAk/ continuum. The features were tested on their ability to enable a simple classifier (Quadratic Discriminant Analysis) to reproduce the observed classification performance of average and dyslexic readers in a speech perception experiment. The ‘classical’ features examined were based on component process accounts of developmental dyslexia such as the supposed deficit in Envelope Rise Time detection and the deficit in the detection of rapid changes in the distribution of energy in the frequency spectrum (formant transitions). Studies examining these temporal processing deficit hypotheses do not employ measures that quantify the temporal dynamics of stimuli. It is shown that measures based on quantification of the dynamics of complex, interaction-dominant systems (Recurrence Quantification Analysis and the multifractal spectrum) enable QDA to classify the stimuli almost identically as observed in dyslexic and average reading participants. It seems unlikely that participants used any of the features that are traditionally associated with accounts of (impaired) speech perception. The nature of the variables quantifying the temporal dynamics of the speech stimuli imply that the classification of speech stimuli cannot be regarded as a linear aggregate of component processes that each parse the acoustic signal independent of one another, as is assumed by the ‘classical’ aetiologies of developmental dyslexia. It is suggested that the results imply that the differences in speech perception performance between average and dyslexic readers represent a scaled continuum rather than being caused by a specific deficient component.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\G5RLG5YL\\Hasselman - 2015 - Classifying acoustic signals into phoneme categori.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\CG3LHAFN\\837.html},
  langid = {english}
}

@software{hastieGamGeneralizedAdditive2019,
  title = {Gam: {{Generalized Additive Models}}},
  shorttitle = {Gam},
  author = {Hastie, Trevor},
  date = {2019-07-03},
  url = {https://CRAN.R-project.org/package=gam},
  urldate = {2020-01-27},
  abstract = {Functions for fitting and working with generalized additive models, as described in chapter 7 of "Statistical Models in S" (Chambers and Hastie (eds), 1991), and "Generalized Additive Models" (Hastie and Tibshirani, 1990).},
  keywords = {Econometrics,Environmetrics,SocialSciences},
  version = {1.16.1}
}

@article{hauserFacultyLanguageWhat2002,
  title = {The {{Faculty}} of {{Language}}: {{What Is It}}, {{Who Has It}}, and {{How Did It Evolve}}?},
  shorttitle = {The {{Faculty}} of {{Language}}},
  author = {Hauser, M. D.},
  date = {2002-11-22},
  journaltitle = {Science},
  volume = {298},
  pages = {1569--1579},
  issn = {00368075, 10959203},
  doi = {10.1126/science.298.5598.1569},
  url = {https://www.sciencemag.org/lookup/doi/10.1126/science.298.5598.1569},
  urldate = {2020-03-27},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YQH6ITE5\\Hauser - 2002 - The Faculty of Language What Is It, Who Has It, a.pdf},
  langid = {english},
  number = {5598}
}

@article{havilandEmergingGrammarNouns2013,
  title = {The Emerging Grammar of Nouns in a First Generation Sign Language: {{Specification}}, Iconicity, and Syntax},
  shorttitle = {The Emerging Grammar of Nouns in a First Generation Sign Language},
  author = {Haviland, John B.},
  date = {2013-12-31},
  journaltitle = {Gesture},
  volume = {13},
  pages = {309--353},
  issn = {1568-1475, 1569-9773},
  doi = {10.1075/gest.13.3.04hav},
  url = {http://www.jbe-platform.com/content/journals/10.1075/gest.13.3.04hav},
  urldate = {2020-03-25},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AQ5YEQ97\\Haviland - 2013 - The emerging grammar of nouns in a first generatio.pdf},
  langid = {english},
  number = {3}
}

@article{healeyRunningRepairsCoordinating2018,
  title = {Running {{Repairs}}: {{Coordinating Meaning}} in {{Dialogue}}},
  shorttitle = {Running {{Repairs}}},
  author = {Healey, Patrick G. T. and Mills, Gregory J. and Eshghi, Arash and Howes, Christine},
  date = {2018},
  journaltitle = {Topics in Cognitive Science},
  volume = {10},
  pages = {367--388},
  issn = {1756-8765},
  doi = {10.1111/tops.12336},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12336},
  urldate = {2020-01-22},
  abstract = {People give feedback in conversation: both positive signals of understanding, such as nods, and negative signals of misunderstanding, such as frowns. How do signals of understanding and misunderstanding affect the coordination of language use in conversation? Using a chat tool and a maze-based reference task, we test two experimental manipulations that selectively interfere with feedback in live conversation: (a) “Attenuation” that replaces positive signals of understanding such as “right” or “okay” with weaker, more provisional signals such as “errr” or “umm” and (2) “Amplification” that replaces relatively specific signals of misunderstanding from clarification requests such as “on the left?” with generic signals of trouble such as “huh?” or “eh?”. The results show that Amplification promotes rapid convergence on more systematic, abstract ways of describing maze locations while Attenuation has no significant effect. We interpret this as evidence that “running repairs”—the processes of dealing with misunderstandings on the fly—are key drivers of semantic coordination in dialogue. This suggests a new direction for experimental work on conversation and a productive way to connect the empirical accounts of Conversation Analysis with the representational and processing concerns of Formal Semantics and Psycholinguistics.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AS38SYQX\\Healey et al. - 2018 - Running Repairs Coordinating Meaning in Dialogue.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\YNE7349C\\tops.html},
  keywords = {Dialogue,Miscommunication,Repair},
  langid = {english},
  number = {2}
}

@inproceedings{heAmplitudeEnvelopeKinematics2017,
  title = {Amplitude Envelope Kinematics of Speech Signal: Parameter Extraction and Applications},
  booktitle = {Konferenz {{Elektronische Sprachsignalverarbeitung}}},
  date = {2017},
  pages = {107--113},
  location = {{Saarbrücken}},
  editora = {He, L. and Dellwo, V.},
  editoratype = {collaborator}
}

@article{heAmplitudeEnvelopeKinematics2017a,
  title = {Amplitude Envelope Kinematics of Speech: {{Parameter}} Extraction and Applications},
  shorttitle = {Amplitude Envelope Kinematics of Speech},
  author = {He, Lei and Dellwo, Volker},
  date = {2017-05-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {141},
  pages = {3582--3582},
  issn = {0001-4966},
  doi = {10.1121/1.4987638},
  url = {https://asa.scitation.org/doi/10.1121/1.4987638},
  urldate = {2019-05-05},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\96CYPDK7\\1.html},
  number = {5}
}

@online{HierarchyAutonomousSystems,
  title = {A {{Hierarchy}} of {{Autonomous Systems}} for {{Vocal Production}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.tins.2019.12.006},
  url = {https://reader.elsevier.com/reader/sd/pii/S0166223619302401?token=944D77B8B1ECDE0BE1D2E949C7ED1446036B5D3B1B7C62D1E7F61AE7791DFA5D78C530C05B641392085395272CECB253},
  urldate = {2020-01-22},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TCMUAANF\\S0166223619302401.html},
  langid = {english}
}

@article{hodgesFeedforwardContractionTransversus1997,
  title = {Feedforward Contraction of Transversus Abdominis Is Not Influenced by the Direction of Arm Movement},
  author = {Hodges, P. W. and Richardson, C. A.},
  date = {1997-04},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {114},
  pages = {362--370},
  issn = {0014-4819},
  doi = {10.1007/pl00005644},
  abstract = {Because the structure of the spine is inherently unstable, muscle activation is essential for the maintenance of trunk posture and intervertebral control when the limbs are moved. To investigate how the central nervous system deals with this situation the temporal components of the response of the muscles of the trunk were evaluated during rapid limb movement performed in response to a visual stimulus. Fine-wire electromyography (EMG) electrodes were inserted into transversus abdominis (TrA), obliquus internus abdominis (OI) and obliquus externus abdominis (OE) of 15 subjects under the guidance of real-time ultrasound imaging. Surface electrodes were placed over rectus abdominis (RA), lumbar multifidus (MF) and the three parts of deltoid. In a standing position, ten repetitions of shoulder flexion, abduction and extension were performed by the subjects as fast as possible in response to a visual stimulus. The onset of TrA EMG occurred in advance of deltoid irrespective of the movement direction. The time to onset of EMG activity of OI, OE, RA and MF varied with the movement direction, being activated earliest when the prime action of the muscle opposed the reactive forces associated with the specific limb movement. It is postulated that the non-direction-specific contraction of TrA may be related to the control of trunk stability independent of the requirement for direction-specific control of the centre of gravity in relation to the base of support.},
  eprint = {9166925},
  eprinttype = {pmid},
  keywords = {Adult,Arm,Electromyography,Female,Homeostasis,Humans,Male,Movement,Muscle Contraction,Muscle; Skeletal,Psychomotor Performance,Shoulder Joint},
  langid = {english},
  number = {2}
}

@article{hoganOrganizingPrincipleClass1984,
  title = {An Organizing Principle for a Class of Voluntary Movements},
  author = {Hogan, N.},
  date = {1984-11},
  journaltitle = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {4},
  pages = {2745--2754},
  issn = {0270-6474},
  abstract = {This paper presents a mathematical model which predicts both the major qualitative features and, within experimental error, the quantitative details of a class of perturbed and unperturbed large-amplitude, voluntary movements performed at intermediate speed by primates. A feature of the mathematical model is that a concise description of the behavioral organization of the movement has been formulated which is separate and distinct from the description of the dynamics of movement execution. Based on observations of voluntary movements in primates, the organization has been described as though the goal were to make the smoothest movement possible under the circumstances, i.e., to minimize the accelerative transients. This has been formalized by using dynamic optimization theory to determine the movement which minimizes the rate of change of acceleration (jerk) of the limb. Based on observations of muscle mechanics, the concept of a "virtual position" determined by the active states of the muscles is rigorously defined as one of the mechanical consequences of the neural commands to the muscles. This provides insight into the mechanics of perturbed and unperturbed movements and is a useful aid in the separation of the descriptions of movement organization and movement execution.},
  eprint = {6502203},
  eprinttype = {pmid},
  keywords = {Animals,Elbow Joint,Forearm,Macaca mulatta,Mathematics,Models; Biological,Motor Neurons,Movement,Muscles,Posture},
  langid = {english},
  number = {11},
  pmcid = {PMC6564718}
}

@article{hoganSensitivitySmoothnessMeasures2009,
  title = {Sensitivity of {{Smoothness Measures}} to {{Movement Duration}}, {{Amplitude}} and {{Arrests}}},
  author = {Hogan, Neville and Sternad, Dagmar},
  date = {2009-11},
  journaltitle = {Journal of motor behavior},
  shortjournal = {J Mot Behav},
  volume = {41},
  pages = {529--534},
  issn = {0022-2895},
  doi = {10.3200/35-09-004-RC},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3470860/},
  urldate = {2020-01-31},
  abstract = {Studies of sensory-motor performance, including those concerned with changes due to age, disease or therapeutic intervention, often use measures based on jerk, the time-derivative of acceleration, to quantify smoothness and coordination. However, results have been mixed, some studies reporting sensitive discrimination of subtle differences, others failing to find significant differences, even when they are obviously present. One reason for this state of affairs is that different measures have been used with different scaling factors. These measures are sensitive to movement amplitude and/or duration to different degrees. We show that jerk-based measures with dimensions vary counter-intuitively with movement smoothness, whereas a dimensionless jerk-based measure properly quantifies common deviations from smooth, coordinated movement.},
  eprint = {19892658},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BR9XVDTG\\Hogan and Sternad - 2009 - Sensitivity of Smoothness Measures to Movement Dur.pdf},
  number = {6},
  pmcid = {PMC3470860}
}

@article{holderiedEcholocationRangeWingbeat2003,
  title = {Echolocation Range and Wingbeat Period Match in Aerial-Hawking Bats},
  author = {Holderied, M. W. and von Helversen, O.},
  date = {2003-11-07},
  journaltitle = {Proceedings. Biological Sciences},
  shortjournal = {Proc. Biol. Sci.},
  volume = {270},
  pages = {2293--2299},
  issn = {0962-8452},
  doi = {10.1098/rspb.2003.2487},
  abstract = {Aerial-hawking bats searching the sky for prey face the problem that flight and echolocation exert independent and possibly conflicting influences on call intervals. These bats can only exploit their full echolocation range unambiguously if they emit their next call when all echoes from the preceding call would have arrived. However, not every call interval is equally available. The need to reduce the high energetic costs of echolocation forces aerial-hawking bats to couple call emission to their wingbeat. We compared the wingbeat periods of 11 aerial-hawking bat species with the delays of the last-expected echoes. Acoustic flight-path tracking was employed to measure the source levels (SLs) of echolocation calls in the field. SLs were very high, extending the known range to 133 dB peak equivalent sound pressure level. We calculated the maximum detection distances for insects, larger flying objects and background targets. Wingbeat periods were derived from call intervals. Small and medium-sized bats in fact matched their maximum detection range for insects and larger flying targets to their wingbeat period. The tendency to skip calls correlated with the species' detection range for background targets. We argue that a species' call frequency is at such a pitch that the resulting detection range matches their wingbeat period.},
  eprint = {14613617},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TQFN5WZX\\Holderied and von Helversen - 2003 - Echolocation range and wingbeat period match in ae.pdf},
  keywords = {Animals,Auditory Perception,Chiroptera,Echolocation,Flight; Animal,Predatory Behavior,Time Factors,Wings; Animal},
  langid = {english},
  number = {1530},
  options = {useprefix=true},
  pmcid = {PMC1691500}
}

@article{hollerExperimentalInvestigationHow2011,
  title = {An Experimental Investigation of How Addressee Feedback Affects Co-Speech Gestures Accompanying Speakers’ Responses},
  author = {Holler, Judith and Wilkin, Katie},
  date = {2011-11-01},
  journaltitle = {Journal of Pragmatics},
  shortjournal = {Journal of Pragmatics},
  volume = {43},
  pages = {3522--3536},
  issn = {0378-2166},
  doi = {10.1016/j.pragma.2011.08.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0378216611002190},
  urldate = {2020-03-18},
  abstract = {There is evidence that co-speech gestures communicate information to addressees and that they are often communicatively intended. However, we still know comparatively little about the role of gestures in the actual process of communication. The present study offers a systematic investigation of speakers’ gesture use before and after addressee feedback. The findings show that when speakers responded to addressees’ feedback gesture rate remained constant when this feedback encouraged clarification, elaboration or correction. However, speakers gestured proportionally less often after feedback when providing confirmatory responses. That is, speakers may not be drawing on gesture in response to addressee feedback per se, but particularly with responses that enhance addressees’ understanding. Further, the large majority of speakers’ gestures changed in their form. They tended to be more precise, larger, or more visually prominent after feedback. Some changes in gesture viewpoint were also observed. In addition, we found that speakers used deixis in speech and gaze to increase the salience of gestures occurring in response to feedback. Speakers appear to conceive of gesture as a useful modality in redesigning utterances to make them more accessible to addressees. The findings further our understanding of recipient design and co-speech gestures in face-to-face dialogue.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SQEQI6BX\\Holler and Wilkin - 2011 - An experimental investigation of how addressee fee.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\PWNYMULC\\S0378216611002190.html},
  keywords = {Addressee feedback,Co-speech gesture,Gaze,Recipient design,Verbal deixis},
  langid = {english},
  number = {14}
}

@article{hollerMultimodalLanguageProcessing2019,
  title = {Multimodal Language Processing in Human Communication},
  author = {Holler, Judith and Levinson, Stephen C.},
  date = {2019-08-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  pages = {639--652},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.05.006},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661319301299},
  urldate = {2019-08-26},
  abstract = {The natural ecology of human language is face-to-face interaction comprising the exchange of a plethora of multimodal signals. Trying to understand the psycholinguistic processing of language in its natural niche raises new issues, first and foremost the binding of multiple, temporally offset signals under tight time constraints posed by a turn-taking system. This might be expected to overload and slow our cognitive system, but the reverse is in fact the case. We propose cognitive mechanisms that may explain this phenomenon and call for a multimodal, situated psycholinguistic framework to unravel the full complexities of human language processing.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\2JPUT5J2\\Holler and Levinson - 2019 - Multimodal Language Processing in Human Communicat.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\LY3C7FWG\\S1364661319301299.html},
  keywords = {binding,cross-level prediction,multimodal gestalts,multimodal language,segregation},
  number = {8}
}

@article{hopkinsHandMouthEvolution2003,
  title = {From Hand to Mouth in the Evolution of Language: {{The}} Influence of Vocal Behavior on Lateralized Hand Use in Manual Gestures by Chimpanzees ({{Pan}} Troglodytes)},
  shorttitle = {From Hand to Mouth in the Evolution of Language},
  author = {Hopkins, William D. and Cantero, Monica},
  date = {2003},
  journaltitle = {Developmental Science},
  volume = {6},
  pages = {55--61},
  publisher = {{Blackwell Publishing}},
  location = {{United Kingdom}},
  issn = {1467-7687(Electronic),1363-755X(Print)},
  doi = {10.1111/1467-7687.00254},
  abstract = {Examined the association between hand use for gestural communication and vocal behavior in chimpanzees. On each test trial, an experimenter approached the S's cage, positioned themselves approximately 1 m from the S's home cage and directly in front of the focal chimpanzee, and offered the S a banana. A 2nd experimenter, standing directly behind the experimenter offering the banana, recorded all communicative behavior of the focal S for a 1-minute sampling period. Each chimpanzee was tested on 10 separate occasions separated by at least 1 day. Results offer evidence that chimpanzees exhibit preferential use of the right hand in gestural communication. Moreover, use of the right hand in gestural communication is significantly enhanced when accompanied by a vocalization, particularly among human-reared chimpanzees. Taken together, the data suggest that the lateralization of manual and speech systems of communication may date back as far as 5 million years ago. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5ZIH58HJ\\2003-04786-013.html},
  keywords = {Animal Vocalizations,Chimpanzees,Gestures,Handedness},
  number = {1}
}

@article{hospelhornMethodAnalyzingGestural2017,
  title = {Method for {{Analyzing Gestural Communication}} in {{Musical Groups}}},
  author = {Hospelhorn, Emma and Radinsky, Josh},
  date = {2017-10-03},
  journaltitle = {Discourse Processes},
  volume = {54},
  pages = {504--523},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2015.1137183},
  url = {https://doi.org/10.1080/0163853X.2015.1137183},
  urldate = {2019-09-20},
  abstract = {Musical performances provide a rich context for studying complex spatial and embodied modes of group learning. This article proposes a framework for analyzing gesture in musical performances to highlight the ways musicians' movements reflect and promote their emerging and changing conceptions of a piece of music. The constructs of expressive musical gesture (at the individual level of analysis) and group expressive musical gesture (at the collective level) are used to analyze a series of five sequential performances of a musical passage by a string quartet during rehearsal. The analysis identifies three functions of embodied gesture for score interpretation: (1) gestures served as a tool for group interpretation in passages that had previously been pointed to by verbal exchanges, (2) gestures served to fine-tune the location and enactment of dynamic markings in the score, and (3) group expressive gestures in the final “take” of the rehearsal incorporated group expressive gestures from other takes, constituting a negotiated set of score interpretations.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8ATUUZ2N\\0163853X.2015.html},
  number = {7}
}

@article{hostetterVisibleEmbodimentGestures2008,
  title = {Visible Embodiment: {{Gestures}} as Simulated Action},
  shorttitle = {Visible Embodiment},
  author = {Hostetter, Autumn B. and Alibali, Martha W.},
  date = {2008-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {15},
  pages = {495--514},
  issn = {1531-5320},
  doi = {10.3758/PBR.15.3.495},
  url = {https://doi.org/10.3758/PBR.15.3.495},
  urldate = {2019-08-26},
  abstract = {Spontaneous gestures that accompany speech are related to both verbal and spatial processes. We argue that gestures emerge from perceptual and motor simulations that underlie embodied language and mental imagery. We first review current thinking about embodied cognition, embodied language, and embodied mental imagery. We then provide evidence that gestures stem from spatial representations and mental images. We then propose the gestures-as-simulated-action framework to explain how gestures might arise from an embodied cognitive system. Finally, we compare this framework with other current models of gesture production, and we briefly outline predictions that derive from the framework.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4VGTSXDV\\Hostetter and Alibali - 2008 - Visible embodiment Gestures as simulated action.pdf},
  keywords = {Lexical Access,Mental Image,Mental Rotation,Motor Imagery,Speech Production},
  langid = {english},
  number = {3}
}

@article{hubscherGesturalProsodicDevelopment2019,
  title = {Gestural and {{Prosodic Development Act}} as {{Sister Systems}} and {{Jointly Pave}} the {{Way}} for {{Children}}’s {{Sociopragmatic Development}}},
  author = {Hübscher, Iris and Prieto, Pilar},
  date = {2019-06-12},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.01259},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6581748/},
  urldate = {2019-09-06},
  abstract = {Children might combine gesture and prosody to express a pragmatic meaning such as a request, information focus, uncertainty or politeness, before they can convey these meanings in speech. However, little is known about the developmental trajectories of gestural and prosodic patterns and how they relate to a child’s growing understanding and propositional use of these sociopragmatic meanings. Do gesture and prosody act as sister systems in pragmatic development? Do children acquire these components of language before they are able to express themselves through spoken language, thus acting as forerunners in children’s pragmatic development? This review article assesses empirical evidence that demonstrates that gesture and prosody act as intimately related systems and, importantly, pave the way for pragmatic acquisition at different developmental stages. The review goes on to explore how the integration of gesture and prosody with semantics and syntax can impact language acquisition and how multimodal interventions can be used effectively in educational settings. Our review findings support the importance of simultaneously assessing both the prosodic and the gestural components of language in the fields of language development, language learning, and language intervention.},
  eprint = {31244716},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\T7A5B36S\\Hübscher and Prieto - 2019 - Gestural and Prosodic Development Act as Sister Sy.pdf},
  pmcid = {PMC6581748}
}

@article{imProbabilisticRelationCospeech2020,
  title = {Probabilistic Relation between Co-Speech Gestures, Pitch Accents and Information Status},
  author = {Im, Suyeon and Baumann, Stefan},
  date = {2020-03-23},
  journaltitle = {Proceedings of the Linguistic Society of America},
  volume = {5},
  pages = {685--697},
  issn = {2473-8689},
  doi = {10.3765/plsa.v5i1.4755},
  url = {http://journals.linguisticsociety.org/proceedings/index.php/PLSA/article/view/4755},
  urldate = {2020-03-30},
  abstract = {This study investigates the occurrence of co-speech gestures as a function of prosodic prominence (pitch accents) and discourse meaning (information status) in a clear and engaging speech style. Among several types of co-speech gestures, we examine non-referential gestures, which are claimed to be prosodic in nature (Shattuck-Hufnagel \& Ren 2018). In particular, we want to find out to what extent these gestures co-occur with specific accent types and whether they are used to encode referential, lexical, or contrastive information. Our results show that the occurrence of gestures was highest for L+H*, followed by H*, !H*, and unaccented words. Gestures were accompanied by L* only in continuations. Also, co-speech gestures were more likely to occur with new or accessible, and especially contrastive, information than with given information. The patterns differed between the referential and lexical level of information status, though. In general, this study suggests that co-speech gestures contribute to the probabilistic encoding of a word’s information status in conjunction with pitch accents.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8ZKMNLIH\\Im and Baumann - 2020 - Probabilistic relation between co-speech gestures,.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\K69ZSHPN\\4755.html},
  issue = {1},
  keywords = {clear speech,co-speech gestures,information status,pitch accents,prosody,RefLex Scheme},
  langid = {english},
  number = {1}
}

@article{indenRapidEntrainmentSpontaneous,
  title = {Rapid Entrainment to Spontaneous Speech: {{A}} Comparison of Oscillator Models},
  author = {Inden, Benjamin},
  pages = {7},
  abstract = {Oscillator models may be used for modeling synchrony between gestures and speech, or timing of backchanneling and turn-taking in dialogues. We find support for the hypothesis that oscillator networks can better predict rhythmic events on the syllable and foot level than single oscillators, but we do not find support for the hypothesis that phase resetting oscillators perform better that phase adapting oscillators. Overall, oscillators can be used to predict rhythmic events in speech, but higher level information needs to be integrated into such models to reach a satisfactory performance.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZDPHZYTH\\Inden - Rapid entrainment to spontaneous speech A compari.pdf},
  langid = {english}
}

@article{ingberTensegrityMechanotransduction2008,
  title = {Tensegrity and Mechanotransduction},
  author = {Ingber, D. W.},
  date = {2008-07},
  journaltitle = {Journal of Bodywork and Movement Therapies},
  volume = {12},
  pages = {198--200},
  issn = {13608592},
  doi = {10.1016/j.jbmt.2008.04.038},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1360859208000788},
  urldate = {2019-08-09},
  abstract = {Anyone who is skilled in the art of physical therapy knows that the mechanical properties, behavior and movement of our bodies are as important for human health as chemicals and genes. However, only recently have scientists and physicians begun to appreciate the key role that mechanical forces play in biological control at the molecular and cellular levels. This article provides a brief overview of a lecture presented at the 1st International Fascia Research Congress that convened at Harvard Medical School in Boston, MA on October 4, 2007. (see figure 1) In this lecture, I described what we have learned over the past thirty years as a result of our research focused on the molecular mechanisms by which cells sense mechanical forces and convert them into changes in intracellular biochemistry and gene expression – a process called “mechanotransduction”. This work has revealed that molecules, cells, tissues, organs, and our entire bodies use “tensegrity” architecture to mechanically stabilize their shape, and to seamlessly integrate structure and function at all size scales. Through use of this tension-dependent building system, mechanical forces applied at the macroscale produce changes in biochemistry and gene expression within individual living cells. This structurebased system provides a mechanistic basis to explain how application of physical therapies might influence cell and tissue physiology.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DK6AVJRE\\Ingber - 2008 - Tensegrity and mechanotransduction.pdf},
  langid = {english},
  number = {3}
}

@unpublished{itoMediumFacilitatesPerception2015,
  title = {Medium {{Facilitates}} the Perception of Affordances for Touch},
  author = {Ito, K. and Sawada, M. and Mishima, Mamoru and Mishima, H. and Takiyama, M. and Kikuchi, Y.;},
  date = {2015-07-18},
  eventtitle = {{{XVIII International Conference}} on {{Perception}}-{{Action}}},
  venue = {{Minneapolis}}
}

@article{iversenSynchronizationAuditoryVisual2015,
  title = {Synchronization to Auditory and Visual Rhythms in Hearing and Deaf Individuals},
  author = {Iversen, John R. and Patel, Aniruddh D. and Nicodemus, Brenda and Emmorey, Karen},
  date = {2015-01},
  journaltitle = {Cognition},
  volume = {134},
  pages = {232--244},
  issn = {00100277},
  doi = {10.1016/j.cognition.2014.10.018},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027714002194},
  urldate = {2020-02-10},
  abstract = {A striking asymmetry in human sensorimotor processing is that humans synchronize movements to rhythmic sound with far greater precision than to temporally equivalent visual stimuli (e.g., to an auditory vs. a flashing visual metronome). Traditionally, this finding is thought to reflect a fundamental difference in auditory vs. visual processing, i.e., superior temporal processing by the auditory system and/or privileged coupling between the auditory and motor systems. It is unclear whether this asymmetry is an inevitable consequence of brain organization or whether it can be modified (or even eliminated) by stimulus characteristics or by experience. With respect to stimulus characteristics, we found that a moving, colliding visual stimulus (a silent image of a bouncing ball with a distinct collision point on the floor) was able to drive synchronization nearly as accurately as sound in hearing participants. To study the role of experience, we compared synchronization to flashing metronomes in hearing and profoundly deaf individuals. Deaf individuals performed better than hearing individuals when synchronizing with visual flashes, suggesting that cross-modal plasticity enhances the ability to synchronize with temporally discrete visual stimuli. Furthermore, when deaf (but not hearing) individuals synchronized with the bouncing ball, their tapping patterns suggest that visual timing may access higherorder beat perception mechanisms for deaf individuals. These results indicate that the auditory advantage in rhythmic synchronization is more experience- and stimulusdependent than has been previously reported.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\X85INBQ3\\Iversen et al. - 2015 - Synchronization to auditory and visual rhythms in .pdf},
  langid = {english}
}

@article{iversonHandMouthBrain2005,
  title = {Hand, Mouth and Brain: {{The}} Dynamic Emergence of Speech and Gesture},
  author = {Iverson, Jana M and Thelen, Esther},
  date = {2005},
  journaltitle = {Journal of Consciousness Studies},
  pages = {22},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NIX9C9GZ\\Iverson and Thelen - 2005 - Journal of Consciousness Studies.pdf},
  langid = {english}
}

@article{iversonInfantVocalmotorCoordination2004,
  title = {Infant Vocal-Motor Coordination: Precursor to the Gesture-Speech System?},
  shorttitle = {Infant Vocal-Motor Coordination},
  author = {Iverson, Jana M. and Fagan, Mary K.},
  year = {2004 Jul-Aug},
  journaltitle = {Child Development},
  shortjournal = {Child Dev},
  volume = {75},
  pages = {1053--1066},
  issn = {0009-3920},
  doi = {10.1111/j.1467-8624.2004.00725.x},
  abstract = {This study was designed to provide a general picture of infant vocal-motor coordination and test predictions generated by Iverson and Thelen's (1999) model of the development of the gesture-speech system. Forty-seven 6- to 9-month-old infants were videotaped with a primary caregiver during rattle and toy play. Results indicated an age-related increase in frequency of vocal-motor coordination, greater coordination with arm (specifically right arm) than leg or torso movements, and a temporal pattern similar to that in adult gesture-speech coproductions. Rhythmic vocalizations (consonant-vowel repetitions) were more likely to occur with than without rhythmic movement, and with rhythmic manual than with nonmanual activity, and the rate of vocal-manual coordination was higher in babblers than in prebabblers.},
  eprint = {15260864},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NLH9WKRR\\Iverson and Fagan - 2004 - Infant vocal-motor coordination precursor to the .pdf},
  keywords = {Child Development,Communication,Extremities,Female,Gestures,Humans,Infant,Male,Motor Skills,Movement,Periodicity,Speech,Videotape Recording},
  langid = {english},
  number = {4}
}

@article{iversonResilienceGestureTalk2001,
  title = {The Resilience of Gesture in Talk: Gesture in Blind Speakers and Listeners},
  shorttitle = {The Resilience of Gesture in Talk},
  author = {Iverson, Jana M. and Goldin‐Meadow, Susan},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {416--422},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00183},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00183},
  urldate = {2019-05-05},
  abstract = {Spontaneous gesture frequently accompanies speech. The question is why. In these studies, we tested two non-mutually exclusive possibilities. First, speakers may gesture simply because they see others gesture and learn from this model to move their hands as they talk. We tested this hypothesis by examining spontaneous communication in congenitally blind children and adolescents. Second, speakers may gesture because they recognize that gestures can be useful to the listener. We tested this hypothesis by examining whether speakers gesture even when communicating with a blind listener who is unable to profit from the information that the hands convey. We found that congenitally blind speakers, who had never seen gestures, nevertheless gestured as they spoke, conveying the same information and producing the same range of gesture forms as sighted speakers. Moreover, blind speakers gestured even when interacting with another blind individual who could not have benefited from the information contained in those gestures. These findings underscore the robustness of gesture in talk and suggest that the gestures that co-occur with speech may serve a function for the speaker as well as for the listener.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BBKPR8QQ\\1467-7687.html},
  langid = {english},
  number = {4}
}

@article{iversonResilienceGestureTalk2001a,
  title = {The Resilience of Gesture in Talk: Gesture in Blind Speakers and Listeners},
  shorttitle = {The Resilience of Gesture in Talk},
  author = {Iverson, Jana M. and Goldin‐Meadow, Susan},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {416--422},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00183},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00183},
  urldate = {2019-10-14},
  abstract = {Spontaneous gesture frequently accompanies speech. The question is why. In these studies, we tested two non-mutually exclusive possibilities. First, speakers may gesture simply because they see others gesture and learn from this model to move their hands as they talk. We tested this hypothesis by examining spontaneous communication in congenitally blind children and adolescents. Second, speakers may gesture because they recognize that gestures can be useful to the listener. We tested this hypothesis by examining whether speakers gesture even when communicating with a blind listener who is unable to profit from the information that the hands convey. We found that congenitally blind speakers, who had never seen gestures, nevertheless gestured as they spoke, conveying the same information and producing the same range of gesture forms as sighted speakers. Moreover, blind speakers gestured even when interacting with another blind individual who could not have benefited from the information contained in those gestures. These findings underscore the robustness of gesture in talk and suggest that the gestures that co-occur with speech may serve a function for the speaker as well as for the listener.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\G6DTJTEE\\1467-7687.html},
  langid = {english},
  number = {4}
}

@article{iversonWhyPeopleGesture1998,
  title = {Why People Gesture When They Speak},
  author = {Iverson, Jana M. and Goldin-Meadow, Susan},
  date = {1998-11},
  journaltitle = {Nature},
  volume = {396},
  pages = {228--228},
  issn = {1476-4687},
  doi = {10.1038/24300},
  url = {https://www.nature.com/articles/24300},
  urldate = {2019-11-30},
  abstract = {People use gestures when they talk, but is this behaviour learned from watching others move their hands when talking? Individuals who are blind from birth never see such gestures and so have no model for gesturing. But here we show that congenitally blind speakers gesture despite their lack of a visual model, even when they speak to a blind listener. Gestures therefore require neither a model nor an observant partner.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\69RUKT7D\\24300.html},
  langid = {english},
  number = {6708}
}

@article{jarvisEvolutionVocalLearning2019,
  title = {Evolution of Vocal Learning and Spoken Language},
  author = {Jarvis, Erich D.},
  date = {2019-10-04},
  journaltitle = {Science},
  volume = {366},
  pages = {50--54},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax0287},
  url = {https://science.sciencemag.org/content/366/6461/50},
  urldate = {2019-10-18},
  abstract = {Although language, and therefore spoken language or speech, is often considered unique to humans, the past several decades have seen a surge in nonhuman animal studies that inform us about human spoken language. Here, I present a modern, evolution-based synthesis of these studies, from behavioral to molecular levels of analyses. Among the key concepts drawn are that components of spoken language are continuous between species, and that the vocal learning component is the most specialized and rarest and evolved by brain pathway duplication from an ancient motor learning pathway. These concepts have important implications for understanding brain mechanisms and disorders of spoken language.},
  eprint = {31604300},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\GGTIKRCC\\50.html},
  langid = {english},
  number = {6461}
}

@online{JointActionInteractive,
  title = {Joint {{Action}}, {{Interactive Alignment}}, and {{Dialog}} - {{Garrod}} - 2009 - {{Topics}} in {{Cognitive Science}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1756-8765.2009.01020.x},
  urldate = {2019-12-18},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HR5LMI5K\\j.1756-8765.2009.01020.html}
}

@article{jollyFlatlandFallacyMoving2019,
  title = {The {{Flatland Fallacy}}: {{Moving Beyond Low}}–{{Dimensional Thinking}}},
  shorttitle = {The {{Flatland Fallacy}}},
  author = {Jolly, Eshin and Chang, Luke J.},
  date = {2019},
  journaltitle = {Topics in Cognitive Science},
  volume = {11},
  pages = {433--454},
  issn = {1756-8765},
  doi = {10.1111/tops.12404},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12404},
  urldate = {2020-05-07},
  abstract = {Psychology is a complicated science. It has no general axioms or mathematical proofs, is rarely directly observable, and is the only discipline in which the subject matter (i.e., human psychological phenomena) is also the tool of investigation. Like the Flatlanders in Edwin Abbot's famous short story (), we may be led to believe that the parsimony offered by our low-dimensional theories reflects the reality of a much higher-dimensional problem. Here we contend that this “Flatland fallacy” leads us to seek out simplified explanations of complex phenomena, limiting our capacity as scientists to build and communicate useful models of human psychology. We suggest that this fallacy can be overcome through (a) the use of quantitative models, which force researchers to formalize their theories to overcome this fallacy, and (b) improved quantitative training, which can build new norms for conducting psychological research.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\W6AYR598\\Jolly and Chang - 2019 - The Flatland Fallacy Moving Beyond Low–Dimensiona.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\T882X5TX\\tops.html},
  keywords = {Computational,Decision-making,Dual-processing,Psychological education,Social},
  langid = {english},
  number = {2}
}

@article{kashiwadateSyntacticStructureInfluences2020,
  title = {Syntactic {{Structure Influences Speech}}-{{Gesture Synchronization}}},
  author = {Kashiwadate, Kei and Yasuda, Tetsuya and Fujita, Koji and Kita, Sotaro and Kobayashi, Harumi},
  date = {2020-03-16},
  journaltitle = {Letters on Evolutionary Behavioral Science},
  volume = {11},
  pages = {10--14},
  issn = {1884-927X},
  doi = {10.5178/lebs.2020.73},
  url = {https://lebs.hbesj.org/index.php/lebs/article/view/lebs.2020.73},
  urldate = {2020-03-26},
  abstract = {It is known that a phrase may have multiple meanings. Phrases such as “green tea cup” may be interpreted with two different meanings—a “green-colored tea cup” or a “cup of green tea.” Then how people know the exact meanings of apparently syntactically ambiguous linguistic expressions? We propose that gesture that accompanies speech may help disambiguate syntactically ambiguous structures. The present study investigated whether the difference in phrase structures influences the production of gestures. Participants produced gestures as they produced a Japanese four-word phrases. We examined all possible synchronization patterns of speech and gestures. We found, for the first time, gestures tended to synchronize with the chunks of words that form a constituent in syntactic structures. Our study suggests that gestures may play an important role in disambiguating syntactically ambiguous phrases. This could be a reason why humans have continuously used gestures even after they acquired a powerful tool of language and why today, they still produce language-redundant gestures.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XZ4IWUUF\\Kashiwadate et al. - 2020 - Syntactic Structure Influences Speech-Gesture Sync.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\LD97IERN\\lebs.2020.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{kashiwadateSyntacticStructureInfluences2020a,
  title = {Syntactic {{Structure Influences Speech}}-{{Gesture Synchronization}}},
  author = {Kashiwadate, Kei and Yasuda, Tetsuya and Fujita, Koji and Kita, Sotaro and Kobayashi, Harumi},
  date = {2020-03-16},
  journaltitle = {Letters on Evolutionary Behavioral Science},
  volume = {11},
  pages = {10--14},
  issn = {1884-927X},
  doi = {10.5178/lebs.2020.73},
  url = {https://lebs.hbesj.org/index.php/lebs/article/view/lebs.2020.73},
  urldate = {2020-03-27},
  abstract = {It is known that a phrase may have multiple meanings. Phrases such as “green tea cup” may be interpreted with two different meanings—a “green-colored tea cup” or a “cup of green tea.” Then how people know the exact meanings of apparently syntactically ambiguous linguistic expressions? We propose that gesture that accompanies speech may help disambiguate syntactically ambiguous structures. The present study investigated whether the difference in phrase structures influences the production of gestures. Participants produced gestures as they produced a Japanese four-word phrases. We examined all possible synchronization patterns of speech and gestures. We found, for the first time, gestures tended to synchronize with the chunks of words that form a constituent in syntactic structures. Our study suggests that gestures may play an important role in disambiguating syntactically ambiguous phrases. This could be a reason why humans have continuously used gestures even after they acquired a powerful tool of language and why today, they still produce language-redundant gestures.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FY7H47HD\\Kashiwadate et al. - 2020 - Syntactic Structure Influences Speech-Gesture Sync.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\YJTVUZC4\\lebs.2020.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@incollection{kelsoActionperceptionPatternFormation1990,
  title = {Action-Perception as a Pattern Formation Process},
  booktitle = {Attention and Performance 13:  {{Motor}} Representation and Control},
  author = {Kelso, S. J. A. and Del Colle, J. D. and Schöner, G.},
  date = {1990},
  pages = {139--169},
  publisher = {{Lawrence Erlbaum Associates, Inc}},
  location = {{Hillsdale, NJ, US}},
  abstract = {aim to identify collective variables (or order parameters) and their dynamics (stability, loss of stability, hysteresis . . . ) for perception-action patterns / to do so we extend earlier results on phase transitions in human bimanual coordination to a perception-action task, synchronizing or syncopating finger flexion with an auditory metronome whose frequency is varied in steps (.25 Hz) over a wide range (1.0 Hz to 3.5 Hz) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Y6BQ6G7X\\1990-97330-005.html},
  isbn = {978-0-8058-0565-9},
  keywords = {Auditory Perception,Perceptual Motor Coordination,Response Parameters}
}

@article{kelsoConvergingEvidenceSupport1984,
  title = {Converging Evidence in Support of Common Dynamical Principles for Speech and Movement Coordination},
  author = {Kelso, J. A. and Tuller, B.},
  date = {1984-06},
  journaltitle = {The American Journal of Physiology},
  shortjournal = {Am. J. Physiol.},
  volume = {246},
  pages = {R928-935},
  issn = {0002-9513},
  doi = {10.1152/ajpregu.1984.246.6.R928},
  abstract = {We suggest that a principled analysis of language and action should begin with an understanding of the rate-dependent, dynamical processes that underlie their implementation. Here we present a summary of our ongoing speech production research, which reveals some striking similarities with other work on limb movements. Four design themes emerge for articulatory systems: 1) they are functionally rather than anatomically specific in the way they work; 2) they exhibit equifinality and in doing so fall under the generic category of a dynamical system called point attractor; 3) across transformations they preserve a relationally invariant topology; and 4) this, combined with their stable cyclic nature, suggests that they can function as nonlinear, limit cycle oscillators (periodic attractors). This brief inventory of regularities, though not mean to be inclusive, hints strongly that speech and other movements share a common, dynamical mode of operation.},
  eprint = {6742170},
  eprinttype = {pmid},
  issue = {6 Pt 2},
  keywords = {Animals,Brain,Humans,Locomotion,Models; Neurological,Models; Psychological,Motor Activity,Movement,Speech},
  langid = {english}
}

@incollection{kelsoDynamicPatternPerspective1983,
  title = {A “{{Dynamic Pattern}}” {{Perspective}} on the {{Control}} and {{Coordination}} of {{Movement}}},
  booktitle = {The Production of Speech},
  author = {Kelso, J. A. S. and Tuller, B. and Harris, K.},
  date = {1983},
  publisher = {{Springer-Verlag}},
  location = {{Berlin}},
  url = {https://link.springer.com/chapter/10.1007/978-1-4613-8202-7_7},
  urldate = {2019-08-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NZWWH973\\978-1-4613-8202-7_7.html}
}

@article{kelsoFunctionallySpecificArticulatory1984,
  title = {Functionally Specific Articulatory Cooperation Following Jaw Perturbations during Speech: Evidence for Coordinative Structures},
  shorttitle = {Functionally Specific Articulatory Cooperation Following Jaw Perturbations during Speech},
  author = {Kelso, J. A. and Tuller, B. and Vatikiotis-Bateson, E. and Fowler, C. A.},
  date = {1984-12},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {10},
  pages = {812--832},
  issn = {0096-1523},
  abstract = {In three experiments we show that articulatory patterns in response to jaw perturbations are specific to the utterance produced. In Experiments 1 and 2, an unexpected constant force load (5.88 N) applied during upward jaw motion for final /b/ closure in the utterance /baeb/ revealed nearly immediate compensation in upper and lower lips, but not the tongue, on the first perturbation trial. The same perturbation applied during the utterance /baez/ evoked rapid and increased tongue-muscle activity for /z/ frication, but no active lip compensation. Although jaw perturbation represented a threat to both utterances, no perceptible distortion of speech occurred. In Experiment 3, the phase of the jaw perturbation was varied during the production of bilabial consonants. Remote reactions in the upper lip were observed only when the jaw was perturbed during the closing phase of motion. These findings provide evidence for flexibly assembled coordinative structures in speech production.},
  eprint = {6239907},
  eprinttype = {pmid},
  keywords = {Adult,Electromyography,Humans,Lip,Male,Mandible,Masticatory Muscles,Phonation,Phonetics,Speech,Speech Production Measurement,Voice},
  langid = {english},
  number = {6}
}

@article{kendonReflectionsGesturefirstHypothesis2017,
  title = {Reflections on the “Gesture-First” Hypothesis of Language Origins},
  author = {Kendon, Adam},
  date = {2017-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {24},
  pages = {163--170},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1117-3},
  url = {https://doi.org/10.3758/s13423-016-1117-3},
  urldate = {2019-10-14},
  abstract = {The main lines of evidence taken as support for the “gesture-first” hypothesis of language origins are briefly evaluated, and the problem that speech poses for this hypothesis is discussed. I conclude that language must have evolved in the oral–aural and kinesic modalities together, with neither modality taking precedence over the other.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\T3SYIPKP\\Kendon - 2017 - Reflections on the “gesture-first” hypothesis of l.pdf},
  keywords = {Gesture,Language origins,Primate communication,Sign language,Speech},
  langid = {english},
  number = {1}
}

@article{kienastACOUSTICALANALYSISSPECTRAL,
  title = {{{ACOUSTICAL ANALYSIS OF SPECTRAL AND TEMPORAL CHANGES IN EMOTIONAL SPEECH}}},
  author = {Kienast, M and Sendlmeier, W F},
  journaltitle = {Analysis ISCA Tutor},
  pages = {92--97},
  abstract = {In the present study, the vocal expressions of the emotions anger, happiness, fear, boredom and sadness are acoustically analyzed in relation to neutral speech. The emotional speech material produced by actors is investigated especially with regard to spectral and segmental changes which are caused by different articulatory behavior accompanying emotional arousal. The findings are interpreted in relation to temporal variations.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BIJ55YAL\\Kienast and Sendlmeier - ACOUSTICAL ANALYSIS OF SPECTRAL AND TEMPORAL CHANG.pdf},
  langid = {english}
}

@article{kirbyCompressionCommunicationCultural2015,
  title = {Compression and Communication in the Cultural Evolution of Linguistic Structure},
  author = {Kirby, S and Tamariz, M. and Cornish, H. and Smith, K.},
  date = {2015},
  journaltitle = {Cognition},
  volume = {141},
  pages = {87--102},
  doi = {10.1016/j.cognition.2015.03.016},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027715000815},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YR8VRAVS\\S0010027715000815.html}
}

@article{kirbyCompressionCommunicationCultural2015a,
  title = {Compression and Communication in the Cultural Evolution of Linguistic Structure},
  author = {Kirby, Simon and Tamariz, Monica and Cornish, Hannah and Smith, Kenny},
  date = {2015-08-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {141},
  pages = {87--102},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.03.016},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027715000815},
  urldate = {2020-03-18},
  abstract = {Language exhibits striking systematic structure. Words are composed of combinations of reusable sounds, and those words in turn are combined to form complex sentences. These properties make language unique among natural communication systems and enable our species to convey an open-ended set of messages. We provide a cultural evolutionary account of the origins of this structure. We show, using simulations of rational learners and laboratory experiments, that structure arises from a trade-off between pressures for compressibility (imposed during learning) and expressivity (imposed during communication). We further demonstrate that the relative strength of these two pressures can be varied in different social contexts, leading to novel predictions about the emergence of structured behaviour in the wild.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CV896X9L\\Kirby et al. - 2015 - Compression and communication in the cultural evol.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\DHT5PRIK\\S0010027715000815.html},
  keywords = {Cultural transmission,Iterated learning,Language evolution},
  langid = {english}
}

@article{kirbyCumulativeCulturalEvolution2008,
  title = {Cumulative Cultural Evolution in the Laboratory: {{An}} Experimental Approach to the Origins of Structure in Human Language},
  shorttitle = {Cumulative Cultural Evolution in the Laboratory},
  author = {Kirby, Simon and Cornish, Hannah and Smith, Kenny},
  date = {2008-08-05},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {105},
  pages = {10681--10686},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0707835105},
  url = {https://www.pnas.org/content/105/31/10681},
  urldate = {2020-03-16},
  abstract = {We introduce an experimental paradigm for studying the cumulative cultural evolution of language. In doing so we provide the first experimental validation for the idea that cultural transmission can lead to the appearance of design without a designer. Our experiments involve the iterated learning of artificial languages by human participants. We show that languages transmitted culturally evolve in such a way as to maximize their own transmissibility: over time, the languages in our experiments become easier to learn and increasingly structured. Furthermore, this structure emerges purely as a consequence of the transmission of language over generations, without any intentional design on the part of individual language learners. Previous computational and mathematical models suggest that iterated learning provides an explanation for the structure of human language and link particular aspects of linguistic structure with particular constraints acting on language during its transmission. The experimental work presented here shows that the predictions of these models, and models of cultural evolution more generally, can be tested in the laboratory.},
  eprint = {18667697},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LMLYGGZK\\Kirby et al. - 2008 - Cumulative cultural evolution in the laboratory A.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\Z2D3EWBR\\10681.html},
  keywords = {cultural transmission,iterated learning,language evolution},
  langid = {english},
  number = {31}
}

@article{kirbyIteratedLearningEvolution2014,
  title = {Iterated Learning and the Evolution of Language},
  author = {Kirby, Simon and Griffiths, Tom and Smith, Kenny},
  date = {2014-10},
  journaltitle = {Current Opinion in Neurobiology},
  volume = {28},
  pages = {108--114},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.07.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438814001421},
  urldate = {2020-03-18},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\WSUJ84ZR\\Kirby et al. - 2014 - Iterated learning and the evolution of language.pdf},
  langid = {english}
}

@incollection{kirbyLanguageLearningLanguage2003,
  title = {From {{Language Learning}} to {{Language Evolution}}},
  booktitle = {Language {{Evolution}}},
  author = {Kirby, Simon and Christiansen, M. H.},
  editor = {Christiansen, M. H. and Kirby, Simon},
  date = {2003-07-24},
  pages = {272--294},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199244843.003.0015},
  url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199244843.001.0001/acprof-9780199244843-chapter-15},
  urldate = {2020-03-16},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QKWSV3BA\\Kirby and Christiansen - 2003 - From Language Learning to Language Evolution.pdf},
  isbn = {978-0-19-924484-3},
  langid = {english}
}

@inproceedings{kitaMovementPhasesSigns1998,
  title = {Movement Phases in Signs and Co-Speech Gestures, and Their Transcription by Human Coders},
  booktitle = {Gesture and {{Sign Language}} in {{Human}}-{{Computer Interaction}}},
  author = {Kita, Sotaro and van Gijn, Ingeborg and van der Hulst, Harry},
  editor = {Wachsmuth, Ipke and Fröhlich, Martin},
  date = {1998},
  pages = {23--35},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The previous literature has suggested that the hand movement in co-speech gestures and signs consists of a series of phases with qualitatively different dynamic characteristics. In this paper, we propose a syntagmatic rule system for movement phases that applies to both co-speech gestures and signs. Descriptive criteria for the rule system were developed for the analysis video-recorded continuous production of signs and gesture. It involves segmenting a stream of body movement into phases and identifying different phase types. Two human coders used the criteria to analyze signs and cospeech gestures that are produced in natural discourse. It was found that the criteria yielded good inter-coder reliability. These criteria can be used for the technology of automatic recognition of signs and co-speech gestures in order to segment continuous production and identify the potentially meaningbearing phase.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\D6R8R2SP\\Kita et al. - 1998 - Movement phases in signs and co-speech gestures, a.pdf},
  isbn = {978-3-540-69782-4},
  keywords = {American Sign,Expressive Phase,Movement Phase,Phase Type,Sign Language},
  langid = {english},
  options = {useprefix=true},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{kitaPointingWhereLanguage2003,
  title = {Pointing:  {{Where}} Language, Culture, and Cognition Meet},
  shorttitle = {Pointing},
  author = {Kita, S.},
  date = {2003},
  publisher = {{Lawrence Erlbaum Associates Publishers}},
  location = {{Mahwah, NJ, US}},
  abstract = {This volume examines pointing gestures from a multidisciplinary viewpoint. Pointing has captured the interest of scholars from different disciplines who study communication, however, ideas and findings have been scattered across diverse journals and researchers are often not aware of results in other disciplines. The aim of this volume is to provide an arena for the interdisciplinary exchange of information on pointing. This volume will be of interest to researchers in linguistics, semiotics, psychology, anthropology, and primatology. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5URMXNHN\\2003-00985-000.html},
  isbn = {978-0-8058-4014-8},
  keywords = {Experimentation,Gestures,Interdisciplinary Research},
  pagetotal = {vii, 339},
  series = {Pointing:  {{Where}} Language, Culture, and Cognition Meet}
}

@article{kitaRelationsSyntacticEncoding2007,
  title = {Relations between Syntactic Encoding and Co-Speech Gestures: {{Implications}} for a Model of Speech and Gesture Production},
  shorttitle = {Relations between Syntactic Encoding and Co-Speech Gestures},
  author = {Kita, Sotaro and Özyürek, Asli and Allen, Shanley and Brown, Amanda and Furman, Reyhan and Ishizuka, Tomoko},
  date = {2007-12-01},
  journaltitle = {Language and Cognitive Processes},
  volume = {22},
  pages = {1212--1236},
  publisher = {{Routledge}},
  issn = {0169-0965},
  doi = {10.1080/01690960701461426},
  url = {https://doi.org/10.1080/01690960701461426},
  urldate = {2020-05-25},
  abstract = {Gestures that accompany speech are known to be tightly coupled with speech production. However little is known about the cognitive processes that underlie this link. Previous cross-linguistic research has provided preliminary evidence for online interaction between the two systems based on the systematic co-variation found between how different languages syntactically package Manner and Path information of a motion event and how gestures represent Manner and Path. Here we elaborate on this finding by testing whether speakers within the same language gesturally express Manner and Path differently according to their online choice of syntactic packaging of Manner and Path, or whether gestural expression is pre-determined by a habitual conceptual schema congruent with the linguistic typology. Typologically congruent and incongruent syntactic structures for expressing Manner and Path (i.e., in a single clause or multiple clauses) were elicited from English speakers. We found that gestural expressions were determined by the online choice of syntactic packaging rather than by a habitual conceptual schema. It is therefore concluded that speech and gesture production processes interface online at the conceptual planning phase. Implications of the findings for models of speech and gesture production are discussed.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TCYK5WBH\\Kita et al. - 2007 - Relations between syntactic encoding and co-speech.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AKFU8BIX\\01690960701461426.html},
  number = {8}
}

@article{kitaWhatDoesCrosslinguistic2003,
  title = {What Does Cross-Linguistic Variation in Semantic Coordination of Speech and Gesture Reveal?: {{Evidence}} for an Interface Representation of Spatial Thinking and Speaking},
  shorttitle = {What Does Cross-Linguistic Variation in Semantic Coordination of Speech and Gesture Reveal?},
  author = {Kita, Sotaro and Özyürek, Asli},
  date = {2003-01-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {48},
  pages = {16--32},
  issn = {0749-596X},
  doi = {10.1016/S0749-596X(02)00505-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0749596X02005053},
  urldate = {2020-05-25},
  abstract = {Gestures that spontaneously accompany speech convey information coordinated with the concurrent speech. There has been considerable theoretical disagreement about the process by which this informational coordination is achieved. Some theories predict that the information encoded in gesture is not influenced by how information is verbally expressed. However, others predict that gestures encode only what is encoded in speech. This paper investigates this issue by comparing informational coordination between speech and gesture across different languages. Narratives in Turkish, Japanese, and English were elicited using an animated cartoon as the stimulus. It was found that gestures used to express the same motion events were influenced simultaneously by (1) how features of motion events were expressed in each language, and (2) spatial information in the stimulus that was never verbalized. From this, it is concluded that gestures are generated from spatio-motoric processes that interact on-line with the speech production process. Through the interaction, spatio-motoric information to be expressed is packaged into chunks that are verbalizable within a processing unit for speech formulation. In addition, we propose a model of speech and gesture production as one of a class of frameworks that are compatible with the data.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\R3BJFMFQ\\Kita and Özyürek - 2003 - What does cross-linguistic variation in semantic c.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8EDDEQ3A\\S0749596X02005053.html},
  keywords = {Cross-linguistic comparison,Gesture production,Motion event,Semantic coordination,Speech production},
  langid = {english},
  number = {1}
}

@software{kleimanEMAtoolsDataManagement2017,
  title = {{{EMAtools}}: {{Data Management Tools}} for {{Real}}-{{Time Monitoring}}/{{Ecological Momentary Assessment Data}}},
  shorttitle = {{{EMAtools}}},
  author = {Kleiman, Evan},
  date = {2017-08-03},
  url = {https://CRAN.R-project.org/package=EMAtools},
  urldate = {2020-05-27},
  abstract = {Do data management functions common in real-time monitoring (also called: ecological momentary assessment, experience sampling, micro-longitudinal) data, including creating power curves for multilevel data, centering on participant means and merging event-level data into momentary data sets where you need the events to correspond to the nearest data point in the momentary data. This is VERY early release software, and more features will be added over time.},
  version = {0.1.3}
}

@article{klingeIncreasedAmygdalaActivation2010,
  title = {Increased Amygdala Activation to Emotional Auditory Stimuli in the Blind},
  author = {Klinge, Corinna and Röder, Brigitte and Büchel, Christian},
  date = {2010-06},
  journaltitle = {Brain},
  volume = {133},
  pages = {1729--1736},
  issn = {1460-2156, 0006-8950},
  doi = {10.1093/brain/awq102},
  url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awq102},
  urldate = {2019-11-22},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SSDBMUSK\\Klinge et al. - 2010 - Increased amygdala activation to emotional auditor.pdf},
  langid = {english},
  number = {6}
}

@article{klingeIncreasedAmygdalaActivation2010a,
  title = {Increased Amygdala Activation to Emotional Auditory Stimuli in the Blind},
  author = {Klinge, Corinna and Röder, Brigitte and Büchel, Christian},
  date = {2010-06-01},
  journaltitle = {Brain},
  shortjournal = {Brain},
  volume = {133},
  pages = {1729--1736},
  issn = {0006-8950},
  doi = {10.1093/brain/awq102},
  url = {https://academic.oup.com/brain/article/133/6/1729/355156},
  urldate = {2019-11-22},
  abstract = {Abstract.  Emotional signals are of pivotal relevance in social interactions. Neuroimaging and lesion studies have established an important role of the amygdala},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PEZPIEYL\\Klinge et al. - 2010 - Increased amygdala activation to emotional auditor.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8VNW26X9\\355156.html},
  langid = {english},
  number = {6}
}

@article{koenigRespiratoryElectroglottographicMeasures2019,
  title = {Respiratory and Electroglottographic Measures of Normal and Loud Speech across Vowels},
  author = {Koenig, Laura L. and Fuchs, Susanne},
  date = {2019-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {145},
  pages = {1931--1931},
  issn = {0001-4966},
  doi = {10.1121/1.5102030},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.5102030},
  urldate = {2019-08-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VVDTB7XM\\1.html},
  number = {3}
}

@article{krahmerEffectsVisualBeats2007,
  title = {The Effects of Visual Beats on Prosodic Prominence: {{Acoustic}} Analyses, Auditory Perception and Visual Perception},
  shorttitle = {The Effects of Visual Beats on Prosodic Prominence},
  author = {Krahmer, Emiel and Swerts, Marc},
  date = {2007-10},
  journaltitle = {Journal of Memory and Language},
  volume = {57},
  pages = {396--414},
  issn = {0749596X},
  doi = {10.1016/j.jml.2007.06.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X07000708},
  urldate = {2019-04-18},
  abstract = {Speakers employ acoustic cues (pitch accents) to indicate that a word is important, but may also use visual cues (beat gestures, head nods, eyebrow movements) for this purpose. Even though these acoustic and visual cues are related, the exact nature of this relationship is far from well understood. We investigate whether producing a visual beat leads to changes in how acoustic prominence is realized in speech, and whether it leads to changes in how prominence is perceived by observers. For Experiment I (‘‘making beats’’) we use an original experimental paradigm in which speakers are instructed to realize a target sentence with different distributions of acoustic and visual cues for prominence. Acoustic analyses reveal that the production of a visual beat indeed has an effect on the acoustic realization of the co-occurring speech, in particular on duration and the higher formants (F2 and F3), independent of the kind of visual beat and of the presence and position of pitch accents. In Experiment II (‘‘hearing beats’’), it is found that visual beats have a significant effect on the perceived prominence of the target words. When a speaker produces a beat gesture, an eyebrow movement or a head nod, the accompanying word is produced with relatively more spoken emphasis. In Experiment III (‘‘seeing beats’’), finally, it is found that when participants see a speaker realize a visual beat on a word, they perceive it as more prominent than when they do not see the beat gesture.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AU3AWR57\\Krahmer and Swerts - 2007 - The effects of visual beats on prosodic prominence.pdf},
  langid = {english},
  number = {3}
}

@incollection{kraussLexicalGesturesLexical2000,
  title = {Lexical Gestures and Lexical {{accessL}} a Process},
  booktitle = {Language and {{Gesture}}},
  author = {Krauss, R. M. and Chen, Y. and Gottesman, R. F.},
  date = {2000},
  url = {https://scholar.googleusercontent.com/scholar.bib?q=info:0fpr8C9_kDMJ:scholar.google.com/&output=citation&scisdr=CgUQcZkfEO3Yy3o0M_I:AAGBfm0AAAAAXZMxK_KEmWpHM-VpMySbsZU2wKqbdaIZ&scisig=AAGBfm0AAAAAXZMxK7vMU-afRHI6nivhKGEXFplyqMZq&scisf=4&ct=citation&cd=-1&hl=en},
  urldate = {2019-10-01},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ECPAAISQ\\scholar.html}
}

@article{krivokapicGesturalCoordinationProsodic2014,
  title = {Gestural Coordination at Prosodic Boundaries and Its Role for Prosodic Structure and Speech Planning Processes},
  author = {Krivokapić, Jelena},
  date = {2014-12-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0397},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4240964/},
  urldate = {2020-01-26},
  abstract = {Prosodic structure is a grammatical component that serves multiple functions in the production, comprehension and acquisition of language. Prosodic boundaries are critical for the understanding of the nature of the prosodic structure of language, and important progress has been made in the past decades in illuminating their properties. We first review recent prosodic boundary research from the point of view of gestural coordination. We then go on to tie in this work to questions of speech planning and manual and head movement. We conclude with an outline of a new direction of research which is needed for a full understanding of prosodic boundaries and their role in the speech production process.},
  eprint = {25385775},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3ZRITNK5\\Krivokapić - 2014 - Gestural coordination at prosodic boundaries and i.pdf},
  number = {1658},
  pmcid = {PMC4240964}
}

@article{krivokapicKinematicStudyProsodic2017,
  title = {A {{Kinematic Study}} of {{Prosodic Structure}} in {{Articulatory}} and {{Manual Gestures}}: {{Results}} from a {{Novel Method}} of {{Data Collection}}},
  shorttitle = {A {{Kinematic Study}} of {{Prosodic Structure}} in {{Articulatory}} and {{Manual Gestures}}},
  author = {Krivokapić, Jelena and Tiede, Mark K. and Tyrone, Martha E.},
  date = {2017},
  journaltitle = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {8},
  pages = {3},
  issn = {1868-6354},
  doi = {10.5334/labphon.75},
  url = {http://www.journal-labphon.org/articles/10.5334/labphon.75/},
  urldate = {2020-01-15},
  abstract = {The primary goal of this work is to examine prosodic structure as expressed concurrently through articulatory and manual gestures. Specifically, we investigated the effects of phrase-level prominence (Experiment 1) and of prosodic boundaries (Experiments 2 and 3) on the kinematic properties of oral constriction and manual gestures. The hypothesis guiding this work is that prosodic structure will be similarly expressed in both modalities. To test this, we have developed a novel method of data collection that simultaneously records speech audio, vocal tract gestures (using electromagnetic articulometry) and manual gestures (using motion capture). This method allows us, for the first time, to investigate kinematic properties of body movement and vocal tract gestures simultaneously, which in turn allows us to examine the relationship between speech and body gestures with great precision. A second goal of the paper is thus to establish the validity of this method. Results from two speakers show that manual and oral gestures lengthen under prominence and at prosodic boundaries, indicating that the effects of prosodic structure extend beyond the vocal tract to include body movement.1},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QZ4WY9ZY\\Krivokapić et al. - 2017 - A Kinematic Study of Prosodic Structure in Articul.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ZCZIUWFM\\labphon.75.html},
  keywords = {electro- magnetic articulometry,EMA,gestures,motion capture,Prosodic boundaries,prosodic prominence,speech production,Vicon},
  langid = {english},
  number = {1}
}

@inproceedings{krivokapicSpeechManualGesture2016,
  title = {Speech and Manual Gesture Coordination in a Pointing Task},
  author = {Krivokapic, Jelena and Tiede, Mark K. and Tyrone, Martha E. and Goldenberg, Dolly},
  date = {2016},
  doi = {10.21437/SpeechProsody.2016-255},
  abstract = {This study explores the coordination between manual pointing gestures and gestures of the vocal tract. Using a novel methodology that allows for concurrent collection of audio, kinematic body and speech articulator trajectories, we ask 1) which particular gesture (vowel gesture, consonant gesture, or tone gesture) the pointing gesture is coordinated with, and 2) with which landmarks the two gestures are coordinated (for example, whether the pointing gesture is coordinated to the speech gesture by the onset or maximum displacement). Preliminary results indicate coordination of the intonation gesture and the pointing gesture.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\JBUHQ7XB\\Krivokapic et al. - 2016 - Speech and manual gesture coordination in a pointi.pdf},
  keywords = {Audio Media,Displacement mapping,Gesture recognition,Onset (audio),Pointing device,Psychologic Displacement,recurrent childhood visual pathway glioma,Tract (literature),vowels}
}

@inproceedings{kucherenkoAnalyzingInputOutput2019,
  title = {Analyzing {{Input}} and {{Output Representations}} for {{Speech}}-{{Driven Gesture Generation}}},
  booktitle = {Proceedings of the 19th {{ACM International Conference}} on {{Intelligent Virtual Agents}}  - {{IVA}} '19},
  author = {Kucherenko, Taras and Hasegawa, Dai and Henter, Gustav Eje and Kaneko, Naoshi and Kjellström, Hedvig},
  date = {2019},
  pages = {97--104},
  publisher = {{ACM Press}},
  location = {{Paris, France}},
  doi = {10.1145/3308532.3329472},
  url = {http://dl.acm.org/citation.cfm?doid=3308532.3329472},
  urldate = {2019-10-01},
  eventtitle = {The 19th {{ACM International Conference}}},
  isbn = {978-1-4503-6672-4},
  langid = {english}
}

@incollection{kuglerConceptCoordinativeStructures1980,
  title = {On the Concept of Coordinative Structures as Dissipative Structures: {{I}}. {{Theoretical}} Lines of Convergence},
  shorttitle = {1 {{On}} the {{Concept}} of {{Coordinative Structures}} as {{Dissipative Structures}}},
  booktitle = {Advances in {{Psychology}}},
  author = {Kugler, P. N. and Scott Kelso, J. A. and Turvey, M. T.},
  editor = {Stelmach, George E. and Requin, Jean},
  date = {1980-01-01},
  volume = {1},
  pages = {3--47},
  publisher = {{North-Holland}},
  doi = {10.1016/S0166-4115(08)61936-6},
  url = {http://www.sciencedirect.com/science/article/pii/S0166411508619366},
  urldate = {2019-08-08},
  abstract = {A model construct for coordination and control is pursued according to three related guidelines: (1) that it directly address Bernstein's problem of how to explain the regulation of the many biokinematic degrees of freedom with minimal recourse to an “intelligent regulator”; (2) that it be miserly on the number of explanatory principles, sui generis; and (3) that it be consistent with established strictures of non-equilibrium thermodynamics, that is, physical principles that inform biological design. Argument is given that a group of muscles constrained to act as a unit, a coordinative structure, is a member of the class of thermodynamic engines qua dissipative structures and that this membership gives a principled basis for understanding the characteristics of coordination and control.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LEMVCFRN\\S0166411508619366.html},
  series = {Tutorials in {{Motor Behavior}}}
}

@incollection{ladefagodLinguisticAspectsRespiratory1968,
  title = {Linguistic Aspects of Respiratory Phenomena},
  booktitle = {Sound {{Production}} in {{Man}} ({{Ed}}. {{A}}. Bouhuys)},
  author = {Ladefagod, P.},
  date = {1968},
  pages = {141--151},
  publisher = {{New York Academy of Sciences}},
  location = {{New York}}
}

@article{lancasterRespiratoryMuscleActivity1995,
  title = {Respiratory Muscle Activity in Relation to Vocalization in Flying Bats.},
  author = {Lancaster, W. C. and Henson, O. W. and Keating, A. W.},
  date = {1995-01-01},
  journaltitle = {Journal of Experimental Biology},
  volume = {198},
  pages = {175--191},
  issn = {0022-0949, 1477-9145},
  url = {https://jeb.biologists.org/content/198/1/175},
  urldate = {2020-01-27},
  abstract = {Skip to Next Section
The structure of the thoracic and abdominal walls of Pteronotus parnellii (Microchiroptera: Mormoopidae) was described with respect to their function in respiration and vocalization. We monitored electromyographic activity of respiratory and flight muscles in relation to echolocative vocalization. In flight, signals were telemetered with a small FM transmitter modified to summate the low-frequency myopotentials with biosonar signals from a ceramic-crystal microphone. Recordings were also made from the same bats confined to a small cage. Vocalizations were used as the parameter by which all muscle activities were correlated. A discrete burst of activity in the lateral abdominal wall muscles accompanied each vocalization. Diaphragmatic myopotentials occurred between groups of calls and did not coincide with activity of the abdominal wall or with vocalizations. Flight muscles were not active in resting bats. During flight, vocalizations and the abdominal muscle activity that accompanied them coincided with myopotentials of the pectoralis and serratus ventralis muscles. We propose that contractions of the lateral abdominal wall provide the primary power for the production of intense biosonar vocalization in flying and in stationary bats. In flight, synchronization of vocalization with activity of the pectoralis and serratus ventralis jointly contribute to the pressurization of the thoraco-abdominal cavity. This utilization of pressure that is normally generated in flight facilitates respiration and allows for the production of intense vocalizations with little additional energetic expenditure.},
  eprint = {7891034},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\S6EMXYR3\\Lancaster et al. - 1995 - Respiratory muscle activity in relation to vocaliz.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\7Q8YBGKR\\175.html},
  langid = {english},
  number = {1}
}

@article{larssonBipedalStepsDevelopment2019,
  title = {Bipedal {{Steps}} in the {{Development}} of {{Rhythmic Behavior}} in {{Humans}}},
  author = {Larsson, Matz and Richter, Joachim and Ravignani, Andrea},
  date = {2019-01-01},
  journaltitle = {Music \& Science},
  shortjournal = {Music \& Science},
  volume = {2},
  pages = {2059204319892617},
  issn = {2059-2043},
  doi = {10.1177/2059204319892617},
  url = {https://doi.org/10.1177/2059204319892617},
  urldate = {2020-01-03},
  abstract = {We contrast two related hypotheses of the evolution of dance: H1: Maternal bipedal walking influenced the fetal experience of sound and associated movement patterns; H2: The human transition to bipedal gait produced more isochronous/predictable locomotion sound resulting in early music-like behavior associated with the acoustic advantages conferred by moving bipedally in pace. The cadence of walking is around 120 beats per minute, similar to the tempo of dance and music. Human walking displays long-term constancies. Dyads often subconsciously synchronize steps. The major amplitude component of the step is a distinctly produced beat. Human locomotion influences, and interacts with, emotions, and passive listening to music activates brain motor areas. Across dance-genres the footwork is most often performed in time to the musical beat. Brain development is largely shaped by early sensory experience, with hearing developed from week 18 of gestation. Newborns reacts to sounds, melodies, and rhythmic poems to which they have been exposed in utero. If the sound and vibrations produced by footfalls of a walking mother are transmitted to the fetus in coordination with the cadence of the motion, a connection between isochronous sound and rhythmical movement may be developed. Rhythmical sounds of the human mother locomotion differ substantially from that of nonhuman primates, while the maternal heartbeat heard is likely to have a similar isochronous character across primates, suggesting a relatively more influential role of footfall in the development of rhythmic/musical abilities in humans. Associations of gait, music, and dance are numerous. The apparent absence of musical and rhythmic abilities in nonhuman primates, which display little bipedal locomotion, corroborates that bipedal gait may be linked to the development of rhythmic abilities in humans. Bipedal stimuli in utero may primarily boost the ontogenetic development. The acoustical advantage hypothesis proposes a mechanism in the phylogenetic development.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\K8Z2Y2G9\\Larsson et al. - 2019 - Bipedal Steps in the Development of Rhythmic Behav.pdf},
  keywords = {Bipedal gait,evolution of dance,hominids,intra-uterine development,music},
  langid = {english}
}

@article{leavellFirefliesThwartBat2018,
  title = {Fireflies Thwart Bat Attack with Multisensory Warnings},
  author = {Leavell, Brian C. and Rubin, Juliette J. and McClure, Christopher J. W. and Miner, Krystie A. and Branham, Marc A. and Barber, Jesse R.},
  date = {2018-08-01},
  journaltitle = {Science Advances},
  volume = {4},
  pages = {eaat6601},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aat6601},
  url = {https://advances.sciencemag.org/content/4/8/eaat6601},
  urldate = {2020-02-28},
  abstract = {Many defended animals prevent attacks by displaying warning signals that are highly conspicuous to their predators. We hypothesized that bioluminescing fireflies, widely known for their vibrant courtship signals, also advertise their noxiousness to echolocating bats. To test this postulate, we pit naïve big brown bats (Eptesicus fuscus) against chemically defended fireflies (Photinus pyralis) to examine whether and how these beetles transmit salient warnings to bats. We demonstrate that these nocturnal predators learn to avoid noxious fireflies using either vision or echolocation and that bats learn faster when integrating information from both sensory streams—providing fundamental evidence that multisensory integration increases the efficacy of warning signals in a natural predator-prey system. Our findings add support for a warning signal origin of firefly bioluminescence and suggest that bat predation may have driven evolution of firefly bioluminescence.
Naïve bats learn to avoid noxious fireflies fastest when integrating bioluminescent and echo-derived warnings.
Naïve bats learn to avoid noxious fireflies fastest when integrating bioluminescent and echo-derived warnings.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\K285A8AT\\Leavell et al. - 2018 - Fireflies thwart bat attack with multisensory warn.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\EA65PT2Y\\eaat6601.html},
  langid = {english},
  number = {8}
}

@article{lenthPackageLsmeans2017,
  title = {Package ‘lsmeans'},
  author = {Lenth, R. and Lenth, M. R.},
  date = {2017},
  journaltitle = {The American Statistician},
  volume = {34},
  pages = {216--221},
  number = {4}
}

@article{leonardTemporalRelationBeat2011,
  title = {The Temporal Relation between Beat Gestures and Speech},
  author = {Leonard, Thomas and Cummins, Fred},
  date = {2011-12-01},
  journaltitle = {Language and Cognitive Processes},
  volume = {26},
  pages = {1457--1471},
  issn = {0169-0965},
  doi = {10.1080/01690965.2010.500218},
  url = {https://doi.org/10.1080/01690965.2010.500218},
  urldate = {2019-04-23},
  abstract = {The temporal relation between beat gestures and accompanying speech is examined in two experiments. In the first, we find that subjects are very quick to spot altered timing between gesture and speech if the gesture is later than normal, but are considerably less sensitive to alterations that result in an earlier gesture. This suggests an asymmetry in the expectation on the part of listeners/watchers and raises immediate questions about which elements within the speech are being perceived as linked to which elements in the gestural series. We therefore examine the variability between several kinematic landmarks in a beat gesture, and three potential anchor points in the accompanying speech. We find the least variable relationship obtains between the point of maximum extension of the gesture and the accompanying pitch accent. Together, these findings contribute to our understanding of both the production and perception of beat gestures along with speech, and support an account of speech communication as a strongly embodied activity.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\RKA5ALLQ\\Leonard and Cummins - 2011 - The temporal relation between beat gestures and sp.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\6TTZCMG9\\01690965.2010.html},
  keywords = {Coordination,Embodiment,Gesture},
  number = {10}
}

@article{leroiRevolutions2020,
  title = {On Revolutions},
  author = {Leroi, Armand M. and Lambert, Ben and Mauch, Matthias and Papadopoulou, Marina and Ananiadou, Sophia and Lindberg, Staffan I. and Lindenfors, Patrik},
  date = {2020-12},
  journaltitle = {Palgrave Communications},
  volume = {6},
  issn = {2055-1045},
  doi = {10.1057/s41599-019-0371-1},
  url = {http://www.nature.com/articles/s41599-019-0371-1},
  urldate = {2020-02-21},
  abstract = {Sometimes the normal course of events is disrupted by a particularly swift and profound change. Historians have often referred to such changes as “revolutions”, and, though they have identified many of them, they have rarely supported their claims with statistical evidence. Here, we present a method to identify revolutions based on a measure of multivariate rate of change called Foote novelty. We define revolutions as those periods of time when the value of this measure is, by a non-parametric test, shown to significantly exceed the background rate. Our method also identifies conservative periods when the rate of change is unusually low. We apply it to several quantitative data sets that capture long-term political, social and cultural changes and, in some of them, identify revolutions — both well known and not. Our method is general and can be applied to any phenomenon captured by multivariate time series data of sufficient quality.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YASBXFRN\\Leroi et al. - 2020 - On revolutions.pdf},
  langid = {english},
  number = {1}
}

@article{levinPuttingShoulderWheel1997,
  title = {Putting the Shoulder to the Wheel: A New Biomechanical Model for the Shoulder Girdle},
  shorttitle = {Putting the Shoulder to the Wheel},
  author = {Levin, S. M.},
  date = {1997},
  journaltitle = {Biomedical Sciences Instrumentation},
  shortjournal = {Biomed Sci Instrum},
  volume = {33},
  pages = {412--417},
  issn = {0067-8856},
  abstract = {The least successfully modeled joint complex has been the shoulder. In multi-segmented mathematical shoulder models rigid beams (the bones) act as a series of columns or levers to transmit forces or loads to the axial skeleton. Forces passing through the almost frictionless joints must, somehow, always be directed perfectly perpendicular to the joints as only loads directed at right angles to the surfaces could transfer across frictionless joints. Loads transmitted to the axial skeleton would have to pass through the moving ribs or the weak jointed clavicle and then through the ribs. A new model of the shoulder girdle, based on the tension icosahedron described by Buckminster Fuller, is proposed that permits the compression loads passing through the arm and shoulder to be transferred to the axial skeleton through its soft tissues. In this model the scapula 'floats' in the tension network of shoulder girdle muscles just as the hub of the wire wheel is suspended in its tension network of spokes. With this construct inefficient beams and levers are eliminated. A more energy efficient, load distributing, integrated, hierarchical system is created.},
  eprint = {9731395},
  eprinttype = {pmid},
  keywords = {Biomechanical Phenomena,Humans,Models; Biological,Models; Structural,Shoulder,Shoulder Joint},
  langid = {english}
}

@article{levinsonOriginHumanMultimodal2014,
  title = {The Origin of Human Multi-Modal Communication},
  author = {Levinson, Stephen C. and Holler, Judith},
  date = {2014-09-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0302},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4123681/},
  urldate = {2020-01-26},
  abstract = {One reason for the apparent gulf between animal and human communication systems is that the focus has been on the presence or the absence of language as a complex expressive system built on speech. But language normally occurs embedded within an interactional exchange of multi-modal signals. If this larger perspective takes central focus, then it becomes apparent that human communication has a layered structure, where the layers may be plausibly assigned different phylogenetic and evolutionary origins—especially in the light of recent thoughts on the emergence of voluntary breathing and spoken language. This perspective helps us to appreciate the different roles that the different modalities play in human communication, as well as how they function as one integrated system despite their different roles and origins. It also offers possibilities for reconciling the ‘gesture-first hypothesis’ with that of gesture and speech having evolved together, hand in hand—or hand in mouth, rather—as one system.},
  eprint = {25092670},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\2UD3JMPX\\Levinson and Holler - 2014 - The origin of human multi-modal communication.pdf},
  number = {1651},
  pmcid = {PMC4123681}
}

@incollection{levinTensegrityNewBiomechanics2006,
  title = {Tensegrity: {{The}} New Biomechanics},
  booktitle = {Textbook of Muscularskeletal Medicine},
  author = {Levin, S. M.},
  editor = {Hutson, M. and Ellis, R.},
  date = {2006},
  pages = {69--80},
  publisher = {{Oxford University Press}},
  location = {{Oxford, England}}
}

@inproceedings{linHowHitThat2020,
  title = {How to Hit That Beat: {{Testing}} Acoustic Anchors of Rhythmic Movement with Speech},
  shorttitle = {How to Hit That Beat},
  booktitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  author = {Lin, Chia-Yuan and Rathcke, Tamara},
  date = {2020-05-25},
  pages = {1--5},
  publisher = {{ISCA}},
  doi = {10.21437/SpeechProsody.2020-1},
  url = {http://www.isca-speech.org/archive/SpeechProsody_2020/abstracts/31.html},
  urldate = {2020-05-26},
  abstract = {Sensorimotor synchronisation with metronome and music have been extensively studied, while synchronisation with speech is still relatively poorly understood. The present study looks into the question how to define the best anchor of synchronised movement (finger tapping) in speech, and compares manually identified vowel onsets with four acoustic landmarks that were derived by different signal processing algorithms. Participants listened to repetitions of natural English sentences and were instructed to tap in synchrony with what they perceived to be the sentence beat. The time course of the sentences was tagged for a number of rhythmically relevant events, including vowel onsets, fastest energy increase (maxD), a combination of high local pitch and periodic energy (PPP), and the largest amplitude of intersyllabic and interstress timescales (IMF1 and IMF2). Vowel onsets and maxD showed consistent tapping patterns, while other landmarks performed worse than vowel onsets. These findings suggest that local energy changes shape sensorimotor synchronisation with speech and that energy contours might serve as anchors of rhythmic attention in spoken language.},
  eventtitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9MFVVMQC\\Lin and Rathcke - 2020 - How to hit that beat Testing acoustic anchors of .pdf},
  langid = {english}
}

@article{loehrTemporalStructuralPragmatic2012,
  title = {Temporal, Structural, and Pragmatic Synchrony between Intonation and Gesture},
  author = {Loehr, Daniel P.},
  date = {2012},
  journaltitle = {Laboratory Phonology},
  volume = {3},
  pages = {71--89},
  issn = {1868-6346},
  doi = {10.1515/lp-2012-0006},
  url = {https://www.degruyter.com/view/j/lp.2012.3.issue-1/lp-2012-0006/lp-2012-0006.xml},
  urldate = {2019-04-23},
  abstract = {This paper explores the interaction between intonation and gesture, noting temporal, structural, and pragmatic synchrony. Videotapes of subjects conversing freely were annotated for intonation according to ToBI (Beckman and Elam 1997), and for gesture according to Kendon (1980) and McNeill (1992). The time-stamped annotations were analyzed statistically, as well as visually in the Anvil tool (Kipp 2001), which allows time-aligned viewing of videos with their annotations. Alignments were investigated between three levels of intonational units and four levels of gestural units. The intonational units were, from smallest to largest, pitch accents, intermediate phrases, and intonational phrases. The gestural units, again from smallest to largest, were apices of strokes, gesture phases, gesture phrases, and gesture units. Of these possible combinations, two pairs aligned. Apices clearly aligned with pitch accents, and gesture phrases tended to align with intermediate phrases. The existence of intermediate phrases in English has been the subject of some debate (Ladd 2008), and this paper suggests that a probable gestural correlate to intermediate phrases provides independent evidence for their existence. In addition, intonation and gesture were found to perform simultaneous complementary pragmatic functions. This temporal, structural, and pragmatic synchrony between the two channels reinforces the claim that speech and gesture are two surface forms of the same underlying and emerging cognitive content.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FKELJLTZ\\Loehr - 2012 - Temporal, structural, and pragmatic synchrony betw.pdf},
  number = {1}
}

@article{lorasTimingContinuousDiscontinuous2012,
  title = {Timing Continuous or Discontinuous Movements across Effectors Specified by Different Pacing Modalities and Intervals},
  author = {Lorås, H. and Sigmundsson, H. and Talcott, J. B. and Öhberg, F. and Stensdotter, A. K.},
  date = {2012-08-01},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {220},
  pages = {335--347},
  issn = {1432-1106},
  doi = {10.1007/s00221-012-3142-4},
  url = {https://doi.org/10.1007/s00221-012-3142-4},
  urldate = {2020-05-19},
  abstract = {Sensorimotor synchronization is hypothesized to arise through two different processes, associated with continuous or discontinuous rhythmic movements. This study investigated synchronization of continuous and discontinuous movements to different pacing signals (auditory or visual), pacing interval (500, 650, 800, 950~ms) and across effectors (non-dominant vs. non-dominant hand). The results showed that mean and variability of asynchronization errors were consistently smaller for discontinuous movements compared to continuous movements. Furthermore, both movement types were timed more accurately with auditory pacing compared to visual pacing and were more accurate with the dominant hand. Shortening the pacing interval also improved sensorimotor synchronization accuracy in both continuous and discontinuous movements. These results show the dependency of temporal control of movements on the nature of the motor task, the type and rate of extrinsic sensory information as well as the efficiency of the motor actuators for sensory integration.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IZFZPSYK\\Lorås et al. - 2012 - Timing continuous or discontinuous movements acros.pdf},
  langid = {english},
  number = {3}
}

@article{lou-magnusonSocialNetworkLimits2018,
  title = {Social {{Network Limits Language Complexity}}},
  author = {Lou‐Magnuson, Matthew and Onnis, Luca},
  date = {2018},
  journaltitle = {Cognitive Science},
  volume = {42},
  pages = {2790--2817},
  issn = {1551-6709},
  doi = {10.1111/cogs.12683},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12683},
  urldate = {2020-03-06},
  abstract = {Natural languages vary widely in the degree to which they make use of nested compositional structure in their grammars. It has long been noted by linguists that the languages historically spoken in small communities develop much deeper levels of compositional embedding than those spoken by larger groups. Recently, this observation has been confirmed by a robust statistical analysis of the World Atlas of Language Structures. In order to examine this connection mechanistically, we propose an agent-based model that accounts for key cultural evolutionary features of language transfer and language change. We identify transitivity as a physical parameter of social networks critical for the evolution of compositional structure and the hierarchical patterning of scale-free distributions as inhibitory.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QKZ2DILP\\Lou‐Magnuson and Onnis - 2018 - Social Network Limits Language Complexity.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ZDB2VL9A\\cogs.html},
  keywords = {Agent-based model,Grammaticalization,Language change,Language complexity,Language evolution,Social network},
  langid = {english},
  number = {8}
}

@article{lugoShoulderBiomechanics2008,
  title = {Shoulder Biomechanics},
  author = {Lugo, Roberto and Kung, Peter and Ma, C. Benjamin},
  date = {2008-10-01},
  journaltitle = {European Journal of Radiology},
  shortjournal = {European Journal of Radiology},
  volume = {68},
  pages = {16--24},
  issn = {0720-048X},
  doi = {10.1016/j.ejrad.2008.02.051},
  url = {http://www.sciencedirect.com/science/article/pii/S0720048X08001277},
  urldate = {2020-04-21},
  abstract = {The biomechanics of the glenohumeral joint depend on the interaction of both static and dynamic-stabilizing structures. Static stabilizers include the bony anatomy, negative intra-articular pressure, the glenoid labrum, and the glenohumeral ligaments along with the joint capsule. The dynamic-stabilizing structures include the rotator cuff muscles and the other muscular structures surrounding the shoulder joint. The combined effect of these stabilizers is to support the multiple degrees of motion within the glenohumeral joint. The goal of this article is to review how these structures interact to provide optimal stability and how failure of some of these mechanisms can lead to shoulder joint pathology.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BD3KB837\\Lugo et al. - 2008 - Shoulder biomechanics.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\MJ3CNYRB\\S0720048X08001277.html},
  keywords = {Biomechanics,Glenohumeral joint,Shoulder},
  langid = {english},
  number = {1},
  series = {Shoulder {{Imaging}}}
}

@article{lumExtractingInsightsShape2013,
  title = {Extracting Insights from the Shape of Complex Data Using Topology},
  author = {Lum, P. Y. and Singh, G. and Lehman, A. and Ishkanov, T. and Vejdemo-Johansson, M. and Alagappan, M. and Carlsson, J. and Carlsson, G.},
  date = {2013-02-07},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {3},
  issn = {2045-2322},
  doi = {10.1038/srep01236},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3566620/},
  urldate = {2020-03-11},
  abstract = {This paper applies topological methods to study complex high dimensional data sets by extracting shapes (patterns) and obtaining insights about them. Our method combines the best features of existing standard methodologies such as principal component and cluster analyses to provide a geometric representation of complex data sets. Through this hybrid method, we often find subgroups in data sets that traditional methodologies fail to find. Our method also permits the analysis of individual data sets as well as the analysis of relationships between related data sets. We illustrate the use of our method by applying it to three very different kinds of data, namely gene expression from breast tumors, voting data from the United States House of Representatives and player performance data from the NBA, in each case finding stratifications of the data which are more refined than those produced by standard methods.},
  eprint = {23393618},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9FGPYUKT\\Lum et al. - 2013 - Extracting insights from the shape of complex data.pdf},
  pmcid = {PMC3566620}
}

@article{luptonMotorLearningSign1990,
  title = {Motor {{Learning}} in {{Sign Language Students}}},
  author = {Lupton, Linda K. and Zelaznik, Howard N.},
  date = {1990},
  journaltitle = {Sign Language Studies},
  volume = {1067},
  pages = {153--174},
  issn = {1533-6263},
  doi = {10.1353/sls.1990.0020},
  url = {http://muse.jhu.edu/content/crossref/journals/sign_language_studies/v1067/67.lupton.html},
  urldate = {2020-03-09},
  abstract = {We examined the changes in movement trajectories of two initially naive students from near the beginning until the end of an introductory course in American Sign Language. The movement patterns increased in speed, symmetry, replicability, and grew more constrained in movement amplitude as the semester progressed. Analysis of phase portraits (graphs of displacement versus velocity) revealed increasing limit-cycle behavior for complicated two-handed signs over the testing sessions. One-handed signs, however, exhibited limit-cycle behavior from the first session.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FFYANREN\\Lupton and Zelaznik - 1990 - Motor Learning in Sign Language Students.pdf},
  langid = {english},
  number = {1}
}

@article{lupyanLanguageStructurePartly2010,
  title = {Language {{Structure Is Partly Determined}} by {{Social Structure}}},
  author = {Lupyan, G. and Dale, R.},
  date = {2010-01-20},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {5},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0008559},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2798932/},
  urldate = {2020-03-18},
  abstract = {Background
Languages differ greatly both in their syntactic and morphological systems and in the social environments in which they exist. We challenge the view that language grammars are unrelated to social environments in which they are learned and used.

Methodology/Principal Findings
We conducted a statistical analysis of {$>$}2,000 languages using a combination of demographic sources and the World Atlas of Language Structures— a database of structural language properties. We found strong relationships between linguistic factors related to morphological complexity, and demographic/socio-historical factors such as the number of language users, geographic spread, and degree of language contact. The analyses suggest that languages spoken by large groups have simpler inflectional morphology than languages spoken by smaller groups as measured on a variety of factors such as case systems and complexity of conjugations. Additionally, languages spoken by large groups are much more likely to use lexical strategies in place of inflectional morphology to encode evidentiality, negation, aspect, and possession. Our findings indicate that just as biological organisms are shaped by ecological niches, language structures appear to adapt to the environment (niche) in which they are being learned and used. As adults learn a language, features that are difficult for them to acquire, are less likely to be passed on to subsequent learners. Languages used for communication in large groups that include adult learners appear to have been subjected to such selection. Conversely, the morphological complexity common to languages used in small groups increases redundancy which may facilitate language learning by infants.

Conclusions/Significance
We hypothesize that language structures are subjected to different evolutionary pressures in different social environments. Just as biological organisms are shaped by ecological niches, language structures appear to adapt to the environment (niche) in which they are being learned and used. The proposed Linguistic Niche Hypothesis has implications for answering the broad question of why languages differ in the way they do and makes empirical predictions regarding language acquisition capacities of children versus adults.},
  eprint = {20098492},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\I5NTQZLW\\Lupyan and Dale - 2010 - Language Structure Is Partly Determined by Social .pdf},
  number = {1},
  pmcid = {PMC2798932}
}

@article{maclarnonEvolutionHumanSpeech1999,
  title = {The Evolution of Human Speech: The Role of Enhanced Breathing Control},
  shorttitle = {The Evolution of Human Speech},
  author = {MacLarnon, A. M. and Hewitt, G. P.},
  date = {1999-07},
  journaltitle = {American Journal of Physical Anthropology},
  shortjournal = {Am. J. Phys. Anthropol.},
  volume = {109},
  pages = {341--363},
  issn = {0002-9483},
  doi = {10.1002/(SICI)1096-8644(199907)109:3<341::AID-AJPA5>3.0.CO;2-2},
  abstract = {Many cognitive and physical features must have undergone change for the evolution of fully modern human language. One neglected aspect is the evolution of increased breathing control. Evidence presented herein shows that modern humans and Neanderthals have an expanded thoracic vertebral canal compared with australopithecines and Homo ergaster, who had canals of the same relative size as extant nonhuman primates. Based on previously published analyses, these results demonstrate that there was an increase in thoracic innervation during human evolution. Possible explanations for this increase include postural control for bipedalism, increased difficulty of parturition, respiration for endurance running, an aquatic phase, and choking avoidance. These can all be ruled out, either because of their evolutionary timing, or because they are insufficiently demanding neurologically. The remaining possible functional cause is increased control of breathing for speech. The main muscles involved in the fine control of human speech breathing are the intercostals and a set of abdominal muscles which are all thoracically innervated. Modifications to quiet breathing are essential for modern human speech, enabling the production of long phrases on single expirations punctuated with quick inspirations at meaningful linguistic breaks. Other linguistically important features affected by variation in subglottal air pressure include emphasis of particular sound units, and control of pitch and intonation. Subtle, complex muscle movements, integrated with cognitive factors, are involved. The vocalizations of nonhuman primates involve markedly less respiratory control. Without sophisticated breath control, early hominids would only have been capable of short, unmodulated utterances, like those of extant nonhuman primates. Fine respiratory control, a necessary component for fully modern language, evolved sometime between 1.6 Mya and 100,000 ya.},
  eprint = {10407464},
  eprinttype = {pmid},
  keywords = {Animals,Biological Evolution,Fossils,Hominidae,Humans,Language,Respiration,Speech,Thorax},
  langid = {english},
  number = {3}
}

@article{macneilageFrameContentTheory1998,
  title = {The Frame/Content Theory of Evolution of Speech Production},
  author = {MacNeilage, Peter F.},
  date = {1998-08},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {21},
  pages = {499--511},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X98001265},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/framecontent-theory-of-evolution-of-speech-production/4863C373F8A52560D49DF516AD20FA17},
  urldate = {2020-04-22},
  abstract = {The species-specific organizational property of speech is a
continual mouth open-close alternation, the two phases of which are
subject to continual articulatory modulation. The cycle constitutes
the syllable, and the open and closed phases are segments –
vowels and consonants, respectively. The fact that segmental serial
ordering errors in normal adults obey syllable structure constraints
suggests that syllabic “frames” and segmental
“content” elements are separately controlled in the
speech production process. The frames may derive from cycles of
mandibular oscillation present in humans from babbling onset, which
are responsible for the open-close alternation. These communication-
related frames perhaps first evolved when the ingestion-related
cyclicities of mandibular oscillation (associated with mastication
[chewing] sucking and licking) took on communicative significance
as lipsmacks, tonguesmacks, and teeth chatters – displays that
are prominent in many nonhuman primates. The new role of Broca's
area and its surround in human vocal communication may have derived
from its evolutionary history as the main cortical center for the
control of ingestive processes. The frame and content components
of speech may have subsequently evolved separate realizations
within two general purpose primate motor control systems: (1) a
motivation-related medial “intrinsic” system, including
anterior cingulate cortex and the supplementary motor area, for
self-generated behavior, formerly responsible for ancestral
vocalization control and now also responsible for frames, and
(2) a lateral “extrinsic” system, including Broca's
area and surround, and Wernicke's area, specialized for response
to external input (and therefore the emergent vocal learning capacity)
and more responsible for content.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\B3G9HCHA\\MacNeilage - 1998 - The framecontent theory of evolution of speech pr.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\KNPGPLFE\\4863C373F8A52560D49DF516AD20FA17.html},
  keywords = {Broca's aphasia,chewing,consonants,lipsmacks,speech evolution syllables,supplementary motor area,vowels,Wernicke's aphasia},
  langid = {english},
  number = {4}
}

@article{macuchsilvaMultimodalityOriginNovel,
  title = {Multimodality and the Origin of a Novel Communication System in Face-to-Face Interaction},
  author = {Macuch Silva, Vinicius and Holler, Judith and Ozyurek, Asli and Roberts, Seán G.},
  journaltitle = {Royal Society Open Science},
  shortjournal = {Royal Society Open Science},
  volume = {7},
  pages = {182056},
  doi = {10.1098/rsos.182056},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.182056},
  urldate = {2020-01-28},
  abstract = {Face-to-face communication is multimodal at its core: it consists of a combination of vocal and visual signalling. However, current evidence suggests that, in the absence of an established communication system, visual signalling, especially in the form of visible gesture, is a more powerful form of communication than vocalization and therefore likely to have played a primary role in the emergence of human language. This argument is based on experimental evidence of how vocal and visual modalities (i.e. gesture) are employed to communicate about familiar concepts when participants cannot use their existing languages. To investigate this further, we introduce an experiment where pairs of participants performed a referential communication task in which they described unfamiliar stimuli in order to reduce reliance on conventional signals. Visual and auditory stimuli were described in three conditions: using visible gestures only, using non-linguistic vocalizations only and given the option to use both (multimodal communication). The results suggest that even in the absence of conventional signals, gesture is a more powerful mode of communication compared with vocalization, but that there are also advantages to multimodality compared to using gesture alone. Participants with an option to produce multimodal signals had comparable accuracy to those using only gesture, but gained an efficiency advantage. The analysis of the interactions between participants showed that interactants developed novel communication systems for unfamiliar stimuli by deploying different modalities flexibly to suit their needs and by taking advantage of multimodality when required.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QQPP464Y\\Macuch Silva et al. - Multimodality and the origin of a novel communicat.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AUYXMJ53\\rsos.html},
  number = {1}
}

@article{madisonCorrelationsIntelligenceComponents2009,
  title = {Correlations between Intelligence and Components of Serial Timing Variability},
  author = {Madison, Guy and Forsman, Lea and Blom, Örjan and Karabanov, Anke and Ullén, Fredrik},
  date = {2009-01-01},
  journaltitle = {Intelligence},
  shortjournal = {Intelligence},
  volume = {37},
  pages = {68--75},
  issn = {0160-2896},
  doi = {10.1016/j.intell.2008.07.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0160289608000937},
  urldate = {2020-05-19},
  abstract = {Psychometric intelligence correlates with reaction time in elementary cognitive tasks, as well as with performance in time discrimination and judgment tasks. It has remained unclear, however, to what extent these correlations are due to top–down mechanisms, such as attention, and bottom–up mechanisms, i.e. basic neural properties that influence both temporal accuracy and cognitive processes. Here, we assessed correlations between intelligence (Raven SPM Plus) and performance in isochronous serial interval production, a simple, automatic timing task where participants first make movements in synchrony with an isochronous sequence of sounds and then continue with self-paced production to produce a sequence of intervals with the same inter-onset interval (IOI). The target IOI varied across trials. A number of different measures of timing variability were considered, all negatively correlated with intelligence. Across all stimulus IOIs, local interval-to-interval variability correlated more strongly with intelligence than drift, i.e. gradual changes in response IOI. The strongest correlations with intelligence were found for IOIs between 400 and 900~ms, rather than above 1~s, which is typically considered a lower limit for cognitive timing. Furthermore, poor trials, i.e. trials arguably most affected by lapses in attention, did not predict intelligence better than the most accurate trials. We discuss these results in relation to the human timing literature, and argue that they support a bottom–up model of the relation between temporal variability of neural activity and intelligence.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\GA35RNT3\\Madison et al. - 2009 - Correlations between intelligence and components o.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\2GXFAP3Q\\S0160289608000937.html},
  keywords = {Duration-specificity,Intelligence,Interval production,Isochronous serial interval production,Neural mechanisms,Neural noise,Noise,Ravens progressive matrices,Tapping,Timing},
  langid = {english},
  number = {1}
}

@article{magnussonBodyLanguageAdults2008,
  title = {The {{Body Language}} of {{Adults Who Are Blind}}},
  author = {Magnusson, Anna-Karin and Karlsson, Gunnar},
  date = {2008-06-01},
  journaltitle = {Scandinavian Journal of Disability Research},
  volume = {10},
  pages = {71--89},
  issn = {1501-7419},
  doi = {10.1080/15017410701685927},
  url = {https://doi.org/10.1080/15017410701685927},
  urldate = {2019-11-30},
  abstract = {The body expressions of adults who are blind have been relatively unexplored. The aim of this study was therefore to deepen the understanding of different forms of body expression, or “body language”, in adults who are blind. More specifically the study aimed at answering the following questions: What forms of body expression do adults who are blind display? What can the conditions for some different forms of body expression be? What importance can individual, social and cultural factors have for different forms of body expression? Data consisted of video-taped interviews with five congenitally blind, two adventitiously blind and two sighted individuals. The data were analysed in a hermeneutical and phenomenological sense. The results consisted of a typology of 19 different forms of body expression. All in all, we found that the congenitally blind participants expressed themselves mainly in a functional and concrete manner. They also seemed to have limited experiences with abstract, symbolic body expressions. The conditions and the importance of different factors for different body expressions are discussed.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\S93QF2EA\\Magnusson and Karlsson - 2008 - The Body Language of Adults Who Are Blind.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\JR8FZJIT\\15017410701685927.html},
  number = {2}
}

@article{magnussonNonverbalConversationRegulatingSignals2006,
  title = {Nonverbal {{Conversation}}-{{Regulating Signals}} of the {{Blind Adult}}},
  author = {Magnusson, Anna-Karin},
  date = {2006-12-01},
  journaltitle = {Communication Studies},
  volume = {57},
  pages = {421--433},
  issn = {1051-0974},
  doi = {10.1080/10510970600946004},
  url = {https://doi.org/10.1080/10510970600946004},
  urldate = {2019-11-30},
  abstract = {The purpose of this study is to describe nonverbal conversation-regulating signals among the blind adult and to describe how these signals are manifested through body movements/positions and paralinguistic sounds/silences. Data consist of videotaped conversations between blind-blind pairs and blind-sighted pairs. The data are analyzed in a hermeneutical/phenomenological sense. The analysis is also inspired by Conversational Analysis. The analysis resulted in descriptions of four major signal types, i.e. the listener's back-channeling signals, the speaker's turn-holding signals, the listener's starting signals, and the speaker's turn-yielding signals. One central conclusion is that earlier experience of vision does not seem to be a prerequisite for showing a variety of different types of nonverbal conversation-regulating signals in a systematic and distinct manner.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SDPMBNKS\\Magnusson - 2006 - Nonverbal Conversation-Regulating Signals of the B.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\UGW4LCVA\\10510970600946004.html},
  keywords = {Blindness,Conversation analysis,Conversation-regulating signals,Hermeneutics,Nonverbal,Phenomenology},
  number = {4}
}

@article{magnussonNonverbalConversationRegulatingSignals2006a,
  title = {Nonverbal {{Conversation}}-{{Regulating Signals}} of the {{Blind Adult}}},
  author = {Magnusson, Anna-Karin},
  date = {2006-12-01},
  journaltitle = {Communication Studies},
  volume = {57},
  pages = {421--433},
  issn = {1051-0974},
  doi = {10.1080/10510970600946004},
  url = {https://doi.org/10.1080/10510970600946004},
  urldate = {2019-11-30},
  abstract = {The purpose of this study is to describe nonverbal conversation-regulating signals among the blind adult and to describe how these signals are manifested through body movements/positions and paralinguistic sounds/silences. Data consist of videotaped conversations between blind-blind pairs and blind-sighted pairs. The data are analyzed in a hermeneutical/phenomenological sense. The analysis is also inspired by Conversational Analysis. The analysis resulted in descriptions of four major signal types, i.e. the listener's back-channeling signals, the speaker's turn-holding signals, the listener's starting signals, and the speaker's turn-yielding signals. One central conclusion is that earlier experience of vision does not seem to be a prerequisite for showing a variety of different types of nonverbal conversation-regulating signals in a systematic and distinct manner.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\UF965FMB\\Magnusson - 2006 - Nonverbal Conversation-Regulating Signals of the B.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\43GRBUV9\\10510970600946004.html},
  keywords = {Blindness,Conversation analysis,Conversation-regulating signals,Hermeneutics,Nonverbal,Phenomenology},
  number = {4}
}

@article{mcclavePitchManualGestures1998,
  title = {Pitch and {{Manual Gestures}}},
  author = {McClave, E.},
  date = {1998},
  journaltitle = {Journal of Psycholinguistic Research},
  volume = {27},
  pages = {69--89},
  doi = {10.1023/A:1023274823974},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DM2EK6C7\\10.html},
  number = {2}
}

@article{mcgurkHearingLipsSeeing1976,
  title = {Hearing Lips and Seeing Voices},
  author = {McGurk, H. and MacDonald, J.},
  date = {1976-12-23/0030},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {264},
  pages = {746--748},
  issn = {0028-0836},
  doi = {10.1038/264746a0},
  eprint = {1012311},
  eprinttype = {pmid},
  keywords = {Adolescent,Adult,Age Factors,Auditory Perception,Child,Child; Preschool,Female,Humans,Illusions,Male,Speech,Visual Perception},
  langid = {english},
  number = {5588}
}

@book{mcneilageOriginSpeech2008,
  title = {The Origin of Speech},
  author = {McNeilage, P.},
  date = {2008},
  publisher = {{Oxford University Press}},
  location = {{New York}}
}

@online{mcneillCatchmentsProsodyDiscourse2001,
  title = {Catchments, Prosody and Discourse},
  author = {McNeill, D. and Quek, F. and McCullough, K.-E. and Duncan, S. and Furuyama, N. and Bryll, R. and Ma, X.-F. and Ansari, R.},
  date = {2001},
  publisher = {{John Benjamins Publishing Company}},
  doi = {info:doi/10.1075/gest.1.1.03mcn},
  url = {https://www.ingentaconnect.com/content/jbp/gest/2001/00000001/00000001/art00002},
  urldate = {2020-03-17},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\6MN3EUT9\\art00002.html},
  langid = {english},
  type = {Text}
}

@book{mcneillGestureThought2005,
  title = {Gesture and {{Thought}}},
  author = {McNeill, David},
  date = {2005},
  publisher = {{The University of Chicago Press}},
  location = {{Chicago}},
  url = {https://www.press.uchicago.edu/ucp/books/book/chicago/G/bo3633713.html},
  urldate = {2019-04-16},
  abstract = {Gesturing is such an integral yet unconscious part of communication that we are mostly oblivious to it. But if you observe anyone in conversation, you are likely to see his or her fingers, hands, and arms in some form of spontaneous motion. Why? David McNeill, a pioneer in the ongoing study of the relationship between gesture and language, set about answering this question over twenty-five years ago. In Gesture and Thought he brings together years of this research, arguing that gesturing, an act which has been popularly understood as an accessory to speech, is actually a dialectical component of language. Gesture and Thought expands on McNeill’s acclaimed classic Hand and Mind. While that earlier work demonstrated what gestures reveal about thought, here gestures are shown to be active participants in both speaking and thinking. Expanding on an approach introduced by Lev Vygotsky in the 1930s, McNeill posits that gestures are key ingredients in an “imagery-language dialectic” that fuels both speech and thought. Gestures are both the “imagery” and components of “language.” The smallest element of this dialectic is the “growth point,” a snapshot~of an utterance at its beginning psychological stage. Utilizing several innovative experiments he created and administered with subjects spanning several different age, gender, and language groups, McNeill shows how growth points organize themselves into utterances and extend to discourse at the moment of speaking.An ambitious project in the ongoing study of the relationship of human communication and thought, Gesture and Thought is a work of such consequence that it will influence all subsequent theory on the subject.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\S7LGLV93\\bo3633713.html},
  pagetotal = {328}
}

@book{mcneillHandMindWhat1992,
  title = {Hand and {{Mind}}: {{What Gestures Reveal}} about {{Thought}}},
  shorttitle = {Hand and {{Mind}}},
  author = {McNeill, David},
  date = {1992},
  publisher = {{University Of Chicago Press}},
  location = {{Chicago}},
  abstract = {Will be shipped from US. Used books may not include companion materials, may have some shelf wear, may contain highlighting/notes, may not include CDs or access codes. 100\% money back guarantee.}
}

@incollection{mcneillIWManWho2010,
  title = {{{IW}} - “{{The Man Who Lost His Body}}”},
  booktitle = {Handbook of {{Phenomenology}} and {{Cognitive Science}}},
  author = {McNeill, David and Quaeghebeur, Liesbet and Duncan, Susan},
  editor = {Schmicking, Daniel and Gallagher, Shaun},
  date = {2010},
  pages = {519--543},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-90-481-2646-0_27},
  url = {https://doi.org/10.1007/978-90-481-2646-0_27},
  urldate = {2019-04-16},
  abstract = {Mr. Ian Waterman, sometimes referred to as ‘IW’, suffered at age 19 a sudden, total deafferentation of his body from the neck down - the near total loss of all the touch, proprioception, and limb spatial position senses that tell you, without looking, where your body is and what it is doing. The loss followed a never-diagnosed fever that is believed to have set off an auto-immune reaction. The immediate behavioral effect was immobility, even though IW’s motor system was unaffected and there was no paralysis. The problem was not lack of movement per se but lack of control. Upon awakening after 3 days, IW nightmarishly found that he had no control over what his body did - he was unable to sit up, walk, feed himself or manipulate objects; none of the ordinary actions of everyday life, let alone the complex actions required for his vocation. To imagine what deafferentation is like, try this experiment suggested by Shaun Gallagher: sit down at a table (something IW could not have done at first) and place your hands below the surface; open and close one hand, close the other and extend a finger; put the open hand over the closed hand, and so forth. You know at all times what your hands are doing and where they are but IW would not know any of this - he would know that he had willed his hands to move but, without vision, would have no idea of what they are doing or where they are located.},
  isbn = {978-90-481-2646-0},
  keywords = {Bodily Expression,Growth Point,Idea Unit,Instrumental Action,Linguistic Meaning},
  langid = {english}
}

@book{mcneillLanguageGesture2000,
  title = {Language and {{Gesture}}},
  author = {McNeill, David},
  date = {2000-08-03},
  publisher = {{Cambridge University Press}},
  abstract = {This landmark study examines the role of gestures in relation to speech and thought. Leading scholars, including psychologists, linguists and anthropologists, offer state-of-the-art analyses to demonstrate that gestures are not merely an embellishment of speech but are integral parts of language itself. Language and Gesture offers a wide range of theoretical approaches, with emphasis not simply on behavioural descriptions but also on the underlying processes. The book has strong cross-linguistic and cross-cultural components, examining gestures by speakers of Mayan, Australian, East Asian as well as English and other European languages. The content is diverse including chapters on gestures during aphasia and severe stuttering, the first emergence of speech-gesture combinations of children, and a section on sign language. In a rapidly growing field of study this volume opens up the agenda for research into a new approach to understanding language, thought and society.},
  eprint = {DRBcMQuSrf8C},
  eprinttype = {googlebooks},
  isbn = {978-0-521-77761-2},
  keywords = {Language Arts & Disciplines / Linguistics / General,Language Arts & Disciplines / Linguistics / Sociolinguistics,Language Arts & Disciplines / Speech},
  langid = {english},
  pagetotal = {424}
}

@article{meltzoffNewbornInfantsImitate1983,
  title = {Newborn Infants Imitate Adult Facial Gestures},
  author = {Meltzoff, A. N. and Moore, M. K.},
  date = {1983},
  journaltitle = {Child Development},
  volume = {54},
  pages = {702--709},
  url = {https://www.jstor.org/stable/1130058?casa_token=-qnMcXXYbLMAAAAA:6Mkw9bqGR5zBtzxZlpZAn5LUZICPlk2Fy2TQxOCLR9HCPgRnK1I2qbICBzd8JKoQINNFy9QKXxaXrWYrnS14K--oSO0Efd-VYtm0IBQZXmri9ZGYNz8B&seq=1#metadata_info_tab_contents},
  urldate = {2019-12-07},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4KFMMS5P\\1130058.html},
  number = {3}
}

@article{mendoza-dentonSemioticLayeringGesture2011,
  title = {Semiotic {{Layering}} through {{Gesture}} and {{Intonation}}: {{A Case Study}} of {{Complementary}} and {{Supplementary Multimodality}} in {{Political Speech}}},
  shorttitle = {Semiotic {{Layering}} through {{Gesture}} and {{Intonation}}},
  author = {Mendoza-Denton, Norma and Jannedy, Stefanie},
  date = {2011-09-01},
  journaltitle = {Journal of English Linguistics},
  shortjournal = {Journal of English Linguistics},
  volume = {39},
  pages = {265--299},
  issn = {0075-4242},
  doi = {10.1177/0075424211405941},
  url = {https://doi.org/10.1177/0075424211405941},
  urldate = {2020-01-26},
  abstract = {Face-to-face communication is multimodal. In face-to-face interaction scholars can observe the interplay of several “semiotic layers,” modalities of information such as syntax, discourse structure, gesture, and intonation. The authors explore the role of gesture in structuring and aligning information in spoken discourse through a study of (1) the complementary co-occurrence of gestural apices and intonational pitch accents and (2) the supplementary co-occurrence of metaphorical gestures and elements in discourse. In the naturally occurring political speech situation the authors examine, metaphorical spatialization through gesture is key in indexing contextual relationships among the speaker, the politicians or government, and other external forces. The use of gestures simultaneously aligns with intonation and metaphorically manipulates political entities in space. Discourse context and social meaning are thus constructed together through the spoken and gestural channels and are supported through fine-grained structural alignment between intonation and gesture.},
  keywords = {embodiment,gesture,intonation,multimodality,political speech,public sphere,semiotics,spoken discourse},
  langid = {english},
  number = {3}
}

@article{michelettaMulticomponentMultimodalLipsmacking2013,
  title = {Multicomponent and {{Multimodal Lipsmacking}} in {{Crested Macaques}} ( {{{\emph{Macaca}}}}{\emph{ Nigra}} ): {{Lipsmacking Behavior}} in {{Crested Macaques}}},
  shorttitle = {Multicomponent and {{Multimodal Lipsmacking}} in {{Crested Macaques}} ( {{{\emph{Macaca}}}}{\emph{ Nigra}} )},
  author = {Micheletta, Jérôme and Engelhardt, Antje and Matthews, Lee and Agil, Muhammad and Waller, Bridget M.},
  date = {2013-07},
  journaltitle = {American Journal of Primatology},
  volume = {75},
  pages = {763--773},
  issn = {02752565},
  doi = {10.1002/ajp.22105},
  url = {http://doi.wiley.com/10.1002/ajp.22105},
  urldate = {2019-09-17},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DFIKF4E2\\Micheletta et al. - 2013 - Multicomponent and Multimodal Lipsmacking in Crest.pdf},
  langid = {english},
  number = {7}
}

@article{millikanDefenseProperFunctions1989,
  title = {In {{Defense}} of {{Proper Functions}}},
  author = {Millikan, Ruth Garrett},
  date = {1989},
  journaltitle = {Philosophy of Science},
  volume = {56},
  pages = {288--302},
  issn = {0031-8248},
  url = {https://www.jstor.org/stable/187875},
  urldate = {2020-01-27},
  abstract = {I defend the historical definition of "function" originally given in my Language, Thought and Other Biological Categories (1984a). The definition was not offered in the spirit of conceptual analysis but is more akin to a theoretical definition of "function". A major theme is that nonhistorical analyses of "function" fail to deal adequately with items that are not capable of performing their functions.},
  number = {2}
}

@article{milneAuditoryVisualSequence2018,
  title = {Auditory and {{Visual Sequence Learning}} in {{Humans}} and {{Monkeys}} Using an {{Artificial Grammar Learning Paradigm}}},
  author = {Milne, Alice E. and Petkov, Christopher I. and Wilson, Benjamin},
  date = {2018-10-01},
  journaltitle = {Neuroscience},
  shortjournal = {Neuroscience},
  volume = {389},
  pages = {104--117},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2017.06.059},
  url = {http://www.sciencedirect.com/science/article/pii/S0306452217304645},
  urldate = {2020-03-18},
  abstract = {Language flexibly supports the human ability to communicate using different sensory modalities, such as writing and reading in the visual modality and speaking and listening in the auditory domain. Although it has been argued that nonhuman primate communication abilities are inherently multisensory, direct behavioural comparisons between human and nonhuman primates are scant. Artificial grammar learning (AGL) tasks and statistical learning experiments can be used to emulate ordering relationships between words in a sentence. However, previous comparative work using such paradigms has primarily investigated sequence learning within a single sensory modality. We used an AGL paradigm to evaluate how humans and macaque monkeys learn and respond to identically structured sequences of either auditory or visual stimuli. In the auditory and visual experiments, we found that both species were sensitive to the ordering relationships between elements in the sequences. Moreover, the humans and monkeys produced largely similar response patterns to the visual and auditory sequences, indicating that the sequences are processed in comparable ways across the sensory modalities. These results provide evidence that human sequence processing abilities stem from an evolutionarily conserved capacity that appears to operate comparably across the sensory modalities in both human and nonhuman primates. The findings set the stage for future neurobiological studies to investigate the multisensory nature of these sequencing operations in nonhuman primates and how they compare to related processes in humans.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NPC4T9E9\\Milne et al. - 2018 - Auditory and Visual Sequence Learning in Humans an.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\GLI6UCE7\\S0306452217304645.html},
  keywords = {auditory,comparative,human,macaque,structured sequence learning,visual},
  langid = {english},
  series = {Sensory {{Sequence Processing}} in the {{Brain}}}
}

@article{morillonMotorOriginTemporal2017,
  title = {Motor Origin of Temporal Predictions in Auditory Attention},
  author = {Morillon, Benjamin and Baillet, Sylvain},
  date = {2017-10-17},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {114},
  pages = {E8913-E8921},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1705373114},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1705373114},
  urldate = {2019-09-17},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FV8UVQLQ\\Morillon and Baillet - 2017 - Motor origin of temporal predictions in auditory a.pdf},
  langid = {english},
  number = {42}
}

@article{motamediEvolvingArtificialSign2019,
  title = {Evolving Artificial Sign Languages in the Lab: {{From}} Improvised Gesture to Systematic Sign},
  shorttitle = {Evolving Artificial Sign Languages in the Lab},
  author = {Motamedi, Yasamin and Schouwstra, Marieke and Smith, Kenny and Culbertson, Jennifer and Kirby, Simon},
  date = {2019-11-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {192},
  pages = {103964},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.05.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027719301234},
  urldate = {2019-12-03},
  abstract = {Recent work on emerging sign languages provides evidence for how key properties of linguistic systems are created. Here we use laboratory experiments to investigate the contribution of two specific mechanisms—interaction and transmission—to the emergence of a manual communication system in silent gesturers. We show that the combined effects of these mechanisms, rather than either alone, maintain communicative efficiency, and lead to a gradual increase of regularity and systematic structure. The gestures initially produced by participants are unsystematic and resemble pantomime, but come to develop key language-like properties similar to those documented in newly emerging sign systems.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9QYZU2GS\\Motamedi et al. - 2019 - Evolving artificial sign languages in the lab Fro.pdf},
  keywords = {Interaction,Iterated learning,Language evolution,Sign language,Silent gesture,Transmission},
  langid = {english}
}

@inproceedings{mueenExtractingOptimalPerformance2016,
  title = {Extracting {{Optimal Performance}} from {{Dynamic Time Warping}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Mueen, Abdullah and Keogh, Eamonn},
  date = {2016},
  pages = {2129--2130},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2939672.2945383},
  url = {http://doi.acm.org/10.1145/2939672.2945383},
  urldate = {2019-06-24},
  abstract = {Dynamic Time Warping (DTW) is a distance measure that compares two time series after optimally aligning them. DTW is being used for decades in thousands of academic and industrial projects despite the very expensive computational complexity, O(n2). These applications include data mining, image processing, signal processing, robotics and computer graphics among many others. In spite of all this research effort, there are many myths and misunderstanding about DTW in the literature, for example "it is too slow to be useful" or "the warping window size does not matter much." In this tutorial, we correct these misunderstandings and we summarize the research efforts in optimizing both the efficiency and effectiveness of both the basic DTW algorithm, and of the higher-level algorithms that exploit DTW such as similarity search, clustering and classification. We will discuss variants of DTW such as constrained DTW, multidimensional DTW and asynchronous DTW, and optimization techniques such as lower bounding, early abandoning, run-length encoding, bounded approximation and hardware optimization. We will discuss a multitude of application areas including physiological monitoring, social media mining, activity recognition and animal sound processing. The optimization techniques are generalizable to other domains on various data types and problems.},
  isbn = {978-1-4503-4232-2},
  keywords = {approximation,dynamic time warping,lower bounds,pruning,time series},
  series = {{{KDD}} '16},
  venue = {San Francisco, California, USA}
}

@inproceedings{mueenExtractingOptimalPerformance2016a,
  title = {Extracting Optimal Performance from Dynamic Time Warping},
  booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Mueen, A. K. and Keogh, E.},
  date = {2016},
  pages = {2129--2130},
  doi = {10.1145/ 2939672.2945383}
}

@article{mullerGestureSignCataclysmic2018,
  title = {Gesture and {{Sign}}: {{Cataclysmic Break}} or {{Dynamic Relations}}?},
  shorttitle = {Gesture and {{Sign}}},
  author = {Müller, Cornelia},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {9},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.01651},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.01651/full},
  urldate = {2019-08-26},
  abstract = {Abstract The goal of the article is to offer a framework against which relations between gesture and sign can be systematically explored beyond the current literature. It does so by a) reconstructing the history of the discussion in the field of gesture studies, focusing on three leading positions (Kendon, McNeill, Goldin-Meadow); and b) by formulating a position to illustrate how this can be achieved. The paper concludes by emphasizing the need for systematic cross-linguistic research on multimodal use of language in its signed and spoken forms.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KM4GGRE6\\Müller - 2018 - Gesture and Sign Cataclysmic Break or Dynamic Rel.pdf},
  keywords = {Conventionalization processes,Emblems,Gesture and sign,McNeill's gesture-sign continua,Multimodality of language use,Recurrent gestures,Silent gestures,Singular gestures},
  langid = {english}
}

@book{mullerInformationRetrievalMusic2007,
  title = {Information Retrieval for Music and Motion},
  author = {Muller, M.},
  date = {2007},
  publisher = {{Springer}},
  location = {{Heidelberg, Germany}}
}

@article{namboodiripadMeasuringConventionalizationManual2016,
  title = {Measuring Conventionalization in the Manual Modality},
  author = {Namboodiripad, S. and Lenzen, D. and Lepic, R. and Verhoef, T.},
  date = {2016},
  journaltitle = {Journal of Language Evolution},
  volume = {1},
  pages = {109--118},
  doi = {10.1093/jole/lzw005},
  number = {2}
}

@incollection{navarrettaBigDataMultimodal2019,
  title = {Big {{Data}} and {{Multimodal Communication}}: {{A Perspective View}}},
  booktitle = {Innovations in {{Big Data Mining}} and {{Embedded Knowledge}}, {{A}}. {{Esposito}} et al. (Eds.),},
  author = {Navarretta, Costanza and Oemig, Lucretia},
  date = {2019},
  pages = {167--184},
  publisher = {{Springer Nature}},
  location = {{Switzerland}},
  url = {https://doi.org/10.1007/978-3-030-15939-9_9}
}

@article{odellModelingTurntakingRhythms2012,
  title = {Modeling Turn-Taking Rhythms with Oscillators},
  author = {O'Dell, M. L. and Nieminen, M. and Mietta, L.},
  date = {2012-09-01},
  journaltitle = {Linguistica Uralica},
  volume = {48},
  pages = {218--228},
  publisher = {{Estonian Academy Publishers}},
  issn = {08684731},
  url = {https://go.gale.com/ps/i.do?p=AONE&sw=w&issn=08684731&v=2.1&it=r&id=GALE%7CA317588557&sid=googleScholar&linkaccess=abs},
  urldate = {2020-05-28},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VG88Q3S5\\anonymous.html},
  langid = {english},
  number = {3}
}

@incollection{ohalaRespiratoryActivitySpeech1990,
  title = {Respiratory {{Activity}} in Speech},
  booktitle = {Speech Production and Speech Modeling ({{Eds}}. {{W}}. {{J}}. {{Hardcastle}} and {{A}}. {{Marchal}})},
  author = {Ohala, J. J.},
  date = {1990},
  pages = {22--53},
  publisher = {{Kluwer}},
  location = {{Dordrecht}}
}

@article{oleszkiewiczVoicebasedAssessmentsTrustworthiness2017,
  title = {Voice-Based Assessments of Trustworthiness, Competence, and Warmth in Blind and Sighted Adults},
  author = {Oleszkiewicz, Anna and Pisanski, Katarzyna and Lachowicz-Tabaczek, Kinga and Sorokowska, Agnieszka},
  date = {2017-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {24},
  pages = {856--862},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1146-y},
  url = {https://doi.org/10.3758/s13423-016-1146-y},
  urldate = {2019-11-30},
  abstract = {The study of voice perception in congenitally blind individuals allows researchers rare insight into how a lifetime of visual deprivation affects the development of voice perception. Previous studies have suggested that blind adults outperform their sighted counterparts in low-level auditory tasks testing spatial localization and pitch discrimination, as well as in verbal speech processing; however, blind persons generally show no advantage in nonverbal voice recognition or discrimination tasks. The present study is the first to examine whether visual experience influences the development of social stereotypes that are formed on the basis of nonverbal vocal characteristics (i.e., voice pitch). Groups of 27 congenitally or early-blind adults and 23 sighted controls assessed the trustworthiness, competence, and warmth of men and women speaking a series of vowels, whose voice pitches had been experimentally raised or lowered. Blind and sighted listeners judged both men’s and women’s voices with lowered pitch as being more competent and trustworthy than voices with raised pitch. In contrast, raised-pitch voices were judged as being warmer than were lowered-pitch voices, but only for women’s voices. Crucially, blind and sighted persons did not differ in their voice-based assessments of competence or warmth, or in their certainty of these assessments, whereas the association between low pitch and trustworthiness in women’s voices was weaker among blind than sighted participants. This latter result suggests that blind persons may rely less heavily on nonverbal cues to trustworthiness compared to sighted persons. Ultimately, our findings suggest that robust perceptual associations that systematically link voice pitch to the social and personal dimensions of a speaker can develop without visual input.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8ERSAHDE\\Oleszkiewicz et al. - 2017 - Voice-based assessments of trustworthiness, compet.pdf},
  keywords = {Blind,Nonverbal communication,Sightedness,Social perception,Voice perception},
  langid = {english},
  number = {3}
}

@article{ollerFunctionalFlexibilityInfant2013,
  title = {Functional Flexibility of Infant Vocalization and the Emergence of Language},
  author = {Oller, D. Kimbrough and Buder, Eugene H. and Ramsdell, Heather L. and Warlaumont, Anne S. and Chorna, Lesya and Bakeman, Roger},
  date = {2013-04-16},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {110},
  pages = {6318--6323},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1300337110},
  url = {https://www.pnas.org/content/110/16/6318},
  urldate = {2020-05-29},
  abstract = {We report on the emergence of functional flexibility in vocalizations of human infants. This vastly underappreciated capability becomes apparent when prelinguistic vocalizations express a full range of emotional content—positive, neutral, and negative. The data show that at least three types of infant vocalizations (squeals, vowel-like sounds, and growls) occur with this full range of expression by 3–4 mo of age. In contrast, infant cry and laughter, which are species-specific signals apparently homologous to vocal calls in other primates, show functional stability, with cry overwhelmingly expressing negative and laughter positive emotional states. Functional flexibility is a sine qua non in spoken language, because all words or sentences can be produced as expressions of varying emotional states and because learning conventional “meanings” requires the ability to produce sounds that are free of any predetermined function. Functional flexibility is a defining characteristic of language, and empirically it appears before syntax, word learning, and even earlier-developing features presumed to be critical to language (e.g., joint attention, syllable imitation, and canonical babbling). The appearance of functional flexibility early in the first year of human life is a critical step in the development of vocal language and may have been a critical step in the evolution of human language, preceding protosyntax and even primitive single words. Such flexible affect expression of vocalizations has not yet been reported for any nonhuman primate but if found to occur would suggest deep roots for functional flexibility of vocalization in our primate heritage.},
  eprint = {23550164},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\U3WNRPUP\\Oller et al. - 2013 - Functional flexibility of infant vocalization and .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\396QDUMR\\6318.html},
  keywords = {evolution of language,flexibility in communication,infant communication,language development,primate communication},
  langid = {english},
  number = {16}
}

@article{ollerLanguageOriginsViewed2019,
  title = {Language {{Origins Viewed}} in {{Spontaneous}} and {{Interactive Vocal Rates}} of {{Human}} and {{Bonobo Infants}}},
  author = {Oller, D. Kimbrough and Griebel, Ulrike and Iyer, Suneeti Nathani and Jhang, Yuna and Warlaumont, Anne S. and Dale, Rick and Call, Josep},
  date = {2019},
  journaltitle = {Frontiers in Psychology},
  volume = {10},
  doi = {10.3389/fpsyg.2019.00729},
  url = {https://www.readcube.com/articles/10.3389%2Ffpsyg.2019.00729},
  urldate = {2020-05-07},
  abstract = {From the first months of life, human infants produce “protophones,” speech-like, non-cry sounds, presumed absent, or only minimally present in other apes. But there have been no direct quantitative comparisons to support this presumption. In addition, by 2 months, human infants show sustained face-to-face interaction using protophones, a pattern thought also absent or very limited in other apes, but again, without quantitative comparison. Such comparison should provide evidence relevant to determining foundations of language, since substantially flexible vocalization, the inclination to explore vocalization, and the ability to interact socially by means of vocalization are foundations for language. Here we quantitatively compare data on vocalization rates in three captive bonobo (Pan paniscus) mother–infant pairs with various sources of data from our laboratories on human infant vocalization. Both humans and bonobos produced distress sounds (cries/screams) and laughter. The bonobo infants also produced sounds that were neither screams nor laughs and that showed acoustic similarities to the human protophones. These protophone-like sounds confirm that bonobo infants share with humans the capacity to produce vocalizations that appear foundational for language. Still, there were dramatic differences between the species in both quantity and function of the protophone and protophone-like sounds. The bonobo protophone-like sounds were far less frequent than the human protophones, and the human protophones were far less likely to be interpreted as complaints and more likely as vocal play. Moreover, we found extensive vocal interaction between human infants and mothers, but no vocal interaction in the bonobo mother–infant pairs—while bonobo mothers were physically responsive to their infants, we observed no case of a bonobo mother vocalization directed to her infant. Our cross-species comparison focuses on low- and moderate-arousal circumstances because we reason the roots of language entail vocalization not triggered by excitement, for example, during fighting or intense play. Language appears to be founded in flexible vocalization, used to regulate comfortable social interaction, to share variable affective states at various levels of arousal, and to explore vocalization itself.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DK39DD4W\\Oller et al. - 2019 - Language Origins Viewed in Spontaneous and Interac.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\MIIBZNLB\\fpsyg.2019.html},
  langid = {english}
}

@article{ollerPretermFullTerm2019,
  title = {Preterm and Full Term Infant Vocalization and the Origin of Language},
  author = {Oller, D. Kimbrough and Caskey, Melinda and Yoo, Hyunjoo and Bene, Edina R. and Jhang, Yuna and Lee, Chia-Cheng and Bowman, Dale D. and Long, Helen L. and Buder, Eugene H. and Vohr, Betty},
  date = {2019-10-14},
  journaltitle = {Scientific Reports},
  volume = {9},
  pages = {14734},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-51352-0},
  url = {https://www.nature.com/articles/s41598-019-51352-0},
  urldate = {2020-05-29},
  abstract = {How did vocal language originate? Before trying to determine how referential vocabulary or syntax may have arisen, it is critical to explain how ancient hominins began to produce vocalization flexibly, without binding to emotions or functions. A crucial factor in the vocal communicative split of hominins from the ape background may thus have been copious, functionally flexible vocalization, starting in infancy and continuing throughout life, long before there were more advanced linguistic features such as referential vocabulary. 2–3 month-old modern human infants produce “protophones”, including at least three types of functionally flexible non-cry precursors to speech rarely reported in other ape infants. But how early in life do protophones actually appear? We report that the most common protophone types emerge abundantly as early as vocalization can be observed in infancy, in preterm infants still in neonatal intensive care. Contrary to the expectation that cries are the predominant vocalizations of infancy, our all-day recordings showed that protophones occurred far more frequently than cries in both preterm and full-term infants. Protophones were not limited to interactive circumstances, but also occurred at high rates when infants were alone, indicating an endogenous inclination to vocalize exploratorily, perhaps the most fundamental capacity underlying vocal language.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\J5UC4JLF\\Oller et al. - 2019 - Preterm and full term infant vocalization and the .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\LQNQSWB4\\s41598-019-51352-0.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{ollerVocalDevelopmentGuide2016,
  title = {Vocal {{Development}} as a {{Guide}} to {{Modeling}} the {{Evolution}} of {{Language}}},
  author = {Oller, D. Kimbrough and Griebel, Ulrike and Warlaumont, Anne S.},
  date = {2016},
  journaltitle = {Topics in Cognitive Science},
  volume = {8},
  pages = {382--392},
  issn = {1756-8765},
  doi = {10.1111/tops.12198},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12198},
  urldate = {2020-05-07},
  abstract = {Modeling of evolution and development of language has principally utilized mature units of spoken language, phonemes and words, as both targets and inputs. This approach cannot address the earliest phases of development because young infants are unable to produce such language features. We argue that units of early vocal development—protophones and their primitive illocutionary/perlocutionary forces—should be targeted in evolutionary modeling because they suggest likely units of hominin vocalization/communication shortly after the split from the chimpanzee/bonobo lineage, and because early development of spontaneous vocal capability is a logically necessary step toward vocal language, a root capability without which other crucial steps toward vocal language capability are impossible. Modeling of language evolution/development must account for dynamic change in early communicative units of form/function across time. We argue for interactive contributions of sender/infants and receiver/caregivers in a feedback loop involving both development and evolution and propose to begin computational modeling at the hominin break from the primate communicative background.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\I8QEHP8Z\\Oller et al. - 2016 - Vocal Development as a Guide to Modeling the Evolu.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\CSFDMG7N\\tops.html},
  keywords = {Computational modeling,Illocution,Language evolution,Parent-infant interaction,Perlocution,Spontaneous vocalization,Vocal development},
  langid = {english},
  number = {2}
}

@article{ortegaHearingNonsignersUse2019,
  title = {Hearing Non-Signers Use Their Gestures to Predict Iconic Form-Meaning Mappings at First Exposure to Signs - {{ScienceDirect}}},
  author = {Ortega, G. and Schiefner, A. and Ozyurek, A.},
  date = {2019},
  journaltitle = {Cognition},
  volume = {191},
  doi = {10.1016/j.cognition.2019.06.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027719301696},
  urldate = {2020-03-24},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9NRYWPQJ\\S0010027719301696.html},
  number = {103996}
}

@article{ortegaSystematicMappingsSemantic2019,
  title = {Systematic Mappings between Semantic Categories and Types of Iconic Representations in the Manual Modality: {{A}} Normed Database of Silent Gesture},
  shorttitle = {Systematic Mappings between Semantic Categories and Types of Iconic Representations in the Manual Modality},
  author = {Ortega, Gerardo and Özyürek, Aslı},
  date = {2019-02-20},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01204-6},
  url = {https://doi.org/10.3758/s13428-019-01204-6},
  urldate = {2019-12-04},
  abstract = {An unprecedented number of empirical studies have shown that iconic gestures—those that mimic the sensorimotor attributes of a referent—contribute significantly to language acquisition, perception, and processing. However, there has been a lack of normed studies describing generalizable principles in gesture production and in comprehension of the mappings of different types of iconic strategies (i.e., modes of representation; Müller, 2013). In Study 1 we elicited silent gestures in order to explore the implementation of different types of iconic representation (i.e., acting, representing, drawing, and personification) to express concepts across five semantic domains. In Study 2 we investigated the degree of meaning transparency (i.e., iconicity ratings) of the gestures elicited in Study 1. We found systematicity in the gestural forms of 109 concepts across all participants, with different types of iconicity aligning with specific semantic domains: Acting was favored for actions and manipulable objects, drawing for nonmanipulable objects, and personification for animate entities. Interpretation of gesture–meaning transparency was modulated by the interaction between mode of representation and semantic domain, with some couplings being more transparent than others: Acting yielded higher ratings for actions, representing for object-related concepts, personification for animate entities, and drawing for nonmanipulable entities. This study provides mapping principles that may extend to all forms of manual communication (gesture and sign). This database includes a list of the most systematic silent gestures in the group of participants, a notation of the form of each gesture based on four features (hand configuration, orientation, placement, and movement), each gesture’s mode of representation, iconicity ratings, and professionally filmed videos that can be used for experimental and clinical endeavors.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XJWS3QGB\\Ortega and Özyürek - 2019 - Systematic mappings between semantic categories an.pdf},
  keywords = {Iconicity,Modes of representation,Normed database,Perception of iconicity,Silent gesture},
  langid = {english}
}

@article{ortegaTypeIconicityMatters2017,
  title = {Type of Iconicity Matters in the Vocabulary Development of Signing Children},
  author = {Ortega, Gerardo and Sümer, Beyza and Özyürek, Aslı},
  date = {2017},
  journaltitle = {Developmental Psychology},
  volume = {53},
  pages = {89--99},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-0599(Electronic),0012-1649(Print)},
  doi = {10.1037/dev0000161},
  abstract = {Recent research on signed as well as spoken language shows that the iconic features of the target language might play a role in language development. Here, we ask further whether different types of iconic depictions modulate children’s preferences for certain types of sign-referent links during vocabulary development in sign language. Results from a picture description task indicate that lexical signs with 2 possible variants are used in different proportions by deaf signers from different age groups. While preschool and school-age children favored variants representing actions associated with their referent (e.g., a writing hand for the sign PEN), adults preferred variants representing the perceptual features of those objects (e.g., upward index finger representing a thin, elongated object for the sign PEN). Deaf parents interacting with their children, however, used action- and perceptual-based variants in equal proportion and favored action variants more than adults signing to other adults. We propose that when children are confronted with 2 variants for the same concept, they initially prefer action-based variants because they give them the opportunity to link a linguistic label to familiar schemas linked to their action/motor experiences. Our results echo findings showing a bias for action-based depictions in the development of iconic co-speech gestures suggesting a modality bias for such representations during development. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BCIXN6MR\\Ortega et al. - 2017 - Type of iconicity matters in the vocabulary develo.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\36QYLMLI\\2016-53086-001.html},
  keywords = {Childhood Development,Communication,Gestures,Language Development,Sign Language,Vocabulary},
  number = {1}
}

@article{ortegaTypesIconicityCombinatorial2020,
  title = {Types of Iconicity and Combinatorial Strategies Distinguish Semantic Categories in Silent Gesture across Cultures},
  author = {Ortega, Gerardo and Özyürek, Asli},
  date = {2020-03},
  journaltitle = {Language and Cognition},
  volume = {12},
  pages = {84--113},
  publisher = {{Cambridge University Press}},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2019.28},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/types-of-iconicity-and-combinatorial-strategies-distinguish-semantic-categories-in-silent-gesture-across-cultures/79A4488F54C5BFCDE9953CD9240183FE},
  urldate = {2020-04-28},
  abstract = {In this study we explore whether different types of iconic gestures (i.e., acting, drawing, representing) and their combinations are used systematically to distinguish between different semantic categories in production and comprehension. In Study 1, we elicited silent gestures from Mexican and Dutch participants to represent concepts from three semantic categories: actions, manipulable objects, and non-manipulable objects. Both groups favoured the acting strategy to represent actions and manipulable objects; while non-manipulable objects were represented through the drawing strategy. Actions elicited primarily single gestures whereas objects elicited combinations of different types of iconic gestures as well as pointing. In Study 2, a different group of participants were shown gestures from Study 1 and were asked to guess their meaning. Single-gesture depictions for actions were more accurately guessed than for objects. Objects represented through two-gesture combinations (e.g., acting + drawing) were more accurately guessed than objects represented with a single gesture. We suggest iconicity is exploited to make direct links with a referent, but when it lends itself to ambiguity, individuals resort to combinatorial structures to clarify the intended referent. Iconicity and the need to communicate a clear signal shape the structure of silent gestures and this in turn supports comprehension.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7P7FUTQE\\Ortega and Özyürek - 2020 - Types of iconicity and combinatorial strategies di.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\9KYKA5C8\\79A4488F54C5BFCDE9953CD9240183FE.html},
  keywords = {combinatorial structure,emerging sign language,iconicity,language emergence,silent gesture},
  langid = {english},
  number = {1}
}

@article{otterRoadmapComputationPersistent2017,
  title = {A Roadmap for the Computation of Persistent Homology},
  author = {Otter, Nina and Porter, Mason A. and Tillmann, Ulrike and Grindrod, Peter and Harrington, Heather A.},
  date = {2017-12},
  journaltitle = {EPJ Data Science},
  shortjournal = {EPJ Data Sci.},
  volume = {6},
  pages = {17},
  issn = {2193-1127},
  doi = {10.1140/epjds/s13688-017-0109-5},
  url = {http://arxiv.org/abs/1506.08903},
  urldate = {2020-03-11},
  abstract = {Persistent homology (PH) is a method used in topological data analysis (TDA) to study qualitative features of data that persist across multiple scales. It is robust to perturbations of input data, independent of dimensions and coordinates, and provides a compact representation of the qualitative features of the input. The computation of PH is an open area with numerous important and fascinating challenges. The field of PH computation is evolving rapidly, and new algorithms and software implementations are being updated and released at a rapid pace. The purposes of our article are to (1) introduce theory and computational methods for PH to a broad range of computational scientists and (2) provide benchmarks of state-of-the-art implementations for the computation of PH. We give a friendly introduction to PH, navigate the pipeline for the computation of PH with an eye towards applications, and use a range of synthetic and real-world data sets to evaluate currently available open-source implementations for the computation of PH. Based on our benchmarking, we indicate which algorithms and implementations are best suited to different types of data sets. In an accompanying tutorial, we provide guidelines for the computation of PH. We make publicly available all scripts that we wrote for the tutorial, and we make available the processed version of the data sets used in the benchmarking.},
  archivePrefix = {arXiv},
  eprint = {1506.08903},
  eprinttype = {arxiv},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XP2TJHTG\\Otter et al. - 2017 - A roadmap for the computation of persistent homolo.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ZQBRPFJR\\1506.html},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Quantitative Methods},
  number = {1}
}

@article{ozcaliskanDoesLanguageShape2016,
  title = {Does Language Shape Silent Gesture?},
  author = {Özçalışkan, Şeyda and Lucero, Ché and Goldin-Meadow, Susan},
  date = {2016-03-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {148},
  pages = {10--18},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.12.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027715301153},
  urldate = {2019-08-26},
  abstract = {Languages differ in how they organize events, particularly in the types of semantic elements they express and the arrangement of those elements within a sentence. Here we ask whether these cross-linguistic differences have an impact on how events are represented nonverbally; more specifically, on how events are represented in gestures produced without speech (silent gesture), compared to gestures produced with speech (co-speech gesture). We observed speech and gesture in 40 adult native speakers of English and Turkish (N=20/per language) asked to describe physical motion events (e.g., running down a path)—a domain known to elicit distinct patterns of speech and co-speech gesture in English- and Turkish-speakers. Replicating previous work (Kita \& Özyürek, 2003), we found an effect of language on gesture when it was produced with speech—co-speech gestures produced by English-speakers differed from co-speech gestures produced by Turkish-speakers. However, we found no effect of language on gesture when it was produced on its own—silent gestures produced by English-speakers were identical in how motion elements were packaged and ordered to silent gestures produced by Turkish-speakers. The findings provide evidence for a natural semantic organization that humans impose on motion events when they convey those events without language.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YBUH3TR5\\Özçalışkan et al. - 2016 - Does language shape silent gesture.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BSUWUPFT\\S0010027715301153.html},
  keywords = {Cross-linguistic differences,Gesture,Language and cognition,Motion events}
}

@article{paddenPatternedIconicitySign2013,
  title = {Patterned Iconicity in Sign Language Lexicons},
  author = {Padden, Carol A. and Meir, Irit and Hwang, So-One and Lepic, Ryan and Seegers, Sharon and Sampson, Tory},
  date = {2013-01-01},
  journaltitle = {Gesture},
  volume = {13},
  pages = {287--308},
  publisher = {{John Benjamins}},
  issn = {1568-1475, 1569-9773},
  doi = {10.1075/gest.13.3.03pad},
  url = {https://www.jbe-platform.com/content/journals/10.1075/gest.13.3.03pad},
  urldate = {2020-04-28},
  abstract = {Iconicity is an acknowledged property of both gesture and sign language. In contrast to the familiar definition of iconicity as a correspondence between individual forms and their referents, we explore iconicity as a shared property among groups of signs, in what we call patterned iconicity. In this paper, we focus on iconic strategies used by hearing silent gesturers and by signers of three unrelated sign languages in an elicitation task featuring pictures of hand-held manufactured tools. As in previous gesture literature, we find that silent gesturers largely prefer a handling strategy, though some use an instrument strategy, in which the handshape represents the shape of the tool. There are additional differences in use of handling and instrument strategies for hand-held tools across the different sign languages, suggesting typological differences in iconic patterning. Iconic patterning in each of the three sign languages demonstrates how gestural iconic resources are organized in the grammars of sign languages.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CNVAKU3G\\gest.13.3.html},
  langid = {english},
  number = {3}
}

@article{pagancanovasQuantifyingSpeechgestureRelation2020,
  title = {Quantifying the Speech-Gesture Relation with Massive Multimodal Datasets: {{Informativity}} in Time Expressions},
  shorttitle = {Quantifying the Speech-Gesture Relation with Massive Multimodal Datasets},
  author = {Pagán Cánovas, Cristóbal and Valenzuela, Javier and Alcaraz Carrión, Daniel and Olza, Inés and Ramscar, Michael},
  editor = {Perlman, Marcus},
  date = {2020-06-02},
  journaltitle = {PLOS ONE},
  volume = {15},
  pages = {e0233892},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0233892},
  url = {https://dx.plos.org/10.1371/journal.pone.0233892},
  urldate = {2020-06-05},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MXLHQGGB\\Pagán Cánovas et al. - 2020 - Quantifying the speech-gesture relation with massi.pdf},
  langid = {english},
  number = {6}
}

@book{paoloLinguisticBodiesContinuity2018,
  title = {Linguistic {{Bodies}}: {{The Continuity}} between {{Life}} and {{Language}}},
  shorttitle = {Linguistic {{Bodies}}},
  author = {Paolo, Ezequiel A. Di and Cuffari, Elena Clare and Jaegher, Hanne De},
  date = {2018-11-06},
  publisher = {{MIT Press}},
  abstract = {A novel theoretical framework for an embodied, non-representational approach to language that extends and deepens enactive theory, bridging the gap between sensorimotor skills and language.Linguistic Bodies offers a fully embodied and fully social treatment of human language without positing mental representations. The authors present the first coherent, overarching theory that connects dynamical explanations of action and perception with language. Arguing from the assumption of a deep continuity between life and mind, they show that this continuity extends to language. Expanding and deepening enactive theory, they offer a constitutive account of language and the co-emergent phenomena of personhood, reflexivity, social normativity, and ideality. Language, they argue, is not something we add to a range of existing cognitive capacities but a new way of being embodied. Each of us is a linguistic body in a community of other linguistic bodies. The book describes three distinct yet entangled kinds of human embodiment, organic, sensorimotor, and intersubjective; it traces the emergence of linguistic sensitivities and introduces the novel concept of linguistic bodies; and it explores the implications of living as linguistic bodies in perpetual becoming, applying the concept of linguistic bodies to questions of language acquisition, parenting, autism, grammar, symbol, narrative, and gesture, and to such ethical concerns as microaggression, institutional speech, and pedagogy.},
  eprint = {_rVyDwAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-262-03816-4},
  keywords = {Language Arts & Disciplines / Linguistics / General,Psychology / Cognitive Psychology & Cognition},
  langid = {english},
  pagetotal = {428}
}

@article{parrellSpatiotemporalCouplingSpeech2014,
  title = {Spatiotemporal Coupling between Speech and Manual Motor Actions},
  author = {Parrell, Benjamin and Goldstein, Louis and Lee, Sungbok and Byrd, Dani},
  date = {2014-01-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {42},
  pages = {1--11},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2013.11.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447013000776},
  urldate = {2019-04-16},
  abstract = {Much evidence has been found for pervasive links between the manual and speech motor systems, including evidence from infant development, deictic pointing, and repetitive tapping and speaking tasks. We expand on the last of these paradigms to look at intra- and cross-modal effects of emphatic stress, as well as the effects of coordination in the absence of explicit rhythm. In this study, subjects repeatedly tapped their finger and synchronously repeated a single spoken syllable. On each trial, subjects placed an emphatic stress on one finger tap or one spoken syllable. Results show that both movement duration and magnitude are affected by emphatic stress regardless of whether that stress is in the same domain (e.g., effects on the oral articulators when a spoken repetition is stressed) or across domains (e.g., effects on the oral articulators when a tap is stressed). Though the size of the effects differs between intra-and cross-domain emphases, the implementation of stress affects both motor domains, indicating a tight connection. This close coupling is seen even in the absence of stress, though it is highlighted under stress. The results of this study support the idea that implementation of prosody is not domain-specific but relies on general aspects of the motor system.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\UZIY6KZ5\\Parrell et al. - 2014 - Spatiotemporal coupling between speech and manual .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BRJQDEAI\\S0095447013000776.html}
}

@article{partanCommunicationGoesMultimodal1999,
  title = {Communication {{Goes Multimodal}}},
  author = {Partan, Sarah and Marler, Peter},
  date = {1999-02-26},
  journaltitle = {Science},
  volume = {283},
  pages = {1272--1273},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.283.5406.1272},
  url = {https://science.sciencemag.org/content/283/5406/1272},
  urldate = {2019-11-15},
  abstract = {{$<$}p{$>$} Communication depends on the simultaneous receipt of multiple sensory stimuli. The Perspective by Partan and Marler in this week9s issue postulates a new classification system for multimodal sensory signals. Combinations of sensory signals are classified according to the behavioral responses they elicit. {$<$}/p{$>$}},
  eprint = {10084931},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EZ48YMV6\\1272.html},
  langid = {english},
  number = {5406}
}

@article{partanCommunicationGoesMultimodal1999a,
  title = {Communication {{Goes Multimodal}}},
  author = {Partan, Sarah and Marler, Peter},
  date = {1999-02-26},
  journaltitle = {Science},
  volume = {283},
  pages = {1272--1273},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.283.5406.1272},
  url = {https://science.sciencemag.org/content/283/5406/1272},
  urldate = {2020-05-29},
  abstract = {{$<$}p{$>$} Communication depends on the simultaneous receipt of multiple sensory stimuli. The Perspective by Partan and Marler in this week9s issue postulates a new classification system for multimodal sensory signals. Combinations of sensory signals are classified according to the behavioral responses they elicit. {$<$}/p{$>$}},
  eprint = {10084931},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CLZC53BA\\1272.html},
  langid = {english},
  number = {5406}
}

@book{patteeLAWSLANGUAGELIFE2012,
  title = {{{LAWS}}, {{LANGUAGE}} and {{LIFE}}: {{Howard Pattee}}’s Classic Papers on the Physics of Symbols with Contemporary Commentary},
  shorttitle = {{{LAWS}}, {{LANGUAGE}} and {{LIFE}}},
  author = {Pattee, Howard Hunt and Rączaszek-Leonardi, Joanna},
  date = {2012-12-09},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Howard Pattee is a physicist who for many years has taken his own path in studying the physics of symbols, which is now a foundation for biosemiotics. By extending von Neumann’s logical requirements for self-replication, to the physical requirements of symbolic instruction at the molecular level, he concludes that a form of quantum measurement is necessary for life. He explains why all non-dynamic symbolic and informational controls act as special (allosteric) constraints on dynamical systems. Pattee also points out that symbols do not exist in isolation but in coordinated symbol systems we call languages. Such insights turn out to be necessary to situate biosemiotics as an objective scientific endeavor. By proposing a way to relate quiescent symbolic constraints to dynamics, Pattee’s work builds a bridge between physical, biological, and psychological models that are based on dynamical systems theory. Pattee’s work awakes new interest in cognitive scientists, where his recognition of the necessary separation—the epistemic cut—between the subject and object provides a basis for a complementary third way of relating the purely symbolic, computational models of cognition and the purely dynamic, non-representational models. This selection of Pattee’s papers also addresses several other fields, including hierarchy theory, artificial life, self-organization, complexity theory, and the complementary epistemologies of the physical and biological sciences.},
  eprint = {raEQodcVdYQC},
  eprinttype = {googlebooks},
  isbn = {978-94-007-5161-3},
  keywords = {Philosophy / Epistemology,Philosophy / Language,Philosophy / Reference,Psychology / Cognitive Psychology & Cognition,Science / History,Science / Life Sciences / Anatomy & Physiology,Science / Life Sciences / Biology,Science / Life Sciences / General,Science / Life Sciences / Molecular Biology,Science / Physics / General},
  langid = {english},
  pagetotal = {338}
}

@article{paxtonArgumentDisruptsInterpersonal2013,
  title = {Argument Disrupts Interpersonal Synchrony},
  author = {Paxton, A. and Dale, Rick},
  date = {2013-11-01},
  journaltitle = {Quarterly Journal of Experimental Psychology},
  shortjournal = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  pages = {2092--2102},
  issn = {1747-0218},
  doi = {10.1080/17470218.2013.853089},
  url = {https://doi.org/10.1080/17470218.2013.853089},
  urldate = {2019-11-30},
  abstract = {Research on interpersonal convergence and synchrony characterizes the way in which interacting individuals come to have more similar affect, behaviour, and cognition over time. Although its dynamics have been explored in many settings, convergence during conflict has been almost entirely overlooked. We present a simple but ecologically valid study comparing how different situational contexts that highlight affiliation and argument impact interpersonal convergence of body movement and to what degree emotional states affect convergence in both conversational settings. Using linear mixed-effect models, we found that in-phase bodily synchrony decreases significantly during argument. However, affective changes did not significantly predict changes in levels of interpersonal synchrony, suggesting that differences in affect valences between affiliation and argument cannot solely explain our results.},
  langid = {english},
  number = {11}
}

@article{paxtonCaseIntersectionalityEcological,
  title = {The Case for Intersectionality in Ecological Psychology},
  author = {Paxton, A. and Blau, J. and Weston, M. L.}
}

@article{paxtonInterpersonalMovementSynchrony2017,
  title = {Interpersonal Movement Synchrony Responds to High- and Low-{{Level}} Conversational Constraints},
  author = {Paxton, A. and Dale, R.},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2017.01135},
  url = {https://www.jair.org/index.php/jair/article/view/10536},
  urldate = {2019-06-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\B2NMR4A4\\10536.html}
}

@article{paxtonMultimodalNetworksInterpersonal,
  title = {Multimodal {{Networks}} of {{Interpersonal Interaction}} and {{Conversational Contexts}}},
  author = {Paxton, Alexandra and Dale, Rick},
  pages = {7},
  abstract = {In interpersonal interaction, the terms synchrony or alignment refer to the way in which communication channels like speech or body movement become intertwined over time, both across interlocutors and within a single individual. A recent trend in alignment research has targeted multimodal alignment, exploring how various communication channels affect one another over time (e.g., Louwerse et al., 2012). While existing research has made significant progress in mapping multimodal alignment during task-based or positively valenced interactions, little is known about the dynamics of multimodal alignment during conflict. We visualize multimodal alignment during naturalistic affiliative and argumentative interactions as networks based on analyses of body movement and speech. Broadly, we find that conversational contexts strongly impact the ways in which interlocutors’ movement and speech systems self-organize interpersonally and intrapersonally.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5BD477BP\\Paxton and Dale - Multimodal Networks of Interpersonal Interaction a.pdf},
  langid = {english}
}

@article{pearsonGestureSonicEvent2013,
  title = {Gesture and the {{Sonic Event}} in {{Karnatak Music}}},
  author = {Pearson, Lara},
  date = {2013-10-24},
  journaltitle = {Empirical Musicology Review},
  volume = {8},
  pages = {2--14},
  issn = {1559-5749},
  doi = {10.18061/emr.v8i1.3918},
  url = {http://emusicology.org/article/view/3918},
  urldate = {2019-09-20},
  abstract = {This paper presents an analysis of the relationship between gesture and music in the context of a Karnatak vocal lesson recorded in Tamil Nadu, South India in September 2011. The study aims to examine instances of correspondence between gesture and sonic event that occur during the lesson. Through this analysis the paper aims to contribute to the wider debate on the factors that determine gesture. Shape and trajectory are used in this study as means of describing and comparing gestures. The teacher’s hand movements are tracked and traced rendering the gestures as static shapes in still images, and developing lines in moving images. The correspondences found between gestures and sonic features are discussed in relation to the physical movement required to produce the music. In addition, the circumstances in which correspondence is not found are analyzed and the extent to which the dynamic form of gesture is also influenced by the phrase as a whole is emphasized.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TCRJQNPC\\Pearson - 2013 - Gesture and the Sonic Event in Karnatak Music.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\X4RGCFSX\\3918.html},
  keywords = {gesture,movement,shape,South Indian music},
  langid = {english},
  number = {1}
}

@book{perez-pereiraLanguageDevelopmentSocial1999,
  title = {Language {{Development}} and {{Social Interaction}} in {{Blind Children}}},
  author = {Perez-Pereira, M. and Conti-Ramsden, G.},
  date = {1999},
  publisher = {{Psychology Press}},
  location = {{New York}}
}

@article{perezConsciousProcessingNarrative2020,
  title = {Conscious Processing of Narrative Stimuli Synchronizes Heart Rate between Individuals},
  author = {Perez, Pauline and Madsen, Jens and Banellis, Leah and Turker, Basak and Raimondo, Federico and Perlbarg, Vincent and Valente, Melanie and Nierat, Marie-Cecile and Puybasset, Louis and Naccache, Lionel and Similowski, Thomas and Cruse, Damian and Parra, Lucas C. and Sitt, Jacobo},
  date = {2020-05-28},
  journaltitle = {bioRxiv},
  pages = {2020.05.26.116079},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.05.26.116079},
  url = {https://www.biorxiv.org/content/10.1101/2020.05.26.116079v1},
  urldate = {2020-05-29},
  abstract = {{$<$}p{$>$}Heart rate has natural fluctuations that are typically ascribed to autonomic function. Recent evidence suggests that conscious processing can affect the timing of the heartbeat. We hypothesized that heart rate is modulated by conscious processing and therefore dependent on attentional focus. To test this we leverage the observation that neural processes can be synchronized between subjects by presenting an identical narrative stimulus. As predicted, we find significant inter-subject correlation of the heartbeat (ISC-HR) when subjects are presented with an auditory or audiovisual narrative. Consistent with the conscious processing hypothesis, we find that ISC-HR is reduced when subjects are distracted from the narrative, and that higher heart rate synchronization predicts better recall of the narrative. Finally, patients with disorders of consciousness who are listening to a story have lower ISC, as compared to healthy individuals, and that individual ISC-HR might predict a patients9 prognosis. We conclude that heart rate fluctuations are partially driven by conscious processing, depend on attentional state, and may represent a simple metric to assess conscious state in unresponsive patients.{$<$}/p{$>$}},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PZDGNWCX\\Perez et al. - 2020 - Conscious processing of narrative stimuli synchron.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\96LMU5TC\\2020.05.26.html},
  langid = {english}
}

@incollection{perrierMotorEquivalenceSpeech2015,
  title = {Motor {{Equivalence}} in {{Speech Production}}},
  booktitle = {The {{Handbook}} of {{Speech Production}}},
  author = {Perrier, Pascal and Fuchs, Susanne},
  date = {2015},
  pages = {223--247},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118584156.ch11},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118584156.ch11},
  urldate = {2019-08-08},
  abstract = {In this chapter, the concepts of “motor equivalence” and “degrees of freedom” are first described and illustrated with a few examples of motor tasks in general and of speech production tasks in particular. After this, methods used to investigate experimentally motor equivalence phenomena in speech production are presented. These are mainly paradigms that perturb the perception-action loop during on-going speech, either by limiting the degrees of freedom of the speech motor system, or by changing the physical conditions of speech production or by modifying the feedback information. Examples are provided for each of these approaches. Implications of these studies for a better understanding of speech production and its interactions with speech perception are presented in a final section of the chapter. Implications are mainly related to characterizing the mechanisms underlying interarticulatory coordination and to the analysis of speech production goals.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MVB5X94U\\Perrier and Fuchs - 2015 - Motor Equivalence in Speech Production.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AA9TEB2L\\9781118584156.html},
  isbn = {978-1-118-58415-6},
  keywords = {auditory feedback perturbations,motor equivalence,motor goals,motor perturbations,speech coordination,speech motor control},
  langid = {english}
}

@article{petroneRelationsSubglottalPressure2017,
  title = {Relations among Subglottal Pressure, Breathing, and Acoustic Parameters of Sentence-Level Prominence in {{German}}},
  author = {Petrone, Caterina and Fuchs, Susanne and Koenig, Laura L.},
  date = {2017-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {141},
  pages = {1715--1725},
  issn = {0001-4966},
  doi = {10.1121/1.4976073},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.4976073},
  urldate = {2019-05-05},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\73SSTAQS\\Petrone et al. - 2017 - Relations among subglottal pressure, breathing, an.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BCEL8HVU\\1.html},
  number = {3}
}

@article{pikaTakingTurnsBridging2018,
  title = {Taking Turns: Bridging the Gap between Human and Animal Communication},
  shorttitle = {Taking Turns},
  author = {Pika, Simone and Wilkinson, Ray and Kendrick, Kobin H. and Vernes, Sonja C.},
  date = {2018-06-13},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {285},
  pages = {20180598},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2018.0598},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2018.0598},
  urldate = {2020-05-18},
  abstract = {Language, humans’ most distinctive trait, still remains a ‘mystery’ for evolutionary theory. It is underpinned by a universal infrastructure—cooperative turn-taking—which has been suggested as an ancient mechanism bridging the existing gap between the articulate human species and their inarticulate primate cousins. However, we know remarkably little about turn-taking systems of non-human animals, and methodological confounds have often prevented meaningful cross-species comparisons. Thus, the extent to which cooperative turn-taking is uniquely human or represents a homologous and/or analogous trait is currently unknown. The present paper draws attention to this promising research avenue by providing an overview of the state of the art of turn-taking in four animal taxa—birds, mammals, insects and anurans. It concludes with a new comparative framework to spur more research into this research domain and to test which elements of the human turn-taking system are shared across species and taxa.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\W83M2UMT\\Pika et al. - 2018 - Taking turns bridging the gap between human and a.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\XCA63SWN\\rspb.2018.html},
  number = {1880}
}

@book{pikovskySynchronizationUniversalConcept2001,
  title = {Synchronization: {{A Universal Concept}} in {{Nonlinear Sciences}}},
  author = {Pikovsky, A and Kurths, J and Rosenblum, M},
  date = {2001},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, Mass}},
  url = {https://books.google.nl/books?hl=en&lr=&id=FuIv845q3QUC&oi=fnd&pg=PP1&dq=pikovsky+synchronization&ots=RM-tIgHgU7&sig=PHOyMBVgtLIao8rdkP5bCTVKwo0#v=onepage&q=pikovsky%20synchronization&f=false},
  urldate = {2019-07-11},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8IGPB5M9\\books.html}
}

@software{pinheiroNlmeLinearNonlinear2019,
  title = {Nlme: {{Linear}} and Nonlinear Mixed Effects Models},
  author = {Pinheiro, J. and Bates, D. and DebRoy, S. and Sarkar, D. and R Team, R. C.},
  date = {2019},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\F38LDCK6\\Cayuela - Modelos lineales mixtos en R.pdf}
}

@article{pisanskiCanBlindPersons2016,
  title = {Can Blind Persons Accurately Assess Body Size from the Voice?},
  author = {Pisanski, Katarzyna and Oleszkiewicz, Anna and Sorokowska, Agnieszka},
  date = {2016-04-30},
  journaltitle = {Biology Letters},
  shortjournal = {Biology Letters},
  volume = {12},
  pages = {20160063},
  doi = {10.1098/rsbl.2016.0063},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsbl.2016.0063},
  urldate = {2019-11-30},
  abstract = {Vocal tract resonances provide reliable information about a speaker's body size that human listeners use for biosocial judgements as well as speech recognition. Although humans can accurately assess men's relative body size from the voice alone, how this ability is acquired remains unknown. In this study, we test the prediction that accurate voice-based size estimation is possible without prior audiovisual experience linking low frequencies to large bodies. Ninety-one healthy congenitally or early blind, late blind and sighted adults (aged 20–65) participated in the study. On the basis of vowel sounds alone, participants assessed the relative body sizes of male pairs of varying heights. Accuracy of voice-based body size assessments significantly exceeded chance and did not differ among participants who were sighted, or congenitally blind or who had lost their sight later in life. Accuracy increased significantly with relative differences in physical height between men, suggesting that both blind and sighted participants used reliable vocal cues to size (i.e. vocal tract resonances). Our findings demonstrate that prior visual experience is not necessary for accurate body size estimation. This capacity, integral to both nonverbal communication and speech perception, may be present at birth or may generalize from broader cross-modal correspondences.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Z6GEIHL2\\Pisanski et al. - 2016 - Can blind persons accurately assess body size from.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\FR5XWK37\\rsbl.2016.html},
  number = {4}
}

@article{pisanskiReturnOzVoice2014,
  title = {Return to {{Oz}}: Voice Pitch Facilitates Assessments of Men's Body Size},
  shorttitle = {Return to {{Oz}}},
  author = {Pisanski, Katarzyna and Fraccaro, Paul J. and Tigue, Cara C. and O'Connor, Jillian J. M. and Feinberg, David R.},
  date = {2014-08},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {40},
  pages = {1316--1331},
  issn = {1939-1277},
  doi = {10.1037/a0036956},
  abstract = {Listeners associate low voice pitch (fundamental frequency and/or harmonics) and formants (vocal-tract resonances) with large body size. Although formants reliably predict size within sexes, pitch does not reliably predict size in groups of same-sex adults. Voice pitch has therefore long been hypothesized to confound within-sex size assessment. Here we performed a knockout test of this hypothesis using whispered and 3-formant sine-wave speech devoid of pitch. Listeners estimated the relative size of men with above-chance accuracy from voiced, whispered, and sine-wave speech. Critically, although men's pitch and physical height were unrelated, the accuracy of listeners' size assessments increased in the presence rather than absence of pitch. Size assessments based on relatively low pitch yielded particularly high accuracy (70\%-80\%). Results of Experiment 2 revealed that amplitude, noise, and signal degradation of unvoiced speech could not explain this effect; listeners readily perceived formant shifts in manipulated whispered speech. Rather, in Experiment 3, we show that the denser harmonic spectrum provided by low pitch allowed for better resolution of formants, aiding formant-based size assessment. These findings demonstrate that pitch does not confuse body size assessment as has been previously suggested, but instead facilitates accurate size assessment by providing a carrier signal for vocal-tract resonances.},
  eprint = {24933617},
  eprinttype = {pmid},
  keywords = {Adolescent,Adult,Body Size,Female,Humans,Male,Pitch Perception,Speech Perception,Voice,Young Adult},
  langid = {english},
  number = {4}
}

@article{pisanskiVoiceModulationWindow2016,
  title = {Voice Modulation: {{A}} Window into the Origins of Human Vocal Control?},
  shorttitle = {Voice {{Modulation}}},
  author = {Pisanski, Katarzyna and Cartei, Valentina and McGettigan, Carolyn and Raine, Jordan and Reby, David},
  date = {2016-04-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {20},
  pages = {304--318},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2016.01.002},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(16)00020-6},
  urldate = {2019-10-17},
  eprint = {26857619},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CIL5832Q\\S1364-6613(16)00020-6.html},
  keywords = {formant scaling,fundamental frequency,nonverbal vocal communication,source–filter theory,speech evolution},
  langid = {english},
  number = {4}
}

@online{PitchAccentTrajectories,
  title = {Pitch {{Accent Trajectories}} across {{Different Conditions}} of {{Visibility}} and {{Information Structure}} - {{Evidence}} from {{Spontaneous Dyadic Interaction}}},
  url = {https://pub.uni-bielefeld.de/record/2936369},
  urldate = {2019-08-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3RXUK7T3\\2936369.html}
}

@article{poeppelSpeechRhythmsTheir2020,
  title = {Speech Rhythms and Their Neural Foundations},
  author = {Poeppel, David and Assaneo, M. Florencia},
  date = {2020-05-06},
  journaltitle = {Nature Reviews Neuroscience},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0304-4},
  url = {http://www.nature.com/articles/s41583-020-0304-4},
  urldate = {2020-05-08},
  abstract = {The recognition of spoken language has typically been studied by focusing on either words or their constituent elements (for example, low-level features or phonemes). More recently{$\mkern1mu$}, the ‘temporal mesoscale’ of speech has been explored, specifically regularities in the envelope of the acoustic signal that correlate with syllabic information and that play a central role in production and perception processes. The temporal structure of speech at this scale is remarkably stable across languages, with a preferred range of rhythmicity of 2– 8\,Hz. Importantly, this rhythmicity is required by the processes underlying the construction of intelligible speech. A lot of current work focuses on audio-m otor interactions in speech, highlighting behavioural and neural evidence that demonstrates how properties of perceptual and motor systems, and their relation, can underlie the mesoscale speech rhythms. The data invite the hypothesis that the speech motor cortex is best modelled as a neural oscillator, a conjecture that aligns well with current proposals highlighting the fundamental role of neural oscillations in perception and cognition. The findings also show motor theories (of speech) in a different light, placing new mechanistic constraints on accounts of the action–perception interface.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VKT6RFNX\\Poeppel and Assaneo - 2020 - Speech rhythms and their neural foundations.pdf},
  langid = {english}
}

@article{poeppelSpeechRhythmsTheir2020a,
  title = {Speech Rhythms and Their Neural Foundations},
  author = {Poeppel, David and Assaneo, M. Florencia},
  date = {2020-05-06},
  journaltitle = {Nature Reviews Neuroscience},
  pages = {1--13},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-020-0304-4},
  url = {https://www.nature.com/articles/s41583-020-0304-4},
  urldate = {2020-05-08},
  abstract = {Syllables play a central role in speech production and perception. In this Review, Poeppel and Assaneo outline how a simple biophysical model of the speech production system as an oscillator explains the remarkably stable rhythmic structure of spoken language.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\I7MIA2RQ\\Poeppel and Assaneo - 2020 - Speech rhythms and their neural foundations.pdf},
  langid = {english}
}

@article{portLanguageSocialInstitution2010,
  title = {Language as a {{Social Institution}}: {{Why Phonemes}} and {{Words Do Not Live}} in the {{Brain}}},
  shorttitle = {Language as a {{Social Institution}}},
  author = {Port, Robert F.},
  date = {2010-10-29},
  journaltitle = {Ecological Psychology},
  volume = {22},
  pages = {304--326},
  issn = {1040-7413},
  doi = {10.1080/10407413.2010.517122},
  url = {https://doi.org/10.1080/10407413.2010.517122},
  urldate = {2020-02-25},
  abstract = {It is proposed that a language, in a rich, high-dimensional form, is part of the cultural environment of the child learner. A language is the product of a community of speakers who develop its phonological, lexical, and phrasal patterns over many generations. The language emerges from the joint behavior of many agents in the community acting as a complex adaptive system. Its form only roughly approximates the low-dimensional structures that our traditional phonology highlights. Those who study spoken language have attempted to approach it as an internal knowledge structure rather than as a communal institution or set of conventions for coordination of activity. We also find it very difficult to avoid being deceived into seeing language in the form employed by our writing system as letters, words, and sentences. But our writing system is a further set of conventions that approximate the high-dimensional spoken language in a consistent and regularized graphical form.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\6R3DRNQ9\\Port - 2010 - Language as a Social Institution Why Phonemes and.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AZJU3YBG\\10407413.2010.html},
  number = {4}
}

@article{pouwAcousticInformationUpper2020a,
  title = {Acoustic Information about Upper Limb Movement in Voicing},
  author = {Pouw, W. and Paxton, A. and Harrison, S. J. and Dixon, J. A.},
  date = {2020-05-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2004163117},
  url = {https://www.pnas.org/content/early/2020/05/05/2004163117},
  urldate = {2020-05-26},
  abstract = {We show that the human voice has complex acoustic qualities that are directly coupled to peripheral musculoskeletal tensioning of the body, such as subtle wrist movements. In this study, human vocalizers produced a steady-state vocalization while rhythmically moving the wrist or the arm at different tempos. Although listeners could only hear and not see the vocalizer, they were able to completely synchronize their own rhythmic wrist or arm movement with the movement of the vocalizer which they perceived in the voice acoustics. This study corroborates recent evidence suggesting that the human voice is constrained by bodily tensioning affecting the respiratory–vocal system. The current results show that the human voice contains a bodily imprint that is directly informative for the interpersonal perception of another’s dynamic physical states.},
  eprint = {32393618},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FXE7ICIZ\\Pouw et al. - 2020 - Acoustic information about upper limb movement in .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\B3ILSP8W\\2004163117.html},
  keywords = {hand gesture,interpersonal synchrony,motion tracking,vocalization acoustics},
  langid = {english}
}

@inproceedings{pouwAcousticSpecificationUpper2019,
  title = {Acoustic Specification of Upper Limb Movement in Voicing},
  booktitle = {Proceedings of the 6th Meeting of {{Gesture}} and {{Speech}} in {{Interaction}}},
  author = {Pouw, W. and Paxton, A. and Harrison, S. J. and Dixon, J. A.},
  date = {2019},
  pages = {75--80},
  publisher = {{Universitaetsbibliothek Paderborn}},
  location = {{Paderborn, Germany}},
  doi = {10.17619/UNIPB/1-812},
  url = {https://psyarxiv.com/5rcdu/},
  urldate = {2019-05-05},
  abstract = {Hand gestures communicate complex information to listeners through the visual information created by movement. In a recent study, however, we found that there are also direct biomechanical effects of high-impetus upper limb movement on voice acoustics. Here we explored whether listeners could detect information about movement in voice acoustics of another person. In this exploratory study, participants listened to a recording of a vocalizer who was simultaneously producing low- (wrist movement) or high- (arm movement) impetus movements at three different tempos. Listeners were asked to synchronize their own movement (wrist or arm movement) with that of the vocalizer. Listeners coupled with the frequency of the vocalizer arm (but not wrist) movements, and showed phase-coupling with vocalizer arm (but not wrist) movements. However, we found that this synchronization occurred regardless of whether the listener was moving their wrist or arm. This study shows that, in principle, there is acoustic specification of arm movements in voicing, but not wrist movements. These results, if replicated, provide novel insight into the possible interpersonal functions of gesture acoustics, which may lie in communicating bodily states. The second part of the paper is a pre-registration for the confirmatory study that will assess the research question in a larger sample with more diverse and naturalistic stimuli.},
  eventtitle = {{{GESPIN}} 6},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\B75VYI5G\\Pouw et al. - 2019 - Acoustic specification of upper limb movement in v.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BUY5LQSE\\5rcdu.html}
}

@article{pouwEnergyFlowsGesturespeech2019,
  title = {Energy Flows in Gesture-Speech Physics: {{Exploratory}} Findings and Pre-Registration of Confirmatory Analysis},
  shorttitle = {Energy Flows in Gesture-Speech Physics},
  author = {Pouw, W. and Harrison, S. J. and Esteve-Gibert, N. and Dixon, J. A.},
  date = {2019},
  doi = {10.31234/osf.io/c7456},
  url = {https://psyarxiv.com/c7456/},
  urldate = {2019-10-13},
  abstract = {A well-known phenomenon of multimodal language is the synchronous coupling of prosodic contours in speech with salient kinematic changes in co-speech hand-gesture motions. Invariably, such coupling has been rendered by psychologists to require a dedicated neural-cognitive mechanism preplanning speech and gesture trajectories. Recently, in a continuous vocalization task, it was found that acoustic peaks unintentionally appear in vocalizations when gesture motions reach peaks in physical impetus, suggesting a biomechanical basis for gesture-speech synchrony (Pouw, Harrison, \& Dixon, 2019). However, from this rudimentary study it is still difficult to draw strong conclusions about gesture-speech dynamics in (more) complex speech and the precise biomechanical nature of these effects. Here we assess how the timing of physical impetus of a gesture relates to its effect on acoustic parameters of mono-syllabic consonant-vowel (CV) vocalization(/pa/). Furthermore, we assess how chest-wall kinematics is affected by gesturing, and whether this modulates the effect of gestures on acoustics. In the current exploratory analysis, we analyze a subset (N = 4) of an already collected dataset (N = 36), which serves as the basis for a pre-registration of the confirmatory analyses yet to be completed. Here we provide exploratory evidence that gestures affect acoustics (amplitude envelope and F0) as well as chest-wall kinematics during mono-syllabic vocalizations. These effects are more extreme when a gesture’s peak impetus occurs closer to the center of the vowel vocalization event. If the current findings can be replicated in confirmatory fashion, there is a more compelling case to be made that gesture-speech physics is important facet of multimodal synchrony.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\J4JHP6YV\\Pouw et al. - 2019 - Energy flows in gesture-speech physics Explorator.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\RTTD5PQQ\\c7456.html}
}

@article{pouwEntrainmentModulationGesture2019,
  title = {Entrainment and Modulation of Gesture–Speech Synchrony under Delayed Auditory Feedback},
  author = {Pouw, W. and Dixon, J. A.},
  date = {2019},
  journaltitle = {Cognitive Science},
  volume = {43},
  pages = {e12721},
  issn = {1551-6709},
  doi = {10.1111/cogs.12721},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12721},
  urldate = {2019-04-16},
  abstract = {Gesture–speech synchrony re-stabilizes when hand movement or speech is disrupted by a delayed feedback manipulation, suggesting strong bidirectional coupling between gesture and speech. Yet it has also been argued from case studies in perceptual–motor pathology that hand gestures are a special kind of action that does not require closed-loop re-afferent feedback to maintain synchrony with speech. In the current pre-registered within-subject study, we used motion tracking to conceptually replicate McNeill's () classic study on gesture–speech synchrony under normal and 150 ms delayed auditory feedback of speech conditions (NO DAF vs. DAF). Consistent with, and extending McNeill's original results, we obtain evidence that (a) gesture-speech synchrony is more stable under DAF versus NO DAF (i.e., increased coupling effect), (b) that gesture and speech variably entrain to the external auditory delay as indicated by a consistent shift in gesture-speech synchrony offsets (i.e., entrainment effect), and (c) that the coupling effect and the entrainment effect are co-dependent. We suggest, therefore, that gesture–speech synchrony provides a way for the cognitive system to stabilize rhythmic activity under interfering conditions.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5VSDHX34\\Pouw and Dixon - 2019 - Entrainment and Modulation of Gesture–Speech Synch.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8B7CACLJ\\cogs.html},
  keywords = {Cross-wavelet analysis,Delayed auditory feedback,Hand-gesture,Speech,Synchrony},
  langid = {english},
  number = {3}
}

@article{pouwGestureNetworksIntroducing2019,
  title = {Gesture Networks: {{Introducing}} Dynamic Time Warping and Network Analysis for the Kinematic Study of Gesture Ensembles},
  author = {Pouw, W. and Dixon, J. A.},
  date = {2019},
  journaltitle = {Discourse Processes},
  doi = {10.1080/0163853X.2019.1678967},
  url = {https://psyarxiv.com/hbnt2},
  urldate = {2019-08-07},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\JYA5DTB6\\hbnt2.html}
}

@article{pouwGesturespeechPhysicsBiomechanical2019,
  title = {Gesture-Speech Physics: {{The}} Biomechanical Basis of the Emergence of Gesture-Speech Synchrony},
  author = {Pouw, W. and Harrison, S. H. and Dixon, J. A.},
  date = {2019},
  journaltitle = {Journal of Experimental Psychology: General},
  doi = {10.1037/xge0000646},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\455PZW9L\\default.html}
}

@software{pouwMaterialsTutorialGespin20192019,
  title = {Materials {{Tutorial Gespin2019}} - {{Using}} Video-Based Motion Tracking to Quantify Speech-Gesture Synchrony},
  author = {Pouw, W. and Trujillo, J. P.},
  date = {2019},
  url = {10.17605/OSF.IO/RXB8J}
}

@article{pouwPhysicalBasisGesturespeech2018,
  title = {The Physical Basis of Gesture-Speech Synchrony: {{Exploratory}} Study and Pre-Registration},
  author = {Pouw, W. and Harrison, S. and Dixon, J.},
  date = {2018},
  journaltitle = {PsyArXiv},
  url = {https://psyarxiv.com/9fzsv},
  urldate = {2019-08-09},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QQU48XH5\\9fzsv.html}
}

@article{pouwQuantificationGesturespeechSynchrony2019,
  title = {The Quantification of Gesture-Speech Synchrony: {{A}} Tutorial and Validation of Multi-Modal Data Acquisition Using Device-Based and Video-Based Motion Tracking},
  author = {Pouw, W. and Trujillo, J. and Dixon, J.A.},
  date = {2019},
  journaltitle = {Behavior Research Methods},
  doi = {10.3758/s13428-019-01271-9},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TAEEFQA8\\jm3hk.html}
}

@inproceedings{pouwQuantifyingGesturespeechSynchrony2019,
  title = {Quantifying Gesture-Speech Synchrony},
  booktitle = {Proceedings of the 6th Meeting of {{Gesture}} and {{Speech}} in {{Interaction}}},
  author = {Pouw, W. and Dixon, J. A.},
  date = {2019},
  pages = {68--74},
  publisher = {{Universitaetsbibliothek Paderborn}},
  location = {{Paderborn}},
  doi = {10.17619/UNIPB/1-812},
  eventtitle = {{{GESPIN}} 6}
}

@article{pouwStabilizingSpeechProduction2018,
  title = {Stabilizing {{Speech Production}} through {{Gesture}}-{{Speech Coordination}}},
  author = {Pouw, W. and De Jonge-Hoekstra, L. and Dixon, J.},
  date = {2018-12-17T16:48:41.054Z},
  doi = {10.31234/osf.io/arzne},
  url = {https://psyarxiv.com/arzne/},
  urldate = {2019-09-20},
  abstract = {Hand-gestures are seamlessly coordinated with speech. Yet, there is only anecdotal support for gestures’ functional role in speech production. Here we explore temporal aspects of speech production when people use hand gesture. We performed exploratory analyses with a naturalistic German-speaking sample from The Bielefeld Speech and Gesture Alignment Corpus (SaGA), which consisted of 67 minutes of narration data and over 500 gesture events (N = 6). We found that the rhythmic timing of speech (defined as the mean and standard deviations of speech onset intervals) is highly correlated with the likelihood of gesturing. Furthermore, we utilized deep learning methods to track gesture motion, and extracted the amplitude envelope of speech, so as to gauge the degree of (continuous) gesture-speech synchrony. We then performed a continuous time-series analysis (recurrence quantification analysis; RQA) to index how temporal properties of speech change when gesture and speech are more or less synchronized. Our analyses revealed that when gesture and speech were more synchronized, the temporal structure of speech was more ordered and less complex, as indexed by classic measures of dynamic temporal stability (e.g., Entropy, Ratio of \%Determinism/Recurrence). We suggest that a fundamental gesture-speech relation is rooted in entrainment, which yields stability in the temporal structure of speech.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\USEMMUQM\\Pouw et al. - 2018 - Stabilizing Speech Production through Gesture-Spee.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\5LEH8GKW\\arzne.html}
}

@article{priceMotionDoesMatter2006,
  title = {Motion Does Matter: An Examination of Speech-Based Text Entry on the Move},
  shorttitle = {Motion Does Matter},
  author = {Price, Kathleen J. and Lin, Min and Feng, Jinjuan and Goldman, Rich and Sears, Andrew and Jacko, Julie A.},
  date = {2006-03-01},
  journaltitle = {Universal Access in the Information Society},
  shortjournal = {Univ Access Inf Soc},
  volume = {4},
  pages = {246--257},
  issn = {1615-5297},
  doi = {10.1007/s10209-005-0006-8},
  url = {https://doi.org/10.1007/s10209-005-0006-8},
  urldate = {2019-10-17},
  abstract = {Desktop interaction solutions are often inappropriate for mobile devices due to small screen size and portability needs. Speech recognition can improve interactions by providing a relatively hands-free solution that can be used in various situations. While mobile systems are designed to be transportable, few have examined the effects of motion on mobile interactions. This paper investigates the effect of motion on automatic speech recognition (ASR) input for mobile devices. Speech recognition error rates (RER) have been examined with subjects walking or seated, while performing text input tasks and the effect of ASR enrollment conditions on RER. The obtained results suggest changes in user training of ASR systems for mobile and seated usage.},
  keywords = {Automatic speech recognition,Mobile device,Motion,Speech recognition errors,Speech-based data entry},
  langid = {english},
  number = {3}
}

@book{prietoDevelopmentProsodyFirst2018,
  title = {The {{Development}} of {{Prosody}} in {{First Language Acquisition}} ({{Trends}} in {{Language Acquisition Research}})},
  author = {Prieto, P. and Esteve-Gibert, N.},
  date = {2018},
  volume = {23},
  publisher = {{John Benjamins}},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EB53FMQX\\ref=sr_1_1.html},
  series = {Trends in {{Language Acquisition Research}}}
}

@article{prieurOriginsGesturesLanguage2019,
  title = {The Origins of Gestures and Language: History, Current Advances and Proposed Theories},
  shorttitle = {The Origins of Gestures and Language},
  author = {Prieur, Jacques and Barbu, Stéphanie and Blois-Heulin, Catherine and Lemasson, Alban},
  date = {2019-12-18},
  journaltitle = {Biological Reviews of the Cambridge Philosophical Society},
  shortjournal = {Biol Rev Camb Philos Soc},
  issn = {1469-185X},
  doi = {10.1111/brv.12576},
  abstract = {Investigating in depth the mechanisms underlying human and non-human primate intentional communication systems (involving gestures, vocalisations, facial expressions and eye behaviours) can shed light on the evolutionary roots of language. Reports on non-human primates, particularly great apes, suggest that gestural communication would have been a crucial prerequisite for the emergence of language, mainly based on the evidence of large communication repertoires and their associated multifaceted nature of intentionality that are key properties of language. Such research fuels important debates on the origins of gestures and language. We review here three non-mutually exclusive processes that can explain mainly great apes' gestural acquisition and development: phylogenetic ritualisation, ontogenetic ritualisation, and learning via social negotiation. We hypothesise the following scenario for the evolutionary origins of gestures: gestures would have appeared gradually through evolution via signal ritualisation following the principle of derived activities, with the key involvement of emotional expression and processing. The increasing level of complexity of socioecological lifestyles and associated daily manipulative activities might then have enabled the acquisition and development of different interactional strategies throughout the life cycle. Many studies support a multimodal origin of language. However, we stress that the origins of language are not only multimodal, but more broadly multicausal. We propose a multicausal theory of language origins which better explains current findings. It postulates that primates' communicative signalling is a complex trait continually shaped by a cost-benefit trade-off of signal production and processing of interactants in relation to four closely interlinked categories of evolutionary and life cycle factors: species, individual and context-related characteristics as well as behaviour and its characteristics. We conclude by suggesting directions for future research to improve our understanding of the evolutionary roots of gestures and language.},
  eprint = {31854102},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7AZS82NH\\Prieur et al. - 2019 - The origins of gestures and language history, cur.pdf},
  keywords = {communication behaviours,emotional and intentional signalling,language evolution,multifactoriality,multimodality},
  langid = {english}
}

@article{prieurOriginsGesturesLanguage2019a,
  title = {The Origins of Gestures and Language: History, Current Advances and Proposed Theories},
  shorttitle = {The Origins of Gestures and Language},
  author = {Prieur, Jacques and Barbu, Stéphanie and Blois-Heulin, Catherine and Lemasson, Alban},
  date = {2019-12-18},
  journaltitle = {Biological Reviews of the Cambridge Philosophical Society},
  shortjournal = {Biol Rev Camb Philos Soc},
  issn = {1469-185X},
  doi = {10.1111/brv.12576},
  abstract = {Investigating in depth the mechanisms underlying human and non-human primate intentional communication systems (involving gestures, vocalisations, facial expressions and eye behaviours) can shed light on the evolutionary roots of language. Reports on non-human primates, particularly great apes, suggest that gestural communication would have been a crucial prerequisite for the emergence of language, mainly based on the evidence of large communication repertoires and their associated multifaceted nature of intentionality that are key properties of language. Such research fuels important debates on the origins of gestures and language. We review here three non-mutually exclusive processes that can explain mainly great apes' gestural acquisition and development: phylogenetic ritualisation, ontogenetic ritualisation, and learning via social negotiation. We hypothesise the following scenario for the evolutionary origins of gestures: gestures would have appeared gradually through evolution via signal ritualisation following the principle of derived activities, with the key involvement of emotional expression and processing. The increasing level of complexity of socioecological lifestyles and associated daily manipulative activities might then have enabled the acquisition and development of different interactional strategies throughout the life cycle. Many studies support a multimodal origin of language. However, we stress that the origins of language are not only multimodal, but more broadly multicausal. We propose a multicausal theory of language origins which better explains current findings. It postulates that primates' communicative signalling is a complex trait continually shaped by a cost-benefit trade-off of signal production and processing of interactants in relation to four closely interlinked categories of evolutionary and life cycle factors: species, individual and context-related characteristics as well as behaviour and its characteristics. We conclude by suggesting directions for future research to improve our understanding of the evolutionary roots of gestures and language.},
  eprint = {31854102},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XDV6ZT32\\Prieur et al. - 2019 - The origins of gestures and language history, cur.pdf},
  keywords = {communication behaviours,emotional and intentional signalling,language evolution,multifactoriality,multimodality},
  langid = {english}
}

@article{profetaBernsteinLevelsMovement2018,
  title = {Bernstein’s Levels of Movement Construction: {{A}} Contemporary Perspective},
  shorttitle = {Bernstein’s Levels of Movement Construction},
  author = {Profeta, Vitor L. S. and Turvey, Michael T.},
  date = {2018-02-01},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {57},
  pages = {111--133},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2017.11.013},
  url = {http://www.sciencedirect.com/science/article/pii/S0167945717305717},
  urldate = {2020-03-31},
  abstract = {Explanation of how goal-directed movements are made manifest is the ultimate aim of the field classically referred to as “motor control”. Essential to the sought-after explanation is comprehension of the supporting functional architecture. Seven decades ago, the Russian physiologist and movement scientist Nikolai A. Bernstein proposed a hierarchical model to explain the construction of movements. In his model, the levels of the hierarchy share a common language (i.e., they are commensurate) and perform complementing functions to bring about dexterous movements. The science of the control and coordination of movement in the phylum Craniata has made considerable progress in the intervening seven decades. The contemporary body of knowledge about each of Bernstein’s hypothesized functional levels is both more detailed and more sophisticated. A natural consequence of this progress, however, is the relatively independent theoretical development of a given level from the other levels. In this essay, we revisit each level of Bernstein’s hierarchy from the joint perspectives of (a) the ecological approach to perception-action and (b) dynamical systems theory. We review a substantial and relevant body of literature produced in different areas of study that are accommodated by this ecological-dynamical version of Bernstein’s levels. Implications for the control and coordination of movement and the challenges to producing a unified theory are discussed.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BWJJTZXU\\S0167945717305717.html},
  keywords = {Bernstein,Control,Coordination,Movement construction,Synergy},
  langid = {english}
}

@article{provineLaughingTicklingEvolution2004,
  title = {Laughing, {{Tickling}}, and the {{Evolution}} of {{Speech}} and {{Self}}},
  author = {Provine, Robert R.},
  date = {2004-12},
  journaltitle = {Current Directions in Psychological Science},
  volume = {13},
  pages = {215--218},
  issn = {0963-7214, 1467-8721},
  doi = {10.1111/j.0963-7214.2004.00311.x},
  url = {http://journals.sagepub.com/doi/10.1111/j.0963-7214.2004.00311.x},
  urldate = {2020-02-25},
  abstract = {Laughter is an instinctive, contagious, stereotyped, unconsciously controlled, social play vocalization that is unusual in solitary settings. Laughter punctuates speech and is not typically humor related, speakers often laugh more often than their audience, and male speakers are the best laugh getters. Laughter evolved from the labored breathing of physical play, with the characteristic ‘‘pant-pant’’ laugh of chimpanzees and derivative ‘‘ha-ha’’ of humans signaling (‘‘ritualizing’’) its rowdy origin. Laughter reveals that breath control is why humans can speak and chimpanzees cannot. The evolution of bipedality in human ancestors freed the thorax of its support role in quadrupedal locomotion, a critical step in uncoupling breathing from running, providing humans with the flexible breath control necessary for speech and our characteristic laugh. Tickle, an ancient laughter stimulus, is a means of communication between preverbal infants and mothers, and between friends, family, and lovers. Because you cannot tickle yourself, tickle involves a neurological self /nonself discrimination, providing the most primitive social scenario.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\M8LTM7W7\\Provine - 2004 - Laughing, Tickling, and the Evolution of Speech an.pdf},
  langid = {english},
  number = {6}
}

@inproceedings{quinnEvolvingCommunicationDedicated2001,
  title = {Evolving Communication without Dedicated Communication Channels. {{Paper}} Presented at The},
  booktitle = {Advances in {{Artificial Life}}: 6th {{European Conference}} on {{Artificial Life}} ({{ECAL}} 2001},
  author = {Quinn, Matt},
  date = {2001},
  abstract = {Abstract. Artificial Life models have consistently implemented com-munication as an exchange of signals over dedicated and functionally isolated channels. I argue that such a feature prevents models from pro-viding a satisfactory account of the origins of communication and present a model in which there are no dedicated channels. Agents controlled by neural networks and equipped with proximity sensors and wheels are pre-sented with a co-ordinated movement task. It is observed that functional, but non-communicative, behaviours which evolve in the early stages of the simulation both make possible, and form the basis of, the commu-nicative behaviour which subsequently evolves. 1},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\WSJJZTJ3\\Quinn - 2001 - Evolving communication without dedicated communica.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\NCBPJ9ET\\summary.html}
}

@article{raineHumanRoarsCommunicate2019,
  title = {Human Roars Communicate Upper-Body Strength More Effectively than Do Screams or Aggressive and Distressed Speech},
  author = {Raine, Jordan and Pisanski, Katarzyna and Bond, Rod and Simner, Julia and Reby, David},
  date = {2019-03-04},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  pages = {e0213034},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0213034},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0213034},
  urldate = {2019-10-17},
  abstract = {Despite widespread evidence that nonverbal components of human speech (e.g., voice pitch) communicate information about physical attributes of vocalizers and that listeners can judge traits such as strength and body size from speech, few studies have examined the communicative functions of human nonverbal vocalizations (such as roars, screams, grunts and laughs). Critically, no previous study has yet to examine the acoustic correlates of strength in nonverbal vocalisations, including roars, nor identified reliable vocal cues to strength in human speech. In addition to being less acoustically constrained than articulated speech, agonistic nonverbal vocalizations function primarily to express motivation and emotion, such as threat, and may therefore communicate strength and body size more effectively than speech. Here, we investigated acoustic cues to strength and size in roars compared to screams and speech sentences produced in both aggressive and distress contexts. Using playback experiments, we then tested whether listeners can reliably infer a vocalizer’s actual strength and height from roars, screams, and valenced speech equivalents, and which acoustic features predicted listeners’ judgments. While there were no consistent acoustic cues to strength in any vocal stimuli, listeners accurately judged inter-individual differences in strength, and did so most effectively from aggressive voice stimuli (roars and aggressive speech). In addition, listeners more accurately judged strength from roars than from aggressive speech. In contrast, listeners’ judgments of height were most accurate for speech stimuli. These results support the prediction that vocalizers maximize impressions of physical strength in aggressive compared to distress contexts, and that inter-individual variation in strength may only be honestly communicated in vocalizations that function to communicate threat, particularly roars. Thus, in continuity with nonhuman mammals, the acoustic structure of human aggressive roars may have been selected to communicate, and to some extent exaggerate, functional cues to physical formidability.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MPXMFGRN\\Raine et al. - 2019 - Human roars communicate upper-body strength more e.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BJ2YTZ3P\\article.html},
  keywords = {Acoustics,Bioacoustics,Hand strength,Physiological parameters,Speech,Speech signal processing,Verbal communication,Vocalization},
  langid = {english},
  number = {3}
}

@article{ramusCorrelatesLinguisticRhythm2000,
  title = {Correlates of Linguistic Rhythm in the Speech Signal},
  author = {Ramus, Franck and Nespor, Marina and Mehler, Jacques},
  date = {2000-04-14},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {75},
  pages = {AD3-AD30},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(00)00101-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027700001013},
  urldate = {2019-09-28},
  abstract = {Spoken languages have been classified by linguists according to their rhythmic properties, and psycholinguists have relied on this classification to account for infants’ capacity to discriminate languages. Although researchers have measured many speech signal properties, they have failed to identify reliable acoustic characteristics for language classes. This paper presents instrumental measurements based on a consonant/vowel segmentation for eight languages. The measurements suggest that intuitive rhythm types reflect specific phonological properties, which in turn are signaled by the acoustic/phonetic properties of speech. The data support the notion of rhythm classes and also allow the simulation of infant language discrimination, consistent with the hypothesis that newborns rely on a coarse segmentation of speech. A hypothesis is proposed regarding the role of rhythm perception in language acquisition.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PL9745Z6\\Ramus et al. - 2000 - Correlates of linguistic rhythm in the speech sign.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8NJPFQUW\\S0010027700001013.html},
  keywords = {Language acquisition,Language discrimination,Phonological bootstrapping,Prosody,Speech rhythm,Syllable structure},
  number = {1}
}

@article{ravignaniEvolutionSpeechRhythm2018,
  title = {Evolution of Speech Rhythm: A Cross-Species Perspective},
  shorttitle = {Evolution of Speech Rhythm},
  author = {Ravignani, Andrea and Dalla Bella, Simone and Falk, Simone and Kello, Chris and Noriega, Florencia and Kotz, Sonja},
  date = {2018},
  doi = {10.7287/peerj.preprints.27539v1},
  url = {https://peerj.com/preprints/27539},
  urldate = {2019-05-07},
  abstract = {Cognition and communication, at the core of human speech rhythm, do not leave a fossil record. However, if the purpose is to understand the origin and evolution of speech rhythm, alternative methods are available. A powerful tool is comparative approach: studying the presence or absence of cognitive/behavioral traits in other species, drawing conclusions on which traits are shared between species, and which are recent human inventions. Here we apply this approach to traits related to human speech rhythm. Many species exhibit temporal structure in their vocalizations but little is known about the range of rhythmic structures perceived and produced, their biological and developmental bases, and communicative functions. We review the literatures on human and non-human studies of rhythm in speech and animal vocalizations to survey similarities and differences. We report important links between vocal perception and motor coordination, and the differentiation of rhythm based on hierarchical temporal structure. We extend this review to quantitative techniques useful for computing rhythmic structure in acoustic sequences and hence facilitating cross-species research. While still far from a full comparative cross-species perspective of speech rhythm, we are closer to fitting missing pieces of the puzzle.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EYVX4IU5\\Ravignani et al. - Evolution of speech rhythm a cross-species perspe.pdf},
  langid = {english}
}

@article{ravignaniInteractiveRhythmsSpecies2019,
  title = {Interactive Rhythms across Species: The Evolutionary Biology of Animal Chorusing and Turn-Taking},
  shorttitle = {Interactive Rhythms across Species},
  author = {Ravignani, Andrea and Verga, Laura and Greenfield, Michael D.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  volume = {1453},
  pages = {12--21},
  issn = {1749-6632},
  doi = {10.1111/nyas.14230},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14230},
  urldate = {2019-10-13},
  abstract = {The study of human language is progressively moving toward comparative and interactive frameworks, extending the concept of turn-taking to animal communication. While such an endeavor will help us understand the interactive origins of language, any theoretical account for cross-species turn-taking should consider three key points. First, animal turn-taking must incorporate biological studies on animal chorusing, namely how different species coordinate their signals over time. Second, while concepts employed in human communication and turn-taking, such as intentionality, are still debated in animal behavior, lower level mechanisms with clear neurobiological bases can explain much of animal interactive behavior. Third, social behavior, interactivity, and cooperation can be orthogonal, and the alternation of animal signals need not be cooperative. Considering turn-taking a subset of chorusing in the rhythmic dimension may avoid overinterpretation and enhance the comparability of future empirical work.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZITD6ZMI\\Ravignani et al. - 2019 - Interactive rhythms across species the evolutiona.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ID8J4AYB\\nyas.html},
  keywords = {bioacoustics,cooperation,interaction,language evolution,speech rhythm,synchrony},
  langid = {english},
  number = {1}
}

@article{ravignaniMusicalEvolutionLab2016,
  title = {Musical Evolution in the Lab Exhibits Rhythmic Universals},
  author = {Ravignani, Andrea and Delgado, Tania and Kirby, Simon},
  date = {2016-12-19},
  journaltitle = {Nature Human Behaviour},
  volume = {1},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0007},
  url = {https://www.nature.com/articles/s41562-016-0007},
  urldate = {2020-03-25},
  abstract = {The authors asked human participants to listen to and imitate randomly generated drumming sequences from each other. Participants turned initially random sequences into rhythmically structured patterns that are characterized by all six statistical universals found in world music.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\UGKMX294\\Ravignani et al. - 2016 - Musical evolution in the lab exhibits rhythmic uni.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\IT6U5FD7\\s41562-016-0007.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{ravignaniRhythmSpeechAnimal2019,
  title = {Rhythm in Speech and Animal Vocalizations: A Cross‐species Perspective},
  author = {Ravignani, A and Dalla Bella, S and Falk, S and Kello, C. T. and Noriega, F. and Kotz, S.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  doi = {10.1111/nyas.14166},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/epdf/10.1111/nyas.14166},
  urldate = {2019-08-30},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VGFKQD2P\\nyas.html}
}

@article{ravivLargerCommunitiesCreate2019,
  title = {Larger Communities Create More Systematic Languages},
  author = {Raviv, Limor and Meyer, Antje and Lev-Ari, Shiri},
  date = {2019-07-24},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {286},
  pages = {20191262},
  doi = {10.1098/rspb.2019.1262},
  url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspb.2019.1262},
  urldate = {2019-12-11},
  abstract = {Understanding worldwide patterns of language diversity has long been a goal for evolutionary scientists, linguists and philosophers. Research over the past decade has suggested that linguistic diversity may result from differences in the social environments in which languages evolve. Specifically, recent work found that languages spoken in larger communities typically have more systematic grammatical structures. However, in the real world, community size is confounded with other social factors such as network structure and the number of second languages learners in the community, and it is often assumed that linguistic simplification is driven by these factors instead. Here, we show that in contrast to previous assumptions, community size has a unique and important influence on linguistic structure. We experimentally examine the live formation of new languages created in the laboratory by small and larger groups, and find that larger groups of interacting participants develop more systematic languages over time, and do so faster and more consistently than small groups. Small groups also vary more in their linguistic behaviours, suggesting that small communities are more vulnerable to drift. These results show that community size predicts patterns of language diversity, and suggest that an increase in community size might have contributed to language evolution.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QA5QVPTR\\rspb.2019.html},
  number = {1907}
}

@movie{rawlenceManWhoLost1997,
  title = {The {{Man Who Lost His Body}}},
  editor = {Rawlence, Chris},
  year = {1997, 16th October},
  publisher = {{BBC Horizon}},
  editortype = {director},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3RE6SZCW\\the-man-who-lost-his-body.html},
  langid = {english}
}

@book{redfordHandbookSpeechProduction2015,
  title = {The Handbook of Speech Production},
  author = {Redford, Melissa A.},
  date = {2015},
  publisher = {{Wiley Blackwell}},
  location = {{West Sussex}}
}

@article{rendallLiftingCurtainWizard2007,
  title = {Lifting the Curtain on the {{Wizard}} of {{Oz}}: Biased Voice-Based Impressions of Speaker Size},
  shorttitle = {Lifting the Curtain on the {{Wizard}} of {{Oz}}},
  author = {Rendall, Drew and Vokey, John R. and Nemeth, Christie},
  date = {2007-10},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {33},
  pages = {1208--1219},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.33.5.1208},
  abstract = {The consistent, but often wrong, impressions people form of the size of unseen speakers are not random but rather point to a consistent misattribution bias, one that the advertising, broadcasting, and entertainment industries also routinely exploit. The authors report 3 experiments examining the perceptual basis of this bias. The results indicate that, under controlled experimental conditions, listeners can make relative size distinctions between male speakers using reliable cues carried in voice formant frequencies (resonant frequencies, or timbre) but that this ability can be perturbed by discordant voice fundamental frequency (F-sub-0, or pitch) differences between speakers. The authors introduce 3 accounts for the perceptual pull that voice F-sub-0 can exert on our routine (mis)attributions of speaker size and consider the role that voice F-sub-0 plays in additional voice-based attributions that may or may not be reliable but that have clear size connotations.},
  eprint = {17924818},
  eprinttype = {pmid},
  keywords = {Adult,Amplifiers; Electronic,Cues,Female,Humans,Literature,Male,Speech Perception,Voice Quality},
  langid = {english},
  number = {5}
}

@article{reppSensorimotorSynchronizationReview2005,
  title = {Sensorimotor Synchronization: {{A}} Review of the Tapping Literature},
  shorttitle = {Sensorimotor Synchronization},
  author = {Repp, Bruno H.},
  date = {2005-12-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {12},
  pages = {969--992},
  issn = {1531-5320},
  doi = {10.3758/BF03206433},
  url = {https://doi.org/10.3758/BF03206433},
  urldate = {2019-04-02},
  abstract = {Sensorimotor synchronization (SMS), the rhythmic coordination of perception and action, occurs in many contexts, but most conspicuously in music performance and dance. In the laboratory, it is most often studied in the form of finger tapping to a sequence of auditory stimuli. This review summarizes theories and empirical findings obtained with the tapping task. Its eight sections deal with the role of intention, rate limits, the negative mean asynchrony, variability, models of error correction, perturbation studies, neural correlates of SMS, and SMS in musical contexts. The central theoretical issue is considered to be how best to characterize the perceptual information and the internal processes that enable people to achieve and maintain SMS. Recent research suggests that SMS is controlled jointly by two error correction processes (phase correction and period correction) that differ in their degrees of cognitive control and may be associated with different brain circuits. They exemplify the general distinction between subconscious mechanisms of action regulation and conscious processes involved in perceptual judgment and action planning.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EYLWBHB4\\Repp - 2005 - Sensorimotor synchronization A review of the tapp.pdf},
  keywords = {Auditory Sequence,Experimental Brain Research,Music Perception,Phase Correction,Target Tone},
  langid = {english},
  number = {6}
}

@online{RespirationWingBeatUltrasonic,
  title = {Respiration, {{Wing}}-{{Beat}} and {{Ultrasonic Pulse Emission}} in an {{Echo}}-{{Locating Bat}} | {{Journal}} of {{Experimental Biology}}},
  url = {https://jeb.biologists.org/content/56/1/37},
  urldate = {2020-01-23},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QSGQKIY8\\37.html}
}

@online{RespiratoryMuscleActivity,
  title = {Respiratory Muscle Activity in Relation to Vocalization in Flying Bats. | {{Journal}} of {{Experimental Biology}}},
  url = {https://jeb.biologists.org/content/198/1/175},
  urldate = {2020-01-23},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7PLEZBXS\\175.html}
}

@software{richardsonPolhemusApplicationsExample2009,
  title = {Polhemus Applications and Example Code},
  author = {Richardson, M.},
  date = {2009},
  url = {http://xkiwilabs.com/softwa re-toolboxes/}
}

@article{robertsSocialEcologicalComplexity,
  title = {Social and Ecological Complexity Is Associated with Gestural Repertoire Size of Wild Chimpanzees},
  author = {Roberts, Sam George Bradley and Roberts, Anna Ilona},
  journaltitle = {Integrative Zoology},
  volume = {n/a},
  issn = {1749-4877},
  doi = {10.1111/1749-4877.12423},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1749-4877.12423},
  urldate = {2019-11-30},
  abstract = {Increasing our understanding of primate gestural communication can provide new insights into language evolution. A key question in primate communication is the association between the social relationships of primates and their repertoire of gestures. Such analyses can reveal how primates use their repertoire of gestural communication to maintain their networks of family and friends, much as humans use language to maintain their social networks. In this study we examined the association between the repertoire of gestures (overall, manual and bodily gestures, gestures of different modalities) and social bonds (presence of reciprocated grooming), coordinated behaviours (travel, resting, co-feeding), and the complexity of ecology (e.g. noise, illumination) and sociality (party size, audience), in wild East African chimpanzees (Pan troglodytes schweinfurthii). A larger repertoire size of manual, visual gestures was associated with the presence of a relationship based on reciprocated grooming and increases in social complexity. A smaller repertoire of manual tactile gestures occurred when relationship was based on reciprocated grooming. A smaller repertoire of bodily gestures occurred between partners who jointly travelled for longer. Whereas gesture repertoire size was associated with social complexity, complex ecology also influenced repertoire size. The evolution of a large repertoire of manual, visual gestures may have been a key factor that enabled for larger social groups to emerge during evolution. Thus, the evolution of the larger brains in hominins may have co-occurred with an increase in the cognitive complexity underpinning gestural communication and this in turn may have enabled hominins to live in more complex social groups. This article is protected by copyright. All rights reserved.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ANUHUNCE\\1749-4877.html},
  keywords = {chimpanzee,ecology,gesture,repertoire size,social network analysis,sociality},
  langid = {english},
  number = {n/a}
}

@article{roch-levecqProductionBasicEmotions2006,
  title = {Production of Basic Emotions by Children with Congenital Blindness: {{Evidence}} for the Embodiment of Theory of Mind},
  shorttitle = {Production of Basic Emotions by Children with Congenital Blindness},
  author = {Roch‐Levecq, Anne-Catherine},
  date = {2006},
  journaltitle = {British Journal of Developmental Psychology},
  volume = {24},
  pages = {507--528},
  issn = {2044-835X},
  doi = {10.1348/026151005X50663},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/026151005X50663},
  urldate = {2019-11-22},
  abstract = {Children with congenital blindness are delayed in understanding other people's minds. The present study examined whether this delay was related to a more primitive form of inter-subjectivity by which infants draw correspondence between parental mirroring of the infant's display and proprioceptive sensations. Twenty children with congenital blindness and 20 typically-developing sighted children aged between 4 and 12 years were administered a series of tasks examining false belief and emotion understanding and production. The blind children scored lower on the false belief tasks and did not convey emotions facially to adult observers as accurately as sighted participants. The adults' ratings of the children's expressions were correlated with the children's scores on the false belief tasks. It is suggested that understanding people's minds might be anchored in primitive embodied forms of relatedness.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VMDL76VZ\\Roch‐Levecq - 2006 - Production of basic emotions by children with cong.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\HJA4M7WY\\026151005X50663.html},
  langid = {english},
  number = {3}
}

@article{rochet-capellanSpeechFocusPosition2008,
  title = {The Speech Focus Position Effect on Jaw–Finger Coordination in a Pointing Task},
  author = {Rochet-Capellan, A. and Laboissière, R. and Galván, A. and Schwartz, Jean-Luc},
  date = {2008-12-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {51},
  pages = {1507--1521},
  doi = {10.1044/1092-4388(2008/07-0173)},
  url = {https://jslhr.pubs.asha.org/doi/full/10.1044/1092-4388%282008/07-0173%29},
  urldate = {2019-04-18},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\22HUYGHV\\Rochet-Capellan Amélie et al. - 2008 - The Speech Focus Position Effect on Jaw–Finger Coo.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\6CD9L55U\\07-0173).html},
  number = {6}
}

@article{rochet-capellanTakeBreathTake2014,
  title = {Take a Breath and Take the Turn: How Breathing Meets Turns in Spontaneous Dialogue},
  shorttitle = {Take a Breath and Take the Turn},
  author = {Rochet-Capellan, A. and Fuchs, S.},
  date = {2014-12-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0399},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4240966/},
  urldate = {2019-11-16},
  abstract = {Physiological rhythms are sensitive to social interactions and could contribute to defining social rhythms. Nevertheless, our knowledge of the implications of breathing in conversational turn exchanges remains limited. In this paper, we addressed the idea that breathing may contribute to timing and coordination between dialogue partners. The relationships between turns and breathing were analysed in unconstrained face-to-face conversations involving female speakers. No overall relationship between breathing and turn-taking rates was observed, as breathing rate was specific to the subjects' activity in dialogue (listening versus taking the turn versus holding the turn). A general inter-personal coordination of breathing over the whole conversation was not evident. However, specific coordinative patterns were observed in shorter time-windows when participants engaged in taking turns. The type of turn-taking had an effect on the respective coordination in breathing. Most of the smooth and interrupted turns were taken just after an inhalation, with specific profiles of alignment to partner breathing. Unsuccessful attempts to take the turn were initiated late in the exhalation phase and with no clear inter-personal coordination. Finally, breathing profiles at turn-taking were different than those at turn-holding. The results support the idea that breathing is actively involved in turn-taking and turn-holding.},
  eprint = {25385777},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\GWEBTNTH\\Rochet-Capellan and Fuchs - 2014 - Take a breath and take the turn how breathing mee.pdf},
  number = {1658},
  pmcid = {PMC4240966}
}

@article{rohrerAvoidingSpuriousSubmovement2003,
  title = {Avoiding Spurious Submovement Decompositions: A Globally Optimal Algorithm},
  shorttitle = {Avoiding Spurious Submovement Decompositions},
  author = {Rohrer, Brandon and Hogan, Neville},
  date = {2003-09-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybern.},
  volume = {89},
  pages = {190--199},
  issn = {1432-0770},
  doi = {10.1007/s00422-003-0428-4},
  url = {https://doi.org/10.1007/s00422-003-0428-4},
  urldate = {2020-01-31},
  abstract = {Evidence for the existence of discrete submovements underlying continuous human movement has motivated many attempts to “extract” them. Although they produce visually convincing results, all of the methodologies that have been employed are prone to produce spurious decompositions. Examples of potential failures are given. A branch-and-bound algorithm for submovement extraction, capable of global nonlinear minimization (and hence capable of avoiding spurious decompositions), is developed and demonstrated.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZJISG8WJ\\Rohrer and Hogan - 2003 - Avoiding spurious submovement decompositions a gl.pdf},
  langid = {english},
  number = {3}
}

@article{roschWaveletCompGuidedTour2014,
  title = {{{WaveletComp}} 1.1: {{A}} Guided Tour through the {{R}} Package},
  author = {Rosch, Angi and Schmidbauer, Harald},
  date = {2014},
  pages = {59},
  abstract = {WaveletComp is an R package for continuous wavelet-based analysis of univariate and bivariate time series. Wavelet functions are implemented in WaveletComp such that a wide range of intermediate and final results are easily accessible. The null hypothesis that there is no (joint) periodicity in the series is tested via p-values obtained from simulation, where the model to be simulated can be chosen from a wide variety of options. The reconstruction, and thus filtering, of a given series from its wavelet decomposition, subject to a range of possible constraints, is also possible. WaveletComp provides extended plotting functionality — which objects should be added to a plot (for example, the ridge of wavelet power, contour lines indicating significant periodicity, arrows indicating the leading/lagging series), which kind and degree of smoothing is desired in wavelet coherence plots, which color palette to use, how to define the layout of the time axis (using POSIXct conventions), and others. Technically, we have developed vector- and matrix-based implementations of algorithms to reduce computation time. Easy and intuitive handling was given high priority.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FXBWTB7H\\Rosch and Schmidbauer - WaveletComp 1.1 A guided tour through the R packa.pdf},
  langid = {english}
}

@article{rucinskaSocialEnactivePerspectives2019,
  title = {Social and {{Enactive Perspectives}} on {{Pretending}} | {{Avant}}},
  author = {Rucińska, Z.},
  date = {2019},
  journaltitle = {Avant},
  volume = {X},
  url = {http://avant.edu.pl/en/2019-03-15},
  urldate = {2020-02-20},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HQRBYRKS\\2019-03-15.html},
  langid = {american},
  number = {3}
}

@online{ruiterProductionGestureSpeech2000,
  title = {The Production of Gesture and Speech},
  author = {de Ruiter, Jan Peter},
  date = {2000-08},
  journaltitle = {Language and Gesture},
  doi = {10.1017/CBO9780511620850.018},
  url = {/core/books/language-and-gesture/production-of-gesture-and-speech/7703D35DC0D8F631AD0E7525AB363841},
  urldate = {2019-05-04},
  abstract = {{$<$}div class="abstract" data-abstract-type="normal"{$><$}p{$>$}Introduction{$<$}/p{$><$}p{$>$}Research topics in the field of speech-related gesture that have received considerable attention are the function of gesture, its synchronization with speech, and its semiotic properties. While the findings of these studies often have interesting implications for theories about the processing of gesture in the human brain, few studies have addressed this issue in the framework of information processing.{$<$}/p{$><$}p{$>$}In this chapter, I will present a general processing architecture for gesture production. It can be used as a starting point for investigating the processes and representations involved in gesture and speech. For convenience, I will use the term ‘model'when referring to ‘processing architecture’ throughout this chapter.{$<$}/p{$><$}p{$>$}Since the use of information-processing models is not believed by every gesture researcher to be an appropriate way of investigating gesture (see, e.g., McNeill 1992), I will first argue that information-processing models are essential theoretical tools for understanding the processing involved in gesture and speech. I will then proceed to formulate a new model for the production of gesture and speech, called the Sketch Model. It is an extension of Levelt's (1989) model for speech production. The modifications and additions to Levelt's model are discussed in detail. At the end of the section, the working of the Sketch Model is demonstrated, using a number of illustrative gesture/speech fragments as examples.{$<$}/p{$><$}p{$>$}Subsequently, I will compare the Sketch Model with both McNeill's (1992) growth-point theory and with the information-processing model by Krauss, Chen \& Gottesman (this volume). While the Sketch Model and the model by Krauss et al. are formulated within the same framework, they are based on fundamentally different assumptions.{$<$}/p{$><$}/div{$>$}},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9FX5U9BF\\7703D35DC0D8F631AD0E7525AB363841.html},
  langid = {english}
}

@article{runesonKinematicSpecificationDynamics1983,
  title = {Kinematic {{Specification}} of {{Dynamics}} as an {{Informational Basis}} for {{Person}}-and-{{Action Perception}}: {{Expectation}}, {{Gender Recognition}}, and {{Deceptive Intention}}},
  author = {Runeson, Sverker and Frykholm, Gunilla},
  date = {1983},
  journaltitle = {Journal of Experimental Psychology: General},
  pages = {31},
  abstract = {The widespread conviction that perceiving another person must rest on ambiguous and fakeable information is challenged. Arguingfrom biomechanical necessities inherent in maintaining balance and coping with reactive impulses, we show that the detailed kinematic pattern is specific to an acting person's anatomical makeup and to the working of his or her motor control system. In this way information is potentially available about gender, identity, expectations, intentions, and what the person is in fact doing. We invoke the lawfulness of human movement, as elucidated by recent advances in motor control theory, to demonstrate the virtual impossibility of performing truly deceptive movements and to argue in general terms for the specification power inherent in human kinematics. The outcome of the analysis is subsumed under a principle of kinematic specification of dynamics (KSD), which states that movements specify the causal factors of events. Generally, a linked multiple degrees-of-freedom system does not exhibit substitutability; a change in one of its "input" factors cannot substitute for, or cancel, the multivariable effects of a change in another factor.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DTGJMNFR\\Runeson and Frykholm - Kinematic Specification of Dynamics as an Informat.pdf},
  langid = {english}
}

@article{rusiewiczEffectsProsodyPosition2013,
  title = {Effects of Prosody and Position on the Timing of Deictic Gestures},
  author = {Rusiewicz, H. L. and Susan, S. and Iverson, J. and Szuminsky, N.},
  date = {2013-04-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {56},
  pages = {458--470},
  doi = {10.1044/1092-4388(2012/11-0283)},
  url = {https://jslhr.pubs.asha.org/doi/full/10.1044/1092-4388%282012/11-0283%29},
  urldate = {2019-04-18},
  abstract = {Purpose
      In this study, the authors investigated the hypothesis that the perceived tight temporal
         synchrony of speech and gesture is evidence of an integrated spoken language and manual
         gesture communication system. It was hypothesized that experimental manipulations
         of the spoken response would affect the timing of deictic gestures.
      
      
      Method
      The authors manipulated syllable position and contrastive stress in compound words
         in multiword utterances by using a repeated-measures design to investigate the degree
         of synchronization of speech and pointing gestures produced by 15 American English
         speakers. Acoustic measures were compared with the gesture movement recorded via capacitance.
      
      
      Results
      Although most participants began a gesture before the target word, the temporal parameters
         of the gesture changed as a function of syllable position and prosody. Syllables with
         contrastive stress in the 2nd position of compound words were the longest in duration
         and also most consistently affected the timing of gestures, as measured by several
         dependent measures.
      
      
      Conclusion
      Increasing the stress of a syllable significantly affected the timing of a corresponding
         gesture, notably for syllables in the 2nd position of words that would not typically
         be stressed. The findings highlight the need to consider the interaction of gestures
         and spoken language production from a motor-based perspective of coordination.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EUJRIGW6\\Rusiewicz Heather Leavy et al. - 2013 - Effects of Prosody and Position on the Timing of D.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ER26DFTE\\11-0283).html},
  number = {2}
}

@incollection{rusiewiczSetTime2018,
  title = {Set in Time},
  booktitle = {The {{Development}} of Prosody in First Language Acquisition},
  author = {Rusiewicz, H. L. and Esteve-Gibert, N.},
  date = {2018},
  pages = {103}
}

@inproceedings{saltzmanTaskdynamicToolkitModeling2008,
  title = {A Task-Dynamic Toolkit for Modeling the Effects of Prosodic Structure on Articulation},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Speech Prosody}}},
  author = {Saltzman, Elliot and Nam, Hosung and Krivokapic, Jelena and Goldstein, Louis},
  date = {2008},
  pages = {175--184},
  location = {{Campina, Brazil}},
  eventtitle = {Speech {{Prosody}} 2009},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9AXZRZSU\\Saltzman et al. - A task-dynamic toolkit for modeling the effects of.pdf},
  langid = {english}
}

@article{samuelPerceptualLearningSpeech2009,
  title = {Perceptual Learning for Speech},
  author = {Samuel, Arthur G. and Kraljic, Tanya},
  date = {2009-08},
  journaltitle = {Attention, Perception, \& Psychophysics},
  volume = {71},
  pages = {1207--1218},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/APP.71.6.1207},
  url = {http://www.springerlink.com/index/10.3758/APP.71.6.1207},
  urldate = {2019-05-07},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BJFJMQS5\\Samuel and Kraljic - 2009 - Perceptual learning for speech.pdf},
  langid = {english},
  number = {6}
}

@article{sandlerBodyEvidenceNature2018,
  title = {The {{Body}} as {{Evidence}} for the {{Nature}} of {{Language}}},
  author = {Sandler, Wendy},
  date = {2018-10-29},
  journaltitle = {Frontiers in Psychology},
  volume = {9},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.01782},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2018.01782/full},
  urldate = {2020-06-08},
  abstract = {Taking its cue from sign languages, this paper proposes that the recruitment and composition of body actions provide evidence for key properties of language and its emergence. Adopting the view that compositionality is the fundamental organizing property of language, we show first that actions of the hands, face, head, and torso in sign languages directly reflect linguistic components, and illuminate certain aspects of compositional organization among them that are relevant for all languages, signed and spoken. Studies of emerging sign languages strengthen the approach by showing that the gradual recruitment of bodily articulators for linguistic functions directly maps the way in which a new language increases in complexity and efficiency over time. While compositional communication is almost exclusively restricted to humans, it is not restricted to language. In the spontaneous, intense emotional displays of athletes, different emotional states are correlated with actions of particular face and body features and feature groupings. These findings indicate a much more ancient communicative compositional capacity, and support a paradigm that includes visible body actions in the quest for core linguistic properties and their origins.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\E3763AJ7\\Sandler - 2018 - The Body as Evidence for the Nature of Language.pdf},
  langid = {english}
}

@article{satoAllAspectsLearning2020,
  title = {Do All Aspects of Learning Benefit from Iconicity? {{Evidence}} from Motion Capture},
  shorttitle = {Do All Aspects of Learning Benefit from Iconicity?},
  author = {Sato, Asha and Schouwstra, Marieke and Flaherty, Molly and Kirby, Simon},
  date = {2020-03},
  journaltitle = {Language and Cognition},
  volume = {12},
  pages = {36--55},
  publisher = {{Cambridge University Press}},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2019.37},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/do-all-aspects-of-learning-benefit-from-iconicity-evidence-from-motion-capture/55EDF990ED0E81A85100F8F01988B7C2},
  urldate = {2020-03-18},
  abstract = {Recent work suggests that not all aspects of learning benefit from an iconicity advantage (Ortega, 2017). We present the results of an artificial sign language learning experiment testing the hypothesis that iconicity may help learners to learn mappings between forms and meanings, whilst having a negative impact on learning specific features of the form. We used a 3D camera (Microsoft Kinect) to capture participants’ gestures and quantify the accuracy with which they reproduce the target gestures in two conditions. In the iconic condition, participants were shown an artificial sign language consisting of congruent gesture–meaning pairs. In the arbitrary condition, the language consisted of non-congruent gesture–meaning pairs. We quantified the accuracy of participants’ gestures using dynamic time warping (Celebi et. al., 2013). Our results show that participants in the iconic condition learn mappings more successfully than participants in the arbitrary condition, but there is no difference in the accuracy with which participants reproduce the forms. While our work confirms that iconicity helps to establish form–meaning mappings, our study did not give conclusive evidence about the effect of iconicity on production; we suggest that iconicity may only have an impact on learning forms when these are complex.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LKDEMR8W\\Sato et al. - 2020 - Do all aspects of learning benefit from iconicity.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\GPZL9V5I\\55EDF990ED0E81A85100F8F01988B7C2.html},
  keywords = {Artificial Sign Language learning,iconicity,motion capture},
  langid = {english},
  number = {1}
}

@article{sauterCrossculturalRecognitionBasic2010,
  title = {Cross-Cultural Recognition of Basic Emotions through Nonverbal Emotional Vocalizations},
  author = {Sauter, Disa A. and Eisner, Frank and Ekman, Paul and Scott, Sophie K.},
  date = {2010-02-09},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {107},
  pages = {2408--2412},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0908239106},
  url = {https://www.pnas.org/content/107/6/2408},
  urldate = {2019-10-22},
  abstract = {Emotional signals are crucial for sharing important information, with conspecifics, for example, to warn humans of danger. Humans use a range of different cues to communicate to others how they feel, including facial, vocal, and gestural signals. We examined the recognition of nonverbal emotional vocalizations, such as screams and laughs, across two dramatically different cultural groups. Western participants were compared to individuals from remote, culturally isolated Namibian villages. Vocalizations communicating the so-called “basic emotions” (anger, disgust, fear, joy, sadness, and surprise) were bidirectionally recognized. In contrast, a set of additional emotions was only recognized within, but not across, cultural boundaries. Our findings indicate that a number of primarily negative emotions have vocalizations that can be recognized across cultures, while most positive emotions are communicated with culture-specific signals.},
  eprint = {20133790},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\L8ZW7D4H\\Sauter et al. - 2010 - Cross-cultural recognition of basic emotions throu.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\6PBF8UUR\\2408.html},
  keywords = {affect,communication,universality,vocal signals},
  langid = {english},
  number = {6}
}

@article{schembriComparingActionGestures2005,
  title = {Comparing Action Gestures and Classifier Verbs of Motion: Evidence from {{Australian Sign Language}}, {{Taiwan Sign Language}}, and Nonsigners' Gestures without Speech},
  shorttitle = {Comparing Action Gestures and Classifier Verbs of Motion},
  author = {Schembri, Adam and Jones, Caroline and Burnham, Denis},
  date = {2005},
  journaltitle = {Journal of Deaf Studies and Deaf Education},
  shortjournal = {J Deaf Stud Deaf Educ},
  volume = {10},
  pages = {272--290},
  issn = {1081-4159},
  doi = {10.1093/deafed/eni029},
  abstract = {Recent research into signed languages indicates that signs may share some properties with gesture, especially in the use of space in classifier constructions. A prediction of this proposal is that there will be similarities in the representation of motion events by sign-naive gesturers and by native signers of unrelated signed languages. This prediction is tested for deaf native signers of Australian Sign Language (Auslan), deaf signers of Taiwan Sign Language (TSL), and hearing nonsigners using the Verbs of Motion Production task from the Test Battery for American Sign Language (ASL) Morphology and Syntax. Results indicate that differences between the responses of nonsigners, Auslan signers, and TSL signers and the expected ASL responses are greatest with handshape units; movement and location units appear to be very similar. Although not definitive, these data are consistent with the claim that classifier constructions are blends of linguistic and gestural elements.},
  eprint = {15858072},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CBXERQKJ\\Schembri et al. - 2005 - Comparing action gestures and classifier verbs of .pdf},
  keywords = {Adolescent,Adult,Case-Control Studies,Deafness,Female,Gestures,Humans,Male,Middle Aged,Multivariate Analysis,Nonverbal Communication,Sign Language,Visual Perception},
  langid = {english},
  number = {3}
}

@article{schererVocalCommunicationEmotion2003,
  title = {Vocal Communication of Emotion: {{A}} Review of Research Paradigms},
  shorttitle = {Vocal Communication of Emotion},
  author = {Scherer, Klaus R},
  date = {2003-04-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {40},
  pages = {227--256},
  issn = {0167-6393},
  doi = {10.1016/S0167-6393(02)00084-5},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639302000845},
  urldate = {2019-10-22},
  abstract = {The current state of research on emotion effects on voice and speech is reviewed and issues for future research efforts are discussed. In particular, it is suggested to use the Brunswikian lens model as a base for research on the vocal communication of emotion. This approach allows one to model the complete process, including both encoding (expression), transmission, and decoding (impression) of vocal emotion communication. Special emphasis is placed on the conceptualization and operationalization of the major elements of the model (i.e., the speaker’s emotional state, the listener’s attribution, and the mediating acoustic cues). In addition, the advantages and disadvantages of research paradigms for the induction or observation of emotional expression in voice and speech and the experimental manipulation of vocal cues are discussed, using pertinent examples drawn from past and present research.
Zusammenfassung
Der Aufsatz gibt einen umfassenden Überblick über den Forschungsstand zum Thema der Beeinflussung von Stimme und Sprechweise durch Emotionen des Sprechers. Allgemein wird vorgeschlagen, die Forschung zur vokalen Kommunikation der Emotionen am Brunswik’schen Linsenmodell zu orientieren. Dieser Ansatz erlaubt den gesamten Kommunikationsprozess zu modellieren, von der Enkodierung (Ausdruck), über die Transmission (Übertragung), bis zur Dekodierung (Eindruck). Besondere Aufmerksamkeit gilt den Problemen der Konzeptualisierung und Operationalisierung der zentralen Elemente des Modells (z.B., dem Emotionszustand des Sprechers, den Inferenzprozessen des Hörers, und den zugrundeliegenden vokalen Hinweisreizen). Anhand ausgewählter Beispiele empirischer Untersuchungen werden die Vor- und Nachteile verschiedener Forschungsparadigmen zur Induktion und Beobachtung des emotionalen Stimmausdrucks sowie zur experimentellen Manipulation vokaler Hinweisreize diskutiert.
Résumé
L’état actuel de la recherche sur l’effet des émotions d’un locuteur sur la voix et la parole est décrit et des approches prometteuses pour le futur identifiées. En particulier, le modèle de perception de Brunswik (dit “de la lentille” est proposé) comme paradigme pour la recherche sur la communication vocale des émotions. Ce modèle permet la modélisation du processus complet, de l’encodage (expression) par la transmission au décodage (impression). La conceptualisation et l’opérationalization des éléments centraux du modèle (l’état émotionnel du locuteur, l’inférence de cet état par l’auditeur, et les indices auditifs) sont discuté en détail. De plus, en analysant des exemples de la recherche dans le domaine, les avantages et désavantages de différentes méthodes pour l’induction et l’observation de l’expression émotionnelle dans la voix et la parole et pour la manipulation expérimentale de différents indices vocaux sont évoqués.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XSGGSSSV\\Scherer - 2003 - Vocal communication of emotion A review of resear.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8ZBAMLDD\\S0167639302000845.html},
  keywords = {Acoustic markers of emotion,Emotion induction,Emotion simulation,Evaluation of emotion effects on voice and speech,Expression of emotion,Perception/decoding,Speaker moods and attitudes,Speech technology,Stress effects on voice,Theories of emotion,Vocal communication},
  langid = {english},
  number = {1}
}

@article{schmidtBodilySynchronizationUnderlying2014,
  title = {Bodily Synchronization Underlying Joke Telling},
  author = {Schmidt, R. C. and Nie, Lin and Franco, Alison and Richardson, Michael J.},
  date = {2014},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {8},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2014.00633},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2014.00633/full},
  urldate = {2019-05-07},
  abstract = {Advances in video and time series analysis have greatly enhanced our ability to study the bodily synchronization that occurs in natural interactions. Past research has demonstrated that the behavioral synchronization involved in social interactions is similar to dynamical synchronization found generically in nature. The present study investigated how the bodily synchronization in a joke telling task is spread across different nested temporal scales. Pairs of participants enacted knock-knock jokes and times series of their bodily activity were recorded. Coherence and relative phase analyses were used to evaluate the synchronization of bodily rhythms for the whole trial as well as at the subsidiary time scales of the whole joke, the setup of the punch line, the two-person exchange and the utterance. The analyses revealed greater than chance entrainment of the joke teller’s and joke responder’s movements at all time scales and that the relative phasing of the teller’s movements led those of the responder at the longer time scales. Moreover, this entrainment was greater when visual information about the partner’s movements was present but was decreased particularly at the shorter time scales when explicit gesturing in telling the joke was performed. In short, the results demonstrate that a complex interpersonal bodily “dance” occurs during structured conversation interactions and that this “dance” is constructed from a set of rhythms associated with the nested behavioral structure of the interaction.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\2WA6HV7G\\Schmidt et al. - 2014 - Bodily synchronization underlying joke telling.pdf},
  keywords = {Motor movements,sensorimotor synchronization,social coordination,social interaction,Spectral decomposition},
  langid = {english}
}

@article{schonerTimingClocksDynamical2002,
  title = {Timing, {{Clocks}}, and {{Dynamical Systems}}},
  author = {Schöner, Gregor},
  date = {2002-02},
  journaltitle = {Brain and Cognition},
  volume = {48},
  pages = {31--51},
  issn = {02782626},
  doi = {10.1006/brcg.2001.1302},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0278262601913028},
  urldate = {2020-05-19},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IE9EE4Y7\\Schöner - 2002 - Timing, Clocks, and Dynamical Systems.pdf},
  langid = {english},
  number = {1}
}

@article{schouwstraTemporalStructureEmerging2017,
  title = {Temporal {{Structure}} in {{Emerging Language}}: {{From Natural Data}} to {{Silent Gesture}}},
  shorttitle = {Temporal {{Structure}} in {{Emerging Language}}},
  author = {Schouwstra, Marieke},
  date = {2017},
  journaltitle = {Cognitive Science},
  volume = {41},
  pages = {928--940},
  issn = {1551-6709},
  doi = {10.1111/cogs.12441},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12441},
  urldate = {2020-03-06},
  abstract = {Many human languages have complex grammatical machinery devoted to temporality, but very little is known about how this came about. This paper investigates how people convey temporal information when they cannot use any conventional languages they know. In a laboratory experiment, adult participants were asked to convey information about simple events taking place at a given time, in spoken language and in silent gesture (i.e., using only gesture and no speech). It was shown that in spoken language, participants formed utterances according to the rules of their native language (Dutch), but in silent gesture, the temporal information was presented initially, and structurally separately, from the other information in the utterance. The experimental results are consistent with findings from natural systems emerging in situations of communicative stress: unsupervised adult second language learning and homesign. This confirms that presenting temporal information separately and initially (directly mirroring how temporal and propositional information can be represented semantically) is a robust strategy to talk about past and future when only sparse communicative means are available.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\R67K2959\\Schouwstra - 2017 - Temporal Structure in Emerging Language From Natu.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\RRB2ZMJY\\cogs.html},
  keywords = {Language evolution,Semantics,Silent gesture,Temporality,Tense,Word order},
  langid = {english},
  number = {S4}
}

@article{scott-phillipsLanguageEvolutionLaboratory2010,
  title = {Language Evolution in the Laboratory},
  author = {Scott-phillips, T. C. and Kirby, S.},
  date = {2010},
  journaltitle = {Trends in Cognitive Sciences},
  volume = {14},
  pages = {411--417},
  doi = {10.1016/j.tics.2010.06.006},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CBG7FTXE\\Scott-phillips and Kirby - Author's personal copy Language evolution in the l.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ICYHGK9W\\summary.html}
}

@article{sellAdaptationsHumansAssessing2010,
  title = {Adaptations in Humans for Assessing Physical Strength from the Voice},
  author = {Sell, Aaron and Bryant, Gregory A. and Cosmides, Leda and Tooby, John and Sznycer, Daniel and von Rueden, Christopher and Krauss, Andre and Gurven, Michael},
  date = {2010-11-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {277},
  pages = {3509--3518},
  doi = {10.1098/rspb.2010.0769},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2010.0769},
  urldate = {2019-10-17},
  abstract = {Recent research has shown that humans, like many other animals, have a specialization for assessing fighting ability from visual cues. Because it is probable that the voice contains cues of strength and formidability that are not available visually, we predicted that selection has also equipped humans with the ability to estimate physical strength from the voice. We found that subjects accurately assessed upper-body strength in voices taken from eight samples across four distinct populations and language groups: the Tsimane of Bolivia, Andean herder-horticulturalists and United States and Romanian college students. Regardless of whether raters were told to assess height, weight, strength or fighting ability, they produced similar ratings that tracked upper-body strength independent of height and weight. Male voices were more accurately assessed than female voices, which is consistent with ethnographic data showing a greater tendency among males to engage in violent aggression. Raters extracted information about strength from the voice that was not supplied from visual cues, and were accurate with both familiar and unfamiliar languages. These results provide, to our knowledge, the first direct evidence that both men and women can accurately assess men's physical strength from the voice, and suggest that estimates of strength are used to assess fighting ability.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\K4KVXIR4\\Sell et al. - 2010 - Adaptations in humans for assessing physical stren.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ARPVBSQY\\rspb.2010.html},
  number = {1699},
  options = {useprefix=true}
}

@article{senghasChildrenCreatingCore2004,
  title = {Children Creating Core Properties of Language: {{Evidence}} from an Emerging Sign Language in Nicaragua},
  shorttitle = {Children {{Creating Core Properties}} of {{Language}}},
  author = {Senghas, Ann and Kita, Sotaro and Özyürek, Asli},
  date = {2004-09-17},
  journaltitle = {Science},
  volume = {305},
  pages = {1779--1782},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1100199},
  url = {https://science.sciencemag.org/content/305/5691/1779},
  urldate = {2019-11-30},
  abstract = {A new sign language has been created by deaf Nicaraguans over the past 25 years, providing an opportunity to observe the inception of universal hallmarks of language. We found that in their initial creation of the language, children analyzed complex events into basic elements and sequenced these elements into hierarchically structured expressions according to principles not observed in gestures accompanying speech in the surrounding language. Successive cohorts of learners extended this procedure, transforming Nicaraguan signing from its early gestural form into a linguistic system. We propose that this early segmentation and recombination reflect mechanisms with which children learn, and thereby perpetuate, language. Thus, children naturally possess learning abilities capable of giving language its fundamental structure.
A sign language developed by deaf children consists of discrete units similar to those of spoken language, perhaps reflecting the fundamental organization of the brain's language centers.
A sign language developed by deaf children consists of discrete units similar to those of spoken language, perhaps reflecting the fundamental organization of the brain's language centers.},
  eprint = {15375269},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8ZZMFZSD\\Senghas et al. - 2004 - Children Creating Core Properties of Language Evi.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\9BBN28AM\\1779.html},
  langid = {english},
  number = {5691}
}

@online{SequenceMemoryConstraints,
  title = {Sequence {{Memory Constraints Give Rise}} to {{Language}}-{{Like Structure}} through {{Iterated Learning}}},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168532},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VIJ5DP3M\\article.html}
}

@article{sharkeyHandGesturesVisually2000,
  title = {Hand {{Gestures}} of {{Visually Impaired}} and {{Sighted Interactants}}},
  author = {Sharkey, William F. and Asamoto, Paula and Tokunaga, Christine and Haraguchi, Gail and McFaddon-Robar, Tammy},
  date = {2000-09-01},
  journaltitle = {Journal of Visual Impairment \& Blindness},
  shortjournal = {Journal of Visual Impairment \& Blindness},
  volume = {94},
  pages = {549--563},
  issn = {0145-482X},
  doi = {10.1177/0145482X0009400902},
  url = {https://doi.org/10.1177/0145482X0009400902},
  urldate = {2019-11-30},
  abstract = {This study investigated the types of gestures used, the frequency of the gestures, and the total time engaged in gestural communication by 11 visually impaired-sighted dyads; 12 sighted-sighted dyads; and 8 visually impaired-visually impaired dyads. Regardless of the type of dyad, the persons who were visually impaired used more adaptors and used gestures, emblems, and illustrators less often than did those who were sighted.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KGADQWEP\\Sharkey et al. - 2000 - Hand Gestures of Visually Impaired and Sighted Int.pdf},
  langid = {english},
  number = {9}
}

@article{sharkeyTurnTakingResources1990,
  title = {Turn‐taking Resources Employed by Congenitally Blind Conversers},
  author = {Sharkey, William F. and Stafford, Laura},
  date = {1990-06-01},
  journaltitle = {Communication Studies},
  volume = {41},
  pages = {161--182},
  issn = {1051-0974},
  doi = {10.1080/10510979009368299},
  url = {https://doi.org/10.1080/10510979009368299},
  urldate = {2019-11-30},
  abstract = {It has been suggested that blind persons lack appropriate communicative social skills. One aspect of social skills is the ability to regulate interaction smoothly. The study examined turn‐taking resources utilized by congenitally blind persons. Conversational Analysis was employed to discover the turn‐taking resources used by six congenitally blind individuals in three dyads (i.e., one male, one female and one mixed dyad). The results were compared with past research on turn‐taking resources utilized by sighted conversers. Overall, the participants utilized the majority of focal resources reported in research on sighted individuals. However, non‐vocal resources deviated from those found in previous research on sighted conversers. Specifically, tactile resources were not used; self‐adaptors, gestures and posture shifts were seldom used; mechanistic movements of the head and atypical use of facial orientation were discovered. Possible implications of these finding are discussed.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\G6XQJHDE\\10510979009368299.html},
  number = {2}
}

@inproceedings{shattuck-hufnagelDimensionalizingCospeechGestures2019,
  title = {Dimensionalizing Co-Speech Gestures},
  booktitle = {Proceedings of the {{International Congress}} of {{Phonetic Sciences}} 2019},
  author = {Shattuck-Hufnagel, S. and Prieto, P.},
  date = {2019},
  pages = {5},
  location = {{Melbourne, Australia}},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IHR6ZBZX\\Shattuck-Hufnagel and Prieto - Dimensionalizing co-speech gestures.pdf},
  langid = {english}
}

@article{shattuck-hufnagelProsodicCharacteristicsNonreferential2018,
  title = {The Prosodic Characteristics of Non-Referential Co-Speech Gestures in a Sample of Academic-Lecture-Style Speech},
  author = {Shattuck-Hufnagel, S. and Ren, Ada},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  volume = {9},
  doi = {10.3389/fpsyg.2018.01514},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HTSWA3AA\\fpsyg.2018.html},
  number = {1514}
}

@article{shattuck-hufnagelProsodyTutorialInvestigators1996,
  title = {A Prosody Tutorial for Investigators of Auditory Sentence Processing},
  author = {Shattuck-Hufnagel, Stefanie and Turk, Alice E.},
  date = {1996-03},
  journaltitle = {Journal of Psycholinguistic Research},
  volume = {25},
  pages = {193--247},
  issn = {0090-6905, 1573-6555},
  doi = {10.1007/BF01708572},
  url = {http://link.springer.com/10.1007/BF01708572},
  urldate = {2020-03-17},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IGUX47UT\\Shattuck-Hufnagel and Turk - 1996 - A prosody tutorial for investigators of auditory s.pdf},
  langid = {english},
  number = {2}
}

@book{sheets-johnstonePrimacyMovement2011,
  title = {The {{Primacy}} of {{Movement}}},
  author = {Sheets-Johnstone, M.},
  date = {2011},
  publisher = {{John Benjamins}},
  location = {{Amsterdam}}
}

@article{shockleyArticulatoryConstraintsInterpersonal2007,
  title = {Articulatory Constraints on Interpersonal Postural Coordination},
  author = {Shockley, Kevin and Baker, Aimee A. and Richardson, Michael J. and Fowler, Carol A.},
  date = {2007},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  pages = {201--208},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.33.1.201},
  abstract = {Cooperative conversation has been shown to foster interpersonal postural coordination. The authors investigated whether such coordination is mediated by the influence of articulation on postural sway. In Experiment 1, talkers produced words in synchrony or in alternation, as the authors varied speaking rate and word similarity. Greater shared postural activity was found for the faster speaking rate. In Experiment 2, the authors demonstrated that shared postural activity also increases when individuals speak the same words or speak words that have similar stress patterns. However, this increase in shared postural activity is present only when participants' data are compared with those of their partner, who was present during the task, but not when compared with the data of a member of a different pair speaking the same word sequences as those of the original partner. The authors' findings suggest that interpersonal postural coordination observed during conversation is mediated by convergent speaking patterns. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KNU5IU2G\\2007-01135-014.html},
  keywords = {Articulation (Speech),Conversation,Motor Coordination,Posture},
  number = {1}
}

@article{shockleyMutualInterpersonalPostural2003,
  title = {Mutual Interpersonal Postural Constraints Are Involved in Cooperative Conversation},
  author = {Shockley, Kevin and Santana, Marie-Vee and Fowler, Carol A.},
  date = {2003-04},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {29},
  pages = {326--332},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.29.2.326},
  abstract = {The research was designed to evaluate interpersonal coordination during conversation with a new measurement tool. The experiment uses an analysis based on recurrence strategies, known as cross recurrence quantification, to evaluate the shared activity between 2 postural time series in reconstructed phase space. Pairs of participants were found to share more locations in phase space (greater recurrence) in conditions where they were conversing with one another to solve a puzzle task than in conditions in which they convened with others. The trajectories of pairs of participants also showed less divergence when they conversed with each other than when they conversed with others well. This is offered as objective evidence of interpersonal coordination of postural sway in the context of a cooperative verbal task.},
  eprint = {12760618},
  eprinttype = {pmid},
  keywords = {Adult,Analysis of Variance,Communication,Cooperative Behavior,Data Collection,Evaluation Studies as Topic,Humans,Interpersonal Relations,Kinesics,Posture,Problem Solving,Verbal Behavior},
  langid = {english},
  number = {2}
}

@software{sievertPlotlyCreateInteractive2019,
  title = {Plotly: {{Create Interactive Web Graphics}} via 'Plotly.Js'},
  shorttitle = {Plotly},
  author = {Sievert, Carson and Parmer, Chris and Hocking, Toby and Chamberlain, Scott and Ram, Karthik and Corvellec, Marianne and Despouy, Pedro and Inc, Plotly Technologies},
  date = {2019-04-10},
  url = {https://CRAN.R-project.org/package=plotly},
  urldate = {2019-04-23},
  abstract = {Create interactive web graphics from 'ggplot2' graphs and/or a custom interface to the (MIT-licensed) JavaScript library 'plotly.js' inspired by the grammar of graphics.},
  keywords = {WebTechnologies},
  version = {4.9.0}
}

@inproceedings{silvaEffectEndpointsDynamic2016,
  title = {On the {{Effect}} of {{Endpoints}} on {{Dynamic Time Warping}}},
  author = {Silva, D. F. and Batista, G. A. E. P. A. and Keogh, E.},
  date = {2016},
  pages = {10},
  location = {{San Francisco}},
  abstract = {While there exist a plethora of classification algorithms for most data types, there is an increasing acceptance that the unique properties of time series mean that the combination of nearest neighbor classifiers and Dynamic Time Warping (DTW) is very competitive across a host of domains, from medicine to astronomy to environmental sensors. While there has been significant progress in improving the efficiency and effectiveness of DTW in recent years, in this work we demonstrate that an underappreciated issue can significantly degrade the accuracy of DTW in real-world deployments. This issue has probably escaped the attention of the very active time series research community because of its reliance on static highly contrived benchmark datasets, rather than real world dynamic datasets where the problem tends to manifest itself. In essence, the issue is that DTW’s eponymous invariance to warping is only true for the main “body” of the two time series being compared. However, for the “head” and “tail” of the time series, the DTW algorithm affords no warping invariance. The effect of this is that tiny differences at the beginning or end of the time series (which may be either consequential or simply the result of poor “cropping”) will tend to contribute disproportionally to the estimated similarity, producing incorrect classifications. In this work, we show that this effect is real, and reduces the performance of the algorithm. We further show that we can fix the issue with a subtle redesign of the DTW algorithm, and that we can learn an appropriate setting for the extra parameter we introduced. We further demonstrate that our generalization is amiable to all the optimizations that make DTW tractable for large datasets.},
  eventtitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  langid = {english}
}

@article{silvaSteadystateStressOne2007,
  title = {Steady-State Stress at One Hand Magnifies the Amplitude, Stiffness, and Non-Linearity of Oscillatory Behavior at the Other Hand},
  author = {Silva, P. and Moreno, M. and Mancini, M. and Fonseca, S. and Turvey, M. T.},
  date = {2007-12-11},
  journaltitle = {Neuroscience Letters},
  shortjournal = {Neuroscience Letters},
  volume = {429},
  pages = {64--68},
  issn = {0304-3940},
  doi = {10.1016/j.neulet.2007.09.066},
  url = {http://www.sciencedirect.com/science/article/pii/S0304394007010750},
  urldate = {2019-04-18},
  abstract = {Stress at one body segment can influence rhythmic movements of non-neighboring body segments. The nervous, circulatory, and fascia (connective tissue) systems are potential mediators of such remote effects. Assessing them begins with a detailed description of the remote effects. Precisely, how do the rhythmic movements change? In our experiment with seven participants, left-hand oscillations of held pendulums at self-selected frequencies were examined as a function of right-hand tonic forces of 0, 10 or 20\% of the maximum voluntary contraction. We evaluated the effect of the right hand's tonic force on the amplitude and frequency, and the stiffness and friction functions of the left hand's oscillations. Our results suggest that (a) amplitude and stiffness (both linear and non-linear) increased with tonic force but frequency and friction (both linear and non-linear) did not, and (b) the stiffness increases due to right hand 10 and 20\% stress were indifferent to the initial (0\%) left-hand stiffness values. Discussion took note of how the nervous system and architectural features of the body (e.g., its network of connective tissue) may produce such effects.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\24CQGZCP\\Silva et al. - 2007 - Steady-state stress at one hand magnifies the ampl.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\QKBQX6EV\\S0304394007010750.html},
  keywords = {Fascia,Remote effects,Rhythmic movements,Tonic force},
  number = {1}
}

@article{sizemoreImportanceWholeTopological2018,
  title = {The Importance of the Whole: Topological Data Analysis for the Network Neuroscientist},
  shorttitle = {The Importance of the Whole},
  author = {Sizemore, Ann E. and Phillips-Cremins, Jennifer and Ghrist, Robert and Bassett, Danielle S.},
  date = {2018-06-13},
  url = {http://arxiv.org/abs/1806.05167},
  urldate = {2020-03-11},
  abstract = {The application of network techniques to the analysis of neural data has greatly improved our ability to quantify and describe these rich interacting systems. Among many important contributions, networks have proven useful in identifying sets of node pairs that are densely connected and that collectively support brain function. Yet the restriction to pairwise interactions prevents us from realizing intrinsic topological features such as cavities within the interconnection structure that may be just as crucial for proper function. To detect and quantify these topological features we must turn to methods from algebraic topology that encode data as a simplicial complex built of sets of interacting nodes called simplices. On this substrate, we can then use the relations between simplices and higher-order connectivity to expose cavities within the complex, thereby summarizing its topological nature. Here we provide an introduction to persistent homology, a fundamental method from applied topology that builds a global descriptor of system structure by chronicling the evolution of cavities as we move through a combinatorial object such as a weighted network. We detail the underlying mathematics and perform demonstrative calculations on the mouse structural connectome, electrical and chemical synapses in \textbackslash textit\{C. elegans\}, and genomic interaction data. Finally we suggest avenues for future work and highlight new advances in mathematics that appear ready for use in revealing the architecture and function of neural systems.},
  archivePrefix = {arXiv},
  eprint = {1806.05167},
  eprinttype = {arxiv},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QVFVEHUB\\Sizemore et al. - 2018 - The importance of the whole topological data anal.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\VCRHS85W\\1806.html},
  keywords = {55-01,Quantitative Biology - Neurons and Cognition,Quantitative Biology - Quantitative Methods},
  primaryClass = {q-bio}
}

@article{slocombeLanguageVoidNeed2011,
  title = {The Language Void: The Need for Multimodality in Primate Communication Research},
  shorttitle = {The Language Void},
  author = {Slocombe, Katie E. and Waller, Bridget M. and Liebal, Katja},
  date = {2011-05-01},
  journaltitle = {Animal Behaviour},
  shortjournal = {Animal Behaviour},
  volume = {81},
  pages = {919--924},
  issn = {0003-3472},
  doi = {10.1016/j.anbehav.2011.02.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0003347211000558},
  urldate = {2020-05-29},
  abstract = {Theories of language evolution often draw heavily on comparative evidence of the communicative abilities of extant nonhuman primates (primates). Many theories have argued exclusively for a unimodal origin of language, usually gestural or vocal. Theories are often strengthened by research on primates that indicates the absence of certain linguistic precursors in the opposing communicative modality. However, a systematic review of the primate communication literature reveals that vocal, gestural and facial signals have attracted differing theoretical and methodological approaches, rendering cross-modal comparisons problematic. The validity of the theories based on such comparisons can therefore be questioned. We propose that these a priori biases, inherent in unimodal research, highlight the need for integrated multimodal research. By examining communicative signals in concert we can both avoid methodological discontinuities as well as better understand the phylogenetic precursors to human language as part of a multimodal system.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VDH57FDP\\Slocombe et al. - 2011 - The language void the need for multimodality in p.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\4SZ5C4Q9\\S0003347211000558.html},
  keywords = {facial expression,gesture,language evolution,multimodal communication,primate communication,vocalization},
  langid = {english},
  number = {5}
}

@article{slonimskaRoleIconicitySimultaneity2020a,
  title = {The Role of Iconicity and Simultaneity for Efficient Communication: {{The}} Case of {{Italian Sign Language}} ({{LIS}})},
  shorttitle = {The Role of Iconicity and Simultaneity for Efficient Communication},
  author = {Slonimska, Anita and Özyürek, Asli and Capirci, Olga},
  date = {2020-07-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {200},
  pages = {104246},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104246},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027720300652},
  urldate = {2020-04-25},
  abstract = {A fundamental assumption about language is that, regardless of language modality, it faces the linearization problem, i.e., an event that occurs simultaneously in the world has to be split in language to be organized on a temporal scale. However, the visual modality of signed languages allows its users not only to express meaning in a linear manner but also to use iconicity and multiple articulators together to encode information simultaneously. Accordingly, in cases when it is necessary to encode informatively rich events, signers can take advantage of simultaneous encoding in order to represent information about different referents and their actions simultaneously. This in turn would lead to more iconic and direct representation. Up to now, there has been no experimental study focusing on simultaneous encoding of information in signed languages and its possible advantage for efficient communication. In the present study, we assessed how many information units can be encoded simultaneously in Italian Sign Language (LIS) and whether the amount of simultaneously encoded information varies based on the amount of information that is required to be expressed. Twenty-three deaf adults participated in a director-matcher game in which they described 30 images of events that varied in amount of information they contained. Results revealed that as the information that had to be encoded increased, signers also increased use of multiple articulators to encode different information (i.e., kinematic simultaneity) and density of simultaneously encoded information in their production. Present findings show how the fundamental properties of signed languages, i.e., iconicity and simultaneity, are used for the purpose of efficient information encoding in Italian Sign Language (LIS).},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QJ67B222\\S0010027720300652.html},
  keywords = {Efficient communication,Iconicity,Linearization problem,Sign language,Simultaneity},
  langid = {english}
}

@article{smithAnatomicalCharacteristicsUpper2003,
  title = {Anatomical {{Characteristics}} of the {{Upper Serratus Anterior}}: {{Cadaver Dissection}}},
  shorttitle = {Anatomical {{Characteristics}} of the {{Upper Serratus Anterior}}},
  author = {Smith, Russell and Nyquist-Battie, Cynthia and Clark, Mark and Rains, Julie},
  date = {2003-08-01},
  journaltitle = {Journal of Orthopaedic \& Sports Physical Therapy},
  shortjournal = {J Orthop Sports Phys Ther},
  volume = {33},
  pages = {449--454},
  publisher = {{Journal of Orthopaedic \& Sports Physical Therapy}},
  issn = {0190-6011},
  doi = {10.2519/jospt.2003.33.8.449},
  url = {https://www.jospt.org/doi/abs/10.2519/jospt.2003.33.8.449},
  urldate = {2020-04-21},
  abstract = {Study DesignA descriptive study of the anatomical characteristics of the upper serratus anterior.ObjectivesTo delineate the upper serratus anterior with comparison to classical descriptions of the anatomy of the muscle as a whole.BackgroundAlthough the serratus anterior has a major role in scapulothoracic stability, description of the separate function and anatomy of the upper, middle, and lower portions of the muscle has been limited.Methods and MeasuresBilateral anatomical dissection of 8 cadavers (3 female and 5 male) exposed 13 serratus anterior and surrounding structures for review. The number of serrations, attachment sites, length, and girth of the upper serratus anterior were measured.ResultsThe upper serratus anterior presented with dual serrations and single serrations in 7 (54\%) and 6 (46\%) of 13 observations, respectively. Attachments to both first and second ribs were noted in 6 (46\%) of the 13 observations. The remaining proximal attachments were to the second rib only, the first rib only, and dual attachments to the second and third ribs. In all cases, cranial attachments were to the superior scapular angle blending with the levator scapulae attachment. Length ranged from 4.8 to 9.0 cm (mean ± SD, 6.9 ± 1.2 cm). The girth ranged from 3.0 to 8.5 cm (mean ± SD, 6.1 ± 1.5 cm). One or more branches of the long thoracic nerve were observed to consistently innervate the upper serratus anterior fibers.ConclusionThe upper serratus anterior demonstrated wide variation in anatomy and was noted to be distinct in appearance and peripheral innervation from the middle and lower serratus anterior.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LWX5XBGH\\Smith et al. - 2003 - Anatomical Characteristics of the Upper Serratus A.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\54C9YN3T\\jospt.2003.33.8.html},
  number = {8}
}

@article{smithInteractionGlottalpulseRate2005,
  title = {The Interaction of Glottal-Pulse Rate and Vocal-Tract Length in Judgements of Speaker Size, Sex, and Age},
  author = {Smith, David R. R. and Patterson, Roy D.},
  date = {2005-11},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {118},
  pages = {3177--3186},
  issn = {0001-4966},
  doi = {10.1121/1.2047107},
  abstract = {Glottal-pulse rate (GPR) and vocal-tract length (VTL) are related to the size, sex, and age of the speaker but it is not clear how the two factors combine to influence our perception of speaker size, sex, and age. This paper describes experiments designed to measure the effect of the interaction of GPR and VTL upon judgements of speaker size, sex, and age. Vowels were scaled to represent people with a wide range of GPRs and VTLs, including many well beyond the normal range of the population, and listeners were asked to judge the size and sex/age of the speaker. The judgements of speaker size show that VTL has a strong influence upon perceived speaker size. The results for the sex and age categorization (man, woman, boy, or girl) show that, for vowels with GPR and VTL values in the normal range, judgements of speaker sex and age are influenced about equally by GPR and VTL. For vowels with abnormal combinations of low GPRs and short VTLs, the VTL information appears to decide the sex/age judgement.},
  eprint = {16334696},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Adult,Age Factors,Child,Female,Glottis,Humans,Male,Organ Size,Sex Characteristics,Speech Perception,Vocal Cords},
  langid = {english},
  number = {5},
  pmcid = {PMC2346770}
}

@article{stennekenSelfinducedReactiveTriggering2002,
  title = {Self-Induced versus Reactive Triggering of Synchronous Movements in a Deafferented Patient and Control Subjects},
  author = {Stenneken, P. and Aschersleben, G. and Cole, J. D. and Prinz, W.},
  date = {2002-02-01},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  volume = {66},
  pages = {40--49},
  issn = {1430-2772},
  doi = {10.1007/s004260100072},
  url = {https://doi.org/10.1007/s004260100072},
  urldate = {2019-04-02},
  abstract = {. The present study investigates the contribution of tactile-kinesthetic information to the timing of movements. The relative timing of simultaneous tapping movements of finger and foot (hand-foot asynchrony) was examined in a simple reaction time task and in discrete self-initiated taps (Experiment 1), and in externally triggered synchronization tapping (Experiment 2). We compared the performance of a deafferented participant (IW) to the performance of two control groups of different ages. The pattern of results in control groups replicates previous findings: Whereas positive hand-foot asynchronies (hand precedes foot) are observed in a simultaneous reaction to an auditory stimulus, hand-foot asynchronies are negative with discrete self-initiated as well as auditorily paced sequences of synchronized finger and foot taps. In the first case, results are explained by a simultaneous triggering of motor commands. In contrast, self-initiated and auditorily paced movements are assumed to be controlled in terms of their afferent consequences, as provided by tactile-kinesthetic information. The performance of the deafferented participant differed from that of healthy participants in some aspects. As expected on the basis of unaffected motor functions, the participant was able to generate finger and foot movements in reaction to an external signal. In spite of the lack of movement-contingent sensory feedback, the deafferented participant showed comparable timing errors in self-initiated and regularly paced tapping as observed in control participants. However, in discrete self-initiated taps IW's hand-foot asynchronies were considerably larger than in control participants, while performance did not differ from that of controls in continuous movement generation. These findings are discussed in terms of an internal generation of the movement's sensory consequences (forward-modeling).},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MNW3S2WF\\Stenneken et al. - 2002 - Self-induced versus reactive triggering of synchro.pdf},
  keywords = {Control Participant,Motor Command,Reactive Trigger,Simple Reaction Time Task,Synchronization Task},
  langid = {english},
  number = {1}
}

@article{stennekenSelfinducedReactiveTriggering2002a,
  title = {Self-Induced versus Reactive Triggering of Synchronous Movements in a Deafferented Patient and Control Subjects},
  author = {Stenneken, P. and Aschersleben, G. and Cole, J. and Prinz, W.},
  date = {2002-02},
  journaltitle = {Psychological Research},
  volume = {66},
  pages = {40--49},
  issn = {0340-0727, 1430-2772},
  doi = {10.1007/s004260100072},
  url = {http://link.springer.com/10.1007/s004260100072},
  urldate = {2019-04-19},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EAPIWRAU\\Stenneken et al. - 2002 - Self-induced versus reactive triggering of synchro.pdf},
  langid = {english},
  number = {1}
}

@article{sterelnyLanguageGestureSkill2012,
  title = {Language, Gesture, Skill: The Co-Evolutionary Foundations of Language},
  shorttitle = {Language, Gesture, Skill},
  author = {Sterelny, Kim},
  date = {2012-08-05},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {367},
  pages = {2141--2151},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2012.0116},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0116},
  urldate = {2020-02-24},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\56LVW5VW\\Sterelny - 2012 - Language, gesture, skill the co-evolutionary foun.pdf},
  langid = {english},
  number = {1599}
}

@book{stetsonMotorPhoneticsStudy1928,
  title = {Motor {{Phonetics}}: {{A Study}} of {{Speech Movements}} in {{Action}}},
  shorttitle = {Motor {{Phonetics}}},
  author = {Stetson, R. H.},
  date = {1928},
  publisher = {{Springer Netherlands}},
  url = {https://www.springer.com/gp/book/9789401521475},
  urldate = {2019-08-08},
  abstract = {Motor Phonetics...},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZEVAZGJY\\9789401521475.html},
  isbn = {978-94-015-2147-5},
  langid = {english}
}

@article{stiversUniversalsCulturalVariation2009,
  title = {Universals and Cultural Variation in Turn-Taking in Conversation},
  author = {Stivers, Tanya and Enfield, N. J. and Brown, Penelope and Englert, Christina and Hayashi, Makoto and Heinemann, Trine and Hoymann, Gertie and Rossano, Federico and de Ruiter, Jan Peter and Yoon, Kyung-Eun and Levinson, Stephen C.},
  date = {2009-06-30},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {106},
  pages = {10587--10592},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0903616106},
  url = {https://www.pnas.org/content/106/26/10587},
  urldate = {2020-05-28},
  abstract = {Informal verbal interaction is the core matrix for human social life. A mechanism for coordinating this basic mode of interaction is a system of turn-taking that regulates who is to speak and when. Yet relatively little is known about how this system varies across cultures. The anthropological literature reports significant cultural differences in the timing of turn-taking in ordinary conversation. We test these claims and show that in fact there are striking universals in the underlying pattern of response latency in conversation. Using a worldwide sample of 10 languages drawn from traditional indigenous communities to major world languages, we show that all of the languages tested provide clear evidence for a general avoidance of overlapping talk and a minimization of silence between conversational turns. In addition, all of the languages show the same factors explaining within-language variation in speed of response. We do, however, find differences across the languages in the average gap between turns, within a range of 250 ms from the cross-language mean. We believe that a natural sensitivity to these tempo differences leads to a subjective perception of dramatic or even fundamental differences as offered in ethnographic reports of conversational style. Our empirical evidence suggests robust human universals in this domain, where local variations are quantitative only, pointing to a single shared infrastructure for language use with likely ethological foundations.},
  eprint = {19553212},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BV6JI9E2\\Stivers et al. - 2009 - Universals and cultural variation in turn-taking i.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\LJZJKGA6\\10587.html},
  isbn = {9780903616102},
  keywords = {cooperation,response speed,social interaction},
  langid = {english},
  number = {26}
}

@article{stoltmannSyllablepointingGestureCoordination2017,
  title = {Syllable-Pointing Gesture Coordination in {{Polish}} Counting out Rhymes: {{The}} Effect of Speech Rate},
  author = {Stoltmann, K. and Fuchs, S.},
  date = {2017},
  journaltitle = {Journal of Multimodal Communication Studies},
  volume = {4},
  pages = {63--68},
  number = {1-2}
}

@article{streeckDepictingGesture2008,
  title = {Depicting by Gesture},
  author = {Streeck, Jürgen},
  date = {2008},
  journaltitle = {Gesture},
  volume = {8},
  pages = {285--301},
  issn = {1569-9773(Electronic),1568-1475(Print)},
  doi = {10.1075/gest.8.3.02str},
  abstract = {This paper deals with ways in which gestural "pictures" are made, i.e., manual depictions of phenomena in the world. The view that "iconic" gestures uniformly function by way of some resemblance between signifier and signified is rejected, giving way to an understanding of depiction by gesture as the achievement of a heterogeneous set of practices, some of which rely on relations of contiguity or indexicality to evoke commonly known objects or scenes. Others seem to be derivative of other representation methods (e.g., drawing on surfaces). The paper reviews some existing work on gestural depiction methods, offers a working heuristics, and illustrates some of its categories. It is suggested that some of the basic ways in which actions of the hands evoke the world in gesture correspond to fundamental modes of existence and activity of human hands in the world: hands depict by enacting their familiar, "real-world" capacities as users, transporters, experiencers, assemblers, molders, and shapers of things. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BVVVHGT5\\2009-01670-001.html},
  keywords = {Gestures,Hand (Anatomy),Heuristics,Iconic Memory},
  number = {3}
}

@incollection{studdert-kennedyLaunchingLanguageGestural2003,
  title = {Launching {{Language}}: {{The Gestural Origin}} of {{Discrete Infinity}}},
  shorttitle = {Launching {{Language}}},
  booktitle = {Language {{Evolution}}},
  author = {Studdert-Kennedy, Michael and Goldstein, Louis},
  editor = {Christiansen, Morten H. and Kirby, Simon},
  date = {2003-07-24},
  pages = {235--254},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199244843.003.0013},
  url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199244843.001.0001/acprof-9780199244843-chapter-13},
  urldate = {2020-03-16},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Y4DEGYFR\\Studdert-Kennedy and Goldstein - 2003 - Launching Language The Gestural Origin of Discret.pdf},
  isbn = {978-0-19-924484-3},
  langid = {english}
}

@article{sugiharaDetectingCausalityComplex2012,
  title = {Detecting {{Causality}} in {{Complex Ecosystems}}},
  author = {Sugihara, George and May, Robert and Ye, Hao and Hsieh, Chih-hao and Deyle, Ethan and Fogarty, Michael and Munch, Stephan},
  date = {2012-10-26},
  journaltitle = {Science},
  volume = {338},
  pages = {496--500},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1227079},
  url = {https://science.sciencemag.org/content/338/6106/496},
  urldate = {2020-01-16},
  abstract = {Identifying causal networks is important for effective policy and management recommendations on climate, epidemiology, financial regulation, and much else. We introduce a method, based on nonlinear state space reconstruction, that can distinguish causality from correlation. It extends to nonseparable weakly connected dynamic systems (cases not covered by the current Granger causality paradigm). The approach is illustrated both by simple models (where, in contrast to the real world, we know the underlying equations/relations and so can check the validity of our method) and by application to real ecological systems, including the controversial sardine-anchovy-temperature problem.
A new method, based on nonlinear state space reconstruction, can distinguish causality from correlation.
A new method, based on nonlinear state space reconstruction, can distinguish causality from correlation.},
  eprint = {22997134},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XMNY2UJX\\Sugihara et al. - 2012 - Detecting Causality in Complex Ecosystems.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\CFDTXH86\\496.html},
  langid = {english},
  number = {6106}
}

@article{sundbergInfluenceBodyPosture1991,
  title = {Influence of Body Posture and Lung Volume on Subglottal Pressure Control during Singing},
  author = {Sundberg, J. and Leanderson, R. and von Euler, C. and Knutsson, E.},
  date = {1991-01-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {5},
  pages = {283--291},
  issn = {0892-1997},
  doi = {10.1016/S0892-1997(05)80057-8},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199705800578},
  urldate = {2019-10-17},
  abstract = {The role of different breathing muscles during singing was investigated by synchronously recording EMG, pressure, and sound signals, using lung volume and gravity as experimental parameters. Surface EMG signals from the external and internal intercostals, the diaphragm, and the abdominal oblique muscles were recorded, while two singer subjects performed various singing tasks associated with rapid and precise changes of subglottal pressure. Esophageal and gastric pressures were measured by pressure transducers, and lung volume by means of impedance plethysmography. The results show that the breathing system efficiently compensates for drastic differences in the mechanics of the breathing apparatus, caused by differences in lung volume and gravity induced by changes of body posture.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\WAMA79AT\\S0892199705800578.html},
  keywords = {Abdominal oblique muscle,Body posture,Breathing,Diaphragm muscle,EMG,Intercostal muscles},
  langid = {english},
  number = {4},
  options = {useprefix=true}
}

@article{sundbergPhonatoryControlMale1993,
  title = {Phonatory Control in Male Singing: {{A}} Study of the Effects of Subglottal Pressure, Fundamental Frequency, and Mode of Phonation on the Voice Source},
  shorttitle = {Phonatory Control in Male Singing},
  author = {Sundberg, J. and Titze, I. and Scherer, R.},
  date = {1993-03-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {7},
  pages = {15--29},
  issn = {0892-1997},
  doi = {10.1016/S0892-1997(05)80108-0},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199705801080},
  urldate = {2020-02-25},
  abstract = {This article describes experiments carried out in order to gain a deeper understanding of the mechanisms underlying variation of vocal loudness in singers. Ten singers, two of whom are famous professional opera tenor soloists, phonated at different pitches and different loudnesses. Their voice source characteristics were analyzed by inverse filtering the oral airflow signal. It was found that the main physiological variable underlying loudness variation is subglottal pressure (Ps). The voice source property determining most of the loudness variation is the amplitude of the negative peak of the differentiated flow signal, as predicted by previous research. Increases in this amplitude are achieved by (a) increasing the pulse amplitude of the flow waveform; (b) moving the moment of vocal fold contact earlier in time, closer to the center of the pulse; and (c) skewing the pulses. The last mentioned alternative seems dependent on both Ps and the ratio between the fundamental frequency and the first formant. On the average, the singers doubled Ps when they increased fundamental frequency by one octave, and a doubling of the excess Ps over threshold caused the sound pressure level (SPL) to increase by 8–9 dB for neutral phonation, less if mode of phonation was changed to pressed. A shift of mode of phonation from flow over neutral to pressed was associated with a reduction of the peak glottal permittance i.e., the ratio between peak transglottal airflow to Ps. Flow phonation had the most favorable relationship between Ps and SPL.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5DRF82RP\\Sundberg et al. - 1993 - Phonatory control in male singing A study of the .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ZQKV67K9\\S0892199705801080.html},
  keywords = {Flow glottogram,Glottal permittance,Mode of phonation-Fundamental frequency,Subglottal pressure,Vocal loudness,Voice source},
  langid = {english},
  number = {1},
  series = {The {{Voice Foundation}}'s 22nd {{Annual Symposium}}}
}

@article{takahashiCoupledOscillatorDynamics2013,
  title = {Coupled {{Oscillator Dynamics}} of {{Vocal Turn}}-{{Taking}} in {{Monkeys}}},
  author = {Takahashi, Daniel Y. and Narayanan, Darshana Z. and Ghazanfar, Asif A.},
  date = {2013-11-04},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {23},
  pages = {2162--2168},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2013.09.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0960982213011172},
  urldate = {2020-05-28},
  abstract = {Cooperation is central to human communication [1, 2, 3]. The foundation of cooperative verbal communication is taking turns to speak. Given the universality of turn-taking [4], it is natural to ask how it evolved. We used marmoset monkeys to explore whether another primate species exhibits cooperative vocal communication by taking turns. Marmosets share with humans a cooperative breeding strategy and volubility. Cooperative care behaviors are thought to scaffold prosocial cognitive processes [5, 6]. Moreover, marmosets and other callitrichid primates are very vocal and readily exchange vocalizations with conspecifics [7, 8, 9, 10, 11]. By measuring the natural statistics of marmoset vocal exchanges, we observed that they take turns in extended sequences and show that this vocal turn-taking has as its foundation dynamics characteristic of coupled oscillators—one that is similar to the dynamics proposed for human conversational turn-taking [12]. As marmoset monkeys are on a different branch of the evolutionary tree that led to humans, our data demonstrate convergent evolution of vocal cooperation. Perhaps more importantly, our data offer a plausible alternative scenario to “gestural origin” hypotheses for how human cooperative vocal communication could have evolved.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IE5LZLMJ\\Takahashi et al. - 2013 - Coupled Oscillator Dynamics of Vocal Turn-Taking i.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\W7TENNMA\\S0960982213011172.html},
  langid = {english},
  number = {21}
}

@article{teshAbdominalMusclesVertebral1987,
  title = {The Abdominal Muscles and Vertebral Stability},
  author = {Tesh, K. M. and Dunn, J. S. and Evans, J. H.},
  date = {1987-06},
  journaltitle = {Spine},
  shortjournal = {Spine},
  volume = {12},
  pages = {501--508},
  issn = {0362-2436},
  doi = {10.1097/00007632-198706000-00014},
  abstract = {It has been suggested that the muscles of the anterolateral abdominal wall increase the stability of the lumbar region of the vertebral column by tensing the thoracolumbar fascia and by raising intra-abdominal pressure. In this report these new mechanisms are reviewed and their contribution to vertebral stability assessed. The thoracolumbar fascia consists of two principal layers of dense fibrous tissue that attach the abdominal muscles to the vertebral column. Each of these layers was dissected in fresh and fixed material and samples chosen for light and scanning electron microscopy to study the arrangement of the component fibers. Computed axial tomography in volunteers showed the changes in spatial organization that occur during flexion of the back and during the Valsalva maneuver. The fascia was then tensed experimentally in isolated unfixed motion segments. The results suggested that the stabilizing action of the thoracolumbar fascia is less than had been thought previously but was consistent with calculations based on the more accurate structural and mechanical information that had been derived from the current study. Abdominal muscle contraction was simulated in whole cadavers in both the flexed and lateral bending positions to compare the stabilizing effect of the thoracolumbar fascia and intra-abdominal pressure mechanisms. These definitive experiments showed that the resistance to bending in the sagittal plane offered by the abdominal muscles acting through fascial tension was of a similar magnitude to that offered by a raised intra-abdominal pressure, both being relatively small in the fully flexed position. The stabilizing influence of the middle layer of the thoracolumbar fascia in lateral bending was clearly demonstrated and warrants further study in vivo.},
  eprint = {2957802},
  eprinttype = {pmid},
  keywords = {Abdominal Muscles,Biomechanical Phenomena,Fascia,Humans,Lumbar Vertebrae,Pressure},
  langid = {english},
  number = {5}
}

@article{tilsenSpeechRhythmAnalysis2013,
  title = {Speech Rhythm Analysis with Decomposition of the Amplitude Envelope: {{Characterizing}} Rhythmic Patterns within and across Languages},
  shorttitle = {Speech Rhythm Analysis with Decomposition of the Amplitude Envelope},
  author = {Tilsen, Sam and Arvaniti, Amalia},
  date = {2013-07-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {134},
  pages = {628--639},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.4807565},
  url = {https://asa.scitation.org/doi/full/10.1121/1.4807565},
  urldate = {2020-05-12},
  abstract = {This study presents a method for analyzing speech rhythm using empirical mode decomposition of the speech amplitude envelope, which allows for extraction and quantification of syllabic- and supra-syllabic time-scale components of the envelope. The method of empirical mode decomposition of a vocalic energy amplitude envelope is illustrated in detail, and several types of rhythm metrics derived from this method are presented. Spontaneous speech extracted from the Buckeye Corpus is used to assess the effect of utterance length on metrics, and it is shown how metrics representing variability in the supra-syllabic time-scale components of the envelope can be used to identify stretches of speech with targeted rhythmic characteristics. Furthermore, the envelope-based metrics are used to characterize cross-linguistic differences in speech rhythm in the UC San Diego Speech Lab corpus of English, German, Greek, Italian, Korean, and Spanish speech elicited in read sentences, read passages, and spontaneous speech. The envelope-based metrics exhibit significant effects of language and elicitation method that argue for a nuanced view of cross-linguistic rhythm patterns.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QERBU8FV\\Tilsen and Arvaniti - 2013 - Speech rhythm analysis with decomposition of the a.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\XL9X9I5D\\1.html},
  number = {1}
}

@book{tomaselloOriginsHumanCommunication2008,
  title = {The Origins of Human Communication},
  author = {Tomasello, M.},
  date = {2008},
  publisher = {{MIT press}},
  location = {{Cambdride, MA}}
}

@article{tomaselloThirtyYearsGreat2019,
  title = {Thirty Years of Great Ape Gestures},
  author = {Tomasello, Michael and Call, Josep},
  date = {2019-07},
  journaltitle = {Animal Cognition},
  volume = {22},
  pages = {461--469},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-018-1167-1},
  url = {http://link.springer.com/10.1007/s10071-018-1167-1},
  urldate = {2020-03-06},
  abstract = {We and our colleagues have been doing studies of great ape gestural communication for more than 30 years. Here we attempt to spell out what we have learned. Some aspects of the process have been reliably established by multiple researchers, for example, its intentional structure and its sensitivity to the attentional state of the recipient. Other aspects are more controversial. We argue here that it is a mistake to assimilate great ape gestures to the species-typical displays of other mammals by claiming that they are fixed action patterns, as there are many differences, including the use of attention-getters. It is also a mistake, we argue, to assimilate great ape gestures to human gestures by claiming that they are used referentially and declaratively in a human-like manner, as apes’ “pointing” gesture has many limitations and they do not gesture iconically. Great ape gestures constitute a unique form of primate communication with their own unique qualities.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QBB3YSIV\\Tomasello and Call - 2019 - Thirty years of great ape gestures.pdf},
  langid = {english},
  number = {4}
}

@article{tormeneMatchingIncompleteTime2009,
  title = {Matching Incomplete Time Series with Dynamic Time Warping: An Algorithm and an Application to Post-Stroke Rehabilitation},
  shorttitle = {Matching Incomplete Time Series with Dynamic Time Warping},
  author = {Tormene, Paolo and Giorgino, Toni and Quaglini, Silvana and Stefanelli, Mario},
  date = {2009-01},
  journaltitle = {Artificial Intelligence in Medicine},
  volume = {45},
  pages = {11--34},
  issn = {09333657},
  doi = {10.1016/j.artmed.2008.11.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365708001772},
  urldate = {2019-05-15},
  abstract = {Objective: The purpose of this study was to assess the performance of a real-time (‘‘open-end’’) version of the dynamic time warping (DTW) algorithm for the recognition of motor exercises. Given a possibly incomplete input stream of data and a reference time series, the open-end DTW algorithm computes both the size of the prefix of reference which is best matched by the input, and the dissimilarity between the matched portions. The algorithm was used to provide real-time feedback to neurological patients undergoing motor rehabilitation.
Methods and materials: We acquired a dataset of multivariate time series from a sensorized long-sleeve shirt which contains 29 strain sensors distributed on the upper limb. Seven typical rehabilitation exercises were recorded in several variations, both correctly and incorrectly executed, and at various speeds, totaling a data set of 840 time series. Nearest-neighbour classifiers were built according to the outputs of openend DTW alignments and their global counterparts on exercise pairs. The classifiers were also tested on well-known public datasets from heterogeneous domains.
Results: Nonparametric tests show that (1) on full time series the two algorithms achieve the same classification accuracy ( p-value ¼ 0:32); (2) on partial time series, classifiers based on open-end DTW have a far higher accuracy (k ¼ 0:898 versus k ¼ 0:447; p {$<$} 10À5); and (3) the prediction of the matched fraction follows closely the ground truth (root mean square {$<$} 10\%). The results hold for the motor rehabilitation and the other datasets tested, as well.
Conclusions: The open-end variant of the DTW algorithm is suitable for the classification of truncated quantitative time series, even in the presence of noise. Early recognition and accurate class prediction can be achieved, provided that enough},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LNSWNC7R\\Tormene et al. - 2009 - Matching incomplete time series with dynamic time .pdf},
  langid = {english},
  number = {1}
}

@article{treffnerIntentionalAttentionalDynamics2002,
  title = {Intentional and Attentional Dynamics of Speech–Hand Coordination},
  author = {Treffner, P. and Peter, M.},
  date = {2002},
  journaltitle = {Human Movement Science},
  volume = {21},
  pages = {641--697},
  doi = {10.1016/S0167-9457(02)00178-1},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0167945702001781},
  urldate = {2019-08-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\M6EBIXEJ\\S0167945702001781.html},
  number = {5-6}
}

@article{trujilloMarkerlessAutomaticAnalysis2019,
  title = {Toward the Markerless and Automatic Analysis of Kinematic Features: {{A}} Toolkit for Gesture and Movement Research},
  shorttitle = {Toward the Markerless and Automatic Analysis of Kinematic Features},
  author = {Trujillo, James P. and Vaitonyte, Julija and Simanova, Irina and Özyürek, Asli},
  date = {2019-04-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {51},
  pages = {769--777},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-1086-8},
  url = {https://doi.org/10.3758/s13428-018-1086-8},
  urldate = {2020-03-19},
  abstract = {Action, gesture, and sign represent unique aspects of human communication that use form and movement to convey meaning. Researchers typically use manual coding of video data to characterize naturalistic, meaningful movements at various levels of description, but the availability of markerless motion-tracking technology allows for quantification of the kinematic features of gestures or any meaningful human movement. We present a novel protocol for extracting a set of kinematic features from movements recorded with Microsoft Kinect. Our protocol captures spatial and temporal features, such as height, velocity, submovements/strokes, and holds. This approach is based on studies of communicative actions and gestures and attempts to capture features that are consistently implicated as important kinematic aspects of communication. We provide open-source code for the protocol, a description of how the features are calculated, a validation of these features as quantified by our protocol versus manual coders, and a discussion of how the protocol can be applied. The protocol effectively quantifies kinematic features that are important in the production (e.g., characterizing different contexts) as well as the comprehension (e.g., used by addressees to understand intent and semantics) of manual acts. The protocol can also be integrated with qualitative analysis, allowing fast and objective demarcation of movement units, providing accurate coding even of complex movements. This can be useful to clinicians, as well as to researchers studying multimodal communication or human–robot interactions. By making this protocol available, we hope to provide a tool that can be applied to understanding meaningful movement characteristics in human communication.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\GVMYESRI\\Trujillo et al. - 2019 - Toward the markerless and automatic analysis of ki.pdf},
  langid = {english},
  number = {2}
}

@article{turveyCoordination1990,
  title = {Coordination},
  author = {Turvey, M. T.},
  date = {1990},
  journaltitle = {American Psychologist},
  volume = {45},
  pages = {938--953},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.45.8.938},
  abstract = {The Russian physiologist N. Bernstein (1967) defined coordination as a problem of mastering the very many degrees of freedom involved in a particular movement—of reducing the number of independent variables to be controlled. The initial theorizing and experimentation on "Bernstein's problem" was conducted largely in terms of how a device of very many independent variables might be regulated without ascribing excessive responsibility to an executive subsystem. A second round of theory and research on Bernstein's problem is now under way. This second round is motivated by similarities between coordination and physical processes in which multiple components become collectively self-organized; it is directed at an explanation of coordination in terms of very general laws and principles. The major achievements of the first round of efforts to address Bernstein's problem are summarized, and six examples of the theory and research typifying the second round are presented. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PXDZCKSF\\1990-30330-001.html},
  keywords = {Experimentation,Motor Coordination,Theories},
  number = {8}
}

@book{turveyLecturesPerceptionEcological2018,
  title = {Lectures on {{Perception}}: {{An Ecological Perspective}}},
  shorttitle = {Lectures on {{Perception}}},
  author = {Turvey, M. T.},
  date = {2018-11-07},
  edition = {1 edition},
  publisher = {{Routledge}},
  abstract = {Lectures on Perception: An Ecological Perspective addresses the generic principles by which each and every kind of life form―from single celled organisms (e.g., difflugia) to multi-celled organisms (e.g., primates)―perceives the circumstances of their living so that they can behave adaptively. It focuses on the fundamental ability that relates each and every organism to its surroundings, namely, the ability to perceive things in the sense of how to get about among them and what to do, or not to do, with them. The book’s core thesis breaks from the conventional interpretation of perception as a form of abduction based on innate hypotheses and acquired knowledge, and from the historical scientific focus on the perceptual abilities of animals, most especially those abilities ascribed to humankind. Specifically, it advances the thesis of perception as a matter of laws and principles at nature’s ecological scale, and gives equal theoretical consideration to the perceptual achievements of all of the classically defined ‘kingdoms’ of organisms―Archaea, Bacteria, Protoctista, Fungi, Plantae, and Animalia.},
  isbn = {978-1-138-33526-4},
  langid = {english},
  pagetotal = {446}
}

@article{turveyMediumHapticPerception2014,
  title = {The {{Medium}} of {{Haptic Perception}}: {{A Tensegrity Hypothesis}}},
  shorttitle = {The {{Medium}} of {{Haptic Perception}}},
  author = {Turvey, M. T. and Fonseca, S. T.},
  date = {2014-05},
  journaltitle = {Journal of Motor Behavior},
  volume = {46},
  pages = {143--187},
  issn = {0022-2895, 1940-1027},
  doi = {10.1080/00222895.2013.798252},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00222895.2013.798252},
  urldate = {2019-04-18},
  abstract = {For any given animal, the sources of mechanical disturbances inducing tissue deformation define environment from the perspective of the animal’s haptic perceptual system. The system’s achievements include perceiving the body, attachments to the body, and the surfaces and substances adjacent to the body. Among the perceptual systems, it stands alone in having no defined medium. There is no articulated functional equivalent to air and water, the media that make possible the energy transmissions and diffusions underpinning the other perceptual systems. To identify the haptic system’s medium the authors focus on connective tissue and the conjunction of muscular, connective tissue net, and skeletal (MCS) as the body’s proper characterization. The challenge is a biophysical formulation of MCS as a continuum that, similar to air and water, is homogeneous and isotropic. The authors hypothesized a multifractal tensegrity (MFT) with the shape and stability of the constituents of each scale, from individual cell to whole body, derivative of continuous tension and discontinuous compression. Each component tensegrity of MFT is an adjustive-receptive unit, and the array of tensions in MFT is information about MCS. The authors extend the MFT hypothesis to body-brain linkages, and to limb perception phenomena attendant to amputation, vibration, anesthesia, neuropathy, and microgravity.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QSVVYB7C\\Turvey and Fonseca - 2014 - The Medium of Haptic Perception A Tensegrity Hypo.pdf},
  langid = {english},
  number = {3}
}

@article{tyronePhoneticsHeadBody2016,
  title = {The {{Phonetics}} of {{Head}} and {{Body Movement}} in the {{Realization}} of {{American Sign Language Signs}}},
  author = {Tyrone, Martha E. and Mauk, Claude E.},
  date = {2016},
  journaltitle = {Phonetica},
  shortjournal = {PHO},
  volume = {73},
  pages = {120--140},
  publisher = {{Karger Publishers}},
  issn = {0031-8388, 1423-0321},
  doi = {10.1159/000443836},
  url = {https://www.karger.com/Article/FullText/443836},
  urldate = {2020-03-09},
  abstract = {Background/Aims: Because the primary articulators for sign languages are the hands, sign phonology and phonetics have focused mainly on them and treated other articulators as passive targets. However, there is abundant research on the role of nonmanual articulators in sign language grammar and prosody. The current study examines how hand and head/body movements are coordinated to realize phonetic targets. Methods: Kinematic data were collected from 5 deaf American Sign Language (ASL) signers to allow the analysis of movements of the hands, head and body during signing. In particular, we examine how the chin, forehead and torso move during the production of ASL signs at those three phonological locations. Results: Our findings suggest that for signs with a lexical movement toward the head, the forehead and chin move to facilitate convergence with the hand. By comparison, the torso does not move to facilitate convergence with the hand for signs located at the torso. Conclusion: These results imply that the nonmanual articulators serve a phonetic as well as a grammatical or prosodic role in sign languages. Future models of sign phonetics and phonology should take into consideration the movements of the nonmanual articulators in the realization of signs.},
  eprint = {27225639},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KNI39GPU\\Tyrone and Mauk - 2016 - The Phonetics of Head and Body Movement in the Rea.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\INQJT5XX\\443836.html},
  langid = {english},
  number = {2}
}

@article{valenteAdultsVisualRecognition2019,
  title = {Adults’ Visual Recognition of Actions Simulations by Finger Gestures ({{ASFGs}}) Produced by Sighted and Blind Individuals},
  author = {Valente, Dannyelle and Palama, Amaya and Malsert, Jennifer and Bolens, Guillemette and Gentaz, Edouard},
  date = {2019-03-28},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  pages = {e0214371},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0214371},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0214371},
  urldate = {2019-11-30},
  abstract = {The present study examines the visual recognition of action simulations by finger gestures (ASFGs) produced by sighted and blind individuals. In ASFGs, fingers simulate legs to represent actions such as jumping, spinning, climbing, etc. The question is to determine whether the common motor experience of one’s own body is sufficient to produce adequate ASFGs or whether the possibility to see gestures from others are also necessary to do it. Three experiments were carried out to address this question. Experiment 1 examined in 74 sighted adults the recognition of 18 types of ASFGs produced by 20 blindfolded sighted adults. Results showed that rates of correct recognition were globally very high, but varied with the type of ASFG. Experiment 2 studied in 91 other sighted adults the recognition of ASFGs produced by 10 early blind and 7 late blind adults. Results also showed a high level of recognition with a similar order of recognizability by type of ASFG. However, ASFGs produced by early blind individuals were more poorly recognized than those produced by late blind individuals. In order to match data of recognition obtained with the form that gestures are produced by individuals, two independant judges evaluated prototypical and atypical attributes of ASFG produced by blindfolded sighted, early blind and late blind individuals in Experiment 3. Results revealed the occurrence of more atypical attributes in ASFG produced by blind individuals: their ASFGs transpose more body movements from a character-viewpoint in less agreement with visual rules. The practical interest of the study relates to the relevance of including ASFGs as a new exploratory procedure in tactile devices which are more apt to convey action concepts to blind users/readers.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\W8QFSG9D\\Valente et al. - 2019 - Adults’ visual recognition of actions simulations .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\Q4LQTI9U\\article.html},
  keywords = {Adults,Blindness,Climbing,Congenital disorders,Fingers,Language,Surveys,Vision},
  langid = {english},
  number = {3}
}

@article{verhoefIconicityEmergenceCombinatorial2016,
  title = {Iconicity and the {{Emergence}} of {{Combinatorial Structure}} in {{Language}}},
  author = {Verhoef, T. and Kirby, S. and de Boer, B.},
  date = {2016-11},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  volume = {40},
  pages = {1969--1994},
  issn = {1551-6709},
  doi = {10.1111/cogs.12326},
  abstract = {In language, recombination of a discrete set of meaningless building blocks forms an unlimited set of possible utterances. How such combinatorial structure emerged in the evolution of human language is increasingly being studied. It has been shown that it can emerge when languages culturally evolve and adapt to human cognitive biases. How the emergence of combinatorial structure interacts with the existence of holistic iconic form-meaning mappings in a language is still unknown. The experiment presented in this paper studies the role of iconicity and human cognitive learning biases in the emergence of combinatorial structure in artificial whistled languages. Participants learned and reproduced whistled words for novel objects with the use of a slide whistle. Their reproductions were used as input for the next participant, to create transmission chains and simulate cultural transmission. Two conditions were studied: one in which the persistence of iconic form-meaning mappings was possible and one in which this was experimentally made impossible. In both conditions, cultural transmission caused the whistled languages to become more learnable and more structured, but this process was slightly delayed in the first condition. Our findings help to gain insight into when and how words may lose their iconic origins when they become part of an organized linguistic system.},
  eprint = {26706244},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MK2NES8M\\Verhoef et al. - 2016 - Iconicity and the Emergence of Combinatorial Struc.pdf},
  keywords = {Adult,Cognition,Cognitive biases,Combinatorial structure,Concept Formation,Cultural evolution,Cultural Evolution,Culture,Female,Humans,Iconicity,Iterated learning,Language,Language Development,Language evolution,Male,Young Adult},
  langid = {english},
  number = {8},
  options = {useprefix=true}
}

@article{vervloedTeachingMeaningWords2014,
  title = {Teaching the {{Meaning}} of {{Words}} to {{Children}} with {{Visual Impairments}}},
  author = {Vervloed, Mathijs P. J. and Loijens, Nancy E. A. and Waller, Sarah E.},
  date = {2014-09-01},
  journaltitle = {Journal of Visual Impairment \& Blindness},
  shortjournal = {Journal of Visual Impairment \& Blindness},
  volume = {108},
  pages = {433--438},
  issn = {0145-482X},
  doi = {10.1177/0145482X1410800508},
  url = {https://doi.org/10.1177/0145482X1410800508},
  urldate = {2019-11-30},
  langid = {english},
  number = {5}
}

@article{voisinClavicleNeglectedBone2006,
  title = {Clavicle, a Neglected Bone: {{Morphology}} and Relation to Arm Movements and Shoulder Architecture in Primates},
  shorttitle = {Clavicle, a Neglected Bone},
  author = {Voisin, Jean-Luc},
  date = {2006},
  journaltitle = {The Anatomical Record Part A: Discoveries in Molecular, Cellular, and Evolutionary Biology},
  volume = {288A},
  pages = {944--953},
  issn = {1552-4892},
  doi = {10.1002/ar.a.20354},
  url = {https://anatomypubs.onlinelibrary.wiley.com/doi/abs/10.1002/ar.a.20354},
  urldate = {2020-04-27},
  abstract = {In spite of its importance for movements of the upper limbs, the clavicle is an infrequently studied shoulder bone. The present study compares clavicular morphology among different extant primates. Methods have included the assessment of clavicular curvatures projected on two perpendicular planes that can be assessed overall as cranial and dorsal primary curvatures. Results showed that in cranial view, three morphologies can be defined. One group exhibited an external curvature considerably more pronounced than the internal one (Gorilla, Papio); a second group was characterized by an internal curvature much more pronounced than the external one (Hylobates, Ateles); and a third group contained those with the two curvatures equally pronounced (Pan, Homo, Pongo, Procolobus, Colobus). Clavicle curvatures projected on the dorsal plane could be placed into four groups. The first group is characterized by two curvatures, an inferior and a superior (Apes, Spider monkeys). The second included monkeys, whose clavicles have an inferior curvature much more pronounced than the superior one. The third group includes only Hylobates, whose clavicles possess only the superior curvature. The last group includes only modern humans, whose clavicles show only the inferior curvature, which is less pronounced than that which exists in monkeys. Curvatures in cranial view relate information regarding the parameters of arm elevation while those in dorsal view offer insights into the position of the scapula related to the thorax. The use of clavicular curvature analysis offers a new dimension in assessment of the functional morphology of the clavicle and its relationship to the shoulder complex. Anat Rec Part A, 288A:944–953, 2006. © 2006 Wiley-Liss, Inc.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YCFW8D39\\Voisin - 2006 - Clavicle, a neglected bone Morphology and relatio.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\PANQMDV9\\ar.a.html},
  keywords = {brachiation,catyrrhine,clavicle,hominoid,locomotion,platyrrhine,shoulder},
  langid = {english},
  number = {9}
}

@software{wadhwaTDAstatsPipelineTopological2019,
  title = {{{TDAstats}}: {{Pipeline}} for {{Topological Data Analysis}}},
  shorttitle = {{{TDAstats}}},
  author = {Wadhwa, Raoul and Dhawan, Andrew and Williamson, Drew and Scott, Jacob and Brunson, Jason Cory and Ochi, Shota},
  date = {2019-12-12},
  url = {https://CRAN.R-project.org/package=TDAstats},
  urldate = {2020-03-11},
  abstract = {A comprehensive toolset for any useR conducting topological data analysis, specifically via the calculation of persistent homology in a Vietoris-Rips complex. The tools this package currently provides can be conveniently split into three main sections: (1) calculating persistent homology; (2) conducting statistical inference on persistent homology calculations; (3) visualizing persistent homology and statistical inference. The published form of TDAstats can be found in Wadhwa et al. (2018) {$<$}doi:10.21105/joss.00860{$>$}. For a general background on computing persistent homology for topological data analysis, see Otter et al. (2017) {$<$}doi:10.1140/epjds/s13688-017-0109-5{$>$}. To learn more about how the permutation test is used for nonparametric statistical inference in topological data analysis, read Robinson \& Turner (2017) {$<$}doi:10.1007/s41468-017-0008-7{$>$}. To learn more about how TDAstats calculates persistent homology, you can visit the GitHub repository for Ripser, the software that works behind the scenes at {$<$}https://github.com/Ripser/ripser{$>$}. This package has been published as Wadhwa et al. (2018) {$<$}doi:10.21105/joss.00860{$>$}.},
  version = {0.4.1}
}

@article{wagnerExploitingSpeechgestureLinkInPress,
  title = {Exploiting the Speech-Gesture Link to Capture Fine-Grained Prosodic Prominence Impressions and Listening Strategies},
  author = {Wagner, Petra and Cwiek, A. and Samlowksi, B.},
  year = {In Press},
  journaltitle = {Journal of Phonetics},
  pages = {74},
  abstract = {In this paper, we explore the possibility to gather perceptual impressions of prosodic prominence by exploiting the strong prosody-gesture link, i.e., by having listeners transform a perceptual impression into a motor movement, namely drumming, for two domains of prominence: word-level and syllablelevel. A feasibility study reveals that such a procedure is indeed easily and speedily mastered by na¨ıve listeners, but more difficult for word-level prominences. We furthermore examine whether “drummed” annotations are comparable to those gathered with more established annotation protocols based on cumulative na¨ıve impressions and fine-grained expert ratings. These comparisons reveal high correspondences across all prominence annotation protocols, thus corroborating the general usefulness of the gestural approach. The analyses also reveal that all annotation protocols are strongly driven by structural linguistic considerations. We then use Random Forest Models to investigate the relative impact of signal and structural cues to prominence annotations. We find that expert ratings of prosodic prominence are guided comparatively more by structural concerns than those of na¨ıve annotators, that word-level annotations are influenced more by structural linguistic cues than syllable-level ones, and that “drummed” annotations are driven least by structural cues. Lastly, we isolate two main listener strategies among our group of “drummers”, namely those integrating structural and signal cues to prominence, and those being guided predominantly by signal cues.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ETFMPUHE\\Wagner - Exploiting the speech-gesture link to capture ﬁne-.pdf},
  langid = {english}
}

@article{wagnerGestureSpeechInteraction2014,
  title = {Gesture and Speech in Interaction: {{An}} Overview},
  shorttitle = {Gesture and Speech in Interaction},
  author = {Wagner, Petra and Malisz, Zofia and Kopp, Stefan},
  date = {2014-02-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {57},
  pages = {209--232},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2013.09.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639313001295},
  urldate = {2019-04-16},
  abstract = {Gestures and speech interact. They are linked in language production and perception, with their interaction contributing to felicitous communication. The multifaceted nature of these interactions has attracted considerable attention from the speech and gesture community. This article provides an overview of our current understanding of manual and head gesture form and function, of the principle functional interactions between gesture and speech aiding communication, transporting meaning and producing speech. Furthermore, we present an overview of research on temporal speech-gesture synchrony, including the special role of prosody in speech-gesture alignment. In addition, we provide a summary of tools and data available for gesture analysis, and describe speech-gesture interaction models and simulations in technical systems. This overview also serves as an introduction to a Special Issue covering a wide range of articles on these topics. We provide links to the Special Issue throughout this paper.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\I8QZH4YS\\S0167639313001295.html}
}

@article{wallotInteractionDominantCausationMind2018,
  title = {Interaction-{{Dominant Causation}} in {{Mind}} and {{Brain}}, and {{Its Implication}} for {{Questions}} of {{Generalization}} and {{Replication}}},
  author = {Wallot, Sebastian and Kelty-Stephen, Damian G.},
  date = {2018-06-01},
  journaltitle = {Minds and Machines},
  shortjournal = {Minds \& Machines},
  volume = {28},
  pages = {353--374},
  issn = {1572-8641},
  doi = {10.1007/s11023-017-9455-0},
  url = {https://doi.org/10.1007/s11023-017-9455-0},
  urldate = {2020-02-12},
  abstract = {The dominant assumption about the causal architecture of the mind is, that it is composed of a stable set of components that contribute independently to relevant observables that are employed to measure cognitive activity. This view has been called component-dominant dynamics. An alternative has been proposed, according to which the different components are not independent, but fundamentally interdependent, and are not stable basic properties of the mind, but rather an emergent feature of the mind given a particular task context. This view has been called interaction-dominant dynamics. In this paper, we review evidence for interaction-dominant dynamics as the causal architecture of the mind. We point out, that such an architecture is consistent with problems of convergence in research on the level of results and theorizing. Moreover, we point out that if interaction-dominant dynamics as the causal architecture of the mind were to be true, this would naturally lead to (some degree of) problems with generalization and replicability in sciences of the mind and brain, and would probably warrant changes in the scientific practice with regard to study-design and data analysis.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DYD768VE\\Wallot and Kelty-Stephen - 2018 - Interaction-Dominant Causation in Mind and Brain, .pdf},
  langid = {english},
  number = {2}
}

@article{wallotRecurrenceQuantificationAnalysis2017,
  title = {Recurrence {{Quantification Analysis}} of {{Processes}} and {{Products}} of {{Discourse}}: {{A Tutorial}} in {{R}}},
  shorttitle = {Recurrence {{Quantification Analysis}} of {{Processes}} and {{Products}} of {{Discourse}}},
  author = {Wallot, Sebastian},
  date = {2017-07-04},
  journaltitle = {Discourse Processes},
  volume = {54},
  pages = {382--405},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2017.1297921},
  url = {https://doi.org/10.1080/0163853X.2017.1297921},
  urldate = {2019-09-20},
  abstract = {Processes of naturalistic reading and writing are based on complex linguistic input, stretch-out over time, and rely on an integrated performance of multiple perceptual, cognitive, and motor processes. Hence, naturalistic reading and writing performance is nonstationary and exhibits fluctuations and transitions. However, instead of being just complications for the analysis of such data, they are also informative about cognitive change, fluency, and reading or writing skill. To use and quantify such dynamics, one needs appropriate statistics that capture these aspects. In this article I introduce Recurrence Quantification Analysis (RQA) as a tool to capture such dynamic structure. After a conceptual introduction of the analysis, I present a step-by-step tutorial on how to run RQA using R. Guidance is given with regard to common issues and best practices using this time-series analysis technique. Finally, I review previous results from studies applying RQA to reading and writing and summarize current hypotheses and interpretations of RQA measures in the context of reading and writing.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BBK6PIN3\\Wallot - 2017 - Recurrence Quantification Analysis of Processes an.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ALHILH8H\\0163853X.2017.html},
  number = {5-6}
}

@article{wangSpeakingRhythmicallyImproves2018,
  title = {Speaking Rhythmically Improves Speech Recognition under "Cocktail-Party" Conditions},
  author = {Wang, Mengyuan and Kong, Lingzhi and Zhang, Changxin and Wu, Xihong and Li, Liang},
  date = {2018-04},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {143},
  pages = {EL255},
  issn = {1520-8524},
  doi = {10.1121/1.5030518},
  abstract = {This study examines whether speech rhythm affects speech recognition under "cocktail-party" conditions. Against a two-talker masker, but not a speech-spectrum noise masker, recognition of the last (third) keyword in a normal rhythmic sentence was significantly better than that of the first keyword. However, this word-position-related speech-recognition improvement disappeared for rhythmically hybrid target sentences that were constructed by grouping parts from different sentences with different artificially modulated rhythms (rates) (fast, normal, or slow). Thus, the normal rhythm with a constant rate plays a role in improving speech recognition against informational speech masking, probably through a build-up of temporal prediction for target words.},
  eprint = {29716270},
  eprinttype = {pmid},
  langid = {english},
  number = {4}
}

@book{webberRecurrenceQuantificationAnalysis2015,
  title = {Recurrence {{Quantification Analysis}}: {{Theory}} and {{Best Practices}}},
  author = {Webber, C. L. and Marwan, N.},
  date = {2015},
  publisher = {{Springer}},
  location = {{Cham}},
  url = {https://doi.org/10.1007/978-3-319-07155-8}
}

@software{wickhamGgplot2CreateElegant2019,
  title = {Ggplot2: {{Create Elegant Data Visualisations Using}} the {{Grammar}} of {{Graphics}}},
  shorttitle = {Ggplot2},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and RStudio},
  date = {2019-04-07},
  url = {https://CRAN.R-project.org/package=ggplot2},
  urldate = {2019-04-23},
  abstract = {A system for 'declaratively' creating graphics, based on "The Grammar of Graphics". You provide the data, tell 'ggplot2' how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.},
  keywords = {Graphics,Phylogenetics,TeachingStatistics},
  version = {3.1.1}
}

@article{wielingAnalyzingDynamicPhonetic2018,
  title = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling: {{A}} Tutorial Focusing on Articulatory Differences between {{L1}} and {{L2}} Speakers of {{English}}},
  shorttitle = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling},
  author = {Wieling, Martijn},
  date = {2018-09},
  journaltitle = {Journal of Phonetics},
  volume = {70},
  pages = {86--116},
  issn = {00954470},
  doi = {10.1016/j.wocn.2018.03.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447017301377},
  urldate = {2019-10-12},
  abstract = {In phonetics, many datasets are encountered which deal with dynamic data collected over time. Examples include diphthongal formant trajectories and articulator trajectories observed using electromagnetic articulography. Traditional approaches for analyzing this type of data generally aggregate data over a certain timespan, or only include measurements at a fixed time point (e.g., formant measurements at the midpoint of a vowel). In this paper, I discuss generalized additive modeling, a non-linear regression method which does not require aggregation or the pre-selection of a fixed time point. Instead, the method is able to identify general patterns over dynamically varying data, while simultaneously accounting for subject and item-related variability. An advantage of this approach is that patterns may be discovered which are hidden when data is aggregated or when a single time point is selected. A corresponding disadvantage is that these analyses are generally more time consuming and complex. This tutorial aims to overcome this disadvantage by providing a hands-on introduction to generalized additive modeling using articulatory trajectories from L1 and L2 speakers of English within the freely available R environment. All data and R code is made available to reproduce the analysis presented in this paper.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3HBTJ6T5\\Wieling - 2018 - Analyzing dynamic phonetic data using generalized .pdf},
  langid = {english}
}

@article{wielingAnalyzingDynamicPhonetic2018a,
  title = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling: {{A}} Tutorial Focusing on Articulatory Differences between {{L1}} and {{L2}} Speakers of {{English}}},
  shorttitle = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling},
  author = {Wieling, Martijn},
  date = {2018-09-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {70},
  pages = {86--116},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2018.03.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447017301377},
  urldate = {2019-10-12},
  abstract = {In phonetics, many datasets are encountered which deal with dynamic data collected over time. Examples include diphthongal formant trajectories and articulator trajectories observed using electromagnetic articulography. Traditional approaches for analyzing this type of data generally aggregate data over a certain timespan, or only include measurements at a fixed time point (e.g., formant measurements at the midpoint of a vowel). This paper discusses generalized additive modeling, a non-linear regression method which does not require aggregation or the pre-selection of a fixed time point. Instead, the method is able to identify general patterns over dynamically varying data, while simultaneously accounting for subject and item-related variability. An advantage of this approach is that patterns may be discovered which are hidden when data is aggregated or when a single time point is selected. A corresponding disadvantage is that these analyses are generally more time consuming and complex. This tutorial aims to overcome this disadvantage by providing a hands-on introduction to generalized additive modeling using articulatory trajectories from L1 and L2 speakers of English within the freely available R environment. All data and R code is made available to reproduce the analysis presented in this paper.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\6ILUPIEI\\Wieling - 2018 - Analyzing dynamic phonetic data using generalized .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\N67F9T36\\S0095447017301377.html},
  keywords = {Dynamic data,Electromagnetic articulography,Generalized additive modeling,Tutorial}
}

@article{wilburExperimentalInvestigationStressed1990,
  title = {An Experimental Investigation of Stressed Sign Production},
  author = {Wilbur, R. B.},
  date = {1990},
  journaltitle = {International Journal of Sign Linguistics},
  volume = {1},
  number = {1}
}

@article{willardThoracolumbarFasciaAnatomy2012,
  title = {The Thoracolumbar Fascia: Anatomy, Function and Clinical Considerations},
  shorttitle = {The Thoracolumbar Fascia},
  author = {Willard, F H and Vleeming, A and Schuenke, M D and Danneels, L and Schleip, R},
  date = {2012-12},
  journaltitle = {Journal of Anatomy},
  shortjournal = {J Anat},
  volume = {221},
  pages = {507--536},
  issn = {0021-8782},
  doi = {10.1111/j.1469-7580.2012.01511.x},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3512278/},
  urldate = {2020-04-03},
  abstract = {In this overview, new and existent material on the organization and composition of the thoracolumbar fascia (TLF) will be evaluated in respect to its anatomy, innervation biomechanics and clinical relevance. The integration of the passive connective tissues of the TLF and active muscular structures surrounding this structure are discussed, and the relevance of their mutual interactions in relation to low back and pelvic pain reviewed. The TLF is a girdling structure consisting of several aponeurotic and fascial layers that separates the paraspinal muscles from the muscles of the posterior abdominal wall. The superficial lamina of the posterior layer of the TLF (PLF) is dominated by the aponeuroses of the latissimus dorsi and the serratus posterior inferior. The deeper lamina of the PLF forms an encapsulating retinacular sheath around the paraspinal muscles. The middle layer of the TLF (MLF) appears to derive from an intermuscular septum that developmentally separates the epaxial from the hypaxial musculature. This septum forms during the fifth and sixth weeks of gestation. The paraspinal retinacular sheath (PRS) is in a key position to act as a ‘hydraulic amplifier’, assisting the paraspinal muscles in supporting the lumbosacral spine. This sheath forms a lumbar interfascial triangle (LIFT) with the MLF and PLF. Along the lateral border of the PRS, a raphe forms where the sheath meets the aponeurosis of the transversus abdominis. This lateral raphe is a thickened complex of dense connective tissue marked by the presence of the LIFT, and represents the junction of the hypaxial myofascial compartment (the abdominal muscles) with the paraspinal sheath of the epaxial muscles. The lateral raphe is in a position to distribute tension from the surrounding hypaxial and extremity muscles into the layers of the TLF. At the base of the lumbar spine all of the layers of the TLF fuse together into a thick composite that attaches firmly to the posterior superior iliac spine and the sacrotuberous ligament. This thoracolumbar composite (TLC) is in a position to assist in maintaining the integrity of the lower lumbar spine and the sacroiliac joint. The three-dimensional structure of the TLF and its caudally positioned composite will be analyzed in light of recent studies concerning the cellular organization of fascia, as well as its innervation. Finally, the concept of a TLC will be used to reassess biomechanical models of lumbopelvic stability, static posture and movement.},
  eprint = {22630613},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9XJCZZ4H\\Willard et al. - 2012 - The thoracolumbar fascia anatomy, function and cl.pdf},
  number = {6},
  pmcid = {PMC3512278}
}

@article{willburExperimentalInvestigationStressed,
  title = {An Experimental Investigation of Stressed Sign Production},
  author = {Willbur, R. B.},
  journaltitle = {International Journal of Sign Linguistics},
  volume = {1},
  pages = {41--60},
  number = {1}
}

@article{wilsonEmbodiedCognitionNot2013,
  title = {Embodied {{Cognition}} Is {{Not What}} You {{Think}} It Is},
  author = {Wilson, Andrew D. and Golonka, Sabrina},
  date = {2013},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {4},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00058},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00058/full},
  urldate = {2020-03-08},
  abstract = {The most exciting hypothesis in cognitive science right now is the theory that cognition is embodied. Like all good ideas in cognitive science, however, embodiment immediately came to mean six different things. The most common definitions involve the straightforward claim that ‘states of the body modify states of the mind’. However, the implications of embodiment are actually much more radical than this. If cognition can span the brain, body and the environment, then the ‘states of mind’ of disembodied cognitive science won’t exist to be modified. Cognition will instead be an extended system assembled from a broad array of resources. Taking embodiment seriously therefore requires both new methods and theory. Here we outline four key steps that research programmes should follow in order to fully engage with the implications of embodiment. The first step is to conduct a task analysis, which characterises from a first person perspective the specific task that a perceiving-acting cognitive agent is faced with. The second step is to identify the task-relevant resources the agent has access to in order to solve the task. These resources can span brain, body and environment. The third step is to identify how the agent can assemble these resources into a system capable of solving the problem at hand. The last step is to test the agent’s performance to confirm that agent is actually using the solution identified in step 3. We explore these steps in more detail with reference to two useful examples (the outfielder problem and the A-not-B error), and introduce how to apply this analysis to the thorny question of language use. Embodied cognition is more than we think it is, and we have the tools we need to realise its full potential.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CPVFCASV\\Wilson and Golonka - 2013 - Embodied Cognition is Not What you Think it is.pdf},
  keywords = {A-not-B error,dynamical systems,Embodied Cognition,Language,outfielder problem,replacement hypothesis,Robotics},
  langid = {english}
}

@article{wilsonOscillatorModelTiming2005,
  title = {An Oscillator Model of the Timing of Turn-Taking},
  author = {Wilson, Margaret and Wilson, Thomas P.},
  date = {2005-12-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {12},
  pages = {957--968},
  issn = {1531-5320},
  doi = {10.3758/BF03206432},
  url = {https://doi.org/10.3758/BF03206432},
  urldate = {2020-05-19},
  abstract = {When humans talk without conventionalized arrangements, they engage in conversation—that is, a continuous and largely nonsimultaneous exchange in which speakers take turns. Turn-taking is ubiquitous in conversation and is the normal case against which alternatives, such as interruptions, are treated as violations that warrant repair. Furthermore, turn-taking involves highly coordinated timing, including a cyclic rise and fall in the probability of initiating speech during brief silences, and involves the notable rarity, especially in two-party conversations, of two speakers’ breaking a silence at once. These phenomena, reported by conversation analysts, have been neglected by cognitive psychologists, and to date there has been no adequate cognitive explanation. Here, we propose that, during conversation, endogenous oscillators in the brains of the speaker and the listeners become mutually entrained, on the basis of the speaker’s rate of syllable production. This entrained cyclic pattern governs the potential for initiating speech at any given instant for the speaker and also for the listeners (as potential next speakers). Furthermore, the readiness functions of the listeners are counterphased with that of the speaker, minimizing the likelihood of simultaneous starts by a listener and the previous speaker. This mutual entrainment continues for a brief period when the speech stream ceases, accounting for the cyclic property of silences. This model not only captures the timing phenomena observed in the literature on conversation analysis, but also converges with findings from the literatures on phoneme timing, syllable organization, and interpersonal coordination.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QMH69846\\Wilson and Wilson - 2005 - An oscillator model of the timing of turn-taking.pdf},
  langid = {english},
  number = {6}
}

@article{wilsonRhythmicEntrainmentWhy2016,
  title = {Rhythmic Entrainment: {{Why}} Humans Want to, Fireflies Can't Help It, Pet Birds Try, and Sea Lions Have to Be Bribed},
  shorttitle = {Rhythmic Entrainment},
  author = {Wilson, Margaret and Cook, Peter F.},
  date = {2016-12},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {23},
  pages = {1647--1659},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1013-x},
  abstract = {Until recently, the literature on rhythmic ability took for granted that only humans are able to synchronize body movements to an external beat-to entrain. This assumption has been undercut by findings of beat-matching in various species of parrots and, more recently, in a sea lion, several species of primates, and possibly horses. This throws open the question of how widespread beat-matching ability is in the animal kingdom. Here we reassess the arguments and evidence for an absence of beat-matching in animals, and conclude that in fact no convincing case against beat-matching in animals has been made. Instead, such evidence as there is suggests that this capacity could be quite widespread. Furthermore, mutual entrainment of oscillations is a general principle of physical systems, both biological and nonbiological, suggesting that entrainment of motor systems by sensory systems may be a default rather than an oddity. The question then becomes, not why a few privileged species are able to beat-match, but why species do not always do so-why they vary in both spontaneous and learned beat-matching. We propose that when entrainment is not driven by fixed, mandatory connections between input and output (as in the case of, e.g., fireflies entraining to each others' flashes), it depends on voluntary control over, and voluntary or learned coupling of, sensory and motor systems, which can paradoxically lead to apparent failures of entrainment. Among the factors that affect whether an animal will entrain are sufficient control over the motor behavior to be entrained, sufficient perceptual sophistication to extract the entraining beat from the overall sensory environment, and the current cognitive state of the animal, including attention and motivation. The extent of entrainment in the animal kingdom potentially has widespread implications, not only for understanding the roots of human dance, but also for understanding the neural and cognitive architectures of animals.},
  eprint = {26920589},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DDMH7YR4\\Wilson and Cook - 2016 - Rhythmic entrainment Why humans want to, fireflie.pdf},
  keywords = {Animal cognition,Animals,Behavior; Animal,Entrainment,Humans,Motor Activity,Time Perception},
  langid = {english},
  number = {6}
}

@article{wilsonStructureSilenceTurns1986,
  title = {The Structure of Silence between Turns in Two‐party Conversation},
  author = {Wilson, Thomas P. and Zimmerman, Don H.},
  date = {1986-10},
  journaltitle = {Discourse Processes},
  volume = {9},
  pages = {375--390},
  issn = {0163-853X, 1532-6950},
  doi = {10.1080/01638538609544649},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01638538609544649},
  urldate = {2020-05-19},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\GW8KUN2S\\Wilson and Zimmerman - 1986 - The structure of silence between turns in two‐part.pdf},
  langid = {english},
  number = {4}
}

@software{winkelmannWrasspInterfaceASSP2018,
  title = {Wrassp: {{Interface}} to the '{{ASSP}}' {{Library}}},
  shorttitle = {Wrassp},
  author = {Winkelmann, Raphael and Bombien, Lasse and Scheffers, Michel},
  date = {2018-08-31},
  url = {https://CRAN.R-project.org/package=wrassp},
  urldate = {2019-09-24},
  abstract = {A wrapper around Michel Scheffers's 'libassp' ({$<$}http://libassp.sourceforge.net/{$>$}). The 'libassp' (Advanced Speech Signal Processor) library aims at providing functionality for handling speech signal files in most common audio formats and for performing analyses common in phonetic science/speech science. This includes the calculation of formants, fundamental frequency, root mean square, auto correlation, a variety of spectral analyses, zero crossing rate, filtering etc. This wrapper provides R with a large subset of 'libassp's signal processing functions and provides them to the user in a (hopefully) user-friendly manner.},
  version = {0.1.8}
}

@article{wittenburgELANProfessionalFramework2006,
  title = {{{ELAN}}: A {{Professional Framework}} for {{Multimodality Research}}},
  author = {Wittenburg, Peter and Brugman, Hennie and Russel, Albert and Klassmann, Alex and Sloetjes, Han},
  date = {2006},
  pages = {4},
  abstract = {Utilization of computer tools in linguistic research has gained importance with the maturation of media frameworks for the handling of digital audio and video. The increased use of these tools in gesture, sign language and multimodal interaction studies has led to stronger requirements on the flexibility, the efficiency and in particular the time accuracy of annotation tools. This paper describes the efforts made to make ELAN a tool that meets these requirements, with special attention to the developments in the area of time accuracy. In subsequent sections an overview will be given of other enhancements in the latest versions of ELAN, that make it a useful tool in multimodality research.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Q84I5QW9\\Wittenburg et al. - ELAN a Professional Framework for Multimodality R.pdf},
  langid = {english}
}

@book{woodGeneralizedAdditiveModels2017,
  title = {Generalized {{Additive Models}}: {{An Introduction}} with {{R}}, {{Second Edition}}},
  shorttitle = {Generalized {{Additive Models}}},
  author = {Wood, S. N.},
  date = {2017},
  publisher = {{Chapman and Hall/CRC}},
  url = {https://www.crcpress.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331},
  urldate = {2019-10-12},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NZSPGFBD\\9781498728331.html},
  langid = {english}
}

@article{yamamotoInfluenceAttractorStability2020,
  title = {The Influence of Attractor Stability of Intrinsic Coordination Patterns on the Adaptation to New Constraints},
  author = {Yamamoto, Kota and Shinya, Masahiro and Kudo, Kazutoshi},
  date = {2020-12},
  journaltitle = {Scientific Reports},
  volume = {10},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-60066-7},
  url = {http://www.nature.com/articles/s41598-020-60066-7},
  urldate = {2020-03-26},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HJNIN4VL\\Yamamoto et al. - 2020 - The influence of attractor stability of intrinsic .pdf},
  langid = {english},
  number = {1}
}

@article{zdrazilovaCommunicatingAbstractMeaning2018,
  title = {Communicating Abstract Meaning: Concepts Revealed in Words and Gestures},
  shorttitle = {Communicating Abstract Meaning},
  author = {Zdrazilova, Lenka and Sidhu, David M. and Pexman, Penny M.},
  date = {2018-08-05},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {373},
  pages = {20170138},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2017.0138},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2017.0138},
  urldate = {2020-03-26},
  abstract = {words refer to concepts that cannot be directly experienced through our senses (e.g. truth, morality). How we ground the meanings of abstract words is one of the deepest problems in cognitive science today. We investigated this question in an experiment in which 62 participants were asked to communicate the meanings of words (20 abstract nouns, e.g. impulse; 10 concrete nouns, e.g. insect) to a partner without using the words themselves (the taboo task). We analysed the speech and associated gestures that participants used to communicate the meaning of each word in the taboo task. Analysis of verbal and gestural data yielded a number of insights. When communicating about the meanings of abstract words, participants' speech referenced more people and introspections. In contrast, the meanings of concrete words were communicated by referencing more objects and entities. Gesture results showed that when participants spoke about abstract word meanings their speech was accompanied by more metaphorical and beat gestures, and speech about concrete word meanings was accompanied by more iconic gestures. Taken together, the results suggest that abstract meanings are best captured by a model that allows dynamic access to multiple representation systems.This article is part of the theme issue ‘Varieties of abstract concepts: development, use and representation in the brain’.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZJU45ZL3\\Zdrazilova et al. - 2018 - Communicating abstract meaning concepts revealed .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\43ZD5MMS\\rstb.2017.html},
  number = {1752}
}

@article{zdrazilovaCommunicatingAbstractMeaning2018a,
  title = {Communicating Abstract Meaning: Concepts Revealed in Words and Gestures},
  shorttitle = {Communicating Abstract Meaning},
  author = {Zdrazilova, Lenka and Sidhu, David M. and Pexman, Penny M.},
  date = {2018-08-05},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {373},
  pages = {20170138},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2017.0138},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rstb.2017.0138},
  urldate = {2020-03-26},
  abstract = {words refer to concepts that cannot be directly experienced through our senses (e.g. truth, morality). How we ground the meanings of abstract words is one of the deepest problems in cognitive science today. We investigated this question in an experiment in which 62 participants were asked to communicate the meanings of words (20 abstract nouns, e.g. impulse; 10 concrete nouns, e.g. insect) to a partner without using the words themselves (the taboo task). We analysed the speech and associated gestures that participants used to communicate the meaning of each word in the taboo task. Analysis of verbal and gestural data yielded a number of insights. When communicating about the meanings of abstract words, participants' speech referenced more people and introspections. In contrast, the meanings of concrete words were communicated by referencing more objects and entities. Gesture results showed that when participants spoke about abstract word meanings their speech was accompanied by more metaphorical and beat gestures, and speech about concrete word meanings was accompanied by more iconic gestures. Taken together, the results suggest that abstract meanings are best captured by a model that allows dynamic access to multiple representation systems.This article is part of the theme issue ‘Varieties of abstract concepts: development, use and representation in the brain’.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\RV686JIY\\Zdrazilova et al. - 2018 - Communicating abstract meaning concepts revealed .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AV2A493J\\rstb.2017.html},
  number = {1752}
}

@article{zelicArticulatoryConstraintsSpontaneous2015,
  title = {Articulatory Constraints on Spontaneous Entrainment between Speech and Manual Gesture},
  author = {Zelic, Gregory and Kim, Jeesun and Davis, Chris},
  date = {2015-08-01},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {42},
  pages = {232--245},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2015.05.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0167945715000937},
  urldate = {2019-05-03},
  abstract = {The present study examined the extent to which speech and manual gestures spontaneously entrain in a non-communicative task. Participants had to repeatedly utter nonsense /CV/ syllables while continuously moving the right index finger in flexion/extension. No instructions to coordinate were given. We manipulated the type of syllable uttered (/ba/ vs. /sa/), and vocalization (phonated vs. silent speech). Assuming principles of coordination dynamics, a stronger entrainment between the fingers oscillations and the jaw motion was predicted (1) for /ba/, due to expected larger amplitude of jaw motion and (2) in phonated speech, due to the auditory feedback. Fifteen out of twenty participants showed simple ratios of speech to finger cycles (1:1, 1:2 or 2:1). In contrast with our predictions, speech–gesture entrainment was stronger when vocalizing /sa/ than /ba/, also more widely distributed on an in-phase mode. Furthermore, results revealed a spatial anchoring and an increased temporal variability in jaw motion when producing /sa/. We suggested that this indicates a greater control of the speech articulators for /sa/, making the speech performance more receptive to environmental forces, resulting in the greater entrainment observed to gesture oscillations. The speech–gesture coordination was maintained in silent speech, suggesting a somatosensory basis for their endogenous coupling.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KVGKEBKU\\S0167945715000937.html},
  keywords = {Rhythmic coordination dynamics,Speech articulatory constraints,Spontaneous entrainment processes}
}

@article{zhangMoreWordsOnline2020,
  title = {More than Words: {{The}} Online Orchestration of Word Predictability, Prosody, Gesture, and Mouth Movements during Natural Language Comprehension},
  shorttitle = {More than Words},
  author = {Zhang, Ye and Frassinelli, Diego and Tuomainen, Jyrki and Skipper, Jeremy I and Vigliocco, Gabriella},
  date = {2020-01-09},
  journaltitle = {bioRxiv},
  doi = {10.1101/2020.01.08.896712},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.01.08.896712},
  urldate = {2020-01-13},
  abstract = {Communication naturally occurs in dynamic face-to-face environments where spoken words are embedded in linguistic discourse and accompanied by multimodal cues. Existing research supports predictive brain models where prior discourse but also prosody, mouth movements, and hand-gestures individually contribute to comprehension. In electroencephalography (EEG) studies, more predictable words show reduced negativity, peaking at approximately 400ms after onset of the word (N400). Meaningful gestures and prosody also reduce the N400, while the effects of rhythmic gestures and mouth movements remain unclear. However, these studies have only focused on individual cues while in the real world, communicative cues co-occur and potentially interact. We measured EEG elicited by words while participants watched videos of a speaker producing short naturalistic passages. For each word, we quantified the information carried by prior linguistic discourse (surprisal), prosody (mean pitch), mouth informativeness (lip-reading), and presence of meaningful and/or rhythmic gestures. Discourse predictability reduced the N400 amplitude and multimodal cues impacted and interacted with this effect. Specifically, higher pitch and meaningful gestures reduced N400 amplitude, beyond the effect of discourse, while rhythmic gestures increased the N400 amplitude. Moreover, higher pitch overall reduced N400 amplitude while informative mouth movements only showed effects when no gestures were present. These results can constrain existing predictive brain models in that they demonstrate that the brain uses cues selectively in a dynamic and contextually determined manner in real-world language comprehension.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\C44EXJ56\\Zhang et al. - 2020 - More than words The online orchestration of word .pdf},
  langid = {english}
}

@report{zhangMoreWordsOnline2020a,
  title = {More than Words: {{The}} Online Orchestration of Word Predictability, Prosody, Gesture, and Mouth Movements during Natural Language Comprehension},
  shorttitle = {More than Words},
  author = {Zhang, Ye and Frassinelli, Diego and Tuomainen, Jyrki and Skipper, Jeremy I and Vigliocco, Gabriella},
  date = {2020-01-09},
  institution = {{Neuroscience}},
  doi = {10.1101/2020.01.08.896712},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.01.08.896712},
  urldate = {2020-03-06},
  abstract = {Communication naturally occurs in dynamic face-to-face environments where spoken words are embedded in linguistic discourse and accompanied by multimodal cues. Existing research supports predictive brain models where prior discourse but also prosody, mouth movements, and hand-gestures individually contribute to comprehension. In electroencephalography (EEG) studies, more predictable words show reduced negativity, peaking at approximately 400ms after onset of the word (N400). Meaningful gestures and prosody also reduce the N400, while the effects of rhythmic gestures and mouth movements remain unclear. However, these studies have only focused on individual cues while in the real world, communicative cues co-occur and potentially interact. We measured EEG elicited by words while participants watched videos of a speaker producing short naturalistic passages. For each word, we quantified the information carried by prior linguistic discourse (surprisal), prosody (mean pitch), mouth informativeness (lip-reading), and presence of meaningful and/or rhythmic gestures. Discourse predictability reduced the N400 amplitude and multimodal cues impacted and interacted with this effect. Specifically, higher pitch and meaningful gestures reduced N400 amplitude, beyond the effect of discourse, while rhythmic gestures increased the N400 amplitude. Moreover, higher pitch overall reduced N400 amplitude while informative mouth movements only showed effects when no gestures were present. These results can constrain existing predictive brain models in that they demonstrate that the brain uses cues selectively in a dynamic and contextually determined manner in real-world language comprehension.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3BESFLPY\\Zhang et al. - 2020 - More than words The online orchestration of word .pdf},
  langid = {english},
  type = {preprint}
}

@article{zhangMultimodalDeepLearning,
  title = {Multimodal {{Deep Learning Framework}} for {{Mental Disorder Recognition}}},
  author = {Zhang, Ziheng and Lin, Weizhe and Liu, Mingyu and Mahmoud, Marwa},
  pages = {7},
  abstract = {Current methods for mental disorder recognition mostly depend on clinical interviews and self-reported scores that can be highly subjective. Building an automatic recognition system can help in early detection of symptoms and providing insights into the biological markers for diagnosis. It is, however, a challenging task as it requires taking into account indicators from different modalities, such as facial expressions, gestures, acoustic features and verbal content. To address this issue, we propose a general-purpose multimodal deep learning framework, in which multiple modalities - including acoustic, visual and textual features - are processed individually with the cross-modality correlation considered. Specifically, a Multimodal Deep Denoising Autoencoder (multiDDAE) is designed to obtain multimodal representations of audio-visual features followed by the Fisher Vector encoding which produces session-level descriptors. For textual modality, a Paragraph Vector (PV) is proposed to embed the transcripts of interview sessions into document representations capturing cues related to mental disorders. Following an early fusion strategy, both audio-visual and textual features are then fused prior to feeding them to a Multitask Deep Neural Network (DNN) as the final classifier. Our framework is evaluated on the automatic detection of two mental disorders: bipolar disorder (BD) and depression, using two datasets: Bipolar Disorder Corpus (BDC) and the Extended Distress Analysis Interview Corpus (E-DAIC), respectively. Our experimental evaluation results showed comparable performance to the state-of-theart in BD and depression detection, thus demonstrating the effective multimodal representation learning and the capability to generalise across different mental disorders.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8KURKRDY\\Zhang et al. - Multimodal Deep Learning Framework for Mental Diso.pdf},
  langid = {english}
}

@article{zhangTopologicalPortraitsMultiscale2020,
  title = {Topological Portraits of Multiscale Coordination Dynamics},
  author = {Zhang, Mengsen and Kalies, William D. and Kelso, J. A. Scott and Tognoli, Emmanuelle},
  date = {2020-06-01},
  journaltitle = {Journal of Neuroscience Methods},
  shortjournal = {Journal of Neuroscience Methods},
  volume = {339},
  pages = {108672},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2020.108672},
  url = {http://www.sciencedirect.com/science/article/pii/S0165027020300947},
  urldate = {2020-05-26},
  abstract = {Living systems exhibit complex yet organized behavior on multiple spatiotemporal scales. To investigate the nature of multiscale coordination in living systems, one needs a meaningful and systematic way to quantify the complex dynamics, a challenge in both theoretical and empirical realms. The present work shows how integrating approaches from computational algebraic topology and dynamical systems may help us meet this challenge. In particular, we focus on the application of multiscale topological analysis to coordinated rhythmic processes. First, theoretical arguments are introduced as to why certain topological features and their scale-dependency are highly relevant to understanding complex collective dynamics. Second, we propose a method to capture such dynamically relevant topological information using persistent homology, which allows us to effectively construct a multiscale topological portrait of rhythmic coordination. Finally, the method is put to test in detecting transitions in real data from an experiment of rhythmic coordination in ensembles of interacting humans. The recurrence plots of topological portraits highlight collective transitions in coordination patterns that were elusive to more traditional methods. This sensitivity to collective transitions would be lost if the behavioral dynamics of individuals were treated as separate degrees of freedom instead of constituents of the topology that they collectively forge. Such multiscale topological portraits highlight collective aspects of coordination patterns that are irreducible to properties of individual parts. The present work demonstrates how the analysis of multiscale coordination dynamics can benefit from topological methods, thereby paving the way for further systematic quantification of complex, high-dimensional dynamics in living systems.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IVYLD3NF\\Zhang et al. - 2020 - Topological portraits of multiscale coordination d.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\LXCDVLSV\\S0165027020300947.html},
  keywords = {Complex systems,Coordination Dynamics,Metastability,Oscillators,Topological data analysis},
  langid = {english}
}

@article{zhangVocalDevelopmentMorphological2018,
  title = {Vocal Development through Morphological Computation},
  author = {Zhang, Yisi S. and Ghazanfar, Asif A.},
  date = {2018-02-20},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {16},
  pages = {e2003933},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2003933},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2003933},
  urldate = {2019-10-17},
  abstract = {The vocal behavior of infants changes dramatically during early life. Whether or not such a change results from the growth of the body during development—as opposed to solely neural changes—has rarely been investigated. In this study of vocal development in marmoset monkeys, we tested the putative causal relationship between bodily growth and vocal development. During the first two months of life, the spontaneous vocalizations of marmosets undergo (1) a gradual disappearance of context-inappropriate call types and (2) an elongation in the duration of context-appropriate contact calls. We hypothesized that both changes are the natural consequences of lung growth and do not require any changes at the neural level. To test this idea, we first present a central pattern generator model of marmoset vocal production to demonstrate that lung growth can affect the temporal and oscillatory dynamics of neural circuits via sensory feedback from the lungs. Lung growth qualitatively shifted vocal behavior in the direction observed in real marmoset monkey vocal development. We then empirically tested this hypothesis by placing the marmoset infants in a helium–oxygen (heliox) environment in which air is much lighter. This simulated a reversal in development by decreasing the effort required to respire, thus increasing the respiration rate (as though the lungs were smaller). The heliox manipulation increased the proportions of inappropriate call types and decreased the duration of contact calls, consistent with a brief reversal of vocal development. These results suggest that bodily growth alone can play a major role in shaping the development of vocal behavior.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4DFQJQJC\\Zhang and Ghazanfar - 2018 - Vocal development through morphological computatio.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\X2PTYECE\\article.html},
  keywords = {Behavior,Lungs,Marmosets,Monkeys,Nervous system,Respiratory physiology,Syllables,Vocalization},
  langid = {english},
  number = {2}
}

@article{zhangVocalStateChange2019,
  title = {Vocal State Change through Laryngeal Development},
  author = {Zhang, Yisi S. and Takahashi, Daniel Y. and Liao, Diana A. and Ghazanfar, Asif A. and Elemans, Coen P. H.},
  date = {2019-12},
  journaltitle = {Nature Communications},
  volume = {10},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-12588-6},
  url = {http://www.nature.com/articles/s41467-019-12588-6},
  urldate = {2019-10-15},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7X5E7J3J\\Zhang et al. - 2019 - Vocal state change through laryngeal development.pdf},
  langid = {english},
  number = {1}
}

@book{zotero-1008,
  type = {book}
}


@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2019},
  url = {https://www.R-project.org/},
}
@Manual{R-cluster,
  title = {cluster: Cluster Analysis Basics and Extensions},
  author = {Martin Maechler and Peter Rousseeuw and Anja Struyf and Mia Hubert and Kurt Hornik},
  year = {2019},
  note = {R package version 2.1.0 --- For new features, see the 'Changelog' file (in the package source)},
}
@Manual{R-cowplot,
  title = {cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2019},
  note = {R package version 1.0.0},
  url = {https://CRAN.R-project.org/package=cowplot},
}
@Manual{R-DescTools,
  title = {DescTools: Tools for Descriptive Statistics},
  author = {Andri Signorell et mult. al.},
  year = {2019},
  note = {R package version 0.99.28},
  url = {https://cran.r-project.org/package=DescTools},
}
@Article{R-dtw_a,
  title = {Computing and Visualizing Dynamic Time Warping Alignments in {R}: The {dtw} Package},
  author = {Toni Giorgino},
  journal = {Journal of Statistical Software},
  year = {2009},
  volume = {31},
  number = {7},
  pages = {1--24},
  url = {http://www.jstatsoft.org/v31/i07/},
}
@Article{R-dtw_b,
  title = {Matching Incomplete Time Series with Dynamic Time Warping: An Algorithm and an Application to Post-Stroke Rehabilitation},
  author = {Paolo Tormene and Toni Giorgino and Silvana Quaglini and Mario Stefanelli},
  journal = {Artificial Intelligence in Medicine},
  year = {2008},
  volume = {45},
  number = {1},
  pages = {11--34},
  doi = {10.1016/j.artmed.2008.11.007},
}
@Manual{R-effsize,
  title = {effsize: Efficient Effect Size Computation},
  author = {Marco Torchiano},
  year = {2019},
  note = {R package version 0.7.6},
  doi = {10.5281/zenodo.1480624},
  url = {https://CRAN.R-project.org/package=effsize},
}
@Manual{R-EMAtools,
  title = {EMAtools: Data Management Tools for Real-Time Monitoring/Ecological
Momentary Assessment Data},
  author = {Evan Kleiman},
  year = {2017},
  note = {R package version 0.1.3},
  url = {https://CRAN.R-project.org/package=EMAtools},
}
@Manual{R-ggbeeswarm,
  title = {ggbeeswarm: Categorical Scatter (Violin Point) Plots},
  author = {Erik Clarke and Scott Sherrill-Mix},
  year = {2017},
  note = {R package version 0.6.0},
  url = {https://CRAN.R-project.org/package=ggbeeswarm},
}
@Manual{R-ggExtra,
  title = {ggExtra: Add Marginal Histograms to 'ggplot2', and More 'ggplot2'
Enhancements},
  author = {Dean Attali and Christopher Baker},
  year = {2018},
  note = {R package version 0.8},
  url = {https://CRAN.R-project.org/package=ggExtra},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}
@Manual{R-gridExtra,
  title = {gridExtra: Miscellaneous Functions for "Grid" Graphics},
  author = {Baptiste Auguie},
  year = {2017},
  note = {R package version 2.3},
  url = {https://CRAN.R-project.org/package=gridExtra},
}
@Article{R-igraph,
  title = {The igraph software package for complex network research},
  author = {Gabor Csardi and Tamas Nepusz},
  journal = {InterJournal},
  volume = {Complex Systems},
  pages = {1695},
  year = {2006},
  url = {http://igraph.org},
}
@Manual{R-nlme,
  title = {{nlme}: Linear and Nonlinear Mixed Effects Models},
  author = {Jose Pinheiro and Douglas Bates and Saikat DebRoy and Deepayan Sarkar and {R Core Team}},
  year = {2019},
  note = {R package version 3.1-140},
  url = {https://CRAN.R-project.org/package=nlme},
}
@Manual{R-papaja,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2018},
  note = {R package version 0.1.0.9842},
  url = {https://github.com/crsh/papaja},
}
@Manual{R-pracma,
  title = {pracma: Practical Numerical Math Functions},
  author = {Hans W. Borchers},
  year = {2019},
  note = {R package version 2.2.5},
  url = {https://CRAN.R-project.org/package=pracma},
}
@Manual{R-proxy,
  title = {proxy: Distance and Similarity Measures},
  author = {David Meyer and Christian Buchta},
  year = {2019},
  note = {R package version 0.4-23},
  url = {https://CRAN.R-project.org/package=proxy},
}
@Manual{R-quantmod,
  title = {quantmod: Quantitative Financial Modelling Framework},
  author = {Jeffrey A. Ryan and Joshua M. Ulrich},
  year = {2019},
  note = {R package version 0.4-15},
  url = {https://CRAN.R-project.org/package=quantmod},
}
@Manual{R-r2glmm,
  title = {r2glmm: Computes R Squared for Mixed (Multilevel) Models},
  author = {Byron Jaeger},
  year = {2017},
  note = {R package version 0.1.2},
  url = {https://CRAN.R-project.org/package=r2glmm},
}
@Manual{R-raster,
  title = {raster: Geographic Data Analysis and Modeling},
  author = {Robert J. Hijmans},
  year = {2019},
  note = {R package version 3.0-7},
  url = {https://CRAN.R-project.org/package=raster},
}
@Manual{R-RColorBrewer,
  title = {RColorBrewer: ColorBrewer Palettes},
  author = {Erich Neuwirth},
  year = {2014},
  note = {R package version 1.1-2},
  url = {https://CRAN.R-project.org/package=RColorBrewer},
}
@Manual{R-scales,
  title = {scales: Scale Functions for Visualization},
  author = {Hadley Wickham and Dana Seidel},
  year = {2019},
  note = {R package version 1.1.0},
  url = {https://CRAN.R-project.org/package=scales},
}
@Manual{R-signal,
  title = {{signal}: Signal processing},
  author = {{signal developers}},
  year = {2014},
  url = {http://r-forge.r-project.org/projects/signal/},
}
@Article{R-sp,
  author = {Edzer J. Pebesma and Roger S. Bivand},
  title = {Classes and methods for spatial data in {R}},
  journal = {R News},
  year = {2005},
  volume = {5},
  number = {2},
  pages = {9--13},
  month = {November},
  url = {https://CRAN.R-project.org/doc/Rnews/},
}
@Article{R-TDAstats,
  title = {{T}{D}{A}stats: {R} pipeline for computing persistent homology in topological data analysis},
  author = {Raoul R. Wadhwa and Drew F. K. Williamson and Andrew Dhawan and Jacob G. Scott},
  journal = {Journal of Open Source Software},
  year = {2018},
  volume = {3},
  number = {28},
  pages = {860},
  url = {https://doi.org/10.21105/joss.00860},
  doi = {10.21105/joss.00860},
}
@Manual{R-TTR,
  title = {TTR: Technical Trading Rules},
  author = {Joshua Ulrich},
  year = {2018},
  note = {R package version 0.23-4},
  url = {https://CRAN.R-project.org/package=TTR},
}
@Manual{R-xts,
  title = {xts: eXtensible Time Series},
  author = {Jeffrey A. Ryan and Joshua M. Ulrich},
  year = {2018},
  note = {R package version 0.11-2},
  url = {https://CRAN.R-project.org/package=xts},
}
@Article{R-zoo,
  title = {zoo: S3 Infrastructure for Regular and Irregular Time Series},
  author = {Achim Zeileis and Gabor Grothendieck},
  journal = {Journal of Statistical Software},
  year = {2005},
  volume = {14},
  number = {6},
  pages = {1--27},
  doi = {10.18637/jss.v014.i06},
}
