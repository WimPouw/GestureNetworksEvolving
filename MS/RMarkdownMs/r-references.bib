
@article{abneyComplexityMatchingDyadic2014,
  title = {Complexity Matching in Dyadic Conversation},
  author = {Abney, D. H. and Paxton, A. and Dale, R. and Kello, Christopher T.},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {143},
  pages = {2304--2315},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/xge0000021},
  abstract = {Recent studies of dyadic interaction have examined phenomena of synchronization, entrainment, alignment, and convergence. All these forms of behavioral matching have been hypothesized to play a supportive role in establishing coordination and common ground between interlocutors. In the present study, evidence is found for a new kind of coordination termed complexity matching. Temporal dynamics in conversational speech signals were analyzed through time series of acoustic onset events. Timing in periods of acoustic energy was found to exhibit behavioral matching that reflects complementary timing in turn-taking. In addition, acoustic onset times were found to exhibit power law clustering across a range of timescales, and these power law functions were found to exhibit complexity matching that is distinct from behavioral matching. Complexity matching is discussed in terms of interactive alignment and other theoretical principles that lead to new hypotheses about information exchange in dyadic conversation and interaction in general. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZG2VIXUT\\2014-41508-001.html},
  keywords = {Acoustics,Conversation,Dyads,Interpersonal Interaction,Interstimulus Interval,Stimulus Complexity},
  number = {6}
}

@article{abneyMovementDynamicsReflect2015,
  title = {Movement Dynamics Reflect a Functional Role for Weak Coupling and Role Structure in Dyadic Problem Solving},
  author = {Abney, D. H. and Paxton, A. and Dale, R. and Kello, C. T.},
  date = {2015},
  journaltitle = {Cognitive Processing},
  volume = {16},
  pages = {325--332},
  doi = {10.1007/s10339-015-0648-2},
  number = {4}
}

@article{acevedo-valleAutonomousDiscoveryMotor2018,
  title = {Autonomous {{Discovery}} of {{Motor Constraints}} in an {{Intrinsically Motivated Vocal Learner}}},
  author = {Acevedo-Valle, J. M. and Angulo, C. and Moulin-Frier, C.},
  date = {2018-06},
  journaltitle = {IEEE Transactions on Cognitive and Developmental Systems},
  volume = {10},
  pages = {314--325},
  issn = {2379-8939},
  doi = {10.1109/TCDS.2017.2699578},
  abstract = {This paper introduces new results on the modeling of early vocal development using artificial intelligent cognitive architectures and a simulated vocal tract. The problem is addressed using intrinsically motivated learning algorithms for autonomous sensorimotor exploration, a kind of algorithm belonging to the active learning architectures family. The artificial agent is able to autonomously select goals to explore its own sensorimotor system in regions, where its competence to execute intended goals is improved. We propose to include a somatosensory system to provide a proprioceptive feedback signal to reinforce learning through the autonomous discovery of motor constraints. Constraints are represented by a somatosensory model which is unknown beforehand to the learner. Both the sensorimotor and somatosensory system are modeled using Gaussian mixture models. We argue that using an architecture which includes a somatosensory model would reduce redundancy in the sensorimotor model and drive the learning process more efficiently than algorithms taking into account only auditory feedback. The role of this proposed system is to predict whether an undesired collision within the vocal tract under a certain motor configuration is likely to occur. Thus, compromised motor configurations are rejected, guaranteeing that the agent is less prone to violate its own constraints.},
  eventtitle = {{{IEEE Transactions}} on {{Cognitive}} and {{Developmental Systems}}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UXL7K3JT\\Acevedo-Valle et al. - 2018 - Autonomous Discovery of Motor Constraints in an In.pdf;C\:\\Users\\u668173\\Zotero\\storage\\K9JGBQLU\\7914655.html},
  keywords = {Active learning,active learning architecture family,artificial agent,artificial intelligent cognitive architectures,autonomous discovery,autonomous sensorimotor exploration,Biological system modeling,cognition,compromised motor configurations,early vocal development,feedback,Gaussian mixture model,Gaussian mixture models,Gaussian mixture models (GMMs),Gaussian processes,humanoid robots,intrinsic motivations,intrinsically motivated vocal learner,learning (artificial intelligence),learning algorithms,mixture models,motor configuration,motor constraints,Production,proprioceptive feedback signal,Robot sensing systems,sensorimotor exploration,sensorimotor model,sensorimotor system,simulated vocal tract,somatosensory model,somatosensory system,Speech},
  number = {2}
}

@article{ackermannCerebellarContributionsSpeech2008,
  title = {Cerebellar Contributions to Speech Production and Speech Perception: Psycholinguistic and Neurobiological Perspectives},
  shorttitle = {Cerebellar Contributions to Speech Production and Speech Perception},
  author = {Ackermann, Hermann},
  date = {2008-06},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends Neurosci.},
  volume = {31},
  pages = {265--272},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2008.02.011},
  abstract = {Articulate speech represents a unique trait of our species. Besides other structures, the cerebellum pertains to the brain network engaged in spoken language production. Data from different sources point at a dual role of this organ within the verbal domain: (i) the cerebellum appears to subserve the online sequencing of syllables into fast, smooth and rhythmically organized larger utterances, and (ii) furthermore, the cerebellum seems to participate in the temporal organization of internal speech, that is, a prearticulatory verbal code. Impaired prearticulatory verbal coding mechanisms could explain at least some of the perceptual and cognitive deficits observed in cerebellar disorders. Recent genetic studies indicate that distinct mutations of a specific regulatory gene (FOXP2) promoted the emergence of articulate speech during the course of hominid evolution. Conceivably, structural changes of the expressed FOXP2 protein supported the 'vocal elaboration' of phylogenetically older brain networks engaged in upper limb motor control, such as the cerebro-cerebellar loops.},
  eprint = {18471906},
  eprinttype = {pmid},
  keywords = {Cerebellum,Humans,Neurobiology,Psycholinguistics,Speech,Speech Perception},
  langid = {english},
  number = {6}
}

@article{alemsegedJuvenileEarlyHominin2006,
  title = {A Juvenile Early Hominin Skeleton from {{Dikika}}, {{Ethiopia}}},
  author = {Alemseged, Zeresenay and Spoor, Fred and Kimbel, William H. and Bobe, René and Geraads, Denis and Reed, Denné and Wynn, Jonathan G.},
  date = {2006-09},
  journaltitle = {Nature},
  volume = {443},
  pages = {296--301},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature05047},
  url = {https://www.nature.com/articles/nature05047/},
  urldate = {2020-09-19},
  abstract = {Understanding changes in ontogenetic development is central to the study of human evolution. With the exception of Neanderthals, the growth patterns of fossil hominins have not been studied comprehensively because the fossil record currently lacks specimens that document both cranial and postcranial development at young ontogenetic stages. Here we describe a well-preserved 3.3-million-year-old juvenile partial skeleton of Australopithecus afarensis discovered in the Dikika research area of Ethiopia. The skull of the approximately three-year-old presumed female shows that most features diagnostic of the species are evident even at this early stage of development. The find includes many previously unknown skeletal elements from the Pliocene hominin record, including a hyoid bone that has a typical African ape morphology. The foot and other evidence from the lower limb provide clear evidence for bipedal locomotion, but the gorilla-like scapula and long and curved manual phalanges raise new questions about the importance of arboreal behaviour in the A. afarensis locomotor repertoire.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6VSS9VAC\\Alemseged et al. - 2006 - A juvenile early hominin skeleton from Dikika, Eth.pdf;C\:\\Users\\u668173\\Zotero\\storage\\U93T2BB9\\nature05047.html},
  issue = {7109},
  langid = {english},
  number = {7109}
}

@article{alexandersonGeneratingCoherentSpontaneous2020,
  title = {Generating Coherent Spontaneous Speech and Gesture from Text},
  author = {Alexanderson, Simon and Székely, Éva and Henter, Gustav Eje and Kucherenko, Taras and Beskow, Jonas},
  date = {2020-10-20},
  journaltitle = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},
  pages = {1--3},
  doi = {10.1145/3383652.3423874},
  url = {http://arxiv.org/abs/2101.05684},
  urldate = {2021-01-27},
  abstract = {Embodied human communication encompasses both verbal (speech) and non-verbal information (e.g., gesture and head movements). Recent advances in machine learning have substantially improved the technologies for generating synthetic versions of both of these types of data: On the speech side, text-to-speech systems are now able to generate highly convincing, spontaneous-sounding speech using unscripted speech audio as the source material. On the motion side, probabilistic motion-generation methods can now synthesise vivid and lifelike speech-driven 3D gesticulation. In this paper, we put these two state-of-the-art technologies together in a coherent fashion for the first time. Concretely, we demonstrate a proof-of-concept system trained on a single-speaker audio and motion-capture dataset, that is able to generate both speech and full-body gestures together from text input. In contrast to previous approaches for joint speech-and-gesture generation, we generate full-body gestures from speech synthesis trained on recordings of spontaneous speech from the same person as the motion-capture data. We illustrate our results by visualising gesture spaces and text-speech-gesture alignments, and through a demonstration video at https://simonalexanderson.github.io/IVA2020 .},
  archiveprefix = {arXiv},
  eprint = {2101.05684},
  eprinttype = {arxiv},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CNRC8DLU\\Alexanderson et al. - 2020 - Generating coherent spontaneous speech and gesture.pdf;C\:\\Users\\u668173\\Zotero\\storage\\LPY3WWZC\\2101.html},
  keywords = {68T07,Computer Science - Graphics,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,I.2.6,I.2.9,I.3.7,J.4}
}

@article{alexandersonStyleControllableSpeechDrivenGesture2020,
  title = {Style-{{Controllable Speech}}-{{Driven Gesture Synthesis Using Normalising Flows}}},
  author = {Alexanderson, Simon and Henter, Gustav Eje and Kucherenko, Taras and Beskow, Jonas},
  date = {2020},
  journaltitle = {Computer graphics forum (Print)},
  volume = {39},
  pages = {487--496},
  publisher = {{Eurographics - European Association for Computer Graphics}},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-268363},
  urldate = {2020-07-17},
  abstract = {DiVA portal er en felles søketjeneste for forskningspublikasjoner og studentoppgaver produsert ved følgende 49 læresteder og forskningsinstitusjoner.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ICDI2HHX\\record.html},
  langid = {english},
  number = {2}
}

@article{alipourPressurefrequencyRelationsExcised2007,
  title = {On Pressure-Frequency Relations in the Excised Larynx},
  author = {Alipour, Fariborz and Scherer, Ronald C.},
  date = {2007-09-26},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {122},
  pages = {2296--2305},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.2772230},
  url = {https://asa.scitation.org/doi/full/10.1121/1.2772230},
  urldate = {2020-12-04},
  abstract = {The purpose of this study was to find relationships between subglottal pressure (Ps)(Ps){$<$}math display="inline" overflow="scroll" altimg="eq-00001.gif"{$><$}mrow{$><$}mo{$>$}({$<$}/mo{$><$}msub{$><$}mi{$>$}P{$<$}/mi{$><$}mi{$>$}s{$<$}/mi{$><$}/msub{$><$}mo{$>$}){$<$}/mo{$><$}/mrow{$><$}/math{$>$} and fundamental frequency (F0)(F0){$<$}math display="inline" overflow="scroll" altimg="eq-00002.gif"{$><$}mrow{$><$}mo{$>$}({$<$}/mo{$><$}msub{$><$}mi{$>$}F{$<$}/mi{$><$}mn{$>$}0{$<$}/mn{$><$}/msub{$><$}mo{$>$}){$<$}/mo{$><$}/mrow{$><$}/math{$>$} of phonation in excised larynx models. This included also the relation between F0F0{$<$}math display="inline" overflow="scroll" altimg="eq-00003.gif"{$><$}mrow{$><$}msub{$><$}mi{$>$}F{$<$}/mi{$><$}mn{$>$}0{$<$}/mn{$><$}/msub{$><$}/mrow{$><$}/math{$>$} and its rate of change with pressure (dF∕dP)(dF∕dP){$<$}math display="inline" overflow="scroll" altimg="eq-00004.gif"{$><$}mrow{$><$}mo{$>$}({$<$}/mo{$><$}mi{$>$}d{$<$}/mi{$><$}mi{$>$}F{$<$}/mi{$><$}mo{$>$}∕{$<$}/mo{$><$}mi{$>$}d{$<$}/mi{$><$}mi{$>$}P{$<$}/mi{$><$}mo{$>$}){$<$}/mo{$><$}/mrow{$><$}/math{$>$}. Canine larynges were prepared and mounted over a tapered tube that supplied pressurized, heated, and humidified air. Glottal adduction was accomplished either by using two-pronged probes to press the arytenoids together or by passing a suture to simulate lateral cricoarytenoid muscle activation. The pressure-frequency relation was obtained through a series of pressure-flow sweep experiments that were conducted for eight excised canine larynges. It was found that, at set adduction and elongation levels, the pressure-frequency relation is nonlinear, and is highly influenced by the adduction and elongation. The results indicated that for the lower phonation mode, the average rate of change of frequency with pressure was 2.9±0.7Hz∕cm2.9±0.7Hz∕cm{$<$}math display="inline" overflow="scroll" altimg="eq-00005.gif"{$><$}mrow{$><$}mn{$>$}2.9{$<$}/mn{$><$}mo{$>$}±{$<$}/mo{$><$}mn{$>$}0.7{$<$}/mn{$><$}mspace width="0.3em"{$><$}/mspace{$><$}mi{$>$}Hz{$<$}/mi{$><$}mo{$>$}∕{$<$}/mo{$><$}mi{$>$}cm{$<$}/mi{$><$}/mrow{$><$}/math{$>$} H2OH2O{$<$}math display="inline" overflow="scroll" altimg="eq-00006.gif"{$><$}mrow{$><$}msub{$><$}mi mathvariant="normal"{$>$}H{$<$}/mi{$><$}mn{$>$}2{$<$}/mn{$><$}/msub{$><$}mi mathvariant="normal"{$>$}O{$<$}/mi{$><$}/mrow{$><$}/math{$>$}, and for the higher mode was 5.3±0.5Hz∕cm5.3±0.5Hz∕cm{$<$}math display="inline" overflow="scroll" altimg="eq-00007.gif"{$><$}mrow{$><$}mn{$>$}5.3{$<$}/mn{$><$}mo{$>$}±{$<$}/mo{$><$}mn{$>$}0.5{$<$}/mn{$><$}mspace width="0.3em"{$><$}/mspace{$><$}mi{$>$}Hz{$<$}/mi{$><$}mo{$>$}∕{$<$}/mo{$><$}mi{$>$}cm{$<$}/mi{$><$}/mrow{$><$}/math{$>$} H2OH2O{$<$}math display="inline" overflow="scroll" altimg="eq-00008.gif"{$><$}mrow{$><$}msub{$><$}mi mathvariant="normal"{$>$}H{$<$}/mi{$><$}mn{$>$}2{$<$}/mn{$><$}/msub{$><$}mi mathvariant="normal"{$>$}O{$<$}/mi{$><$}/mrow{$><$}/math{$>$} for adduction changes and 8.2±4.4Hz∕cm8.2±4.4Hz∕cm{$<$}math display="inline" overflow="scroll" altimg="eq-00009.gif"{$><$}mrow{$><$}mn{$>$}8.2{$<$}/mn{$><$}mo{$>$}±{$<$}/mo{$><$}mn{$>$}4.4{$<$}/mn{$><$}mspace width="0.3em"{$><$}/mspace{$><$}mi{$>$}Hz{$<$}/mi{$><$}mo{$>$}∕{$<$}/mo{$><$}mi{$>$}cm{$<$}/mi{$><$}/mrow{$><$}/math{$>$} H2OH2O{$<$}math display="inline" overflow="scroll" altimg="eq-00010.gif"{$><$}mrow{$><$}msub{$><$}mi mathvariant="normal"{$>$}H{$<$}/mi{$><$}mn{$>$}2{$<$}/mn{$><$}/msub{$><$}mi mathvariant="normal"{$>$}O{$<$}/mi{$><$}/mrow{$><$}/math{$>$} for elongation changes. The results suggest that during speech and singing, the dF∕dPdF∕dP{$<$}math display="inline" overflow="scroll" altimg="eq-00011.gif"{$><$}mrow{$><$}mi{$>$}d{$<$}/mi{$><$}mi{$>$}F{$<$}/mi{$><$}mo{$>$}∕{$<$}/mo{$><$}mi{$>$}d{$<$}/mi{$><$}mi{$>$}P{$<$}/mi{$><$}/mrow{$><$}/math{$>$} relationships are taken into account.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GHGRFQ4P\\Alipour and Scherer - 2007 - On pressure-frequency relations in the excised lar.pdf},
  number = {4}
}

@article{allenAspectsRhythmASL1991,
  title = {Aspects of {{Rhythm}} in {{ASL}}},
  author = {Allen, George D. and Wilbur, Ronnie B. and Schick, Brenda B.},
  date = {1991},
  journaltitle = {Sign Language Studies},
  volume = {1072},
  pages = {297--320},
  issn = {1533-6263},
  doi = {10.1353/sls.1991.0020},
  url = {http://muse.jhu.edu/content/crossref/journals/sign_language_studies/v1072/72.allen.html},
  urldate = {2020-03-09},
  abstract = {The fluent production of American Sign Language (ASL), like speech involves highly skilled, complex motor activity. Thus, like all skilled motor acts, it is rhythmically structured. This paper presents the results of an experiment designed to determine whether the rhythm of ASL can be associated with rhythmic beats. Three groups of adult observer subjects, ASL-fluent deaf, ASL-fluent normally-hearing children of deaf parents, and sign-naive normally hearing, tapped a small metal stylus in time to the rhythm of five short ASL narratives 30 times repeated; temporal locations of the observers' taps were compared statistically for differences related to observer group membership and to various properties of the target signs. There was great overall agreement among subjects that repeated signs, signs with primary stress, and phrase-final signs played a major role in the rhythm of these ASL narratives. The ASL-fluent subjects tapped less often to signs with secondary or weak stress than did the ASL-naive subjects, however, confirming that knowledge of ASL isnecessary for full appreciation of the rhythm of ASL.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZW9NTRAD\\Allen et al. - 1991 - Aspects of Rhythm in ASL.pdf},
  langid = {english},
  number = {1}
}

@article{alviarComplexCommunicationDynamics2019,
  title = {Complex Communication Dynamics: {{Exploring}} the Structure of an Academic Talk},
  shorttitle = {Complex {{Communication Dynamics}}},
  author = {Alviar, Camila and Dale, Rick and Galati, Alexia},
  date = {2019},
  journaltitle = {Cognitive Science},
  volume = {43},
  pages = {e12718},
  issn = {1551-6709},
  doi = {10.1111/cogs.12718},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12718},
  urldate = {2020-05-25},
  abstract = {Communication is a multimodal phenomenon. The cognitive mechanisms supporting it are still understudied. We explored a natural dataset of academic lectures to determine how communication modalities are used and coordinated during the presentation of complex information. Using automated and semi-automated techniques, we extracted and analyzed, from the videos of 30 speakers, measures capturing the dynamics of their body movement, their slide change rate, and various aspects of their speech (speech rate, articulation rate, fundamental frequency, and intensity). There were consistent but statistically subtle patterns in the use of speech rate, articulation rate, intensity, and body motion across the presentation. Principal component analysis also revealed patterns of system-like covariation among modalities. These findings, although tentative, do suggest that the cognitive system is integrating body, slides, and speech in a coordinated manner during natural language use. Further research is needed to clarify the specific coordination patterns that occur between the different modalities.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12718},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LYFZYMDE\\Alviar et al. - 2019 - Complex Communication Dynamics Exploring the Stru.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8IAXEDEZ\\cogs.html},
  keywords = {Communication,Dynamic complex systems,Extended mind,Lecturing,Multimodality,Situated cognition},
  langid = {english},
  number = {3}
}

@article{alviarMultimodalCoordinationSound2020a,
  title = {Multimodal {{Coordination}} of {{Sound}} and {{Movement}} in {{Music}} and {{Speech}}},
  author = {Alviar, Camila and Dale, Rick and Dewitt, Akeiylah and Kello, Christopher},
  date = {2020-09-13},
  journaltitle = {Discourse Processes},
  volume = {57},
  pages = {682--702},
  publisher = {{Routledge}},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2020.1768500},
  url = {https://doi.org/10.1080/0163853X.2020.1768500},
  urldate = {2020-12-13},
  abstract = {Speech and music emerge from a spectrum of nested motor and perceptual coordination patterns across timescales of brief movements to actions. Intuitively, this nested clustering in movements should be reflected in sound. We examined similarities and differences in multimodal, multiscale coordination of speech and music using two complementary measures: We computed spectra for envelopes of acoustic amplitudes and motion amplitudes and correlated spectral powers across modalities as a function of frequency. We also correlated smoothed envelopes and examined peaks in their cross-correlation functions. YouTube videos of five different modes of speaking and five different types of music were analyzed. Speech performances yielded stronger, more reliable relationships between sound and movement compared with music. Interestingly, a cappella singing patterned more with music, and improvisational jazz piano patterned more with speech. Results suggest that nested temporal structures in sound and movement are coordinated as a function of communicative aspects of performance.},
  annotation = {\_eprint: https://doi.org/10.1080/0163853X.2020.1768500},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6J6HG4MB\\0163853X.2020.html},
  number = {8}
}

@article{AmandaGormanInauguration2021,
  title = {Amanda {{Gorman}}: {{Inauguration}} Poet Calls for 'Unity and Togetherness'},
  shorttitle = {Amanda {{Gorman}}},
  date = {2021-01-20},
  journaltitle = {BBC News},
  url = {https://www.bbc.com/news/entertainment-arts-55738564},
  urldate = {2021-01-27},
  abstract = {The 22-year-old from LA is the youngest poet to perform at a presidential inauguration.},
  entrysubtype = {newspaper},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AENEACCN\\entertainment-arts-55738564.html},
  journalsubtitle = {Entertainment \& Arts},
  langid = {british}
}

@article{amazeenCouplingBreathingMovement2001,
  title = {Coupling of Breathing and Movement during Manual Wheelchair Propulsion},
  author = {Amazeen, P. G. and Amazeen, E. L. and Beek, P. J.},
  date = {2001},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {27},
  pages = {1243--1259},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7VME6RK3\\Amazeen et al. - Coupling of Breathing and Movement During Manual W.pdf},
  langid = {english},
  number = {5}
}

@article{amazeenLocomotorrespiratoryCouplingManual1999,
  title = {Locomotor-Respiratory Coupling during Manual Wheelchair Propulsion},
  author = {Amazeen, P. and Amazeen, E. L. and Beek, P. J.},
  date = {1999},
  journaltitle = {Ergonomics of manual wheelchair propulsion. The state of the art II},
  pages = {202--207},
  publisher = {{IOS Press}},
  url = {https://research.vu.nl/en/publications/locomotor-respiratory-coupling-during-manual-wheelchair-propulsio},
  urldate = {2020-09-19},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HUBBJ83C\\locomotor-respiratory-coupling-during-manual-wheelchair-propulsio.html},
  langid = {english}
}

@inproceedings{ambrazaitisWordProminenceRatings2020,
  title = {Word Prominence Ratings in {{Swedish}} Television News Readings – Effects of Pitch Accents and Head Movements},
  booktitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  author = {Ambrazaitis, Gilbert and Frid, Johan and House, David},
  date = {2020-05-25},
  pages = {314--318},
  publisher = {{ISCA}},
  doi = {10.21437/SpeechProsody.2020-64},
  url = {http://www.isca-speech.org/archive/SpeechProsody_2020/abstracts/216.html},
  urldate = {2020-05-26},
  abstract = {Prosodic prominence is a multimodal phenomenon where pitch accents are frequently aligned with visible movements by the hands, head, or eyebrows. However, little is known about how such movements function as visible prominence cues in multimodal speech perception with most previous studies being restricted to experimental settings. In this study, we are piloting the acquisition of multimodal prominence ratings for a corpus of natural speech (Swedish television news readings).},
  eventtitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\APE5P74D\\Ambrazaitis et al. - 2020 - Word prominence ratings in Swedish television news.pdf},
  langid = {english}
}

@article{amiriparianSynchronizationInterpersonalSpeech2019,
  title = {Synchronization in {{Interpersonal Speech}}},
  author = {Amiriparian, Shahin and Han, Jing and Schmitt, Maximilian and Baird, Alice and Mallol-Ragolta, Adria and Milling, Manuel and Gerczuk, Maurice and Schuller, Björn},
  date = {2019},
  journaltitle = {Frontiers in Robotics and AI},
  shortjournal = {Front. Robot. AI},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {2296-9144},
  doi = {10.3389/frobt.2019.00116},
  url = {https://www.frontiersin.org/articles/10.3389/frobt.2019.00116/full},
  urldate = {2020-09-25},
  abstract = {During both positive and negative dyadic exchanges, individuals will often unconsciously imitate their partner. A substantial amount of research has been made on this phenomenon and such studies have shown that synchronisation between communication partners can improve interpersonal relationships. Automatic computational approaches for recognising synchrony are still in their infancy. In this study, we extend on previous work in which we applied a novel method utilising hand-crafted low-level acoustic descriptors and autoencoders (AEs) to analyse synchrony in the speech domain. For this purpose, a database consisting of 394 in-the-wild speakers from 6 different cultures, is used. For each speaker in the dyadic exchange, two AEs are implemented. Post the training phase, the acoustic features for one of the speakers is tested using the AE trained on their dyadic partner. In this same way, we also explore the benefits that deep representations from audio may have, implementing the state-of-the-art Deep Spectrum toolkit. For all speakers at varied time-points during their interaction, the calculation of reconstruction error from the AE trained on their respective dyadic partner is made. The results obtained from this acoustic analysis are then compared with the linguistic experiments based on word counts and word embeddings generated by our word2vec approach. The results demonstrate that there is a degree of synchrony during all interactions. We also find that, this degree varies across the 6 cultures found in the investigated database. These findings are further substantiated through the use of 4096 dimensional Deep Spectrum features.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZZUMMAF3\\Amiriparian et al. - 2019 - Synchronization in Interpersonal Speech.pdf},
  keywords = {Autoencoders,Computational paralinguistics,human-human interaction,machine learning,Speech Processing,Speech Synchronisation},
  langid = {english}
}

@article{amisAnalysisElbowForces1980,
  title = {Analysis of Elbow Forces Due to High-Speed Forearm Movements},
  author = {Amis, A. A. and Dowson, D. and Wright, V.},
  date = {1980-01-01},
  journaltitle = {Journal of Biomechanics},
  shortjournal = {Journal of Biomechanics},
  volume = {13},
  pages = {825--831},
  issn = {0021-9290},
  doi = {10.1016/0021-9290(80)90170-0},
  url = {https://www.sciencedirect.com/science/article/pii/0021929080901700},
  urldate = {2021-03-05},
  abstract = {Maximal speed flexion and extension of the forearm, through the full range of movement, was analysed kinematically. For movements lasting 0.25 s, angular velocities of 18 r/s, and angular accelerations of 570 r/s2 were seen. An initial acceleration peak was normally followed by a prolonged constant-torque phase. The motion was arrested abruptly: the fastest movements sometimes caused decelerations of 1100 r/s2. It was shown that such decelerations may be produced by the muscles alone. Analysis of elbow joint forces during these actions suggested that the articulations were not subjected to forces beyond those seen during maximal isometric efforts (3.2 kN maximum).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\P5Y49P64\\Amis et al. - 1980 - Analysis of elbow forces due to high-speed forearm.pdf},
  langid = {english},
  number = {10}
}

@online{AnalysisElbowForces,
  title = {Analysis of Elbow Forces Due to High-Speed Forearm Movements - {{ScienceDirect}}},
  url = {https://www.sciencedirect.com/science/article/pii/0021929080901700},
  urldate = {2021-03-05},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PHRTZBAZ\\0021929080901700.html}
}

@book{andersonPhrenologyNeuralReuse2014,
  title = {After {{Phrenology}}: {{Neural}} Reuse and the Interactive Brain},
  shorttitle = {After {{Phrenology}}},
  author = {Anderson, Michael L.},
  date = {2014},
  publisher = {{The MIT Press}},
  abstract = {The computer analogy of the mind has been as widely adopted in contemporary cognitive neuroscience as was the analogy of the brain as a collection of organs in phrenology. Just as the phrenologist would insist that each organ must have its particular function, so contemporary cognitive neuroscience is committed to the notion that each brain region must have its fundamental computation. In \emph{After Phrenology} , Michael Anderson argues that to achieve a fully post-phrenological science of the brain, we need to reassess this commitment and devise an alternate, neuroscientifically grounded taxonomy of mental function. Anderson contends that the cognitive roles played by each region of the brain are highly various, reflecting different neural partnerships established under different circumstances. He proposes quantifying the functional properties of neural assemblies in terms of their dispositional tendencies rather than their computational or information-processing operations. Exploring larger-scale issues, and drawing on evidence from embodied cognition, Anderson develops a picture of thinking rooted in the exploitation and extension of our early-evolving capacity for iterated interaction with the world. He argues that the multidimensional approach to the brain he describes offers a much better fit for these findings, and a more promising road toward a unified science of minded organisms.},
  eprint = {j.ctt1287hqk},
  eprinttype = {jstor},
  isbn = {978-0-262-02810-3}
}

@article{andreouHandednessAsthmaAllergic2002,
  title = {Handedness, Asthma and Allergic Disorders: {{Is}} There an Association?},
  shorttitle = {Handedness, Asthma and Allergic Disorders},
  author = {Andreou, G. and Krommydas, G. and Gourgoulianis, K. I. and Karapetsas, A. and Molyvdas, P. A.},
  date = {2002-02-01},
  journaltitle = {Psychology, Health \& Medicine},
  volume = {7},
  pages = {53--60},
  publisher = {{Taylor \& Francis}},
  issn = {1354-8506},
  doi = {10.1080/13548500120101559},
  url = {https://doi.org/10.1080/13548500120101559},
  urldate = {2020-09-11},
  abstract = {Left-handedness has often been associated with asthma and allergic disorders. In view of previous findings, we investigated the distribution of laterality scores, using the Edinburgh Handedness Inventory, among 172 children. These children were asthmatic and allergic children of pre-school age and adolescence, who visited the Lung Function Laboratory of the Physiology Department of the University of Thessaly, and their controls. We failed to find an association of left-handedness and asthma at pre-school age. We found an association of left-handedness and other allergies at pre-school age and an association of left-handedness and asthma in adolescence, due to hereditary reasons. Our data suggest that both asthma or other allergies and handedness are inherited, especially through the maternal line.},
  annotation = {\_eprint: https://doi.org/10.1080/13548500120101559},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8MT5YS3K\\Andreou et al. - 2002 - Handedness, asthma and allergic disorders Is ther.pdf;C\:\\Users\\u668173\\Zotero\\storage\\NWTQR3H3\\13548500120101559.html},
  number = {1}
}

@article{andresdottirIrregularAmplitudeFrequency2017,
  title = {Irregular Amplitude and Frequency of Respiratory Movements in Hemispheric Stroke},
  author = {Andresdottir, Gudbjorg Thora and Hjaltason, Haukur and Ragnarsdottir, Maria},
  date = {2017-04-03},
  journaltitle = {European Journal of Physiotherapy},
  volume = {19},
  pages = {84--89},
  publisher = {{Taylor \& Francis}},
  issn = {2167-9169},
  doi = {10.1080/21679169.2016.1261367},
  url = {https://doi.org/10.1080/21679169.2016.1261367},
  urldate = {2020-06-10},
  abstract = {Objectives: The aims of the study were to investigate respiratory movement patterns among patients with right versus left hemispheric stroke with emphasis on regularity of respiratory frequency and amplitude of movements during both quiet and voluntary deep breathing and to compare the results with reference values.Materials and methods: Eighteen patients with severe stroke were measured with the Respiratory Movement Measuring Instrument. Respiratory frequencies, movements, regularity of amplitude and frequency were compared with individuals with no neurological disease.Results: The demographics and physical performance were comparable for 8 left and 10 right hemispheric stroke. Deep respiratory movements were significantly decreased (p {$<$} 0.001) and frequency significantly increased (p {$<$} 0.03 for quiet and p = 0.002 for deep breathing), for the group (n = 18) compared with reference values. Fifty-six percent of our stroke patients had irregular amplitude of deep respiratory movements (p = 0.003), and 33\% had irregular frequency (p = 0.058), but none of the individuals with no neurological diseases (p = 0.003). Seventy percent of RHS patients had irregular amplitude of deep respiratory movements, compared with 38\% of LHS patients (p = 0.34).Conclusion: Results show significantly decreased amplitude of deep breathing in hemispheric stroke and indicate a possible effect on regularity of range and frequency of respiratory movements.},
  annotation = {\_eprint: https://doi.org/10.1080/21679169.2016.1261367},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\D5ITAUGT\\21679169.2016.html},
  keywords = {Abnormal breathing,hemispheric stroke,irregular breathing,respiratory movements},
  number = {2}
}

@article{anegawaLateralCephalometricAnalysis2008,
  title = {Lateral Cephalometric Analysis of the Pharyngeal Airway Space Affected by Head Posture},
  author = {Anegawa, E. and Tsuyama, H. and Kusukawa, J.},
  date = {2008-09},
  journaltitle = {International Journal of Oral and Maxillofacial Surgery},
  volume = {37},
  pages = {805--809},
  issn = {0901-5027},
  doi = {10.1016/j.ijom.2008.03.006},
  abstract = {To clarify the relationship between head posture and pharyngeal airway space (PAS), the cephalometric parameters at different head postures were examined. Twelve normal Japanese adults (6 males and 6 females) were examined. Lateral cephalometric radiographs were taken at five different head postures in each person. All radiographs were traced, and the measurements were analyzed statistically. PAS was significantly increased by forward inclination of the cervical spine. The most significant correlation was found between the change in CVT/NSL (cranio-cervical inclination in the second and fourth vertebrae) and the change in PAS-TP (the minimal pharyngeal airway space) (r(2)=0.79 in males, r(2)=0.67 in females). The mean CVT/NSL when the head was in the natural (neutral) position was 100.9 degrees in males and 103.5 degrees in females. Linear regression analysis revealed DeltaPAS (mm)=0.37DeltaCVT/NSL (degree) (r(2)=0.79, p{$<$}0.0001) in males, and DeltaPAS (mm)=0.33DeltaCVT/NSL (degree) (r(2)=0.51, p{$<$}0.0001) in females. The correlation equations were obtained as follows: the corrected PAS (mm)=the actual PAS (mm)+0.37[100.9-the actual NSL/CVT (degree)] in males, and the corrected PAS (mm)=the actual PAS (mm)+0.33[103.5-the actual NSL/CVT (degree)] in females. These results will contribute to obtaining an accurate assessment of the PAS that should be corrected by the cranio-cervical inclination.},
  eprint = {18468864},
  eprinttype = {pmid},
  keywords = {Adult,Airway Resistance,Cephalometry,Cervical Vertebrae,Female,Head,Humans,Male,Neck,Pharynx,Posture,Radiography,Reference Values},
  langid = {english},
  number = {9}
}

@article{anibleIconicityAmericanSign2020,
  title = {Iconicity in {{American Sign Language}}–{{English}} Translation Recognition},
  author = {Anible, Benjamin},
  date = {2020-03},
  journaltitle = {Language and Cognition},
  volume = {12},
  pages = {138--163},
  publisher = {{Cambridge University Press}},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2019.51},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/iconicity-in-american-sign-languageenglish-translation-recognition/0096C4538CD48E5AA546C453655E8F44},
  urldate = {2020-03-06},
  abstract = {Reaction times for a translation recognition study are reported where novice to expert English–ASL bilinguals rejected English translation distractors for ASL signs that were related to the correct translations through phonology, semantics, or both form and meaning (diagrammatic iconicity). Imageability ratings of concepts impacted performance in all conditions; when imageability was high, participants showed interference for phonologically related distractors, and when imageability was low participants showed interference for semantically related distractors, regardless of proficiency. For diagrammatically related distractors high imageability caused interference in experts, but low imageability caused interference in novices. These patterns suggest that imageability and diagrammaticity interact with proficiency – experts process diagrammatic related distractors phonologically, but novices process them semantically. This implies that motivated signs are dependent on the entrenchment of language systematicity; rather than decreasing their impact on language processing as proficiency grows, they build on the original benefit conferred by iconic mappings.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I5TC9KTD\\Anible - 2020 - Iconicity in American Sign Language–English transl.pdf;C\:\\Users\\u668173\\Zotero\\storage\\KTNRNRG6\\0096C4538CD48E5AA546C453655E8F44.html},
  keywords = {bilingualism,cognitive linguistics,iconicity,signed language,translation recognition,usage-based},
  langid = {english},
  number = {1}
}

@article{antoniadisPlayingMentalRepresentations2020,
  title = {Playing without Mental Representations: {{Embodied}} Navigation and the {{GesTCom}} as a Case Study for Radical Embodied Cognition in Piano Performance},
  shorttitle = {Playing without Mental Representations},
  author = {Antoniadis, Pavlos and Chemero, Anthony},
  date = {2020},
  publisher = {{Universität Graz}},
  doi = {10.25364/24.10:2020.1.7},
  url = {https://unipub.uni-graz.at/jims/periodical/titleinfo/5758295},
  urldate = {2021-02-16},
  abstract = {Implications for musicological interdisciplinarity. The contribution above is inscribed in what today constitutes a paradigm-shifting web of knowledge around musical performance. The relevant fields of the project include both the humanities and the sciences, as well as artistic research. In humanities, traditional approaches stemming from historic and systematic musicology and music pedagogy are complemented by the performative turn in musicology, the wider field of performance studies, and aspects of complexity in post-1950 compositional and performative aesthetics. In sciences, the role of embodiment in cognitive processes (embodied cognition/cognitive psychology), the study of physical movement through interactive technologies (human-computer interaction) and the creation of new interfaces for musical expression, are combined with computational approaches in musicology and dynamic systems theory. In terms of interdisciplinarity between musicology and cognitive science, both the theory and the interactive system presented here are inspired by radical embodied cognition. Inversely, they also serve as a case study for many of the non-musical claims of this theory.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HP93ZQ9Z\\Antoniadis and Chemero - 2020 - Playing without mental representations Embodied n.pdf},
  langid = {english},
  version = {Version of record}
}

@article{arensburgMiddlePalaeolithicHuman1989,
  title = {A {{Middle Palaeolithic}} Human Hyoid Bone},
  author = {Arensburg, B. and Tillier, A. M. and Vandermeersch, B. and Duday, H. and Schepartz, L. A. and Rak, Y.},
  date = {1989-04},
  journaltitle = {Nature},
  volume = {338},
  pages = {758--760},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/338758a0},
  url = {https://www.nature.com/articles/338758a0},
  urldate = {2020-09-19},
  abstract = {THE origin of human language, and in particular the question of whether or not Neanderthal man was capable of language/speech, is of major interest to anthropologists but remains an area of great controversy1, 2. Despite palaeoneurological evidence to the contrary3, 4, many researchers hold to the view that Neanderthals were incapable of language/speech, basing their arguments largely on studies of laryngeal/basicranial morphology1, 5, 6. Studies, however, have been hampered by the absence of unambiguous fossil evidence. We now report the discovery of a well-preserved human hyoid bone from Middle Palaeolithic layers of Kebara Cave, Mount Carmel, Israel, dating from about 60,000 years BP. The bone is almost identical in size and shape to the hyoid of present-day populations, suggesting that there has been little or no change in the visceral skeleton (including the hyoid, middle ear ossicles, and inferentially the larynx) during the past 60,000 years of human evolution. We conclude that the morphological basis for human speech capability appears to have been fully developed during the Middle Palaeolithic.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6QVH8XQK\\Arensburg et al. - 1989 - A Middle Palaeolithic human hyoid bone.pdf;C\:\\Users\\u668173\\Zotero\\storage\\LKNTPM4B\\338758a0.html},
  issue = {6218},
  langid = {english},
  number = {6218}
}

@article{armstrongSensorimotorSynchronizationAudiovisual2014,
  title = {Sensorimotor Synchronization with Audio-Visual Stimuli: Limited Multisensory Integration},
  shorttitle = {Sensorimotor Synchronization with Audio-Visual Stimuli},
  author = {Armstrong, Alan and Issartel, Johann},
  date = {2014-11},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {232},
  pages = {3453--3463},
  issn = {1432-1106},
  doi = {10.1007/s00221-014-4031-9},
  abstract = {Understanding how we synchronize our actions with stimuli from different sensory modalities plays a central role in helping to establish how we interact with our multisensory environment. Recent research has shown better performance with multisensory over unisensory stimuli; however, the type of stimuli used has mainly been auditory and tactile. The aim of this article was to expand our understanding of sensorimotor synchronization with multisensory audio-visual stimuli and compare these findings to their individual unisensory counterparts. This research also aims to assess the role of spatio-temporal structure for each sensory modality. The visual and/or auditory stimuli had either temporal or spatio-temporal information available and were presented to the participants in unimodal and bimodal conditions. Globally, the performance was significantly better for the bimodal compared to the unimodal conditions; however, this benefit was limited to only one of the bimodal conditions. In terms of the unimodal conditions, the level of synchronization with visual stimuli was better than auditory, and while there was an observed benefit with the spatio-temporal compared to temporal visual stimulus, this was not replicated with the auditory stimulus.},
  eprint = {25027792},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Adolescent,Adult,Analysis of Variance,Auditory Perception,Female,Functional Laterality,Humans,Male,Movement,Photic Stimulation,Psychomotor Performance,Reaction Time,Space Perception,Visual Perception,Young Adult},
  langid = {english},
  number = {11}
}

@inproceedings{arnoldUsingGeneralizedAdditive2013,
  title = {Using Generalized Additive Models and Random Forests to Model Prosodic Prominence in {{German}}},
  booktitle = {14th {{Annual Conference}} of the {{International Speech Communication Association}}},
  author = {Arnold, D. and Wagner, P. and Baayen, R. H.},
  date = {2013},
  pages = {5},
  location = {{Lyon, France}},
  abstract = {The perception of prosodic prominence is influenced by different sources like different acoustic cues, linguistic expectations and context. We use a generalized additive model and a random forest to model the perceived prominence on a corpus of spoken German. Both models are able to explain over 80\% of the variance. While the random forests give us some insights on the relative importance of the cues, the general additive model gives us insights on the interaction between different cues to prominence.},
  eventtitle = {{{INTERSPEECH}} 2013},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HFUMCEN4\\Arnold et al. - Using generalized additive models and random fores.pdf},
  langid = {english}
}

@article{aronoffRootsLinguisticOrganization2008,
  title = {The Roots of Linguistic Organization in a New Language},
  author = {Aronoff, Mark and Meir, Irit and Padden, Carol A. and Sandler, Wendy},
  date = {2008-01-01},
  journaltitle = {Interaction Studies},
  volume = {9},
  pages = {133--153},
  publisher = {{John Benjamins}},
  issn = {1572-0373, 1572-0381},
  doi = {10.1075/is.9.1.10aro},
  url = {https://www.jbe-platform.com/content/journals/10.1075/is.9.1.10aro},
  urldate = {2020-03-10},
  abstract = {It is possible for a language to emerge with no direct linguistic history or outside linguistic influence. Al-Sayyid Bedouin Sign Language (ABSL) arose about 70 years ago in a small, insular community with a high incidence of profound prelingual neurosensory deafness. In ABSL, we have been able to identify the beginnings of phonology, morphology, syntax, and prosody. The linguistic elements we find in ABSL are not exclusively holistic, nor are they all compositional, but a combination of both. We do not, however, find in ABSL certain features that have been posited as essential even for a proto-language. ABSL has a highly regular syntax as well as word-internal compounding, also highly regular but quite distinct from syntax in its patterns. ABSL, however, has no discernable word-internal structure of the kind observed in more mature sign languages: no spatially organized morphology and no evident duality of phonological ­patterning.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LGXC3LUA\\Aronoff et al. - 2008 - The roots of linguistic organization in a new lang.pdf;C\:\\Users\\u668173\\Zotero\\storage\\J3HZU43T\\is.9.1.html},
  langid = {english},
  number = {1}
}

@article{aruinDirectionalSpecificityPostural1995,
  title = {Directional Specificity of Postural Muscles in Feed-Forward Postural Reactions during Fast Voluntary Arm Movements},
  author = {Aruin, Alexander S. and Latash, Mark L.},
  date = {1995-03-01},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {103},
  pages = {323--332},
  issn = {1432-1106},
  doi = {10.1007/BF00231718},
  url = {https://doi.org/10.1007/BF00231718},
  urldate = {2020-06-12},
  abstract = {Healthy subjects performed bilateral fast shoulder movements in different directions while standing on a force platform. Anticipatory postural adjustments were seen as changes in the electrical activity of postural muscles as well as displacements of the center of pressure and center of gravity. Postural muscle pairs of agonist-antagonist commonly demonstrated triphasic patterns starting prior to the first electromyographic (EMG) burst in the prime-mover muscle. Proximal postural muscles demonstrated the largest anticipatory increase in the background activity during movements in one of the two opposite directions (forward or backwards). These changes progressively decreased when movements deviated from the preferred direction and frequently disappeared during movements in the opposite direction. The patterns in distal muscles varied across subjects and could demonstrate larger anticipatory changes during movements forward and backwards as compared to movements in intermediate directions. Bilateral addition of inertial loads to the wrists did not change the general anticipatory patterns, while making some of their features more pronounced. Anticipatory postural adjustments were followed by later changes in the activity of postural muscles, also reflected in the mechanical variables. Changes in leg joint angles revealed a „hip-ankle strategy” during shoulder flexions and an „ankle strategy” during shoulder extensions. The study demonstrates different behaviors of proximal and distal muscles during anticipatory postural adjustments in preparation for fast arm movements. We suggest that the proximal muscles produce a general pattern of postural adjustments, while distal muscles take care of fine adjustments that are more likely to vary across subjects.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WTIB5KAP\\Aruin and Latash - 1995 - Directional specificity of postural muscles in fee.pdf},
  langid = {english},
  number = {2}
}

@article{ascherslebenTemporalControlMovements2002,
  title = {Temporal {{Control}} of {{Movements}} in {{Sensorimotor Synchronization}}},
  author = {Aschersleben, Gisa},
  date = {2002-02-01},
  journaltitle = {Brain and Cognition},
  shortjournal = {Brain and Cognition},
  volume = {48},
  pages = {66--79},
  issn = {0278-2626},
  doi = {10.1006/brcg.2001.1304},
  url = {http://www.sciencedirect.com/science/article/pii/S0278262601913041},
  urldate = {2019-04-02},
  abstract = {Under conditions in which the temporal structure of events (e.g., a sequence of tones) is predictable, performing movements in synchrony with this sequence of events (e.g., dancing) is an easy task. A rather simplified version of this task is studied in the sensorimotor synchronization paradigm. Participants are instructed to synchronize their finger taps with an isochronous sequence of signals (e.g., clicks). Although this is an easy task, a systematic error is observed: Taps usually precede clicks by several tens of milliseconds. Different models have been proposed to account for this effect (“negative asynchrony” or “synchronization error”). One group of explanations is based on the idea that synchrony is established at the level of central representations (and not at the level of external events), and that the timing of an action is determined by the (anticipated) action effect. These assumptions are tested by manipulating the amount of sensory feedback available from the tap as well as its temporal characteristics. This article presents an overview of these representational models and the empirical evidence supporting them. It also discusses other accounts briefly in the light of further evidence.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MUE975DA\\Aschersleben - 2002 - Temporal Control of Movements in Sensorimotor Sync.pdf;C\:\\Users\\u668173\\Zotero\\storage\\CAAEWIZD\\S0278262601913041.html},
  number = {1}
}

@article{ascherslebenTimingMechanismsSensorimotor2002,
  title = {Timing Mechanisms in Sensorimotor Synchronization},
  author = {Aschersleben, Gisa and Stenneken, Prisca and Cole, J. D. and Prinz, Wolfgang},
  date = {2002},
  journaltitle = {Common mechanisms in perception and action},
  pages = {227--244},
  url = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_727550},
  urldate = {2019-04-02},
  abstract = {Author: Aschersleben, Gisa et al.; Genre: Book Chapter; Published in Print: 2002; Title: Timing mechanisms in sensorimotor synchronization},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LZMPZVPB\\ViewItemOverviewPage.html},
  langid = {english}
}

@article{assaneoSpeakingRhythmicallyCan2020,
  title = {Speaking Rhythmically Can Shape Hearing},
  author = {Assaneo, M. Florencia and Rimmele, Johanna M. and Sanz Perl, Yonatan and Poeppel, David},
  date = {2020-10-12},
  journaltitle = {Nature Human Behaviour},
  pages = {1--12},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00962-0},
  url = {https://www.nature.com/articles/s41562-020-00962-0},
  urldate = {2020-10-26},
  abstract = {Evidence suggests that temporal predictions arising from the motor system can enhance auditory perception. However, in speech perception, we lack evidence of perception being modulated by production. Here we show a behavioural protocol that captures the existence of such auditory–motor interactions. Participants performed a syllable discrimination task immediately after producing periodic syllable sequences. Two speech rates were explored: a ‘natural’ (individually preferred) and a fixed ‘non-natural’ (2\,Hz) rate. Using a decoding approach, we show that perceptual performance is modulated by the stimulus phase determined by a participant’s own motor rhythm. Remarkably, for ‘natural’ and ‘non-natural’ rates, this finding is restricted to a subgroup of the population with quantifiable auditory–motor coupling. The observed pattern is compatible with a neural model assuming a bidirectional interaction of auditory and speech motor cortices. Crucially, the model matches the experimental results only if it incorporates individual differences in the strength of the auditory–motor connection.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2QSC7VGU\\s41562-020-00962-0.html},
  langid = {english}
}

@article{atilganIntegrationVisualInformation2018,
  title = {Integration of {{Visual Information}} in {{Auditory Cortex Promotes Auditory Scene Analysis}} through {{Multisensory Binding}}},
  author = {Atilgan, Huriye and Town, Stephen M. and Wood, Katherine C. and Jones, Gareth P. and Maddox, Ross K. and Lee, Adrian K. C. and Bizley, Jennifer K.},
  date = {2018-02-07},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {97},
  pages = {640-655.e4},
  issn = {1097-4199},
  doi = {10.1016/j.neuron.2017.12.034},
  abstract = {How and where in the brain audio-visual signals are bound to create multimodal objects remains unknown. One hypothesis is that temporal coherence between dynamic multisensory signals provides a mechanism for binding stimulus features across sensory modalities. Here, we report that when the luminance of a visual stimulus is temporally coherent with the amplitude fluctuations of one sound in a~mixture, the representation of that sound is enhanced in auditory cortex. Critically, this enhancement extends to include both binding and non-binding features of the sound. We demonstrate that visual information conveyed from visual cortex via the phase of the local field potential is combined with auditory information within auditory cortex. These data provide evidence that early cross-sensory binding provides a bottom-up mechanism for the formation of cross-sensory objects and that one role for multisensory binding in auditory cortex is to support auditory scene analysis.},
  eprint = {29395914},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8JGEYYSF\\Atilgan et al. - 2018 - Integration of Visual Information in Auditory Cort.pdf},
  keywords = {Acoustic Stimulation,Action Potentials,Animals,attention,auditory cortex,Auditory Perception,auditory-visual,binding,cross-modal,Female,ferret,Ferrets,multisensory,Neurons,Photic Stimulation,sensory cortex,visual cortex,Visual Cortex,Visual Perception},
  langid = {english},
  number = {3},
  pmcid = {PMC5814679}
}

@inproceedings{attinaTemporalMeasuresHand2006,
  title = {Temporal {{Measures}} of {{Hand}} and {{Speech Coordination During French Cued Speech Production}}},
  booktitle = {Gesture in {{Human}}-{{Computer Interaction}} and {{Simulation}}},
  author = {Attina, Virginie and Cathiard, Marie-Agnès and Beautemps, Denis},
  editor = {Gibet, Sylvie and Courty, Nicolas and Kamp, Jean-François},
  date = {2006},
  pages = {13--24},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11678816_2},
  abstract = {Cued Speech is an efficient method that allows orally educated deaf people to perceive a complete oral message through the visual channel. Using this system, speakers can clarify what they say with the complement of hand cues near the face; similar lip shapes are disambiguated by the addition of a manual cue. In this context, Cued Speech represents a unique system that closely links hand movements and speech since it is based on spoken language. In a previous study, we investigated the temporal organization of French Cued Speech production for a single cueing talker. A specific pattern of coordination was found: the hand anticipates the lips and speech sounds. In the present study, we investigated the cueing behavior of three additional professional cueing talkers. The same pattern of hand cues anticipation was found. Results are discussed with respect to inter-subject variability. A general pattern of coordination is proposed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KY52EB8D\\Attina et al. - 2006 - Temporal Measures of Hand and Speech Coordination .pdf},
  isbn = {978-3-540-32625-0},
  keywords = {Automatic Speech Recognition,Deaf People,Hand Gesture,Hand Position,Speech Perception},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{ayRobustnessComplexityCoconstructed2007,
  title = {Robustness and Complexity Co-Constructed in Multimodal Signalling Networks},
  author = {Ay, Nihat and Flack, Jessica and Krakauer, David C},
  date = {2007-03-29},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {362},
  pages = {441--447},
  issn = {0962-8436},
  doi = {10.1098/rstb.2006.1971},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2323562/},
  urldate = {2019-08-15},
  abstract = {In animal communication, signals are frequently emitted using different channels (e.g. frequencies in a vocalization) and different modalities (e.g. gestures can accompany vocalizations). We explore two explanations that have been provided for multimodality: (i) selection for high information transfer through dedicated channels and (ii) increasing fault tolerance or robustness through multichannel signals. Robustness relates to an accurate decoding of a signal when parts of a signal are occluded. We show analytically in simple feed-forward neural networks that while a multichannel signal can solve the robustness problem, a multimodal signal does so more effectively because it can maximize the contribution made by each channel while minimizing the effects of exclusion. Multimodality refers to sets of channels where within each set information is highly correlated. We show that the robustness property ensures correlations among channels producing complex, associative networks as a by-product. We refer to this as the principle of robust overdesign. We discuss the biological implications of this for the evolution of combinatorial signalling systems; in particular, how robustness promotes enough redundancy to allow for a subsequent specialization of redundant components into novel signals.},
  eprint = {17255020},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\W59H23UJ\\Ay et al. - 2007 - Robustness and complexity co-constructed in multim.pdf},
  number = {1479},
  pmcid = {PMC2323562}
}

@article{ayRobustnessComplexityCoconstructed2007a,
  title = {Robustness and Complexity Co-Constructed in Multimodal Signalling Networks},
  author = {Ay, Nihat and Flack, Jessica and Krakauer, David C},
  date = {2007-03-29},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {362},
  pages = {441--447},
  issn = {0962-8436},
  doi = {10.1098/rstb.2006.1971},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2323562/},
  urldate = {2020-10-13},
  abstract = {In animal communication, signals are frequently emitted using different channels (e.g. frequencies in a vocalization) and different modalities (e.g. gestures can accompany vocalizations). We explore two explanations that have been provided for multimodality: (i) selection for high information transfer through dedicated channels and (ii) increasing fault tolerance or robustness through multichannel signals. Robustness relates to an accurate decoding of a signal when parts of a signal are occluded. We show analytically in simple feed-forward neural networks that while a multichannel signal can solve the robustness problem, a multimodal signal does so more effectively because it can maximize the contribution made by each channel while minimizing the effects of exclusion. Multimodality refers to sets of channels where within each set information is highly correlated. We show that the robustness property ensures correlations among channels producing complex, associative networks as a by-product. We refer to this as the principle of robust overdesign. We discuss the biological implications of this for the evolution of combinatorial signalling systems; in particular, how robustness promotes enough redundancy to allow for a subsequent specialization of redundant components into novel signals.},
  eprint = {17255020},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5ZVZARRK\\Ay et al. - 2007 - Robustness and complexity co-constructed in multim.pdf},
  number = {1479},
  pmcid = {PMC2323562}
}

@article{azarLanguageContactDoes2019,
  title = {Language Contact Does Not Drive Gesture Transfer: {{Heritage}} Speakers Maintain Language Specific Gesture Patterns in Each Language},
  shorttitle = {Language Contact Does Not Drive Gesture Transfer},
  author = {Azar, Zeynep and Backus, Ad and Özyürek, Aslı},
  date = {2019-04-30},
  journaltitle = {Bilingualism: Language and Cognition},
  pages = {1--15},
  issn = {1366-7289, 1469-1841},
  doi = {10.1017/S136672891900018X},
  url = {https://www.cambridge.org/core/product/identifier/S136672891900018X/type/journal_article},
  urldate = {2019-08-30},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SIHT5VTL\\Azar et al. - 2019 - Language contact does not drive gesture transfer .pdf},
  langid = {english}
}

@article{baerReflexActivationLaryngeal1979,
  title = {Reflex Activation of Laryngeal Muscles by Sudden Induced Subglottal Pressure Changes},
  author = {Baer, T.},
  date = {1979-05},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {65},
  pages = {1271--1275},
  issn = {0001-4966},
  doi = {10.1121/1.382795},
  abstract = {In measuring the effect of subglottal pressure changes on fundamental frequency (Fo) of phonation, the effects of changing laryngeal muscle activity must be eliminated. Several investigators have used a strategy in which pulsatile increases of subglottal pressure are induced by pushing on the chest or abdomen of a phonating subject. Fundamental frequency is then correlated with subglottal pressure changes during an interval before laryngeal response is assumed to occur. The present study was undertaken to repeat such an experiment while monitoring electromyographic (EMG) activity of some laryngeal muscles, to discover empirically the latency of the laryngeal response. The results showed a consistent response to each push, with a latency of about 30 ms. Despite this response, analyses of fundamental frequency versus subglottal pressure changes during the interval of constant EMG activity were in general agreement with previously published values. With respect to the nature of the electromyographic response itself, its timing was found to be within the range of latencies appropriate for peripheral feedback, and was also similar to that for an acoustically--or tactually--elicited startle reflex.},
  eprint = {458049},
  eprinttype = {pmid},
  keywords = {Electromyography,Glottis,Humans,Laryngeal Muscles,Muscles,Phonation,Pressure,Reflex,Voice},
  langid = {english},
  number = {5}
}

@article{bakersusane.VentilationSpeechCharacteristics2008,
  title = {Ventilation and {{Speech Characteristics During Submaximal Aerobic Exercise}}},
  author = {{Baker Susan E.} and {Hipp Jenny} and {Alessio Helaine}},
  date = {2008-10-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {51},
  pages = {1203--1214},
  doi = {10.1044/1092-4388(2008/06-0223)},
  url = {https://pubs.asha.org/doi/10.1044/1092-4388%282008/06-0223%29},
  urldate = {2019-10-17},
  abstract = {Purpose       This study examined alterations in ventilation and speech characteristics as well          as perceived dyspnea during submaximal aerobic exercise tasks.                     Method       Twelve healthy participants completed aerobic exercise-only and simultaneous speaking          and aerobic exercise tasks at 50\% and 75\% of their maximum oxygen consumption (VO2 max). Measures of ventilation, oxygen consumption, heart rate, perceived dyspnea,          syllables per phrase, articulation rate, and inappropriate linguistic pause placements          were obtained at baseline and throughout the experimental tasks.                     Results       Ventilation was significantly lower during the speaking tasks compared with the nonspeaking          tasks. Oxygen consumption, however, did not significantly differ between speaking          and nonspeaking tasks. The perception of dyspnea was significantly higher during the          speaking tasks compared with the nonspeaking tasks. All speech parameters were significantly          altered over time at both task intensities.                     Conclusions       It is speculated that decreased ventilation without a reduction in oxygen consumption          implies that utilization of oxygen by the working muscles was increased during the          speaking tasks to meet the metabolic needs. A greater ability to utilize oxygen from          inspired air is found in individuals who are at higher fitness levels, and therefore          these findings may have implications for individuals who must complete simultaneous          speech and exercise for occupational purposes (e.g., fitness/military drill instructors,          singers performing choreography).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5Y2ALGCH\\06-0223).html},
  number = {5}
}

@article{balasubramanianAnalysisMovementSmoothness2015,
  title = {On the Analysis of Movement Smoothness},
  author = {Balasubramanian, Sivakumar and Melendez-Calderon, Alejandro and Roby-Brami, Agnes and Burdet, Etienne},
  date = {2015-12},
  journaltitle = {Journal of NeuroEngineering and Rehabilitation},
  volume = {12},
  issn = {1743-0003},
  doi = {10.1186/s12984-015-0090-9},
  url = {http://www.jneuroengrehab.com/content/12/1/112},
  urldate = {2020-01-31},
  abstract = {Quantitative measures of smoothness play an important role in the assessment of sensorimotor impairment and motor learning. Traditionally, movement smoothness has been computed mainly for discrete movements, in particular arm, reaching and circle drawing, using kinematic data. There are currently very few studies investigating smoothness of rhythmic movements, and there is no systematic way of analysing the smoothness of such movements. There is also very little work on the smoothness of other movement related variables such as force, impedance etc. In this context, this paper presents the first step towards a unified framework for the analysis of smoothness of arbitrary movements and using various data. It starts with a systematic definition of movement smoothness and the different factors that influence smoothness, followed by a review of existing methods for quantifying the smoothness of discrete movements. A method is then introduced to analyse the smoothness of rhythmic movements by generalising the techniques developed for discrete movements. We finally propose recommendations for analysing smoothness of any general sensorimotor behaviour.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\A64EVWVB\\Balasubramanian et al. - 2015 - On the analysis of movement smoothness.pdf},
  langid = {english},
  number = {1}
}

@article{baldisseraAnticipatoryPosturalAdjustments2008,
  title = {Anticipatory Postural Adjustments in Arm Muscles Associated with Movements of the Contralateral Limb and Their Possible Role in Interlimb Coordination},
  author = {Baldissera, Fausto and Rota, Viviana and Esposti, Roberto},
  date = {2008-02},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {185},
  pages = {63--74},
  issn = {1432-1106},
  doi = {10.1007/s00221-007-1131-9},
  abstract = {While sitting on a turnable stool, with both shoulders flexed at 90 degrees or, alternatively, with arms parallel to the trunk and the elbows flexed at 90 degrees--the hands being semisupine--subjects performed unidirectional and cyclic movements on the horizontal plane of the right arm (adduction-abduction) or hand (flexion-extension). The left arm was still, in a position symmetrical to that of the right limb and with the hand contacting a fixed support by the palmar or dorsal surface. During both unidirectional and cyclic arm or hand movements, activation of the prime mover muscles (right Pectoralis Major for arm adduction and Infraspinatus for abduction; right Flexor Carpi Radialis and Extensor Carpi Radialis for the hand movements) was accompanied by activation of the homologous muscles of the contralateral arm and inhibition of antagonists. The contralateral activities (1) regularly preceded the burst in the movement prime movers and (2) were organised in fixation chains that, exerting forces on the hand fixed support, will counterbalance the rotatory action exerted on the trunk by the primary movement. Based on these features, these activities may be classified as anticipatory postural adjustments (APAs). The observed APAs distribution is such as to favour the preferential (mirror symmetrical) coupling of upper limb movements on the horizontal plane. The possible role of these APAs in determining the different constraints experienced when performing mirror symmetrical versus isodirectional coupling is discussed.},
  eprint = {17912507},
  eprinttype = {pmid},
  keywords = {Adult,Arm,Electromyography,Female,Humans,Male,Middle Aged,Movement,Muscle; Skeletal,Postural Balance,Posture,Psychomotor Performance},
  langid = {english},
  number = {1}
}

@article{banseAcousticProfilesVocal,
  title = {Acoustic {{Profiles}} in {{Vocal Emotion Expression}}},
  author = {Banse, Rainer and Scherer, Klaus R},
  journaltitle = {Journal of Personality and Social Psychology},
  volume = {70},
  pages = {614--636},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\X8RRIRXT\\Banse and Scherer - Acoustic Profiles in Vocal Emotion Expression.pdf},
  langid = {english}
}

@article{banzettLocomotionMenHas1992,
  title = {Locomotion in Men Has No Appreciable Mechanical Effect on Breathing},
  author = {Banzett, R. B. and Mead, J. and Reid, M. B. and Topulos, G. P.},
  date = {1992-05},
  journaltitle = {Journal of Applied Physiology (Bethesda, Md.: 1985)},
  shortjournal = {J. Appl. Physiol.},
  volume = {72},
  pages = {1922--1926},
  issn = {8750-7587},
  doi = {10.1152/jappl.1992.72.5.1922},
  abstract = {It has been suggested that the act of taking a stride produces substantial respiratory volume displacement and that this assists the respiratory muscles during locomotion. We measured the flow at the mouth associated with stride in walking and running humans and found it to be 1-2\% of respiratory tidal volume, which is too small to make an appreciable contribution to pulmonary ventilation.},
  eprint = {1601801},
  eprinttype = {pmid},
  keywords = {Adult,Aged,Humans,Locomotion,Male,Middle Aged,Periodicity,Respiratory Mechanics,Running,Tidal Volume,Walking},
  langid = {english},
  number = {5}
}

@article{barcelo-coblijnBiolinguisticApproachVocalizations2011,
  title = {A {{Biolinguistic Approach}} to the {{Vocalizations}} of {{H}}. {{Neanderthalensis}} and the {{Genus Homo}}},
  author = {Barceló-Coblijn, Llu?s},
  date = {2011-12-20},
  journaltitle = {BIOLINGUISTICS},
  shortjournal = {BL},
  volume = {5},
  pages = {286--334},
  issn = {1450-3417},
  url = {https://www.biolinguistics.eu/index.php/biolinguistics/article/view/188},
  urldate = {2020-09-19},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8G78YXDI\\Barceló-Coblijn - 2011 - A Biolinguistic Approach to the Vocalizations of H.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8295D38F\\188.html},
  issue = {4},
  keywords = {von Economo neurons},
  langid = {english},
  number = {4}
}

@article{bardRoleAfferentInformation1992,
  title = {Role of Afferent Information in the Timing of Motor Commands: A Comparative Study with a Deafferented Patient},
  shorttitle = {Role of Afferent Information in the Timing of Motor Commands},
  author = {Bard, C. and Paillard, J. and Lajoie, Y. and Fleury, M. and Teasdale, N. and Forget, R. and Lamarre, Y.},
  date = {1992-02},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  volume = {30},
  pages = {201--206},
  issn = {0028-3932},
  abstract = {The accuracy of the motor system in synchronizing simultaneous movements initiations was tested in two conditions: (1) when the motor commands were triggered by an external signal (reactive condition), and (2) when subjects self-paced their movement onsets (self-paced condition). The task consisted of initiating simultaneously ipsilateral finger extension and heel raising. Eight normal subjects and a deafferented patient were tested. In the reactive condition, both normal subjects and the deafferented patient exhibited a precession of finger initiation over heel raising. This delay corresponds to the difference observed in the reaction time of the two limbs when measured independently. It reflects the difference in conduction times of the efferent pathways, as if the two motor commands were released simultaneously through a common triggering signal in the motor cortex. In contrast, in the self-paced condition normal subjects showed precession of heel over finger onsets, suggesting that synchrony is based upon the evaluation of afferent information. Unlike normal subjects, the patient showed no heel precession in the self-paced condition. These findings suggest that reactive and self-paced responses are produced through two different control modes and that afferent information contributes to the timing of motor commands in the self-paced mode.},
  eprint = {1560897},
  eprinttype = {pmid},
  keywords = {Adult,Afferent Pathways,Demyelinating Diseases,Electromyography,Female,Humans,Male,Mental Processes,Movement,Reaction Time,Time Factors},
  langid = {english},
  number = {2}
}

@article{bardyMovingUnisonPerceptual2020,
  title = {Moving in Unison after Perceptual Interruption},
  author = {Bardy, Benoît G. and Calabrese, Carmela and De Lellis, Pietro and Bourgeaud, Stella and Colomer, Clémentine and Pla, Simon and di Bernardo, Mario},
  date = {2020-10-22},
  journaltitle = {Scientific Reports},
  volume = {10},
  pages = {18032},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-74914-z},
  url = {https://www.nature.com/articles/s41598-020-74914-z},
  urldate = {2020-11-02},
  abstract = {Humans interact in groups through various perception and action channels. The continuity of interaction despite a transient loss of perceptual contact often exists and contributes to goal achievement. Here, we study the dynamics of this continuity, in two experiments involving groups of participants (\$\$N=7\$\$N=7) synchronizing their movements in space and in time. We show that behavioural unison can be maintained after perceptual contact has been lost, for about 7s. Agent similarity and spatial configuration in the group modulated synchronization performance, differently so when perceptual interaction was present or when it was memorized. Modelling these data through a network of oscillators enabled us to clarify the double origin of this memory effect, of individual and social nature. These results shed new light into why humans continue to move in unison after perceptual interruption, and are consequential for a wide variety of applications at work, in art and in sport.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NUNWXN6K\\Bardy et al. - 2020 - Moving in unison after perceptual interruption.pdf},
  issue = {1},
  langid = {english},
  number = {1},
  options = {useprefix=true}
}

@article{barkerTensileTransmissionLumbar2004,
  title = {Tensile {{Transmission Across}} the {{Lumbar Fasciae}} in {{Unembalmed Cadavers}}: {{Effects}} of {{Tension}} to {{Various Muscular Attachments}}},
  shorttitle = {Tensile {{Transmission Across}} the {{Lumbar Fasciae}} in {{Unembalmed Cadavers}}},
  author = {Barker, Priscilla J. and Briggs, Christopher A. and Bogeski, Goce},
  date = {2004-01-15},
  journaltitle = {Spine},
  volume = {29},
  pages = {129--138},
  issn = {0362-2436},
  doi = {10.1097/01.BRS.0000107005.62513.32},
  url = {https://journals.lww.com/spinejournal/Fulltext/2004/01150/The_Relation_Between_the_Transversus_Abdominis.5.aspx?casa_token=ma0ldjmjKIoAAAAA:bfObNe6Lmxf1A2InwLNv0sx9uFRZBB_XDwztOJj8GSZg1aUE-hmZxP0jR5Eeh2ziAa2WJCnGEBSQrpUMhF4uqGPz},
  urldate = {2020-04-03},
  abstract = {Study Design. ~Traction was applied to muscles attaching to the posterior and middle layers of lumbar fascia (PLF, MLF). Effects on fasciae were determined via tensile force measures and movement of markers.         Objectives. ~To document tensile transmission to the PLF and MLF when traction was applied to latissimus dorsi (LD), gluteus maximus (GM), external and internal oblique (EO, IO), and transversus abdominis (TrA) in unembalmed cadavers.         Summary of Background Data. ~A previous study on embalmed cadavers applied traction to muscle attachments while monitoring fascial movement but did not test TrA or the MLF.         Methods. ~The PLF and MLF were dissected then marked on eight unembalmed cadavers. A strain gauge was inserted through fascia at L3; 10N traction was applied to each muscle attachment while photographs and tension measures were taken. Movement of fascial markers was detected photographically. Fascial widths were also measured.         Results. ~Tension was clearly transmitted to fascial vertebral attachments. Tensile forces and fascial areas affected were highest for traction on LD and TrA in the PLF and for TrA in the MLF. Movement of PLF markers from tension on LD and TrA occurred bilaterally between T12 and S1. Effects from other muscles were variably bilateral, with those from GM and IO occurring below L3 and those from EO occurring above L3. Tensile forces were relatively high in the MLF and its width was less than half that of the PLF.         Conclusions. ~Low levels of tension are effectively transmitted between TrA and the MLF or PLF. Via them, TrA may influence intersegmental movement.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GSE3CXK3\\The_Relation_Between_the_Transversus_Abdominis.5.html},
  langid = {american},
  number = {2}
}

@article{barnettActivityAntagonistMuscles1955,
  title = {The Activity of Antagonist Muscles during Voluntary Movement},
  author = {Barnett, C. H. and Harding, D.},
  date = {1955-10},
  journaltitle = {Annals of Physical Medicine},
  shortjournal = {Ann Phys Med},
  volume = {2},
  pages = {290--293},
  issn = {0365-5547},
  doi = {10.1093/rheumatology/2.8.290},
  eprint = {13275868},
  eprinttype = {pmid},
  keywords = {Humans,Muscles,MUSCLES/physiology,Musculoskeletal Physiological Phenomena},
  langid = {english},
  number = {8}
}

@article{baronchelliNetworksCognitiveScience2013,
  title = {Networks in {{Cognitive Science}}},
  author = {Baronchelli, Andrea and Ferrer-i-Cancho, Ramon and Pastor-Satorras, Romualdo and Chater, Nick and Christiansen, Morten H.},
  date = {2013-07},
  journaltitle = {Trends in Cognitive Sciences},
  volume = {17},
  pages = {348--360},
  issn = {13646613},
  doi = {10.1016/j.tics.2013.04.010},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S136466131300096X},
  urldate = {2020-01-21},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RFFKLKMX\\Baronchelli et al. - 2013 - Networks in Cognitive Science.pdf},
  langid = {english},
  number = {7}
}

@article{barryMeetFribblesNovel2014,
  title = {Meet the {{Fribbles}}: Novel Stimuli for Use within Behavioural Research},
  shorttitle = {Meet the {{Fribbles}}},
  author = {Barry, Tom Joseph and Griffith, James W. and De Rossi, Stephanie and Hermans, Dirk},
  date = {2014},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00103},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00103/full},
  urldate = {2021-01-04},
  abstract = {Clinical researchers make use of experimental models for mental disorders. In many cases, these models use stimuli that are relevant to the disorder under scrutiny, which allows one to experimentally investigate the factors that contribute to the development of the disorder. For example, one might use spiders or spider-like stimuli in the study of specific phobia. More broadly, researchers often make use of real-world stimuli such as images of animals, geometrical shapes or emotional words. However, these stimuli are often limited in their experimental controllability and their applicability to the disorder in question. We present a novel set of animal-like stimuli, called Fribbles, for use within behavioural research. Fribbles have desirable properties for use in research because they are similar to real-world stimuli, but due to their novelty, participants will not have had previous experience with them. They also have known properties that can be experimentally manipulated. We present an investigation into similarity between Fribbles in order to illustrate their utility in research that relies on comparisons between similar stimuli. Fribbles offer both experimental control and generalisability to the real world, although some consideration must be made concerning the properties that influence similarity between Fribbles when selecting them along a dimension of similarity.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MVEINLWQ\\Barry et al. - 2014 - Meet the Fribbles novel stimuli for use within be.pdf},
  keywords = {Anxiety,Attention,avoidance,behaviour,generalisation,Memory,phobia,Stimuli},
  langid = {english}
}

@article{barthelNextSpeakersPlan2017,
  title = {Next {{Speakers Plan Their Turn Early}} and {{Speak}} after {{Turn}}-{{Final}} “{{Go}}-{{Signals}}”},
  author = {Barthel, Mathias and Meyer, Antje S. and Levinson, Stephen C.},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00393},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00393/full},
  urldate = {2020-12-09},
  abstract = {In conversation, turn-taking is usually fluid, with next speakers taking their turn right after the end of the previous turn. One reason for this fluency is early content planning of the next turn, if possible while the current turn is still coming in, as found by Barthel et al. (2016) using the list-completion paradigm. The present study makes use of the same paradigm, analyzing speech onset latencies and eye-movements of participants in a task-oriented dialogue with a confederate. Participants named objects visible on their computer screen in response to utterances that did or did not contain cues to the end of the incoming turn. Participants were found to start planning their response as early as possible, replicating the findings of Barthel et al. (2016), and to use turn-final cues to turn-completion as ‘go-signals’ to initiate their response. The results are consistent with models of turn-taking that assume next speakers to start planning their response as soon as the incoming turn’s message can be understood and to monitor the incoming turn for cues to turn-completion so as to initiate their response when turn-transition becomes relevant.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NWU8LMML\\Barthel et al. - 2017 - Next Speakers Plan Their Turn Early and Speak afte.pdf},
  keywords = {Eye-movements,intonation,planning,production,Task-oriented dialogue,turn-taking},
  langid = {english}
}

@article{barthelNextSpeakersPlan2020,
  title = {Next Speakers Plan Word Forms in Overlap with the Incoming Turn: {{Evidence}} from Gaze-Contingent Switch Task Performance},
  shorttitle = {Next Speakers Plan Word Forms in Overlap with the Incoming Turn},
  author = {Barthel, Mathias and Levinson, Stephen C.},
  date = {2020},
  journaltitle = {Language, Cognition and Neuroscience},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{Taylor \& Francis}},
  location = {{United Kingdom}},
  issn = {2327-3801(Electronic),2327-3798(Print)},
  doi = {10.1080/23273798.2020.1716030},
  abstract = {To ensure short gaps between turns in conversation, next speakers regularly start planning their utterance in overlap with the incoming turn. Three experiments investigate which stages of utterance planning are executed in overlap. E1 establishes effects of associative and phonological relatedness of pictures and words in a switch-task from picture naming to lexical decision. E2 focuses on effects of phonological relatedness and investigates potential shifts in the time-course of production planning during background speech. E3 required participants to verbally answer questions as a base task. In critical trials, however, participants switched to visual lexical decision just after they began planning their answer. The task-switch was time-locked to participants' gaze for response planning. Results show that word form encoding is done as early as possible and not postponed until the end of the incoming turn. Hence, planning a response during the incoming turn is executed at least until word form activation. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6VB8PRBZ\\Barthel and Levinson - 2020 - Next speakers plan word forms in overlap with the .pdf;C\:\\Users\\u668173\\Zotero\\storage\\8974WZWQ\\2020-06024-001.html}
}

@article{barthelTimingUtterancePlanning2016,
  title = {The {{Timing}} of {{Utterance Planning}} in {{Task}}-{{Oriented Dialogue}}: {{Evidence}} from a {{Novel List}}-{{Completion Paradigm}}},
  shorttitle = {The {{Timing}} of {{Utterance Planning}} in {{Task}}-{{Oriented Dialogue}}},
  author = {Barthel, Mathias and Sauppe, Sebastian and Levinson, Stephen C. and Meyer, Antje S.},
  date = {2016},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {7},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.01858},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01858/full},
  urldate = {2020-12-08},
  abstract = {In conversation, interlocutors rarely leave long gaps between turns, suggesting that next speak- ers begin to plan their turns while listening to the previous speaker. The present experiment used analyses of speech onset latencies and eye-movements in a task-oriented dialogue paradigm to investigate when speakers start planning their response. Adult German participants heard a confederate describe sets of objects in utterances that either ended in a noun (e.g. Ich habe eine Tür und ein Fahrrad (‘I have a door and a bicycle’)) or a verb form (Ich habe eine Tür und ein Fahrrad besorgt (‘I have gotten a door and a bicycle’)), while the presence or absence of the final verb either was or was not predictable from the preceding sentence structure. In response, participants had to name any unnamed objects they could see in their own display in utterances such as Ich habe ein Ei (‘I have an egg’). The main question was when participants started to plan their response. The results are consistent with the view that speakers begin to plan their turn as soon as sufficient information is available to do so, irrespective of further incoming words.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NFYDZQZE\\Barthel et al. - 2016 - The Timing of Utterance Planning in Task-Oriented .pdf},
  keywords = {Eye-movements,planning,prediction,production,Task-oriented dialogue,timing of turn-taking},
  langid = {english}
}

@article{barthelTimingUtterancePlanning2016a,
  title = {The Timing of Utterance Planning in Task-Oriented Dialogue: {{Evidence}} from a Novel List-Completion Paradigm},
  shorttitle = {The Timing of Utterance Planning in Task-Oriented Dialogue},
  author = {Barthel, Mathias and Sauppe, Sebastian and Levinson, Stephen C. and Meyer, Antje S.},
  date = {2016},
  journaltitle = {Frontiers in Psychology},
  volume = {7},
  publisher = {{Frontiers Media S.A.}},
  location = {{Switzerland}},
  issn = {1664-1078(Electronic)},
  abstract = {In conversation, interlocutors rarely leave long gaps between turns, suggesting that next speakers begin to plan their turns while listening to the previous speaker. The present experiment used analyses of speech onset latencies and eye-movements in a task-oriented dialogue paradigm to investigate when speakers start planning their responses.German speakers heard a confederate describe sets of objects in utterances that either ended in a noun [e.g., Ich habe eine Tür und ein Fahrrad (“I have a door and a bicycle”)]or a verb form [e.g., Ich habe eine Tür und ein Fahrrad besorgt(“I have gotten a door and a bicycle”)], while the presence or absence of the final verb either was or was not predictable from the preceding sentence structure. In response, participants had to name any unnamed objects they could see in their own displays with utterances such as Ich habe ein Ei (“I have an egg”). The results show that speakers begin to plan their turns as soon as sufficient information is available to do so, irrespective of further incoming words. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6N48MXN4\\2017-00774-001.html},
  keywords = {Eye Movements,Oral Communication,Prediction}
}

@incollection{barthSlightestWhiffAir2014,
  title = {The {{Slightest Whiff}} of {{Air}}: {{Airflow Sensing}} in {{Arthropods}}},
  shorttitle = {The {{Slightest Whiff}} of {{Air}}},
  booktitle = {Flow {{Sensing}} in {{Air}} and {{Water}}: {{Behavioral}}, {{Neural}} and {{Engineering Principles}} of {{Operation}}},
  author = {Barth, Friedrich G.},
  editor = {Bleckmann, Horst and Mogdans, Joachim and Coombs, Sheryl L.},
  date = {2014},
  pages = {169--196},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-41446-6_7},
  url = {https://doi.org/10.1007/978-3-642-41446-6_7},
  urldate = {2019-05-07},
  abstract = {The perception of medium flows has received ever increasing attention during the last two decades and has increasingly been recognized as a sensory capacity of its own. A combination of experimental work and physical–mathematical modeling has deepened our understanding of the workings of airflow sensors, mainly represented by insect filiform hairs and arachnid trichobothria, both as individual sensors and sensor arrays. This chapter points to the diversity of arthropod airflow sensors and stresses the importance of comparative studies. These should include animal groups so far largely neglected by sensory biology and neuroethology. Another need is to analyze biologically relevant flow patterns and to relate these to the functional properties of the various patterns of sensor arrangement found in different animal taxa. Finally, the capture of a freely flying fly by a wandering spider is taken to illustrate the challenges and promises of studies that aim to reveal the relation between a particular airflow pattern and a specific behavior.},
  isbn = {978-3-642-41446-6},
  keywords = {Digital Particle Image Velocimetry,Hair Shaft,High Frequency Component,Medium Flow,Oribatid Mite},
  langid = {english}
}

@book{basmajianMusclesAliveTheir1985,
  title = {Muscles {{Alive}}: Their Functions Revealed by Electromyography},
  shorttitle = {Muscles {{Alive}}},
  author = {Basmajian, John V and de Luca, Carlo J.},
  date = {1985},
  publisher = {{Williams \& Wilkins}},
  location = {{Baltimore}},
  annotation = {OCLC: 805568666},
  isbn = {978-0-683-00414-4},
  langid = {english}
}

@article{bauerSynchronisationNeuralOscillations2020,
  title = {Synchronisation of Neural Oscillations and Cross-Modal Influences},
  author = {Bauer, Anna-Katharina R. and Debener, Stefan and Nobre, Anna C.},
  date = {2020-06-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {24},
  pages = {481--495},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2020.03.003},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661320300784},
  urldate = {2020-09-17},
  abstract = {At any given moment, we receive multiple signals from our different senses. Prior research has shown that signals in one sensory modality can influence neural activity and behavioural performance associated with another sensory modality. Recent human and nonhuman primate studies suggest that such cross-modal influences in sensory cortices are mediated by the synchronisation of ongoing neural oscillations. In this review, we consider two mechanisms proposed to facilitate cross-modal influences on sensory processing, namely cross-modal phase resetting and neural entrainment. We consider how top-down processes may further influence cross-modal processing in a flexible manner, and we highlight fruitful directions for further research.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\86UHHQ2J\\Bauer et al. - 2020 - Synchronisation of Neural Oscillations and Cross-m.pdf;C\:\\Users\\u668173\\Zotero\\storage\\B94Y4G5X\\S1364661320300784.html},
  keywords = {causal inference,cross-modal influence,multisensory,neural entrainment,neural oscillations,phase reset},
  langid = {english},
  number = {6}
}

@article{bauerSynchronisationNeuralOscillations2020a,
  title = {Synchronisation of {{Neural Oscillations}} and {{Cross}}-Modal {{Influences}}},
  author = {Bauer, Anna-Katharina R. and Debener, Stefan and Nobre, Anna C.},
  date = {2020-06-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {24},
  pages = {481--495},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2020.03.003},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661320300784},
  urldate = {2020-12-05},
  abstract = {At any given moment, we receive multiple signals from our different senses. Prior research has shown that signals in one sensory modality can influence neural activity and behavioural performance associated with another sensory modality. Recent human and nonhuman primate studies suggest that such cross-modal influences in sensory cortices are mediated by the synchronisation of ongoing neural oscillations. In this review, we consider two mechanisms proposed to facilitate cross-modal influences on sensory processing, namely cross-modal phase resetting and neural entrainment. We consider how top-down processes may further influence cross-modal processing in a flexible manner, and we highlight fruitful directions for further research.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6H4ESWLW\\Bauer et al. - 2020 - Synchronisation of Neural Oscillations and Cross-m.pdf;C\:\\Users\\u668173\\Zotero\\storage\\Z92WMDEH\\S1364661320300784.html},
  keywords = {causal inference,cross-modal influence,multisensory,neural entrainment,neural oscillations,phase reset},
  langid = {english},
  number = {6}
}

@article{bausWhenDoesIconicity2013,
  title = {When Does {{Iconicity}} in {{Sign Language Matter}}?},
  author = {Baus, Cristina and Carreiras, Manuel and Emmorey, Karen},
  date = {2013-03-01},
  journaltitle = {Language and Cognitive Processes},
  shortjournal = {Lang Cogn Process},
  volume = {28},
  pages = {261--271},
  issn = {0169-0965},
  doi = {10.1080/01690965.2011.620374},
  abstract = {We examined whether iconicity in American Sign Language (ASL) enhances translation performance for new learners and proficient signers. Fifteen hearing nonsigners and 15 proficient ASL-English bilinguals performed a translation recognition task and a production translation task. Nonsigners were taught 28 ASL verbs (14 iconic; 14 non-iconic) prior to performing these tasks. Only new learners benefited from sign iconicity, recognizing iconic translations faster and more accurately and exhibiting faster forward (English-ASL) and backward (ASL-English) translation times for iconic signs. In contrast, proficient ASL-English bilinguals exhibited slower recognition and translation times for iconic signs. We suggest iconicity aids memorization in the early stages of adult sign language learning, but for fluent L2 signers, iconicity interacts with other variables that slow translation (specifically, the iconic signs had more translation equivalents than the non-iconic signs). Iconicity may also have slowed translation performance by forcing conceptual mediation for iconic signs, which is slower than translating via direct lexical links.},
  eprint = {23543899},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Z8ELZT78\\Baus et al. - 2013 - When does Iconicity in Sign Language Matter.pdf},
  keywords = {American Sign Language,bilingualism,iconicity,translation},
  langid = {english},
  number = {3},
  pmcid = {PMC3608132}
}

@article{bavelasGesturingTelephoneIndependent2008,
  title = {Gesturing on the Telephone: {{Independent}} Effects of Dialogue and Visibility},
  shorttitle = {Gesturing on the Telephone},
  author = {Bavelas, Janet and Gerwing, Jennifer and Sutton, Chantelle and Prevost, Danielle},
  date = {2008-02-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {58},
  pages = {495--520},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2007.02.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0749596X07000289},
  urldate = {2019-10-14},
  abstract = {Speakers often gesture in telephone conversations, even though they are not visible to their addressees. To test whether this effect is due to being in a dialogue, we separated visibility and dialogue with three conditions: face-to-face dialogue (10 dyads), telephone dialogue (10 dyads), and monologue to a tape recorder (10 individuals). For the rate of gesturing, both dialogue and visibility had significant, independent effects, with the telephone condition consistently higher than the tape recorder. Also, as predicted, visibility alone significantly affected how speakers gestured: face-to-face speakers were more likely to make life-size gestures, to put information in their gestures that was not in their words, to make verbal reference to their gestures, and to use more gestures referring to the interaction itself. We speculate that demonstration, as a modality, may underlie these findings and may be intimately tied to dialogue while being suppressed in monologue.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FFGN2YU7\\S0749596X07000289.html},
  keywords = {Demonstration,Face-to-face dialogue,Gestures,Telephone,Visibility},
  number = {2}
}

@article{beatyAutomatingCreativityAssessment2020,
  title = {Automating Creativity Assessment with {{SemDis}}: {{An}} Open Platform for Computing Semantic Distance},
  shorttitle = {Automating Creativity Assessment with {{SemDis}}},
  author = {Beaty, Roger E. and Johnson, Dan R.},
  date = {2020-08-31},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  issn = {1554-3528},
  doi = {10.3758/s13428-020-01453-w},
  url = {https://doi.org/10.3758/s13428-020-01453-w},
  urldate = {2021-01-29},
  abstract = {Creativity research requires assessing the quality of ideas and products. In practice, conducting creativity research often involves asking several human raters to judge participants’ responses to creativity tasks, such as judging the novelty of ideas from the alternate uses task (AUT). Although such subjective scoring methods have proved useful, they have two inherent limitations—labor cost (raters typically code thousands of responses) and subjectivity (raters vary on their perceptions and preferences)—raising classic psychometric threats to reliability and validity. We sought to address the limitations of subjective scoring by capitalizing on recent developments in automated scoring of verbal creativity via semantic distance, a computational method that uses natural language processing to quantify the semantic relatedness of texts. In five studies, we compare the top performing semantic models (e.g., GloVe, continuous bag of words) previously shown to have the highest correspondence to human relatedness judgements. We assessed these semantic models in relation to human creativity ratings from a canonical verbal creativity task (AUT; Studies 1–3) and novelty/creativity ratings from two word association tasks (Studies 4–5). We find that a latent semantic distance factor—comprised of the common variance from five semantic models—reliably and strongly predicts human creativity and novelty ratings across a range of creativity tasks. We also replicate an established experimental effect in the creativity literature (i.e., the serial order effect) and show that semantic distance correlates with other creativity measures, demonstrating convergent validity. We provide an open platform to efficiently compute semantic distance, including tutorials and documentation (https://osf.io/gz4fc/).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WTAFTRQS\\Beaty and Johnson - 2020 - Automating creativity assessment with SemDis An o.pdf},
  langid = {english}
}

@article{beckageSmallWorldsSemantic2011,
  title = {Small {{Worlds}} and {{Semantic Network Growth}} in {{Typical}} and {{Late Talkers}}},
  author = {Beckage, Nicole and Smith, Linda and Hills, Thomas},
  editor = {Perc, Matjaz},
  date = {2011-05-11},
  journaltitle = {PLoS ONE},
  volume = {6},
  pages = {e19348},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0019348},
  url = {http://dx.plos.org/10.1371/journal.pone.0019348},
  urldate = {2020-01-21},
  abstract = {Network analysis has demonstrated that systems ranging from social networks to electric power grids often involve a small world structure-with local clustering but global ac cess. Critically, small world structure has also been shown to characterize adult human semantic networks. Moreover, the connectivity pattern of these mature networks is consistent with lexical growth processes in which children add new words to their vocabulary based on the structure of the language-learning environment. However, thus far, there is no direct evidence that a child’s individual semantic network structure is associated with their early language learning. Here we show that, while typically developing children’s early networks show small world structure as early as 15 months and with as few as 55 words, children with language delay (late talkers) have this structure to a smaller degree. This implicates a maladaptive bias in word acquisition for late talkers, potentially indicating a preference for ‘‘oddball’’ words. The findings provide the first evidence of a link between small-world connectivity and lexical development in individual children.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BCG8ME59\\Beckage et al. - 2011 - Small Worlds and Semantic Network Growth in Typica.pdf},
  langid = {english},
  number = {5}
}

@article{beecksEfficientQueryProcessing2016,
  title = {Efficient Query Processing in {{3D}} Motion Capture Gesture Databases},
  author = {Beecks, Christian and Hassani, Marwan and Brenger, Bela and Hinnell, Jennifer and Schüller, Daniel and Mittelberg, Irene and Seidl, Thomas},
  date = {2016-03-01},
  journaltitle = {International Journal of Semantic Computing},
  shortjournal = {Int. J. Semantic Computing},
  volume = {10},
  pages = {5--25},
  publisher = {{World Scientific Publishing Co.}},
  issn = {1793-351X},
  doi = {10.1142/S1793351X16400018},
  url = {https://www.worldscientific.com/doi/10.1142/S1793351X16400018},
  urldate = {2021-01-31},
  abstract = {One of the most fundamental challenges when accessing gestural patterns in 3D motion capture databases is the definition of spatiotemporal similarity. While distance-based similarity models such as the Gesture Matching Distance on gesture signatures are able to leverage the spatial and temporal characteristics of gestural patterns, their applicability to large 3D motion capture databases is limited due to their high computational complexity. To this end, we present a lower bound approximation of the Gesture Matching Distance that can be utilized in an optimal multi-step query processing architecture in order to support efficient query processing. We investigate the performance in terms of accuracy and efficiency based on 3D motion capture databases and show that our approach is able to achieve an increase in efficiency of more than one order of magnitude with a negligible loss in accuracy. In addition, we discuss different applications in the digital humanities in order to highlight the significance of similarity search approaches in the research field of gestural pattern analysis.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\443TPH4B\\S1793351X16400018.html},
  number = {01}
}

@inproceedings{beecksSpatiotemporalSimilaritySearch2015,
  title = {Spatiotemporal Similarity Search in {{3D}} Motion Capture Gesture Streams},
  booktitle = {Advances in {{Spatial}} and {{Temporal Databases}}},
  author = {Beecks, Christian and Hassani, Marwan and Hinnell, Jennifer and Schüller, Daniel and Brenger, Bela and Mittelberg, Irene and Seidl, Thomas},
  editor = {Claramunt, Christophe and Schneider, Markus and Wong, Raymond Chi-Wing and Xiong, Li and Loh, Woong-Kee and Shahabi, Cyrus and Li, Ki-Joune},
  date = {2015},
  pages = {355--372},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-22363-6_19},
  abstract = {The question of how to model spatiotemporal similarity between gestures arising in 3D motion capture data streams is of major significance in currently ongoing research in the domain of human communication. While qualitative perceptual analyses of co-speech gestures, which are manual gestures emerging spontaneously and unconsciously during face-to-face conversation, are feasible in a small-to-moderate scale, these analyses are inapplicable to larger scenarios due to the lack of efficient query processing techniques for spatiotemporal similarity search. In order to support qualitative analyses of co-speech gestures, we propose and investigate a simple yet effective distance-based similarity model that leverages the spatial and temporal characteristics of co-speech gestures and enables similarity search in 3D motion capture data streams in a query-by-example manner. Experiments on real conversational 3D motion capture data evidence the appropriateness of the proposal in terms of accuracy and efficiency.},
  isbn = {978-3-319-22363-6},
  keywords = {3D motion capture data,Co-speech gestures,Dynamic time warping,Gesture matching distance,Gesture signature,Similarity search,Spatiotemporal data,Streams},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@report{bekkePredictivePotentialHand2020,
  title = {The Predictive Potential of Hand Gestures during Conversation: {{An}} Investigation of the Timing of Gestures in Relation to Speech},
  shorttitle = {The Predictive Potential of Hand Gestures during Conversation},
  author = {ter Bekke, Marlijn and Drijvers, Linda and Holler, Judith},
  date = {2020-06-19T07:32:28},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/b5zq7},
  url = {https://psyarxiv.com/b5zq7/},
  urldate = {2020-12-08},
  abstract = {In face-to-face conversation, recipients might use the bodily movements of the speaker (e.g. gestures) to facilitate language processing. It has been suggested that one way through which this facilitation may happen is prediction. However, for this to be possible, gestures would need to precede speech, and it is unclear whether this is true during natural conversation. In a corpus of Dutch conversations, we annotated hand gestures that represent semantic information and occurred during questions, and the word(s) which corresponded most closely to the gesturally depicted meaning. Thus, we tested whether representational gestures temporally precede their lexical affiliates. Further, to see whether preceding gestures may indeed facilitate language processing, we asked whether the gesture-speech asynchrony predicts the response time to the question the gesture is part of. Gestures and their strokes (most meaningful movement component) indeed preceded the corresponding lexical information, thus demonstrating their predictive potential. However, while questions with gestures got faster responses than questions without, there was no evidence that questions with larger gesture-speech asynchronies get faster responses. These results suggest that gestures indeed have the potential to facilitate predictive language processing, but further analyses on larger datasets are needed to test for links between asynchrony and processing advantages.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XG5YGVVT\\Bekke et al. - 2020 - The predictive potential of hand gestures during c.pdf},
  keywords = {co-speech gesture,Cognitive Psychology,Language,language prediction,Linguistics,multimodal communication,Nonverbal Behavior,other,Psychology,Social and Behavioral Sciences,Social and Personality Psychology}
}

@article{ben-talLogicNeuralControl2019,
  title = {The Logic behind Neural Control of Breathing Pattern},
  author = {Ben-Tal, Alona and Wang, Yunjiao and Leite, Maria C. A.},
  date = {2019-06-24},
  journaltitle = {Scientific Reports},
  volume = {9},
  pages = {9078},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-45011-7},
  url = {https://www.nature.com/articles/s41598-019-45011-7},
  urldate = {2020-10-27},
  abstract = {The respiratory rhythm generator is spectacular in its ability to support a wide range of activities and adapt to changing environmental conditions, yet its operating mechanisms remain elusive. We show how selective control of inspiration and expiration times can be achieved in a new representation of the neural system (called a Boolean network). The new framework enables us to predict the behavior of neural networks based on properties of neurons, not their values. Hence, it reveals the logic behind the neural mechanisms that control the breathing pattern. Our network mimics many features seen in the respiratory network such as the transition from a 3-phase to 2-phase to 1-phase rhythm, providing novel insights and new testable predictions.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CGNSVK8V\\Ben-Tal et al. - 2019 - The logic behind neural control of breathing patte.pdf;C\:\\Users\\u668173\\Zotero\\storage\\4GEWWDZU\\s41598-019-45011-7.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{bendichPersistentHomologyAnalysis2016,
  title = {Persistent {{Homology Analysis}} of {{Brain Artery Trees}}},
  author = {Bendich, Paul and Marron, J. S. and Miller, Ezra and Pieloch, Alex and Skwerer, Sean},
  date = {2016},
  journaltitle = {The annals of applied statistics},
  shortjournal = {Ann Appl Stat},
  volume = {10},
  pages = {198--218},
  issn = {1932-6157},
  doi = {10.1214/15-AOAS886},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5026243/},
  urldate = {2020-03-11},
  abstract = {New representations of tree-structured data objects, using ideas from topological data analysis, enable improved statistical analyses of a population of brain artery trees. A number of representations of each data tree arise from persistence diagrams that quantify branching and looping of vessels at multiple scales. Novel approaches to the statistical analysis, through various summaries of the persistence diagrams, lead to heightened correlations with covariates such as age and sex, relative to earlier analyses of this data set. The correlation with age continues to be significant even after controlling for correlations from earlier significant summaries.},
  eprint = {27642379},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GI5CR3WI\\Bendich et al. - 2016 - Persistent Homology Analysis of Brain Artery Trees.pdf},
  number = {1},
  pmcid = {PMC5026243}
}

@article{benusPragmaticAspectsTemporal2011,
  title = {Pragmatic Aspects of Temporal Accommodation in Turn-Taking},
  author = {Beňuš, Štefan and Gravano, Agustín and Hirschberg, Julia},
  date = {2011-09},
  journaltitle = {Journal of Pragmatics},
  volume = {43},
  pages = {3001--3027},
  issn = {03782166},
  doi = {10.1016/j.pragma.2011.05.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378216611001469},
  urldate = {2020-05-28},
  abstract = {This study investigates the relationship between the variability in the temporal alignment of turn initiations and the pragmatics of interpersonal communication. The data come from spontaneous, task-oriented dialogues in Standard American English. In addition to analyzing the temporal aspects of turn-taking behavior in general, we focus on the timing of turn-initial single word grounding responses such as mmhm, okay, or yeah, and conversational fillers such as um or uh. Based on qualitative and quantitative analyses of temporal and rhythmic alignment patterns, we propose that these patterns are linked to the achievement of pragmatic goals by interlocutors. More specifically, we examine the role of timing in establishing common ground, and test the hypothesis that the degree of accommodation to temporal and metrical characteristics of an interlocutor’s speech is one aspect of turn-taking behavior that signals asymmetrical dominance relationships between interlocutors. Our results show that dominance relationships linked to floorcontrol, as well as mutual common ground, are pragmatically constructed in part through the accommodation patterns in timing of turn-initial single word utterances.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RY23RBIA\\Beňuš et al. - 2011 - Pragmatic aspects of temporal accommodation in tur.pdf},
  langid = {english},
  number = {12}
}

@article{bergesonAbsolutePitchTempo2002,
  title = {Absolute Pitch and Tempo in Mothers' Songs to Infants},
  author = {Bergeson, Tonya R. and Trehub, Sandra E.},
  date = {2002-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {13},
  pages = {72--75},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00413},
  abstract = {We examined the relative stability of pitch, tempo, and rhythm in maternal speech and singing to prelinguistic infants. Mothers were recorded speaking and singing to their infants on two occasions separated by 1 week or more. The pitch level and tempo of identical utterances were highly variable across the 1-week period, but these features were virtually unchanged in song repetitions. Rhythmic patterning was largely maintained in speech, as in song. Mothers' accurate reproduction of their sung performances can be considered a form of absolute pitch and absolute tempo.},
  eprint = {11892783},
  eprinttype = {pmid},
  keywords = {Communication,Humans,Infant,Mother-Child Relations,Mothers,Periodicity,Speech,Verbal Behavior},
  langid = {english},
  number = {1}
}

@online{bergmannVerbalVisualHow2006,
  title = {Verbal or {{Visual}}? {{How Information}} Is {{Distributed}} across {{Speech}} and {{Gesture}} in {{Spatial Dialog}}},
  shorttitle = {Verbal or {{Visual}}?},
  author = {Bergmann, K. and Kopp, Stefan},
  date = {2006},
  url = {/paper/Verbal-or-Visual-How-Information-is-Distributed-and-Bergmann-Kopp/88420722e8afff17c970b361944476249bafb402},
  urldate = {2020-12-08},
  abstract = {In spatial dialog like in direction giving humans make frequent use of speechaccompanying gestures. Some gestures  convey largely the same information as speech while others complement speech. This paper reports a study on how speakers distribute meaning across speech and gesture,  and depending on what factors. Utterance meaning and the wider dialog context were tested by statistically analyzing  a corpus of direction-giving dialogs. Problems of speech production (as indicated by discourse markers and disfluencies), the communicative goals, and the information  status were found to be influential, while feedback signals by the addressee do not have any influence.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TSNZDYX6\\88420722e8afff17c970b361944476249bafb402.html},
  langid = {english},
  organization = {{undefined}}
}

@article{bernardBeatMetaphoricGestures2015,
  title = {Beat and Metaphoric Gestures Are Differentially Associated with Regional Cerebellar and Cortical Volumes},
  author = {Bernard, Jessica A. and B Millman, Zachary and Mittal, Vijay A.},
  date = {2015-10},
  journaltitle = {Human Brain Mapping},
  shortjournal = {Hum Brain Mapp},
  volume = {36},
  pages = {4016--4030},
  issn = {1097-0193},
  doi = {10.1002/hbm.22894},
  abstract = {Gestures represent an integral aspect of interpersonal communication, and they are closely linked with language and thought. Brain regions for language processing overlap with those for gesture processing. Two types of gesticulation, beat gestures and metaphoric gestures are particularly important for understanding the taxonomy of co-speech gestures. Here, we investigated gesture production during taped interviews with respect to regional brain volume. First, we were interested in whether beat gesture production is associated with similar regions as metaphoric gesture. Second, we investigated whether cortical regions associated with metaphoric gesture processing are linked to gesture production based on correlations with brain volumes. We found that beat gestures are uniquely related to regional volume in cerebellar regions previously implicated in discrete motor timing. We suggest that these gestures may be an artifact of the timing processes of the cerebellum that are important for the timing of vocalizations. Second, our findings indicate that brain volumes in regions of the left hemisphere previously implicated in metaphoric gesture processing are positively correlated with metaphoric gesture production. Together, this novel work extends our understanding of left hemisphere regions associated with gesture to indicate their importance in gesture production, and also suggests that beat gestures may be especially unique. This provides important insight into the taxonomy of co-speech gestures, and also further insight into the general role of the cerebellum in language.},
  eprint = {26174599},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MJ2CWZXP\\Bernard et al. - 2015 - Beat and metaphoric gestures are differentially as.pdf},
  keywords = {Adolescent,Artifacts,beat,Brain Mapping,cerebellum,Cerebellum,Cerebral Cortex,Communication,Female,Functional Laterality,gesture,Gestures,Humans,language,Magnetic Resonance Imaging,Male,Metaphor,neuroimaging,Speech,superior temporal gyrus,timing,Young Adult},
  langid = {english},
  number = {10},
  pmcid = {PMC4583373}
}

@article{bernardisSpeechGestureShare2006,
  title = {Speech and Gesture Share the Same Communication System},
  author = {Bernardis, Paolo and Gentilucci, Maurizio},
  date = {2006-01},
  journaltitle = {Neuropsychologia},
  volume = {44},
  pages = {178--190},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2005.05.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0028393205001892},
  urldate = {2020-03-17},
  abstract = {Humans speak and produce symbolic gestures. Do these two forms of communication interact, and how? First, we tested whether the two communication signals influenced each other when emitted simultaneously. Participants either pronounced words, or executed symbolic gestures, or emitted the two communication signals simultaneously. Relative to the unimodal conditions, multimodal voice spectra were enhanced by gestures, whereas multimodal gesture parameters were reduced by words. In other words, gesture reinforced word, whereas word inhibited gesture. In contrast, aimless arm movements and pseudo-words had no comparable effects. Next, we tested whether observing word pronunciation during gesture execution affected verbal responses in the same way as emitting the two signals. Participants responded verbally to either spoken words, or to gestures, or to the simultaneous presentation of the two signals. We observed the same reinforcement in the voice spectra as during simultaneous emission. These results suggest that spoken word and symbolic gesture are coded as single signal by a unique communication system. This signal represents the intention to engage a closer interaction with a hypothetical interlocutor and it may have a meaning different from when word and gesture are encoded singly.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HXN2VM6I\\Bernardis and Gentilucci - 2006 - Speech and gesture share the same communication sy.pdf},
  langid = {english},
  number = {2}
}

@book{bernsteinCoordinationRegulationsMovements1967,
  title = {The {{Co}}-Ordination and {{Regulations}} of {{Movements}}},
  author = {Bernstein, N.},
  date = {1967},
  edition = {[1st English ed.] edition},
  publisher = {{Pergamon Press}},
  langid = {english}
}

@article{bertranProsodicTypologyDichotomy1999,
  title = {Prosodic {{Typology}}: {{On}} the {{Dichotomy}} between {{Stress}}-{{Timed}} and {{Syllable}}-{{Timed Languages}}},
  author = {Bertran, A. P.},
  date = {1999},
  journaltitle = {Language Design},
  volume = {2},
  pages = {103--130}
}

@article{beugherSemiautomaticAnnotationTool2018,
  title = {A Semi-Automatic Annotation Tool for Unobtrusive Gesture Analysis},
  author = {Beugher, Stijn De and Brône, Geert and Goedemé, Toon},
  date = {2018-06},
  journaltitle = {Language Resources and Evaluation},
  volume = {52},
  pages = {433--460},
  issn = {1574-020X, 1574-0218},
  doi = {10.1007/s10579-017-9404-9},
  url = {http://link.springer.com/10.1007/s10579-017-9404-9},
  urldate = {2019-06-21},
  abstract = {In a variety of research fields, including linguistics, human–computer interaction research, psychology, sociology and behavioral studies, there is a growing interest in the role of gestural behavior related to speech and other modalities. The analysis of multimodal communication requires high-quality video data and detailed annotation of the different semiotic resources under scrutiny. In the majority of cases, the annotation of hand position, hand motion, gesture type, etc. is done manually, which is a time-consuming enterprise requiring multiple annotators and substantial resources. In this paper we present a semi-automatic alternative, in which the focus lies on minimizing the manual workload while guaranteeing highly accurate annotations. First, we discuss our approach, which consists of several processing steps such as identifying the hands in images, calculating motion of the hands, segmenting the recording in gesture and non-gesture events, etc. Second, we validate our approach against existing corpora in terms of accuracy and usefulness. The proposed approach is designed to provide annotations according to the McNeill (Hand and mind: what gestures reveal about thought, University of Chicago Press, Chicago, 1992) gesture space and the output is compatible with annotation tools such as ELAN or ANVIL.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\75PKN23Z\\Beugher et al. - 2018 - A semi-automatic annotation tool for unobtrusive g.pdf},
  langid = {english},
  number = {2}
}

@article{bianchiEfficacyGlossopharyngealBreathing2004,
  title = {Efficacy of Glossopharyngeal Breathing for a Ventilator-Dependent, High-Level Tetraplegic Patient after Cervical Cord Tumor Resection and Tracheotomy.},
  author = {Bianchi, C. and Grandi, M. and Felisari, G.},
  date = {2004-03-01},
  journaltitle = {American Journal of Physical Medicine \& Rehabilitation},
  shortjournal = {Am J Phys Med Rehabil},
  volume = {83},
  pages = {216--219},
  issn = {0894-9115, 1537-7385},
  doi = {10.1097/01.phm.0000113408.96258.06},
  url = {https://europepmc.org/article/med/15043357},
  urldate = {2020-09-19},
  abstract = {Europe PMC is an archive of life sciences journal literature., Efficacy of glossopharyngeal breathing for a ventilator-dependent, high-level tetraplegic patient after cervical cord tumor resection and tracheotomy.},
  eprint = {15043357},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6NWDEQQ8\\15043357.html},
  langid = {english},
  number = {3}
}

@article{biauBeatGesturesSyntactic2018,
  title = {Beat {{Gestures}} and {{Syntactic Parsing}}: {{An ERP Study}}},
  shorttitle = {Beat {{Gestures}} and {{Syntactic Parsing}}},
  author = {Biau, Emmanuel and Fromont, Lauren A. and Soto‐Faraco, Salvador},
  date = {2018},
  journaltitle = {Language Learning},
  volume = {68},
  pages = {102--126},
  issn = {1467-9922},
  doi = {10.1111/lang.12257},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12257},
  urldate = {2020-10-13},
  abstract = {We tested the prosodic hypothesis that the temporal alignment of a speaker's beat gestures in a sentence influences syntactic parsing by driving the listener's attention. Participants chose between two possible interpretations of relative-clause (RC) ambiguous sentences, while their electroencephalogram (EEG) was recorded. We manipulated the alignment of the beat within sentences where auditory prosody was removed. Behavioral performance showed no effect of beat placement on the sentences’ interpretation, while event-related potentials (ERPs) revealed a positive shift of the signal in the windows corresponding to N100 and P200 components. Additionally, post hoc analyses of the ERPs time locked to the RC revealed a modulation of the P600 component as a function of gesture. These results suggest that beats modulate early processing of affiliate words in continuous speech and potentially have a global impact at the level of sentence-parsing components. We speculate that beats must be synergistic with auditory prosody to be fully consequential in behavior.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lang.12257},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YYACI8NQ\\Biau et al. - 2018 - Beat Gestures and Syntactic Parsing An ERP Study.pdf;C\:\\Users\\u668173\\Zotero\\storage\\YP5ZP4B8\\lang.html},
  keywords = {audiovisual speech,ERPs,gestures,P600,prosody,syntactic parsing},
  langid = {english},
  number = {S1}
}

@book{bickertonAdamTongue2009,
  title = {Adam's {{Tongue}}},
  author = {Bickerton, D.},
  date = {2009},
  publisher = {{Hill \& Wang}},
  location = {{New York}}
}

@article{bickhardLanguageInteractionSystem2007,
  title = {Language as an Interaction System},
  author = {Bickhard, Mark H.},
  date = {2007-08},
  journaltitle = {New Ideas in Psychology},
  volume = {25},
  pages = {171--187},
  issn = {0732118X},
  doi = {10.1016/j.newideapsych.2007.02.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0732118X07000244},
  urldate = {2020-02-25},
  abstract = {In this article I present a programmatic outline of a new kind of model of language, and offer some criticisms of standard approaches. The discussion begins with issues concerning representation because, so I argue, problems with standard approaches to representation are at the heart of notions of and problems with language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZHWBLIFB\\Bickhard - 2007 - Language as an interaction system.pdf},
  langid = {english},
  number = {2}
}

@article{billonRoleSensoryInformation1996,
  title = {The Role of Sensory Information in the Production of Periodic Finger-Tapping Sequences},
  author = {Billon, M. and Semjen, A. and Cole, J. and Gauthier, G.},
  date = {1996-06-01},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {110},
  pages = {117--130},
  issn = {1432-1106},
  doi = {10.1007/BF00241381},
  url = {https://doi.org/10.1007/BF00241381},
  urldate = {2019-04-02},
  abstract = {A subject lacking proprioceptive and tactile sensibility below the neck and a group of control subjects performed sequences of periodic finger taps involving a pattern of accentuation. The required intertap interval was 700 ms. In some situations, the taps were synchronized with the clicks of a metronome. Feedback conditions were manipulated by either allowing or not allowing the subjects to hear the taps and see their finger movements. We recorded the trajectory of the subjects' finger displacement in the vertical plane, and the force and moment of contact of the finger with the response key. The control subjects achieved precise timing of the finger taps by trading off downstroke onset for movement duration, e.g., they initiated shorter-duration tapping movements with a delay. This strategy did not vary depending on task demands (e.g., synchronization) or feedback conditions. The deafferented patient produced intertap intervals on average close to the required value. However, his tap timing was characterized by increased variability and severe distortion (lengthening) after the accentuated tap, regardless of feedback conditions. He did not manifest the compensatory strategy whereby, in control subjects, movement onset was adjusted to movement duration. Thus, such a strategy in controls seems to depend on intact proprioceptive and/or tactile information from the moving limb. Upon withdrawal of visual and acoustic feedback, the deafferented subject increased the force of the taps and the amplitude of tapping movements; his mean synchronization error with the metronome also increased. However, he did not lose correct phasing between the taps and the clicks of the metronome. These findings suggest that, under normal circumstances, sequential movements are timed by an internal timekeeper which paces sensory consequences relating to the occurrence of behaviorally important events (e.g., finger taps), and not the onset of the movements eliciting those events. In the synchronization task, the timekeeper may be phase locked to the periodic acoustic stimuli by direct entrainment. Feedback information may be needed, however, for keeping any synchronization error as small as possible.},
  keywords = {Internal clock,Movement timing,Sequence timing,Synchronization},
  langid = {english},
  number = {1}
}

@article{billonSimultaneityTwoEffectors1996,
  title = {Simultaneity of Two Effectors in Synchronization with a Periodic External Signal},
  author = {Billon, M. and Bard, C. and Fleury, M. and Blouin, J. and Teasdale, N.},
  date = {1996-02-01},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {15},
  pages = {25--38},
  issn = {0167-9457},
  doi = {10.1016/0167-9457(95)00037-2},
  url = {http://www.sciencedirect.com/science/article/pii/0167945795000372},
  urldate = {2019-04-02},
  abstract = {Several studies have shown that the control of simultaneous movements differ according to the execution context. For instance, when subjects raise simultaneously their index finger and heel as fast as possible after an auditory signal, the simultaneity is controlled by sending synchronously the motor commands to both effectors. On the other hand, when subjects self-pace their movements, the simultaneity is controlled by processing the delay between afferent signals from both movements at the central level (Paillard, 1948). It has been hypothesized that a mode of control similar to the self-paced condition is also used when subjects produce simultaneous and repetitive movements in synchronization with a metronome (Fraisse, 1980). We examined this hypothesis by asking subjects to move simultaneously the index finger and heel in synchronization with metronome sounds. Results showed that the events chronology (i.e., heel movement first, finger movement second and metronome sound third) was a function of the relative distance of the effectors and auditory organ from the central comparator. We deduced that the synchronization and simultaneity was evaluated by computing the time elapsed between the arrival of the sensory feedback of the movement and auditory signal. The second goal of the study was to assess whether, in such a task, each effector is synchronized separately to the metronome sound or together as an unit. A strong positive correlation was found between finger and heel synchronization errors. This supports the hypothesis that finger and heel movements are synchronized as an unit to the metronome rather than independently. In conclusion, simultaneity between effectors and synchronization between effectors and an external signal, although they share similar processes based on afferent information, are likely to be controlled separately.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TQNZ46K7\\0167945795000372.html},
  keywords = {Movement sequence,Sensorimotor process,Tapping,Temporal coordination,Timing},
  number = {1}
}

@article{bishopWhenTheyListen2015,
  title = {When They Listen and When They Watch: {{Pianists}}' Use of Nonverbal Audio and Visual Cues during Duet Performance},
  shorttitle = {When They Listen and When They Watch},
  author = {Bishop, Laura and Goebl, Werner},
  date = {2015-03},
  journaltitle = {Musicae Scientiae: The Journal of the European Society for the Cognitive Sciences of Music},
  shortjournal = {Music Sci},
  volume = {19},
  pages = {84--110},
  issn = {1029-8649},
  doi = {10.1177/1029864915570355},
  abstract = {Nonverbal auditory and visual communication helps ensemble musicians predict each other's intentions and coordinate their actions. When structural characteristics of the music make predicting co-performers' intentions difficult (e.g., following long pauses or during ritardandi), reliance on incoming auditory and visual signals may change. This study tested whether attention to visual cues during piano-piano and piano-violin duet performance increases in such situations. Pianists performed the secondo part to three duets, synchronizing with recordings of violinists or pianists playing the primo parts. Secondos' access to incoming audio and visual signals and to their own auditory feedback was manipulated. Synchronization was most successful when primo audio was available, deteriorating when primo audio was removed and only cues from primo visual signals were available. Visual cues were used effectively following long pauses in the music, however, even in the absence of primo audio. Synchronization was unaffected by the removal of secondos' own auditory feedback. Differences were observed in how successfully piano-piano and piano-violin duos synchronized, but these effects of instrument pairing were not consistent across pieces. Pianists' success at synchronizing with violinists and other pianists is likely moderated by piece characteristics and individual differences in the clarity of cueing gestures used.},
  eprint = {26279610},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8A8H7P7S\\Bishop and Goebl - 2015 - When they listen and when they watch Pianists' us.pdf},
  keywords = {action prediction,ensemble performance,interpersonal coordination,musical communication,musical expertise,musical gesture,sensorimotor synchronization},
  langid = {english},
  number = {1},
  pmcid = {PMC4526249}
}

@article{blasiHumanSoundSystems2019,
  title = {Human Sound Systems Are Shaped by Post-{{Neolithic}} Changes in Bite Configuration},
  author = {Blasi, D. E. and Moran, S. and Moisik, S. R. and Widmer, P. and Dediu, D. and Bickel, B.},
  date = {2019-03-15},
  journaltitle = {Science},
  volume = {363},
  pages = {eaav3218},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aav3218},
  url = {https://science.sciencemag.org/content/363/6432/eaav3218},
  urldate = {2019-05-04},
  abstract = {The first fricatives In 1985, the linguist Charles Hockett proposed that the use of teeth and jaws as tools in hunter-gatherer populations makes consonants produced with lower lip and upper teeth (“f” and “v” sounds) hard to produce. He thus conjectured that these sounds were a recent innovation in human language. Blasi et al. combined paleoanthropology, speech sciences, historical linguistics, and methods from evolutionary biology to provide evidence for a Neolithic global change in the sound systems of the world's languages. Spoken languages have thus been shaped by changes in the human bite configuration owing to changes in dietary and behavioral practices since the Neolithic. Science, this issue p. eaav3218 Structured Abstract INTRODUCTIONHuman speech manifests itself in spectacular diversity, ranging from ubiquitous sounds such as “m” and “a” to the rare click consonants in some languages of southern Africa. This range is generally thought to have been fixed by biological constraints since at least the emergence of Homo sapiens. At the same time, the abundance of each sound in the languages of the world is commonly taken to depend on how easy the sound is to produce, perceive, and learn. This dependency is also regarded as fixed at the species level. RATIONALEGiven this dependency, we expect that any change in the human apparatus for production, perception, or learning affects the probability—or even the range—of the sounds that languages have. Paleoanthropological evidence suggests that the production apparatus has undergone a fundamental change of just this kind since the Neolithic. Although humans generally start out with vertical and horizontal overlap in their bite configuration (overbite and overjet, respectively), masticatory exertion in the Paleolithic gave rise to an edge-to-edge bite after adolescence. Preservation of overbite and overjet began to persist long into adulthood only with the softer diets that started to become prevalent in the wake of agriculture and intensified food processing. We hypothesize that this post-Neolithic decline of edge-to-edge bite enabled the innovation and spread of a new class of speech sounds that is now present in nearly half of the world’s languages: labiodentals, produced by positioning the lower lip against the upper teeth, such as in “f” or “v.” RESULTSBiomechanical models of the speech apparatus show that labiodentals incur about 30\% less muscular effort in the overbite and overjet configuration than in the edge-to-edge bite configuration. This difference is not present in similar articulations that place the upper lip, instead of the teeth, against the lower lip (as in bilabial “m,” “w,” or “p”). Our models also show that the overbite and overjet configuration reduces the incidental tooth/lip distance in bilabial articulations to 24 to 70\% of their original values, inviting accidental production of labiodentals. The joint effect of a decrease in muscular effort and an increase in accidental production predicts a higher probability of labiodentals in the language of populations where overbite and overjet persist into adulthood. When the persistence of overbite and overjet in a population is approximated by the prevalence of agriculturally produced food, we find that societies described as hunter-gatherers indeed have, on average, only about one-fourth the number of labiodentals exhibited by food-producing societies, after controlling for spatial and phylogenetic correlation. When the persistence is approximated by the increase in food-processing technology over the history of one well-researched language family, Indo-European, we likewise observe a steady increase of the reconstructed probability of labiodental sounds, from a median estimate of about 3\% in the proto-language (6000 to 8000 years ago) to a presence of 76\% in extant languages. CONCLUSIONOur findings reveal that the transition from prehistoric foragers to contemporary societies has had an impact on the human speech apparatus, and therefore on our species’ main mode of communication and social differentiation: spoken language. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/363/6432/eaav3218/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Labiodentals depend on bite configuration.Biomechanical modeling shows that labiodental sounds like “f” are easier to produce (and to accidentally arise) under overbite and overjet (A) than under the edge-to-edge bite (B) that prevailed before the Neolithic (C). Overbite and overjet persisted only when exposed to the softer diets that became characteristic with food production (D versus E) and more recently with intensified food processing (F). Both developments led to a spread of labiodental sounds. Linguistic diversity, now and in the past, is widely regarded to be independent of biological changes that took place after the emergence of Homo sapiens. We show converging evidence from paleoanthropology, speech biomechanics, ethnography, and historical linguistics that labiodental sounds (such as “f” and “v”) were innovated after the Neolithic. Changes in diet attributable to food-processing technologies modified the human bite from an edge-to-edge configuration to one that preserves adolescent overbite and overjet into adulthood. This change favored the emergence and maintenance of labiodentals. Our findings suggest that language is shaped not only by the contingencies of its history, but also by culturally induced changes in human biology. Diet-induced changes in the human bite over recent millennia led to the spread of new speech sounds, including “f” and “v.” Diet-induced changes in the human bite over recent millennia led to the spread of new speech sounds, including “f” and “v.”},
  eprint = {30872490},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SNSELIZ9\\eaav3218.html},
  langid = {english},
  number = {6432}
}

@article{boerAcousticModelsOrangutan2015,
  title = {Acoustic Models of Orangutan Hand-Assisted Alarm Calls},
  author = {de Boer, Bart and Wich, Serge A. and Hardus, Madeleine E. and Lameira, Adriano R.},
  date = {2015-03-15},
  journaltitle = {Journal of Experimental Biology},
  volume = {218},
  pages = {907--914},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.110577},
  url = {https://jeb.biologists.org/content/218/6/907},
  urldate = {2020-04-23},
  abstract = {Skip to Next Section Orangutans produce alarm calls called kiss-squeaks, which they sometimes modify by putting a hand in front of their mouth. Through theoretical models and observational evidence, we show that using the hand when making a kiss-squeak alters the acoustics of the production in such a way that more formants per kilohertz are produced. Our theoretical models suggest that cylindrical wave propagation is created with the use of the hand and face as they act as a cylindrical extension of the lips. The use of cylindrical wave propagation in animal calls appears to be extremely rare, but is an effective way to lengthen the acoustic system; it causes the number of resonances per kilohertz to increase. This increase is associated with larger animals, and thus using the hand in kiss-squeak production may be effective in exaggerating the size of the producer. Using the hand appears to be a culturally learned behavior, and therefore orangutans may be able to associate the acoustic effect of using the hand with potentially more effective deterrence of predators.},
  eprint = {25788727},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6ZLCTNCG\\Boer et al. - 2015 - Acoustic models of orangutan hand-assisted alarm c.pdf;C\:\\Users\\u668173\\Zotero\\storage\\WVTNFPWB\\907.html},
  langid = {english},
  number = {6}
}

@article{boerAirSacsVocal2012,
  title = {Air Sacs and Vocal Fold Vibration: {{Implications}} for Evolution of Speech},
  shorttitle = {Air Sacs and Vocal Fold Vibration},
  author = {Boer, Bart De},
  date = {2012-12-19},
  journaltitle = {Theoria et Historia Scientiarum},
  volume = {9},
  pages = {13--28},
  issn = {2392-1196},
  doi = {10.12775/v10235-011-0002-5},
  url = {https://apcz.umk.pl/czasopisma/index.php/THS/article/view/v10235-011-0002-5},
  urldate = {2020-07-07},
  abstract = {Interaction between vocal fold vibration and the vocal tract was modeled for vocal tracts with air sacs, and it was investigated how the properties of the upper vocal tract influence the regularity of vocal fold vibration. It was foundthat for constant vocal tract shape, a first determinant of whether voicing was irregular was lung pressure. Although it appears that in general the larger the lung pressure, the more likely it is that vocal fold vibration becomes irregular, this is certainly not a simple relationship: there were cases of lowlung pressure with irregular voicing as well as cases of high lung pressure with regular voicing for both soft and hard-walled air sacs, and for vocal tracts without air sacs.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\C7RM4YCC\\Boer - 2012 - Air sacs and vocal fold vibration Implications fo.pdf;C\:\\Users\\u668173\\Zotero\\storage\\MCHX2GQD\\v10235-011-0002-5.html},
  issue = {0},
  langid = {american},
  number = {0}
}

@software{boersmaPraatDoingPhonetics2019,
  title = {Praat: Doing Phonetics by Computer [{{Computer}} Program]},
  author = {Boersma, P. and Weenink, D.},
  date = {2019},
  url = {http://www.praat.org/},
  version = {6.1.03}
}

@article{boeWhichWayDawn2019,
  title = {Which Way to the Dawn of Speech?: {{Reanalyzing}} Half a Century of Debates and Data in Light of Speech Science},
  shorttitle = {Which Way to the Dawn of Speech?},
  author = {Boë, Louis-Jean and Sawallis, Thomas R. and Fagot, Joël and Badin, Pierre and Barbier, Guillaume and Captier, Guillaume and Ménard, Lucie and Heim, Jean-Louis and Schwartz, Jean-Luc},
  date = {2019-12-01},
  journaltitle = {Science Advances},
  volume = {5},
  pages = {eaaw3916},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aaw3916},
  url = {https://advances.sciencemag.org/content/5/12/eaaw3916},
  urldate = {2020-01-03},
  abstract = {Recent articles on primate articulatory abilities are revolutionary regarding speech emergence, a crucial aspect of language evolution, by revealing a human-like system of proto-vowels in nonhuman primates and implicitly throughout our hominid ancestry. This article presents both a schematic history and the state of the art in primate vocalization research and its importance for speech emergence. Recent speech research advances allow more incisive comparison of phylogeny and ontogeny and also an illuminating reinterpretation of vintage primate vocalization data. This review produces three major findings. First, even among primates, laryngeal descent is not uniquely human. Second, laryngeal descent is not required to produce contrasting formant patterns in vocalizations. Third, living nonhuman primates produce vocalizations with contrasting formant patterns. Thus, evidence now overwhelmingly refutes the long-standing laryngeal descent theory, which pushes back “the dawn of speech” beyond \textasciitilde 200 ka ago to over \textasciitilde 20 Ma ago, a difference of two orders of magnitude. Fresh analysis of primate calls shows that speech dawned in monkeys some 100 times earlier than the appearance of modern humans. Fresh analysis of primate calls shows that speech dawned in monkeys some 100 times earlier than the appearance of modern humans.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\G3TPISG8\\Boë et al. - 2019 - Which way to the dawn of speech Reanalyzing half.pdf;C\:\\Users\\u668173\\Zotero\\storage\\AXFU8KRA\\eaaw3916.html},
  langid = {english},
  number = {12}
}

@article{bogelsNeuralCorrelatesTurntaking2020,
  title = {Neural Correlates of Turn-Taking in the Wild: {{Response}} Planning Starts Early in Free Interviews},
  shorttitle = {Neural Correlates of Turn-Taking in the Wild},
  author = {Bögels, Sara},
  date = {2020-10-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {203},
  pages = {104347},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104347},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027720301669},
  urldate = {2020-12-08},
  abstract = {Conversation is generally characterized by smooth transitions between turns, with only very short gaps. This entails that responders often begin planning their response before the ongoing turn is finished. However, controversy exists about whether they start planning as early as they can, to make sure they respond on time, or as late as possible, to minimize the overlap between comprehension and production planning. Two earlier EEG studies have found neural correlates of response planning (positive ERP and alpha decrease) as soon as listeners could start planning their response, already midway through the current turn. However, in these studies, the questions asked were highly controlled with respect to the position where planning could start (e.g., very early) and required short and easy responses. The present study measured participants' EEG while an experimenter interviewed them in a spontaneous interaction. Coding the questions in the interviews showed that, under these natural circumstances, listeners can, in principle, start planning a response relatively early, on average after only about one third of the question has passed. Furthermore, ERP results showed a large positivity, interpreted before as an early neural signature of response planning, starting about half a second after the start of the word that allowed listeners to start planning a response. A second neural signature of response planning, an alpha decrease, was not replicated as reliably. In conclusion, listeners appear to start planning their response early during the ongoing turn, also under natural circumstances, presumably in order to keep the gap between turns short and respond on time. These results have several important implications for turn-taking theories, which need to explain how interlocutors deal with the overlap between comprehension and production, how they manage to come in on time, and the sources that lead to variability between conversationalists in the start of planning.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JCB5TYYW\\Bögels - 2020 - Neural correlates of turn-taking in the wild Resp.pdf;C\:\\Users\\u668173\\Zotero\\storage\\7WK2BUS6\\S0010027720301669.html},
  keywords = {Conversation,EEG,Language production,Pragmatics,Turn-taking},
  langid = {english}
}

@article{bogelsNeuralSignaturesResponse2015,
  title = {Neural Signatures of Response Planning Occur Midway through an Incoming Question in Conversation},
  author = {Bögels, Sara and Magyari, Lilla and Levinson, Stephen C.},
  date = {2015-08-05},
  journaltitle = {Scientific Reports},
  volume = {5},
  pages = {12881},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep12881},
  url = {https://www.nature.com/articles/srep12881},
  urldate = {2020-12-08},
  abstract = {A striking puzzle about language use in everyday conversation is that turn-taking latencies are usually very short, whereas planning language production takes much longer. This implies overlap between language comprehension and production processes, but the nature and extent of such overlap has never been studied directly. Combining an interactive quiz paradigm with EEG measurements in an innovative way, we show that production planning processes start as soon as possible, that is, within half a second after the answer to a question can be retrieved (up to several seconds before the end of the question). Localization of ERP data shows early activation even of brain areas related to late stages of production planning (e.g., syllabification). Finally, oscillation results suggest an attention switch from comprehension to production around the same time frame. This perspective from interactive language use throws new light on the performance characteristics that language competence involves.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YN9MKP2Z\\Bögels et al. - 2015 - Neural signatures of response planning occur midwa.pdf;C\:\\Users\\u668173\\Zotero\\storage\\RMUASBCM\\srep12881.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{boggsInteractionsLocomotionVentilation2002,
  title = {Interactions between Locomotion and Ventilation in Tetrapods},
  author = {Boggs, Dona F},
  date = {2002-10-01},
  journaltitle = {Comparative Biochemistry and Physiology Part A: Molecular \& Integrative Physiology},
  shortjournal = {Comparative Biochemistry and Physiology Part A: Molecular \& Integrative Physiology},
  volume = {133},
  pages = {269--288},
  issn = {1095-6433},
  doi = {10.1016/S1095-6433(02)00160-5},
  url = {http://www.sciencedirect.com/science/article/pii/S1095643302001605},
  urldate = {2020-09-19},
  abstract = {Interactions between locomotion and ventilation have now been studied in several species of reptiles, birds and mammals, from a variety of perspectives. Among these perspectives are neural interactions of separate but linked central controllers; mechanical impacts of locomotion upon ventilatory pressures and flows; and the extent to which the latter may affect gas exchange and the energetics of exercise. A synchrony, i.e. 1:1 pattern of coordination, is observed in many running mammals once they achieve galloping speeds, as well as in flying bats, some flying birds and hopping marsupials. Other, non-1:1, patterns of coordination are seen in trotting and walking quadrupeds, as well as running bipedal humans and running and flying birds. There is evidence for an energetic advantage to coordination of locomotor and respiratory cycles for flying birds and running mammals. There is evidence for a mechanical constraint upon ventilation by locomotion for some reptiles (e.g. iguana), but not for others (e.g. varanids and crocodilians). In diving birds the impact of wing flapping or foot paddling on differential air sac pressures enhances gas exchange during the breath hold by improving diffusive and convective movement of air sac oxygen to parabronchi. This paper will review the current state of our knowledge of such influences of locomotion upon respiratory system function.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2MWWHPVB\\Boggs - 2002 - Interactions between locomotion and ventilation in.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ID6QJ2EA\\S1095643302001605.html},
  keywords = {Birds,Breathing,Diving,Entrainment,Flying,Locomotion,Locomotor respiratory coupling,Mammals,Reptiles,Running},
  langid = {english},
  number = {2}
}

@article{bohnYoungChildrenSpontaneously2019,
  title = {Young Children Spontaneously Recreate Core Properties of Language in a New Modality},
  author = {Bohn, Manuel and Kachel, Gregor and Tomasello, Michael},
  date = {2019-11-27},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1904871116},
  url = {https://www.pnas.org/content/early/2019/11/26/1904871116},
  urldate = {2019-12-03},
  abstract = {How the world’s 6,000+ natural languages have arisen is mostly unknown. Yet, new sign languages have emerged recently among deaf people brought together in a community, offering insights into the dynamics of language evolution. However, documenting the emergence of these languages has mostly consisted of studying the end product; the process by which ad hoc signs are transformed into a structured communication system has not been directly observed. Here we show how young children create new communication systems that exhibit core features of natural languages in less than 30 min. In a controlled setting, we blocked the possibility of using spoken language. In order to communicate novel messages, including abstract concepts, dyads of children spontaneously created novel gestural signs. Over usage, these signs became increasingly arbitrary and conventionalized. When confronted with the need to communicate more complex meanings, children began to grammatically structure their gestures. Together with previous work, these results suggest that children have the basic skills necessary, not only to acquire a natural language, but also to spontaneously create a new one. The speed with which children create these structured systems has profound implications for theorizing about language evolution, a process which is generally thought to span across many generations, if not millennia.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\B342IPFB\\Bohn et al. - 2019 - Young children spontaneously recreate core propert.pdf;C\:\\Users\\u668173\\Zotero\\storage\\MXCZ975A\\1904871116.html},
  keywords = {cognitive development,evolution,gesture,language},
  langid = {english}
}

@book{bolingerAspectsLanguage1968,
  title = {Aspects of {{Language}}},
  author = {Bolinger, Dwight},
  date = {1968},
  publisher = {{Harcourt, Brace, and World}},
  location = {{New York}},
  langid = {english}
}

@article{bolingerIntonationGesture1983,
  title = {Intonation and {{Gesture}}},
  author = {Bolinger, Dwight},
  date = {1983},
  journaltitle = {American Speech},
  volume = {58},
  pages = {156--174},
  publisher = {{[Duke University Press, American Dialect Society]}},
  issn = {0003-1283},
  doi = {10.2307/455326},
  eprint = {455326},
  eprinttype = {jstor},
  number = {2}
}

@book{bolingerIntonationItsParts1986,
  title = {Intonation and {{Its Parts}}: {{Melody}} in {{Spoken English}}},
  shorttitle = {Intonation and {{Its Parts}}},
  author = {Bolinger, Dwight},
  date = {1986},
  publisher = {{Stanford University Press}},
  isbn = {978-0-8047-1241-5},
  keywords = {Language Arts & Disciplines / Linguistics / Phonetics & Phonology},
  langid = {english},
  pagetotal = {442}
}

@article{bonnetMentalSimulationAction1997,
  title = {Mental Simulation of an Action Modulates the Excitability of Spinal Reflex Pathways in Man},
  author = {Bonnet, M and Decety, J and Jeannerod, M and Requin, J},
  date = {1997-03-01},
  journaltitle = {Cognitive Brain Research},
  shortjournal = {Cognitive Brain Research},
  volume = {5},
  pages = {221--228},
  issn = {0926-6410},
  doi = {10.1016/S0926-6410(96)00072-9},
  url = {https://www.sciencedirect.com/science/article/pii/S0926641096000729},
  urldate = {2021-03-03},
  abstract = {The question of whether mental simulation of an action has an effect on the spinal reflex circuits was examined in normal humans. Subjects were instructed either to exert or to mentally simulate a strong or a weak pressure on a pedal with the left or the right foot. Changes in the H- and T-reflexes activated by electrical and mechanical stimuli were measured on both legs during motor performance as well as during mental simulation of the same task. Asynchronous EMG activity of the soleus muscles was simultaneously recorded. Reflex excitability increased during performance of the pressure. It was larger when the H-reflex was triggered in the muscle involved in the task as compared to the contralateral side. Because actual performance modified the tension of the tendon and the location of the stimulus, ipsilateral changes of T-reflex amplitude could not be evaluated. Mental simulation of foot presure in this condition resulted in a large increase of spinal reflex excitability, which was only slightly weaker than the reflex facilitation associated with the actual performance. Changes in T-reflex amplitude, but not in H-reflex amplitude, depended upon the lateralization and force of the simulated pressure, being larger in the leg involved in the simulation than in the contralateral leg, and larger for a strong than for a weak simulated movement. EMG activity was found to be weakly increased during mental imagery. This increase was significantly, although slightly, modulated by the lateralization and intensity of the imagined movement. However, no correlation was found across subjects between reflex amplitude and the amplitude of EMG activity.},
  keywords = {Cognitive neuroscience,Human,Motor imagery,Spinal reflex},
  langid = {english},
  number = {3}
}

@article{bordoniFascialBreath,
  title = {The {{Fascial Breath}}},
  author = {Bordoni, Bruno and Simonelli, Marta and Morabito, Bruno},
  journaltitle = {Cureus},
  shortjournal = {Cureus},
  volume = {11},
  issn = {2168-8184},
  doi = {10.7759/cureus.5208},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6758955/},
  urldate = {2020-04-03},
  abstract = {The word diaphragm comes from the Greek (διάϕραγμα), which meant something that divides, but also expressed a concept related to emotions and intellect. Breath is part of a concept of symmorphosis, that is the maximum ability to adapt to multiple functional questions in a defined biological context. The act of breathing determines and defines our holobiont: how we react and who we are. The article reviews the fascial structure that involves and forms the diaphragm muscle with the aim of changing the vision of this complex muscle: from an anatomical and mechanistic form to a fractal and asynchronous form. Another step forward for understanding the diaphragm muscle is that it is not only covered, penetrated and made up of connective tissue, but the contractile tissue itself is a fascial tissue~with the same embryological derivation. All the diaphragm muscle is fascia.},
  eprint = {31565613},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YYLN4BER\\Bordoni et al. - The Fascial Breath.pdf},
  number = {7},
  pmcid = {PMC6758955}
}

@article{bosbachInferringAnotherExpectation2005,
  title = {Inferring Another's Expectation from Action: The Role of Peripheral Sensation},
  shorttitle = {Inferring Another's Expectation from Action},
  author = {Bosbach, S. and Cole, J. D. and Prinz, W. and Knoblich, G.},
  date = {2005-10},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat. Neurosci.},
  volume = {8},
  pages = {1295--1297},
  issn = {1097-6256},
  doi = {10.1038/nn1535},
  abstract = {It is unclear how knowledge of one's actions and one's body contribute to the understanding of others' actions. Here we show that two subjects lacking cutaneous touch and sense of movement and position show a selective deficit in interpreting another person's anticipation of weight when seeing him lifting boxes. We suggest that this ability occurs through mental simulation of action dependent on internal motor representations, which require peripheral sensation for their maintenance.},
  eprint = {16136040},
  eprinttype = {pmid},
  keywords = {Female,Forecasting,Humans,Judgment,Kinesthesis,Male,Mental Processes,Sensation,Sensory Deprivation,Weight Lifting},
  langid = {english},
  number = {10}
}

@article{bosbachInferringAnotherExpectation2005a,
  title = {Inferring Another's Expectation from Action: The Role of Peripheral Sensation},
  shorttitle = {Inferring Another's Expectation from Action},
  author = {Bosbach, Simone and Cole, Jonathan and Prinz, Wolfgang and Knoblich, Günther},
  date = {2005-10},
  journaltitle = {Nature Neuroscience},
  volume = {8},
  pages = {1295--1297},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn1535},
  url = {https://www.nature.com/articles/nn1535},
  urldate = {2020-11-14},
  abstract = {It is unclear how knowledge of one's actions and one's body contribute to the understanding of others' actions. Here we show that two subjects lacking cutaneous touch and sense of movement and position show a selective deficit in interpreting another person's anticipation of weight when seeing him lifting boxes. We suggest that this ability occurs through mental simulation of action dependent on internal motor representations, which require peripheral sensation for their maintenance.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AUWRSSTA\\Bosbach et al. - 2005 - Inferring another's expectation from action the r.pdf;C\:\\Users\\u668173\\Zotero\\storage\\N9A5WRJM\\nn1535.html},
  issue = {10},
  langid = {english},
  number = {10}
}

@article{boschTemporalAspectsTurn2005,
  title = {On Temporal Aspects of Turn Taking in Conversational Dialogues},
  author = {ten Bosch, Louis and Oostdijk, Nelleke and Boves, Lou},
  date = {2005-09-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {47},
  pages = {80--86},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2005.05.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639305001330},
  urldate = {2020-06-15},
  abstract = {In this short communication we show how shallow annotations in large speech corpora can be used to derive data about the temporal aspects of turn taking. Within the limitations of such a speech corpus, we show that the average durations of between-turn pauses made by speakers in a dyad are statistically related, and our data suggest the existence of gender effects in the temporal aspects of turn taking. Also, clear differences in turn taking behaviour between face-to-face and telephone dialogues can be detected using shallow analyses. We discuss the most important limitations imposed by the shallowness of the annotations in large corpora, and the possibility for enriching those annotations in a semi-automatic iterative manner.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7YCCZJJ9\\S0167639305001330.html},
  keywords = {Dialogues,Spontaneous speech,Temporal structure,Turn taking phenomena},
  langid = {english},
  number = {1},
  series = {In {{Honour}} of {{Louis Pols}}}
}

@article{boskerBeatGesturesInfluence2020,
  title = {Beat Gestures Influence Which Speech Sounds You Hear},
  author = {Bosker, Hans Rutger and Peeters, David},
  date = {2020},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.07.13.200543},
  url = {https://www.biorxiv.org/content/10.1101/2020.07.13.200543v1},
  urldate = {2020-07-14},
  abstract = {{$<$}p{$>$}Beat gestures - spontaneously produced biphasic movements of the hand - are among the most frequently encountered co-speech gestures in human communication. They are closely temporally aligned to the prosodic characteristics of the speech signal, typically occurring on lexically stressed syllables. Despite their prevalence across speakers of the world9s languages, how beat gestures impact spoken word recognition is unclear. Can these simple 9flicks of the hand9 influence speech perception? Across six experiments, we demonstrate that beat gestures influence the explicit and implicit perception of lexical stress (e.g., distinguishing OBject from obJECT), and in turn, can influence what vowels listeners hear. Thus, we provide converging evidence for a manual McGurk effect: even the simplest 9flicks of the hands9 influence which speech sounds we hear.{$<$}/p{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QLH7KWJT\\Bosker and Peeters - 2020 - Beat gestures influence which speech sounds you he.pdf;C\:\\Users\\u668173\\Zotero\\storage\\3DJ5HZME\\2020.07.13.html},
  langid = {english}
}

@article{boucherEffectsGrowthBreath2015,
  title = {Effects of the Growth of Breath Capacities on Mean Length of Utterances: {{How}} Maturing Production Processes Influence Indices of Language Development},
  shorttitle = {Effects of the Growth of Breath Capacities on Mean Length of Utterances},
  author = {Boucher, Victor J. and Lalonde, Brigitte},
  date = {2015-09},
  journaltitle = {Journal of Phonetics},
  volume = {52},
  pages = {58--69},
  issn = {00954470},
  doi = {10.1016/j.wocn.2015.04.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447015000315},
  urldate = {2020-10-27},
  abstract = {Measures of “mean length of utterance” (MLU) involving morpheme counts in transcripts are widely applied to speakers of all ages and are generally interpreted as an index of developing grammar. Yet no study has examined how the growth of respiratory capacities influences MLU and numbers of forms in utterances. We review longstanding problems of MLU counts and investigate the effects of growing breath capacities using speech samples and measures of vital capacity (VC) of 50 speakers aged 5 to 27 years. The results show that VC correlates strongly with MLU, which associates with rising numbers of long lexemes. This suggests that, in normal development, the growth of VC offers the possibility of producing increasingly long utterances that can influence lexical diversity. Hence, interpreting MLU and co-varying indices of lexical development requires a consideration of the effects of maturing production processes in a perspective where developing speech and language are seen to intertwine.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\P43J4IPX\\Boucher and Lalonde - 2015 - Effects of the growth of breath capacities on mean.pdf},
  langid = {english}
}

@article{brainerdFunctionalMorphologyEvolution2006,
  title = {Functional Morphology and Evolution of Aspiration Breathing in Tetrapods},
  author = {Brainerd, E. L. and Owerkowicz, T.},
  date = {2006-11},
  journaltitle = {Respiratory Physiology \& Neurobiology},
  volume = {154},
  pages = {73--88},
  issn = {15699048},
  doi = {10.1016/j.resp.2006.06.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1569904806001856},
  urldate = {2020-09-19},
  abstract = {In the evolution of aspiration breathing, the responsibility for lung ventilation gradually shifted from the hyobranchial to the axial musculoskeletal system, with axial muscles taking over exhalation first, at the base of Tetrapoda, and then inhalation as well at the base of Amniota. This shift from hyobranchial to axial breathing freed the tongue and head to adapt to more diverse feeding styles, but generated a mechanical conflict between costal ventilation and high-speed locomotion. Some “lizards” (nonserpentine squamates) have been shown to circumvent this speed-dependent axial constraint with accessory gular pumping during locomotion, and here we present a new survey of gular pumping behavior in the tuatara and 40 lizard species. We observed gular pumping behavior in 32 of the 40 lizards and in the tuatara, indicating that the ability to inflate the lungs by gular pumping is a shared-derived character for Lepidosauria. Gular pump breathing in lepidosaurs may be homologous with buccal pumping in amphibians, but non-ventilatory buccal oscillation and gular flutter have persisted throughout amniote evolution and gular pumping may have evolved independently by modification of buccal oscillation. In addition to gular pumping in some lizards, three other innovations have evolved repeatedly in the major amniote clades to circumvent the speed-dependent axial constraint: accessory inspiratory muscles (mammals, crocodylians and turtles), changing locomotor posture (mammals and birds) and respiratory-locomotor phase coupling to reduce the mechanical conflict between aspiration breathing and locomotion (mammals and birds).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9ZWM8C8M\\Brainerd and Owerkowicz - 2006 - Functional morphology and evolution of aspiration .pdf},
  langid = {english},
  number = {1-2}
}

@article{brambleRunningBreathingMammals1983,
  title = {Running and Breathing in Mammals},
  author = {Bramble, D. and Carrier, D. R.},
  date = {1983-01-21},
  journaltitle = {Science},
  volume = {219},
  pages = {251--256},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.6849136},
  url = {https://www.sciencemag.org/lookup/doi/10.1126/science.6849136},
  urldate = {2020-02-25},
  langid = {english},
  number = {4582}
}

@article{brandlerCommonVariantsLeft2013,
  title = {Common {{Variants}} in {{Left}}/{{Right Asymmetry Genes}} and {{Pathways Are Associated}} with {{Relative Hand Skill}}},
  author = {Brandler, William M. and Morris, Andrew P. and Evans, David M. and Scerri, Thomas S. and Kemp, John P. and Timpson, Nicholas J. and St Pourcain, Beate and Smith, George Davey and Ring, Susan M. and Stein, John and Monaco, Anthony P. and Talcott, Joel B. and Fisher, Simon E. and Webber, Caleb and Paracchini, Silvia},
  editor = {Geschwind, Daniel H.},
  date = {2013-09-12},
  journaltitle = {PLoS Genetics},
  volume = {9},
  pages = {e1003751},
  issn = {1553-7404},
  doi = {10.1371/journal.pgen.1003751},
  url = {https://dx.plos.org/10.1371/journal.pgen.1003751},
  urldate = {2020-06-10},
  abstract = {Humans display structural and functional asymmetries in brain organization, strikingly with respect to language and handedness. The molecular basis of these asymmetries is unknown. We report a genome-wide association study metaanalysis for a quantitative measure of relative hand skill in individuals with dyslexia [reading disability (RD)] (n = 728). The most strongly associated variant, rs7182874 (P = 8.6861029), is located in PCSK6, further supporting an association we previously reported. We also confirmed the specificity of this association in individuals with RD; the same locus was not associated with relative hand skill in a general population cohort (n = 2,666). As PCSK6 is known to regulate NODAL in the development of left/right (LR) asymmetry in mice, we developed a novel approach to GWAS pathway analysis, using geneset enrichment to test for an over-representation of highly associated variants within the orthologs of genes whose disruption in mice yields LR asymmetry phenotypes. Four out of 15 LR asymmetry phenotypes showed an overrepresentation (FDR\#5\%). We replicated three of these phenotypes; situs inversus, heterotaxia, and double outlet right ventricle, in the general population cohort (FDR\#5\%). Our findings lead us to propose that handedness is a polygenic trait controlled in part by the molecular mechanisms that establish LR body asymmetry early in development.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QWVD6T2B\\Brandler et al. - 2013 - Common Variants in LeftRight Asymmetry Genes and .pdf},
  langid = {english},
  number = {9}
}

@article{brittonPosturalElectromyographicResponses1993,
  title = {Postural Electromyographic Responses in the Arm and Leg Following Galvanic Vestibular Stimulation in Man},
  author = {Britton, T.C. and Day, B.L. and Brown, P. and Rothwell, J.C. and Thompson, P.D. and Marsden, C.D.},
  date = {1993-05},
  journaltitle = {Experimental Brain Research},
  volume = {94},
  issn = {0014-4819, 1432-1106},
  doi = {10.1007/BF00230477},
  url = {http://link.springer.com/10.1007/BF00230477},
  urldate = {2020-06-30},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FJCVZXNT\\Britton et al. - 1993 - Postural electromyographic responses in the arm an.pdf},
  langid = {english},
  number = {1}
}

@article{brookshireVisualCortexEntrains2017,
  title = {Visual Cortex Entrains to Sign Language},
  author = {Brookshire, Geoffrey and Lu, Jenny and Nusbaum, Howard C. and Goldin-Meadow, Susan and Casasanto, Daniel},
  date = {2017-05-24},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  pages = {201620350},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1620350114},
  url = {https://www.pnas.org/content/early/2017/05/23/1620350114},
  urldate = {2019-08-22},
  abstract = {Despite immense variability across languages, people can learn to understand any human language, spoken or signed. What neural mechanisms allow people to comprehend language across sensory modalities? When people listen to speech, electrophysiological oscillations in auditory cortex entrain to slow ({$<<<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>\&$}lt;{$<$}/mml:mo{$><$}/mml:math{$>$}8 Hz) fluctuations in the acoustic envelope. Entrainment to the speech envelope may reflect mechanisms specialized for auditory perception. Alternatively, flexible entrainment may be a general-purpose cortical mechanism that optimizes sensitivity to rhythmic information regardless of modality. Here, we test these proposals by examining cortical coherence to visual information in sign language. First, we develop a metric to quantify visual change over time. We find quasiperiodic fluctuations in sign language, characterized by lower frequencies than fluctuations in speech. Next, we test for entrainment of neural oscillations to visual change in sign language, using electroencephalography (EEG) in fluent speakers of American Sign Language (ASL) as they watch videos in ASL. We find significant cortical entrainment to visual oscillations in sign language {$<$}5 Hz, peaking at ∼∼{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>$}∼{$<$}/mml:mo{$><$}/mml:math{$>$}1 Hz. Coherence to sign is strongest over occipital and parietal cortex, in contrast to speech, where coherence is strongest over the auditory cortex. Nonsigners also show coherence to sign language, but entrainment at frontal sites is reduced relative to fluent signers. These results demonstrate that flexible cortical entrainment to language does not depend on neural processes that are specific to auditory speech perception. Low-frequency oscillatory entrainment may reflect a general cortical mechanism that maximizes sensitivity to informational peaks in time-varying signals.},
  eprint = {28559320},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ECNYBPH7\\Brookshire et al. - 2017 - Visual cortex entrains to sign language.pdf;C\:\\Users\\u668173\\Zotero\\storage\\WUCDMU8F\\1620350114.html},
  keywords = {cortical entrainment,EEG,oscillations,sign language},
  langid = {english}
}

@article{brookshireVisualCortexEntrains2017a,
  title = {Visual Cortex Entrains to Sign Language},
  author = {Brookshire, Geoffrey and Lu, Jenny and Nusbaum, Howard C. and Goldin-Meadow, Susan and Casasanto, Daniel},
  date = {2017-06-13},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {114},
  pages = {6352--6357},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1620350114},
  url = {https://www.pnas.org/content/114/24/6352},
  urldate = {2020-03-09},
  abstract = {Despite immense variability across languages, people can learn to understand any human language, spoken or signed. What neural mechanisms allow people to comprehend language across sensory modalities? When people listen to speech, electrophysiological oscillations in auditory cortex entrain to slow ({$<<<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>\&$}lt;{$<$}/mml:mo{$><$}/mml:math{$>$}8 Hz) fluctuations in the acoustic envelope. Entrainment to the speech envelope may reflect mechanisms specialized for auditory perception. Alternatively, flexible entrainment may be a general-purpose cortical mechanism that optimizes sensitivity to rhythmic information regardless of modality. Here, we test these proposals by examining cortical coherence to visual information in sign language. First, we develop a metric to quantify visual change over time. We find quasiperiodic fluctuations in sign language, characterized by lower frequencies than fluctuations in speech. Next, we test for entrainment of neural oscillations to visual change in sign language, using electroencephalography (EEG) in fluent speakers of American Sign Language (ASL) as they watch videos in ASL. We find significant cortical entrainment to visual oscillations in sign language {$<$}5 Hz, peaking at ∼∼{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>$}∼{$<$}/mml:mo{$><$}/mml:math{$>$}1 Hz. Coherence to sign is strongest over occipital and parietal cortex, in contrast to speech, where coherence is strongest over the auditory cortex. Nonsigners also show coherence to sign language, but entrainment at frontal sites is reduced relative to fluent signers. These results demonstrate that flexible cortical entrainment to language does not depend on neural processes that are specific to auditory speech perception. Low-frequency oscillatory entrainment may reflect a general cortical mechanism that maximizes sensitivity to informational peaks in time-varying signals.},
  eprint = {28559320},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZW55PX63\\Brookshire et al. - 2017 - Visual cortex entrains to sign language.pdf;C\:\\Users\\u668173\\Zotero\\storage\\K3USRCKB\\6352.html},
  isbn = {9781620350119},
  keywords = {cortical entrainment,EEG,oscillations,sign language},
  langid = {english},
  number = {24}
}

@article{browmanArticulatoryPhonology1986,
  title = {Towards an {{Articulatory Phonology}}},
  author = {Browman, Catherine P. and Goldstein, Louis M.},
  date = {1986},
  journaltitle = {Phonology Yearbook},
  volume = {3},
  pages = {219--252},
  publisher = {{Cambridge University Press}},
  issn = {0265-8062},
  abstract = {We propose an approach to phonological representation based on describing an utterance as an organised pattern of overlapping articulatory gestures. Because movement is inherent in our definition of gestures, these gestural 'constellations' can account for both spatial and temporal properties of speech in a relatively simple way. At the same time, taken as phonological representations, such gestural analyses offer many of the same advantages provided by recent nonlinear phonological theories, and we give examples of how gestural analyses simplify the description of such 'complex segments' as /s/--stop clusters and prenasalised stops. Thus, gestural structures can be seen as providing a principled link between phonological and physical description.},
  eprint = {4615400},
  eprinttype = {jstor}
}

@article{brownAuditoryMotorImagery2013,
  title = {Auditory and Motor Imagery Modulate Learning in Music Performance},
  author = {Brown, Rachel M. and Palmer, Caroline},
  date = {2013},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {7},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00320},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2013.00320/full},
  urldate = {2020-09-17},
  abstract = {Skilled performers such as athletes or musicians can improve their performance by imagining the actions or sensory outcomes associated with their skill. Performers vary widely in their auditory and motor imagery abilities, and these individual differences influence sensorimotor learning. It is unknown whether imagery abilities influence both memory encoding and retrieval. We examined how auditory and motor imagery abilities influence musicians’ encoding (during Learning, as they practiced novel melodies), and retrieval (during Recall of those melodies). Pianists learned melodies by listening without performing (auditory learning) or performing without sound (motor learning); following Learning, pianists performed the melodies from memory with auditory feedback (Recall). During either Learning (Experiment 1) or Recall (Experiment 2), pianists experienced either auditory interference, motor interference, or no interference. Pitch accuracy (percentage of correct pitches produced) and temporal regularity (variability of quarter-note interonset intervals) were measured at Recall. Independent tests measured auditory and motor imagery skills. Pianists’ pitch accuracy was higher following auditory learning than following motor learning and lower in motor interference conditions (Experiments 1 and 2). Both auditory and motor imagery skills improved pitch accuracy overall. Auditory imagery skills modulated pitch accuracy encoding (Experiment 1): Higher auditory imagery skill corresponded to higher pitch accuracy following auditory learning with auditory or motor interference, and following motor learning with motor or no interference. These findings suggest that auditory imagery abilities decrease vulnerability to interference and compensate for missing auditory feedback at encoding. Auditory imagery skills also influenced temporal regularity at retrieval (Experiment 2): Higher auditory imagery skill predicted greater temporal regularity during Recall in the presence of audit},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4TR4PAKS\\Brown and Palmer - 2013 - Auditory and motor imagery modulate learning in mu.pdf},
  keywords = {auditory imagery,individual differences,Motor Imagery,music performance,sensorimotor learning},
  langid = {english}
}

@article{brownEfficacyAuditoryMotor2018,
  title = {Efficacy of Auditory versus Motor Learning for Skilled and Novice Performers},
  author = {Brown, Rachel M. and Penhune, Virginia B.},
  date = {2018-11},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {J Cogn Neurosci},
  volume = {30},
  pages = {1657--1682},
  issn = {1530-8898},
  doi = {10.1162/jocn_a_01309},
  abstract = {Humans must learn a variety of sensorimotor skills, yet the relative contributions of sensory and motor information to skill acquisition remain unclear. Here we compare the behavioral and neural contributions of perceptual learning to that of motor learning, and we test whether these contributions depend on the expertise of the learner. Pianists and nonmusicians learned to perform novel melodies on a piano during fMRI scanning in four learning conditions: listening (auditory learning), performing without auditory feedback (motor learning), performing with auditory feedback (auditory-motor learning), or observing visual cues without performing or listening (cue-only learning). Visual cues were present in every learning condition and consisted of musical notation for pianists and spatial cues for nonmusicians. Melodies were performed from memory with no visual cues and with auditory feedback (recall) five times during learning. Pianists showed greater improvements in pitch and rhythm accuracy at recall during auditory learning compared with motor learning. Nonmusicians demonstrated greater rhythm improvements at recall during auditory learning compared with all other learning conditions. Pianists showed greater primary motor response at recall during auditory learning compared with motor learning, and response in this region during auditory learning correlated with pitch accuracy at recall and with auditory-premotor network response during auditory learning. Nonmusicians showed greater inferior parietal response during auditory compared with auditory-motor learning, and response in this region correlated with pitch accuracy at recall. Results suggest an advantage for perceptual learning compared with motor learning that is both general and expertise-dependent. This advantage is hypothesized to depend on feedforward motor control systems that can be used during learning to transform sensory information into motor production.},
  eprint = {30156505},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Auditory Perception,Brain,Female,Humans,Learning,Magnetic Resonance Imaging,Male,Motor Skills,Music,Photic Stimulation,Psychomotor Performance,Young Adult},
  langid = {english},
  number = {11}
}

@article{brownEvolutionSpeechreadyBrain2021,
  title = {Evolution of the Speech-Ready Brain: {{The}} Voice/Jaw Connection in the Human Motor Cortex},
  shorttitle = {Evolution of the Speech-Ready Brain},
  author = {Brown, Steven and Yuan, Ye and Belyk, Michel},
  date = {2021},
  journaltitle = {Journal of Comparative Neurology},
  volume = {529},
  pages = {1018--1028},
  issn = {1096-9861},
  doi = {10.1002/cne.24997},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.24997},
  urldate = {2021-02-16},
  abstract = {A prominent model of the origins of speech, known as the “frame/content” theory, posits that oscillatory lowering and raising of the jaw provided an evolutionary scaffold for the development of syllable structure in speech. Because such oscillations are nonvocal in most nonhuman primates, the evolution of speech required the addition of vocalization onto this scaffold in order to turn such jaw oscillations into vocalized syllables. In the present functional MRI study, we demonstrate overlapping somatotopic representations between the larynx and the jaw muscles in the human primary motor cortex. This proximity between the larynx and jaw in the brain might support the coupling between vocalization and jaw oscillations to generate syllable structure. This model suggests that humans inherited voluntary control of jaw oscillations from ancestral species, but added voluntary control of vocalization onto this via the evolution of a new brain area that came to be situated near the jaw region in the human motor cortex.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.24997},
  keywords = {evolution,fMRI,jaw,larynx,speech,vocalization},
  langid = {english},
  number = {5}
}

@article{brownEvolutionSpeechreadyBrain2021a,
  title = {Evolution of the Speech-Ready Brain: {{The}} Voice/Jaw Connection in the Human Motor Cortex},
  shorttitle = {Evolution of the Speech-Ready Brain},
  author = {Brown, Steven and Yuan, Ye and Belyk, Michel},
  date = {2021},
  journaltitle = {Journal of Comparative Neurology},
  volume = {529},
  pages = {1018--1028},
  issn = {1096-9861},
  doi = {10.1002/cne.24997},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.24997},
  urldate = {2021-02-16},
  abstract = {A prominent model of the origins of speech, known as the “frame/content” theory, posits that oscillatory lowering and raising of the jaw provided an evolutionary scaffold for the development of syllable structure in speech. Because such oscillations are nonvocal in most nonhuman primates, the evolution of speech required the addition of vocalization onto this scaffold in order to turn such jaw oscillations into vocalized syllables. In the present functional MRI study, we demonstrate overlapping somatotopic representations between the larynx and the jaw muscles in the human primary motor cortex. This proximity between the larynx and jaw in the brain might support the coupling between vocalization and jaw oscillations to generate syllable structure. This model suggests that humans inherited voluntary control of jaw oscillations from ancestral species, but added voluntary control of vocalization onto this via the evolution of a new brain area that came to be situated near the jaw region in the human motor cortex.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.24997},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\K84UKAKS\\Brown et al. - 2021 - Evolution of the speech-ready brain The voicejaw.pdf;C\:\\Users\\u668173\\Zotero\\storage\\NHM446ZE\\cne.html},
  keywords = {evolution,fMRI,jaw,larynx,speech,vocalization},
  langid = {english},
  number = {5}
}

@article{brownNeuralBasisHuman2006,
  title = {The Neural Basis of Human Dance},
  author = {Brown, Steven and Martinez, Michael J. and Parsons, Lawrence M.},
  date = {2006-08},
  journaltitle = {Cerebral Cortex (New York, N.Y.: 1991)},
  shortjournal = {Cereb Cortex},
  volume = {16},
  pages = {1157--1167},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhj057},
  abstract = {Human dance was investigated with positron emission tomography to identify its systems-level organization. Three core aspects of dance were examined: entrainment, meter and patterned movement. Amateur dancers performed small-scale, cyclically repeated tango steps on an inclined surface to the beat of tango music, without visual guidance. Entrainment of dance steps to music, compared to self-pacing of movement, was supported by anterior cerebellar vermis. Movement to a regular, metric rhythm, compared to movement to an irregular rhythm, implicated the right putamen in the voluntary control of metric motion. Spatial navigation of leg movement during dance, when controlling for muscle contraction, activated the medial superior parietal lobule, reflecting proprioceptive and somatosensory contributions to spatial cognition in dance. Finally, additional cortical, subcortical and cerebellar regions were active at the systems level. Consistent with recent work on simpler, rhythmic, motor-sensory behaviors, these data reveal the interacting network of brain areas active during spatially patterned, bipedal, rhythmic movements that are integrated in dance.},
  eprint = {16221923},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HSRH7WWW\\Brown et al. - 2006 - The neural basis of human dance.pdf},
  keywords = {Adult,Biological Clocks,Brain,Brain Mapping,Dancing,Female,Humans,Leg,Male,Motor Cortex,Movement,Music,Proprioception,Psychomotor Performance,Somatosensory Cortex},
  langid = {english},
  number = {8}
}

@article{burchardtGeneralIsochronousRhythm2019,
  title = {General Isochronous Rhythm in Echolocation Calls and Social Vocalizations of the Bat {{{\emph{Saccopteryx}}}}{\emph{ Bilineata}}},
  author = {Burchardt, Lara S. and Norton, Philipp and Behr, Oliver and Scharff, Constance and Knörnschild, Mirjam},
  date = {2019-01},
  journaltitle = {Royal Society Open Science},
  volume = {6},
  pages = {181076},
  issn = {2054-5703, 2054-5703},
  doi = {10.1098/rsos.181076},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.181076},
  urldate = {2020-10-07},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YHT8C83N\\Burchardt et al. - 2019 - General isochronous rhythm in echolocation calls a.pdf},
  langid = {english},
  number = {1}
}

@article{burkhardt-reedRelativeRolesVoice,
  title = {The {{Relative Roles}} of {{Voice}} and {{Gesture}} in {{Early Communication Development}}},
  author = {Burkhardt-Reed, Megan M},
  pages = {29},
  abstract = {Both vocalization and gesture are universal modes of communication and fundamental features of language development. Many believe that language evolved out of early gestural use; however, evidence reported here suggests vocalization precedes gesture in human communication and forms the predominant foundation for language. To our knowledge no prior research has investigated the rates of emergence of both gesture and vocalization in human infants to evaluate this question. We evaluated the rates of gesture and speech-like vocalizations (protophones) of 10 infants at 4, 7, and 11 months of age using parent-infant laboratory recordings. We found that infant protophones outnumbered gestures substantially at all three ages, ranging from {$>$}30 times more protophones than gestures at 3 months, to more than twice as many protophones as gestures at 11 months. The results suggest that vocalization is the predominant mode of communication in human infants from the beginning of life.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8V5J698U\\Burkhardt-Reed - The Relative Roles of Voice and Gesture in Early C.pdf},
  langid = {english}
}

@article{burkhardt-reedRelativeRolesVoice2020,
  title = {The {{Relative Roles}} of {{Voice}} and {{Gesture}} in {{Early Communication Development}}},
  author = {Burkhardt-Reed, Megan M. and Long, Helen L. and Bowman, Dale D. and Bene, Edina R. and Oller, D. Kimbrough},
  date = {2020-12-07},
  journaltitle = {bioRxiv},
  pages = {2020.12.07.415232},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.07.415232},
  url = {https://www.biorxiv.org/content/10.1101/2020.12.07.415232v1},
  urldate = {2020-12-16},
  abstract = {{$<$}p{$>$}Both vocalization and gesture are universal modes of communication and fundamental features of language development. Many believe that language evolved out of early gestural use; however, evidence reported here suggests vocalization precedes gesture in human communication and forms the predominant foundation for language. To our knowledge no prior research has investigated the rates of emergence of both gesture and vocalization in human infants to evaluate this question. We evaluated the rates of gesture and speech-like vocalizations (protophones) of 10 infants at 4, 7, and 11 months of age using parent-infant laboratory recordings. We found that infant protophones outnumbered gestures substantially at all three ages, ranging from \&gt;30 times more protophones than gestures at 3 months, to more than twice as many protophones as gestures at 11 months. The results suggest that vocalization is the predominant mode of communication in human infants from the beginning of life.{$<$}/p{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\D2889KZ7\\Burkhardt-Reed et al. - 2020 - The Relative Roles of Voice and Gesture in Early C.pdf},
  langid = {english}
}

@article{burnettExcitotoxicLesionsSuperior2007,
  title = {Excitotoxic Lesions of the Superior Colliculus Preferentially Impact Multisensory Neurons and Multisensory Integration},
  author = {Burnett, Luke R. and Stein, Barry E. and Perrault, Thomas J. and Wallace, Mark T.},
  date = {2007-05},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {179},
  pages = {325--338},
  issn = {0014-4819},
  doi = {10.1007/s00221-006-0789-8},
  abstract = {The superior colliculus (SC) plays an important role in integrating visual, auditory and somatosensory information, and in guiding the orientation of the eyes, ears and head. Previously we have shown that cats with unilateral SC lesions showed a preferential loss of multisensory orientation behaviors for stimuli contralateral to the lesion. Surprisingly, this behavioral loss was seen even under circumstances where the SC lesion was far from complete. To assess the physiological changes induced by these lesions, we employed single unit electrophysiological methods to record from individual neurons in both the intact and damaged SC following behavioral testing in two animals. In the damaged SC of these animals, multisensory neurons were preferentially reduced in incidence, comprising less than 25\% of the sensory-responsive population (as compared with 49\% on the control side). In those multisensory neurons that remained following the lesion, receptive fields were nearly twofold larger, and less than 25\% showed normal patterns of multisensory integration, with those that did being found in areas outside of the lesion. These results strongly suggest that the multisensory behavioral deficits seen following SC lesions are the combined result of a loss of multisensory neurons and a loss of multisensory integration in those neurons that remain.},
  eprint = {17146648},
  eprinttype = {pmid},
  keywords = {Animals,Brain Mapping,Cats,Cerebral Cortex,Electrodes; Implanted,Electrophysiology,Excitatory Amino Acid Agonists,Functional Laterality,Immunohistochemistry,N-Methylaspartate,Neuronal Plasticity,Neurons; Afferent,Photic Stimulation,Superior Colliculi},
  langid = {english},
  number = {2}
}

@article{burnettVoiceF0Responses1997,
  title = {Voice {{F0}} Responses to Pitch-Shifted Auditory Feedback: A Preliminary Study},
  shorttitle = {Voice {{F0}} Responses to Pitch-Shifted Auditory Feedback},
  author = {Burnett, Theresa A. and Senner, Jill E. and Larson, Charles R.},
  date = {1997-06-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {11},
  pages = {202--211},
  issn = {0892-1997},
  doi = {10.1016/S0892-1997(97)80079-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0892199797800793},
  urldate = {2021-03-03},
  abstract = {Auditory feedback has been suggested to be important for voice fundamental frequency (F0) control. The present study featured a new technique for testing this hypothesis by which the pitch of a subject's voice was modulated, fed back over earphones, and the resultant change in the emitted voice F0 was measured. The responses of 67 normal, healthy young adults were recorded as they attempted to ignore intermittent upward or downward shifts in pitch feedback while they sustained steady vowel sounds (/a/) or sang musical scales. Ninety-six percent of subjects increased their F0 when the feedback pitch was decreased, and 78\% of subjects decreased their F0 when the pitch feedback was increased. Latencies of responses ranged from 104 to 223 ms. Results indicate people normally rely on pitch feedback to control voice F0.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\T5KTB28D\\S0892199797800793.html},
  keywords = {Pitch feedback,Voice},
  langid = {english},
  number = {2}
}

@article{burnettVoiceF0Responses1998,
  title = {Voice {{F0}} Responses to Manipulations in Pitch Feedback},
  author = {Burnett, Theresa A. and Freedland, Marcia B. and Larson, Charles R. and Hain, Timothy C.},
  date = {1998-05-27},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {103},
  pages = {3153--3161},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.423073},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.423073},
  urldate = {2021-03-03},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VHR78P4L\\Burnett et al. - 1998 - Voice F0 responses to manipulations in pitch feedb.pdf;C\:\\Users\\u668173\\Zotero\\storage\\KAUTRUAH\\1.html},
  number = {6}
}

@incollection{butcherGestureTransitionOne2000,
  title = {Gesture and the Transition from One- to Two-Word Speech: {{When}} Hand and Mouth Come Together.},
  booktitle = {Language and {{Gesture}}},
  author = {Butcher, S. and Goldin-Meadow, S.},
  editor = {McNeill, D.},
  date = {2000},
  pages = {235--258},
  publisher = {{Cambridge University Press}},
  location = {{New York}}
}

@article{byrneGreatApeGestures2017,
  title = {Great Ape Gestures: Intentional Communication with a Rich Set of Innate Signals},
  shorttitle = {Great Ape Gestures},
  author = {Byrne, R. W. and Cartmill, E. and Genty, E. and Graham, K. E. and Hobaiter, C. and Tanner, J.},
  date = {2017},
  journaltitle = {Animal Cognition},
  shortjournal = {Anim Cogn},
  volume = {20},
  pages = {755--769},
  issn = {1435-9448},
  doi = {10.1007/s10071-017-1096-4},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5486474/},
  urldate = {2019-11-18},
  abstract = {Great apes give gestures deliberately and voluntarily, in order to influence particular target audiences, whose direction of attention they take into account when choosing which type of gesture to use. These facts make the study of ape gesture directly relevant to understanding the evolutionary precursors of human language; here we present an assessment of ape gesture from that perspective, focusing on the work of the “St Andrews Group” of researchers. Intended meanings of ape gestures are relatively few and simple. As with human words, ape gestures often have several distinct meanings, which are effectively disambiguated by behavioural context. Compared to the signalling of most other animals, great ape gestural repertoires are large. Because of this, and the relatively small number of intended meanings they achieve, ape gestures are redundant, with extensive overlaps in meaning. The great majority of gestures are innate, in the sense that the species’ biological inheritance includes the potential to develop each gestural form and use it for a specific range of purposes. Moreover, the phylogenetic origin of many gestures is relatively old, since gestures are extensively shared between different genera in the great ape family. Acquisition of an adult repertoire is a process of first exploring the innate species potential for many gestures and then gradual restriction to a final (active) repertoire that is much smaller. No evidence of syntactic structure has yet been detected.},
  eprint = {28502063},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BRYNHC2D\\Byrne et al. - 2017 - Great ape gestures intentional communication with.pdf},
  number = {4},
  pmcid = {PMC5486474}
}

@inproceedings{cafaroNoXiDatabaseMultimodal2017,
  title = {The {{NoXi}} Database: Multimodal Recordings of Mediated Novice-Expert Interactions},
  shorttitle = {The {{NoXi}} Database},
  booktitle = {Proceedings of the 19th {{ACM International Conference}} on {{Multimodal Interaction}}},
  author = {Cafaro, Angelo and Wagner, Johannes and Baur, Tobias and Dermouche, Soumia and Torres Torres, Mercedes and Pelachaud, Catherine and André, Elisabeth and Valstar, Michel},
  date = {2017-11-03},
  pages = {350--359},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3136755.3136780},
  url = {https://doi.org/10.1145/3136755.3136780},
  urldate = {2021-03-09},
  abstract = {We present a novel multi-lingual database of natural dyadic novice-expert interactions, named NoXi, featuring screen-mediated dyadic human interactions in the context of information exchange and retrieval. NoXi is designed to provide spontaneous interactions with emphasis on adaptive behaviors and unexpected situations (e.g. conversational interruptions). A rich set of audio-visual data, as well as continuous and discrete annotations are publicly available through a web interface. Descriptors include low level social signals (e.g. gestures, smiles), functional descriptors (e.g. turn-taking, dialogue acts) and interaction descriptors (e.g. engagement, interest, and fluidity).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HH9GVRIY\\Cafaro et al. - 2017 - The NoXi database multimodal recordings of mediat.pdf},
  isbn = {978-1-4503-5543-8},
  keywords = {Affective computing,multimedia databases,multimodal corpora},
  series = {{{ICMI}} '17}
}

@book{cangelosiSimulatingEvolutionLanguage2002,
  title = {Simulating the {{Evolution}} of {{Language}}},
  editor = {Cangelosi, Angelo and Parisi, Domenico},
  date = {2002},
  publisher = {{Springer-Verlag}},
  location = {{London}},
  doi = {10.1007/978-1-4471-0663-0},
  url = {https://www.springer.com/gp/book/9781852334284},
  urldate = {2020-03-16},
  abstract = {This book is the first to provide a comprehensive survey of the computational models and methodologies used for studying the evolution and origin of language and communication. Comprising contributions from the most influential figures in the field, it presents and summarises the state-of-the-art in computational approaches to language evolution, and highlights new lines of development.Essential reading for researchers and students in the fields of evolutionary and adaptive systems, language evolution modelling and linguistics, it will also be of interest to researchers working on applications of neural networks to language problems. Furthermore, due to the fact that language evolution models use multi-agent methodologies, it will also be of great interest to computer scientists working on multi-agent systems, robotics and internet agents.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BK5ELUDL\\9781852334284.html},
  isbn = {978-1-85233-428-4},
  langid = {english}
}

@article{cannonSpecifyingMotorSystem2020,
  title = {Specifying the Motor System’s Role in Beat Perception},
  author = {Cannon, Jonathan J. and Patel, Aniruddh D.},
  date = {2020-04-18},
  journaltitle = {bioRxiv},
  pages = {805838},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/805838},
  url = {https://www.biorxiv.org/content/10.1101/805838v2},
  urldate = {2020-11-02},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}A fundamental aspect of music cognition is the perception of an underlying metronome-like pulse (“the beat”) in complex rhythmic patterns. Neuroimaging shows that beat perception strongly engages the motor system \emph{even in the absence of movement}, but few efforts have been made to connect neurocomputational properties of the motor system to beat perception. Here we hypothesize three specific, interacting roles for the motor system in beat perception: predictive timing of beats by neuronal population clocks reset through the basal ganglia’s direct pathway, periodic motor facilitation by entrained hyperdirect pathway drive to basal ganglia, and balancing top-down and bottom-up influences on beat perception through striatal dopaminergic modulation. This leads to testable predictions about the motor system’s contribution to auditory cognition.{$<$}/p{$><$}h3{$>$}Highlights{$<$}/h3{$>$} {$<$}p{$>$}The Action Simulation for Auditory Prediction (ASAP) hypothesis posits that covert anticipatory timing of a musical beat is performed by the motor system.{$<$}/p{$><$}p{$>$}Beats may be anticipated by an adjustable-speed (“relative”) timer in the supplementary motor area, initiated and reset like a movement via the basal ganglia direct pathway.{$<$}/p{$><$}p{$>$}Beta oscillations (known to fluctuate with the beat) may be an anti-kinetic “hold” signal that scaffolds motor entrainment and beat timer resetting.{$<$}/p{$><$}p{$>$}Striatal dopamine may serve as an index of confidence in the beat tempo and phase, allowing the brain to balance between bottom-up and top-down influences on the beat percept.{$<$}/p{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8853ESHQ\\Cannon and Patel - 2020 - Specifying the motor system’s role in beat percept.pdf;C\:\\Users\\u668173\\Zotero\\storage\\Z8S4AE4A\\805838v2.html},
  langid = {english}
}

@inproceedings{caoRealtimeMultiperson2D2017,
  title = {Realtime {{Multi}}-Person {{2D Pose Estimation Using Part Affinity Fields}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2017-07},
  pages = {1302--1310},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.143},
  url = {http://ieeexplore.ieee.org/document/8099626/},
  urldate = {2019-12-11},
  abstract = {We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII MultiPerson benchmark, both in performance and efficiency.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\86MLSR4E\\Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf},
  isbn = {978-1-5386-0457-1},
  langid = {english}
}

@inproceedings{caoRealtimeMultiperson2D2017a,
  title = {Realtime Multi-Person {{2D}} Pose Estimation Using Part Affinity Fields},
  author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2017},
  pages = {7291--7299},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html},
  urldate = {2019-04-17},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\F9E2SARJ\\Cao et al. - 2017 - Realtime Multi-Person 2D Pose Estimation Using Par.pdf;C\:\\Users\\u668173\\Zotero\\storage\\LC5IZ8SB\\Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html}
}

@article{cardosoAssociationsPostureVoice2019,
  title = {Associations between {{Posture}}, {{Voice}}, and {{Dysphonia}}: {{A Systematic Review}}},
  shorttitle = {Associations between {{Posture}}, {{Voice}}, and {{Dysphonia}}},
  author = {Cardoso, Ricardo and Lumini-Oliveira, José and Meneses, Rute F.},
  date = {2019-01-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {33},
  pages = {124.e1-124.e12},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2017.08.030},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199717301832},
  urldate = {2020-11-06},
  abstract = {Objective The study aimed to systematize the associations between posture, voice, and dysphonia in order to support future research directions and possible clinical interventions. Study Design The study is a systematic review. Methods According to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses flowchart, a search on PubMed/Medline, SciELO, RCAAP, LILACS, Cochrane Library, PEDro, and Isi Web of Knowledge was performed from their inception through January of 2017 using the key words “posture” and (“voice” or “dysphonia”). The inclusion criteria were full-text journal articles in French, English, Portuguese, or Spanish, exploring the relationship between posture and voice or dysphonia, in adult human beings. The exclusion criteria coupled treatments for voice disorders, literature reviews and meta-analyses, case studies, opinion articles, and studies linking breathing with posture without assessing voice. Studies were analyzed using a modified version of the Newcastle–Ottawa Scale (NOS). Results Twelve papers met the inclusion criteria with high methodological quality through the NOS. The review shows that a correct posture is necessary for an efficient voice production; however, the relation between dysphonia and posture seems to be contradictory. Conclusion An effective posture allows a subject in a static posture or while moving to more easily shift the tension between muscles, allowing for a free movement of the larynx without blockages and with benefits to voice production.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QH82WMRR\\Cardoso et al. - 2019 - Associations between Posture, Voice, and Dysphonia.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XSCSNBY5\\S0892199717301832.html},
  keywords = {Dysphonia,Posture,Systematic review,Voice,Voice disorders},
  langid = {english},
  number = {1}
}

@inproceedings{carelloEcologicalAcousticsAcoustic2001,
  title = {Ecological Acoustics 1 {{Acoustic Specification}} of {{Object Properties}}},
  author = {Carello, C. and Wagman, J. B. and Turvey, M. T.},
  date = {2001},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DNTEG4F7\\Carello et al. - 2001 - Ecological acoustics 1 Acoustic Specification of O.pdf},
  keywords = {Acoustics}
}

@article{carelloMusclebasedPerceptionTheory2008,
  title = {Muscle-Based Perception: Theory, Research and Implications for Rehabilitation},
  shorttitle = {Muscle-Based Perception},
  author = {Carello, C. and Silva, P. L. and Kinsella-Shaw, J. M. and Turvey, M. T.},
  date = {2008-10},
  journaltitle = {Brazilian Journal of Physical Therapy},
  volume = {12},
  pages = {339--350},
  publisher = {{Brazilian Journal of Physical Therapy}},
  issn = {1413-3555},
  doi = {10.1590/S1413-35552008000500002},
  url = {http://www.scielo.br/scielo.php?script=sci_abstract&pid=S1413-35552008000500002&lng=en&nrm=iso&tlng=en},
  urldate = {2020-10-19},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VXRQ6N4B\\Carello et al. - 2008 - Muscle-based perception theory, research and impl.pdf;C\:\\Users\\u668173\\Zotero\\storage\\GH72WVIQ\\scielo.html;C\:\\Users\\u668173\\Zotero\\storage\\NZIBMFKC\\scielo.html},
  langid = {english},
  number = {5}
}

@article{carelloPerceptionObjectLength1998,
  title = {Perception of Object Length by Sound},
  author = {Carello, Claudia and Anderson, Krista L. and Kunkler-Peck, Andrew J.},
  date = {1998},
  journaltitle = {Psychological Science},
  volume = {9},
  pages = {211--214},
  issn = {1467-9280(Electronic),0956-7976(Print)},
  doi = {10.1111/1467-9280.00040},
  abstract = {The goal of the present research was to provide an empirical evaluation of the basic capability of size perception by sound (in particular, perception of the lengths of dropped wooden dowels) and to identify the physical properties of the objects that constrain that perception. In Exp 1, 8 Ss were told to listen to a rod dropped 5 times and move the adjustable surface out from the proximal edge of the desk to a position that could just be reached with the rod, so its length would fit between the desk and the surface. The location of the surface was recorded. The same procedure was followed in a 2nd experiment using 6 Ss. Results show the ordinal and metrical success of naive listeners was related to length but not to the simple acoustic variables (duration, amplitude, frequency) likely to be related to it. Additional analysis suggests the potential relevance of an object's inertia tensor in constraining perception of that object's length, analogous to the case that has been made for perceiving length by effortful touch. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CJAAPZYY\\Carello et al. - 1998 - Perception of object length by sound.pdf;C\:\\Users\\u668173\\Zotero\\storage\\AXRWYI7M\\1998-02495-009.html},
  keywords = {Auditory Perception,Auditory Stimulation,Stimulus Parameters},
  number = {3}
}

@article{carelloPeripheralNeuropathyObject2006,
  title = {Peripheral Neuropathy and Object Length Perception by Effortful (Dynamic) Touch: {{A}} Case Study},
  shorttitle = {Peripheral Neuropathy and Object Length Perception by Effortful (Dynamic) Touch},
  author = {Carello, Claudia and Kinsella-Shaw, Jeffrey and Amazeen, Eric L. and Turvey, M.T.},
  date = {2006-09},
  journaltitle = {Neuroscience Letters},
  volume = {405},
  pages = {159--163},
  issn = {03043940},
  doi = {10.1016/j.neulet.2006.06.047},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0304394006006367},
  urldate = {2020-10-19},
  abstract = {The spatial extents of hand-held objects can be perceived nonvisually by wielding them. This ability of effortful or dynamic touch to exploit the mass moments of an object to perceive its length was evaluated with a 40-years old right-handed woman with surgically treated Arnold-Chiari Type 1 Malformation and cervical syrinx. At the time of the experiment she presented with loss of discriminative touch in the left arm but no comparable sensory deficits in the right arm or the lower extremities. She could neither identify objects in her left hand nor tell that they were in the hand while manipulating them. She could, however, grasp an object tightly and wield it on request. In the experiment she wielded weighted rods of 45, 60, and 80 cm length about the wrist. There were two main results. First, her nonvisual perception of rod length by the insensate left arm scaled systematically with rod moment of inertia. The scaling matched that of the intact right arm and the nondominant arm of haptically unimpaired controls tested with rods of similar dimensions. Second, her right arm was superior in accuracy and reliability than her insensate left arm and was equal to or better than the dominant arm of the control group on key measures of nonvisual length perception. The first result was evaluated in respect to the notions of numb touch and differences in the neural bases of discriminative and effortful touch. The second result was discussed in terms of contralateral cortical enhancement by deafferentation.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EYJAIFCP\\Carello et al. - 2006 - Peripheral neuropathy and object length perception.pdf},
  langid = {english},
  number = {3}
}

@article{carlsonDanceYourOwn2020,
  title = {Dance to Your Own Drum: {{Identification}} of Musical Genre and Individual Dancer from Motion Capture Using Machine Learning},
  shorttitle = {Dance to Your Own Drum},
  author = {Carlson, Emily and Saari, Pasi and Burger, Birgitta and Toiviainen, Petri},
  date = {2020-03-14},
  journaltitle = {Journal of New Music Research},
  volume = {49},
  pages = {162--177},
  publisher = {{Routledge}},
  issn = {0929-8215},
  doi = {10.1080/09298215.2020.1711778},
  url = {https://doi.org/10.1080/09298215.2020.1711778},
  urldate = {2020-09-18},
  abstract = {Machine learning has been used to accurately classify musical genre using features derived from audio signals. Musical genre, as well as lower-level audio features of music, have also been shown to influence music-induced movement, however, the degree to which such movements are genre-specific has not been explored. The current paper addresses this using motion capture data from participants dancing freely to eight genres. Using a Support Vector Machine model, data were classified by genre and by individual dancer. Against expectations, individual classification was notably more accurate than genre classification. Results are discussed in terms of embodied cognition and culture.},
  annotation = {\_eprint: https://doi.org/10.1080/09298215.2020.1711778},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NU3CBR8C\\Carlson et al. - 2020 - Dance to your own drum Identification of musical .pdf;C\:\\Users\\u668173\\Zotero\\storage\\CSSSCANR\\09298215.2020.html},
  keywords = {embodied cognition,machine learning,Motion capture},
  number = {2}
}

@article{carrierActivityHypaxialMuscles1990,
  title = {Activity of the Hypaxial Muscles during Walking in the Lizard {{Iguana}} Iguana},
  author = {Carrier, D. R.},
  date = {1990-09-01},
  journaltitle = {Journal of Experimental Biology},
  volume = {152},
  pages = {453--470},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0022-0949, 1477-9145},
  url = {https://jeb.biologists.org/content/152/1/453},
  urldate = {2020-09-19},
  abstract = {Skip to Next Section The role that the hypaxial muscles play in locomotion has been largely ignored by biologists. In tetrapods, there are at least three possibilities. First, the hypaxial muscles might bend the trunk laterally to increase stride length. Second, they might stabilize the trunk against the horizontal, lateral and vertical components of the propulsive force. Alternatively, they might not be involved in locomotion. This study evaluated these three hypotheses by analyzing the activity of the hypaxial muscles of green iguanas (Iguana iguana). During walking, the rectus abdominis, obliquus externus superficialis and profundus, intercostales externi, and ventral portion of the intercostales interni on one side of the trunk acted synergistically with the lateral portion of the intercostales interni and obliquus internus on the other side of the trunk. This pattern supports the hypothesis that the hypaxial muscles act to stabilize the trunk during locomotion. Specifically, the longitudinally oriented rectus abdominis, obliquus externus profundus and ventral portion of the intercostales interni appear to stabilize the trunk against the horizontal and lateral components of the propulsive force, which tend to rotate the girdles in the horizontal plane. The obliquely oriented obliquus externus superficialis, intercostales externi, lateral portion of the intercostales interni and obliquus internus appear to stabilize the trunk against the vertical component, which induces long-axis torsion in the trunk. Thus, the demands of locomotion may provide a functional explanation for the basic organization of the hypaxial muscles of tetrapods.},
  eprint = {2146360},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NTHRFXCI\\Carrier - 1990 - Activity of the hypaxial muscles during walking in.pdf;C\:\\Users\\u668173\\Zotero\\storage\\TEDQVQGG\\453.html},
  langid = {english},
  number = {1}
}

@article{carrierEnergeticParadoxHuman1984,
  title = {The {{Energetic Paradox}} of {{Human Running}} and {{Hominid Evolution}} [and {{Comments}} and {{Reply}}]},
  author = {Carrier, D. R. and Kapoor, A. K. and Kimura, Tasuku and Nickels, Martin K. and {Satwanti} and Scott, Eugenie C. and So, Joseph K. and Trinkaus, Erik},
  date = {1984},
  journaltitle = {Current Anthropology},
  volume = {25},
  pages = {483--495},
  issn = {0011-3204},
  abstract = {The energetic cost of running is relatively high in man. In spite of this, humans are adept endurance runners, capable of running down, for example, zebra and kangaroo. Distance running is made possible for man in part by an exceptional ability to dissipate exercise heat loads. Most mammals lose heat by panting, which is coupled to breathing and locomotor cycles during running. This interdependence may limit the effectiveness of panting as a means of heat dissipation. Because sweating is not dependent on respiration, it may be more compatible with running as a thermoregulatory mechanism. Furthermore, man's lack of body hair improves thermal conductance while running, as it facilitates convection at the skin surface. While horses, for example, have been shown to possess energetically optimal speeds in each gait, the energetic cost for a man to run a given distance does not change with speed. It is hypothesized that this is because bipedality allows breathing frequency to vary relative to stride frequency. Man's constant cost of transport may enable human hunters to pursue the prey animal at speeds that force it to run inefficiently, thereby expediting its eventual fatigue. Given what is known of heat dissipation in Old World Anthropoidea, the bipedality of early hominids, and human exercise physiology, one factor important in the origin of the Hominidae may have been the occupation of a new niche as a diurnal endurance predator.},
  eprint = {2742907},
  eprinttype = {jstor},
  number = {4}
}

@article{carriereVisualDeprivationAlters2007,
  title = {Visual Deprivation Alters the Development of Cortical Multisensory Integration},
  author = {Carriere, Brian N. and Royal, David W. and Perrault, Thomas J. and Morrison, Stephen P. and Vaughan, J. William and Stein, Barry E. and Wallace, Mark T.},
  date = {2007-11},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {J Neurophysiol},
  volume = {98},
  pages = {2858--2867},
  issn = {0022-3077},
  doi = {10.1152/jn.00587.2007},
  abstract = {It has recently been demonstrated that the maturation of normal multisensory circuits in the cortex of the cat takes place over an extended period of postnatal life. Such a finding suggests that the sensory experiences received during this time may play an important role in this developmental process. To test the necessity of sensory experience for normal cortical multisensory development, cats were raised in the absence of visual experience from birth until adulthood, effectively precluding all visual and visual-nonvisual multisensory experiences. As adults, semichronic single-unit recording experiments targeting the anterior ectosylvian sulcus (AES), a well-defined multisensory cortical area in the cat, were initiated and continued at weekly intervals in anesthetized animals. Despite having very little impact on the overall sensory representations in AES, dark-rearing had a substantial impact on the integrative capabilities of multisensory AES neurons. A significant increase was seen in the proportion of multisensory neurons that were modulated by, rather than driven by, a second sensory modality. More important, perhaps, there was a dramatic shift in the percentage of these modulated neurons in which the pairing of weakly effective and spatially and temporally coincident stimuli resulted in response depressions. In normally reared animals such combinations typically give rise to robust response enhancements. These results illustrate the important role sensory experience plays in shaping the development of mature multisensory cortical circuits and suggest that dark-rearing shifts the relative balance of excitation and inhibition in these circuits.},
  eprint = {17728386},
  eprinttype = {pmid},
  keywords = {Action Potentials,Animals,Cats,Cerebral Cortex,Dose-Response Relationship; Radiation,Neurons; Afferent,Physical Stimulation,Reaction Time,Sensation,Sensory Deprivation},
  langid = {english},
  number = {5}
}

@article{casasantoGoodBadHands2010,
  title = {Good and {{Bad}} in the {{Hands}} of {{Politicians}}: {{Spontaneous Gestures}} during {{Positive}} and {{Negative Speech}}},
  shorttitle = {Good and {{Bad}} in the {{Hands}} of {{Politicians}}},
  author = {Casasanto, Daniel and Jasmin, K.},
  date = {2010-07-28},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {5},
  pages = {e11805},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0011805},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0011805},
  urldate = {2020-07-14},
  abstract = {Background According to the body-specificity hypothesis, people with different bodily characteristics should form correspondingly different mental representations, even in highly abstract conceptual domains. In a previous test of this proposal, right- and left-handers were found to associate positive ideas like intelligence, attractiveness, and honesty with their dominant side and negative ideas with their non-dominant side. The goal of the present study was to determine whether ‘body-specific’ associations of space and valence can be observed beyond the laboratory in spontaneous behavior, and whether these implicit associations have visible consequences. Methodology and Principal Findings We analyzed speech and gesture (3012 spoken clauses, 1747 gestures) from the final debates of the 2004 and 2008 US presidential elections, which involved two right-handers (Kerry, Bush) and two left-handers (Obama, McCain). Blind, independent coding of speech and gesture allowed objective hypothesis testing. Right- and left-handed candidates showed contrasting associations between gesture and speech. In both of the left-handed candidates, left-hand gestures were associated more strongly with positive-valence clauses and right-hand gestures with negative-valence clauses; the opposite pattern was found in both right-handed candidates. Conclusions Speakers associate positive messages more strongly with dominant hand gestures and negative messages with non-dominant hand gestures, revealing a hidden link between action and emotion. This pattern cannot be explained by conventions in language or culture, which associate ‘good’ with ‘right’ but not with ‘left’; rather, results support and extend the body-specificity hypothesis. Furthermore, results suggest that the hand speakers use to gesture may have unexpected (and probably unintended) communicative value, providing the listener with a subtle index of how the speaker feels about the content of the co-occurring speech.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PWS2HRMC\\Casasanto and Jasmin - 2010 - Good and Bad in the Hands of Politicians Spontane.pdf;C\:\\Users\\u668173\\Zotero\\storage\\3IECGKBD\\article.html},
  keywords = {Behavior,Culture,Emotions,Language,Nonverbal communication,Political parties,Social communication,Speech},
  langid = {english},
  number = {7}
}

@article{castielloTemporalDissociationMotor1991,
  title = {Temporal Dissociation of Motor Responses and Subjective Awareness: {{A}} Study in Normal Subjects},
  author = {Castiello, U. and Paulignan, Y. and Jeannerod, M.},
  date = {1991-12-01},
  journaltitle = {Brain},
  shortjournal = {Brain},
  volume = {114},
  pages = {2639--2655},
  issn = {0006-8950},
  doi = {10.1093/brain/114.6.2639},
  url = {https://doi.org/10.1093/brain/114.6.2639},
  urldate = {2021-02-27},
  abstract = {The aim of the present study was to examine the timing of different responses given simultaneously to a single event, the sudden displacement of a visual object occurring at the onset of the grasping movement directed at that object. The subjects were requested to correct their movement in order to reach accurately for the object and to signal the time at which they became aware of its displacement by a simple vocal utterance (Tah!). The onset of the motor adjustment was measured using kinematic landmarks obtained from the hand trajectory. Movements executed during trials where the object was displaced had an earlier peak in acceleration (107 ms) than movements executed during control trials (120 ms). By contrast, the vocal signal occurred 420 ms following object displacement, that was more than 300 ms after the onset of the motor correction. Control experiments were performed in order to verify the influence of possible interferences between the two tasks. Motor corrections performed without vocal utterance had the same timing as when the vocal signal was produced. Vocal signals produced in response to object's displacements but in the absence of reaching movements had the same latency as when movements were performed. We conclude from these results that the two responses were generated independently of each other. Assuming that the vocal responses in this experiment did signal the subject's awareness, the observed delay between motor corrections and these responses suggests that neural activity must be processed during a significant and quantifiable amount of time before it can give rise to conscious experience. This dissociation between motor responses and awareness in normal subjects is discussed in the light of clinical cases where overt behaviour and conscious experience are dissociated by cerebral lesions.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BV6I3G93\\371465.html},
  number = {6}
}

@article{cerqueiraElectromyographicStudyPectoralis1999,
  title = {Electromyographic Study of the Pectoralis Major, Serratus Anterior and External Oblique Muscles during Respiratory Activity in Humans},
  author = {Cerqueira, E. P. and Garbelline, D.},
  date = {1999},
  journaltitle = {Electromyography Clinical Neurophysioloy},
  volume = {39},
  pages = {131-137.},
  number = {3}
}

@article{chandrasekaranNaturalStatisticsAudiovisual2009,
  title = {The Natural Statistics of Audiovisual Speech},
  author = {Chandrasekaran, Chandramouli and Trubanova, Andrea and Stillittano, Sébastien and Caplier, Alice and Ghazanfar, Asif A.},
  editor = {Friston, Karl J.},
  date = {2009-07-17},
  journaltitle = {PLoS Computational Biology},
  volume = {5},
  pages = {e1000436},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000436},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1000436},
  urldate = {2019-07-11},
  abstract = {Humans, like other animals, are exposed to a continuous stream of signals, which are dynamic, multimodal, extended, and time varying in nature. This complex input space must be transduced and sampled by our sensory systems and transmitted to the brain where it can guide the selection of appropriate actions. To simplify this process, it’s been suggested that the brain exploits statistical regularities in the stimulus space. Tests of this idea have largely been confined to unimodal signals and natural scenes. One important class of multisensory signals for which a quantitative input space characterization is unavailable is human speech. We do not understand what signals our brain has to actively piece together from an audiovisual speech stream to arrive at a percept versus what is already embedded in the signal structure of the stream itself. In essence, we do not have a clear understanding of the natural statistics of audiovisual speech. In the present study, we identified the following major statistical features of audiovisual speech. First, we observed robust correlations and close temporal correspondence between the area of the mouth opening and the acoustic envelope. Second, we found the strongest correlation between the area of the mouth opening and vocal tract resonances. Third, we observed that both area of the mouth opening and the voice envelope are temporally modulated in the 2–7 Hz frequency range. Finally, we show that the timing of mouth movements relative to the onset of the voice is consistently between 100 and 300 ms. We interpret these data in the context of recent neural theories of speech which suggest that speech communication is a reciprocally coupled, multisensory event, whereby the outputs of the signaler are matched to the neural processes of the receiver.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QWRX4JBI\\Chandrasekaran et al. - 2009 - The Natural Statistics of Audiovisual Speech.pdf},
  langid = {english},
  number = {7}
}

@article{changMutualInteractionsSpeech1987,
  title = {Mutual Interactions between Speech and Finger Movements},
  author = {Chang, P. and Hammond, G. R.},
  date = {1987-06},
  journaltitle = {Journal of Motor Behavior},
  shortjournal = {J Mot Behav},
  volume = {19},
  pages = {265--274},
  issn = {0022-2895},
  doi = {10.1080/00222895.1987.10735411},
  abstract = {Speech output and finger movements were recorded as right-handed males repeated a syllable while making cyclical finger movements in three experimental conditions: (2) maintaining constant amplitude in both response systems; (b) alternating speech amplitude while attempting to maintain constant finger movement amplitude; and (c) alternating finger movement amplitude while attempting to maintain constant speech amplitude. Observations showed that output of the two response systems was coupled (one syllable was uttered with each finger movement) and entrained in amplitude (the amplitude pattern of the response that the subject attempted to keep constant followed that of the concurrently-active amplitude-modulated response). These interactions were bidirectional and were present with both left-handed and right-handed finger movements. The interactions are more extensive and subtle than mere interference wtih one response system by the other, and apparently do not depend on anatomical overlap of the responding neural systems.},
  eprint = {14988062},
  eprinttype = {pmid},
  langid = {english},
  number = {2}
}

@article{chanNetworkStructureInfluences,
  title = {Network {{Structure Influences Speech Production}}},
  author = {Chan, K. Y. and Vitevich, M. S.},
  journaltitle = {Cognitive Science},
  volume = {43},
  pages = {685--697},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01100.x},
  urldate = {2020-01-21},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9T6NVBGM\\j.1551-6709.2010.01100.html}
}

@online{chanThoracicShapeHominoids2014,
  title = {The {{Thoracic Shape}} of {{Hominoids}}},
  author = {Chan, Lap Ki},
  date = {2014-04-09},
  volume = {2014},
  pages = {e324850},
  publisher = {{Hindawi}},
  issn = {2090-2743},
  doi = {10.1155/2014/324850},
  url = {https://www.hindawi.com/journals/ari/2014/324850/},
  urldate = {2020-11-15},
  abstract = {In hominoids, the broad thorax has been assumed to contribute to their dorsal scapular position. However, the dorsoventral diameter of their cranial thorax was found in one study to be longer in hominoids. There are insufficient data on thoracic shape to explain the relationship between broad thorax and dorsal scapular position. The current study presents data on multilevel cross-sectional shape and volume distribution in a range of primates. Biplanar radiographs of intact fluid-preserved cadavers were taken to measure the cross-sectional shape of ten equally spaced levels through the sternum (called decisternal levels) and the relative volume of the nine intervening thoracic segments. It was found that the cranial thorax of hominoids is larger and broader (except in the first two decisternal levels) than that of other primates. The cranial thorax of hominoids has a longer dorsoventral diameter because the increase in dorsoventral diameter caused by the increase in the volume of the cranial thorax overcompensates for the decrease caused by the broadening of the cranial thorax. The larger and broader cranial thorax in hominoids can be explained as a locomotor adaptation for scapular gliding and as a respiratory adaptation for reducing the effects of orthograde posture on ventilation-perfusion inequality.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\86VLJL7L\\Chan - 2014 - The Thoracic Shape of Hominoids.pdf;C\:\\Users\\u668173\\Zotero\\storage\\RHX2FJ7C\\324850.html},
  langid = {english},
  organization = {{Anatomy Research International}},
  type = {Research Article}
}

@online{chanThoracicShapeHominoids2014a,
  title = {The {{Thoracic Shape}} of {{Hominoids}}},
  author = {Chan, Lap Ki},
  date = {2014-04-09},
  volume = {2014},
  pages = {e324850},
  publisher = {{Hindawi}},
  issn = {2090-2743},
  doi = {10.1155/2014/324850},
  url = {https://www.hindawi.com/journals/ari/2014/324850/},
  urldate = {2020-11-20},
  abstract = {In hominoids, the broad thorax has been assumed to contribute to their dorsal scapular position. However, the dorsoventral diameter of their cranial thorax was found in one study to be longer in hominoids. There are insufficient data on thoracic shape to explain the relationship between broad thorax and dorsal scapular position. The current study presents data on multilevel cross-sectional shape and volume distribution in a range of primates. Biplanar radiographs of intact fluid-preserved cadavers were taken to measure the cross-sectional shape of ten equally spaced levels through the sternum (called decisternal levels) and the relative volume of the nine intervening thoracic segments. It was found that the cranial thorax of hominoids is larger and broader (except in the first two decisternal levels) than that of other primates. The cranial thorax of hominoids has a longer dorsoventral diameter because the increase in dorsoventral diameter caused by the increase in the volume of the cranial thorax overcompensates for the decrease caused by the broadening of the cranial thorax. The larger and broader cranial thorax in hominoids can be explained as a locomotor adaptation for scapular gliding and as a respiratory adaptation for reducing the effects of orthograde posture on ventilation-perfusion inequality.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AKYGI3MK\\Chan - 2014 - The Thoracic Shape of Hominoids.pdf},
  langid = {english},
  organization = {{Anatomy Research International}},
  type = {Research Article}
}

@article{charltonAreMenBetter2013,
  title = {Are Men Better than Women at Acoustic Size Judgements?},
  author = {Charlton, Benjamin D. and Taylor, Anna M. and Reby, David},
  date = {2013-08-23},
  journaltitle = {Biology Letters},
  shortjournal = {Biology Letters},
  volume = {9},
  pages = {20130270},
  doi = {10.1098/rsbl.2013.0270},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsbl.2013.0270},
  urldate = {2019-10-17},
  abstract = {Formants are important phonetic elements of human speech that are also used by humans and non-human mammals to assess the body size of potential mates and rivals. As a consequence, it has been suggested that formant perception, which is crucial for speech perception, may have evolved through sexual selection. Somewhat surprisingly, though, no previous studies have examined whether sexes differ in their ability to use formants for size evaluation. Here, we investigated whether men and women differ in their ability to use the formant frequency spacing of synthetic vocal stimuli to make auditory size judgements over a wide range of fundamental frequencies (the main determinant of vocal pitch). Our results reveal that men are significantly better than women at comparing the apparent size of stimuli, and that lower pitch improves the ability of both men and women to perform these acoustic size judgements. These findings constitute the first demonstration of a sex difference in formant perception, and lend support to the idea that acoustic size normalization, a crucial prerequisite for speech perception, may have been sexually selected through male competition. We also provide the first evidence that vocalizations with relatively low pitch improve the perception of size-related formant information.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JMAPCEK8\\Charlton et al. - 2013 - Are men better than women at acoustic size judgeme.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ADS7BQVN\\rsbl.2013.html},
  number = {4}
}

@article{chenInteractionsAuditoryDorsal2006,
  title = {Interactions between Auditory and Dorsal Premotor Cortex during Synchronization to Musical Rhythms},
  author = {Chen, Joyce L. and Zatorre, Robert J. and Penhune, Virginia B.},
  date = {2006-10-01},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  volume = {32},
  pages = {1771--1781},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2006.04.207},
  abstract = {When listening to music, we often spontaneously synchronize our body movements to a rhythm's beat (e.g. tapping our feet). The goals of this study were to determine how features of a rhythm such as metric structure, can facilitate motor responses, and to elucidate the neural correlates of these auditory-motor interactions using fMRI. Five variants of an isochronous rhythm were created by increasing the contrast in sound amplitude between accented and unaccented tones, progressively highlighting the rhythm's metric structure. Subjects tapped in synchrony to these rhythms, and as metric saliency increased across the five levels, louder tones evoked longer tap durations with concomitant increases in the BOLD response at auditory and dorsal premotor cortices. The functional connectivity between these regions was also modulated by the stimulus manipulation. These results show that metric organization, as manipulated via intensity accentuation, modulates motor behavior and neural responses in auditory and dorsal premotor cortex. Auditory-motor interactions may take place at these regions with the dorsal premotor cortex interfacing sensory cues with temporally organized movement.},
  eprint = {16777432},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Adult,Analysis of Variance,Auditory Cortex,Cues,Female,Fingers,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Male,Motor Cortex,Music,Nerve Net,Oxygen,Psychomotor Performance},
  langid = {english},
  number = {4}
}

@article{chenMovingTimeBrain2008,
  title = {Moving on Time: Brain Network for Auditory-Motor Synchronization Is Modulated by Rhythm Complexity and Musical Training},
  shorttitle = {Moving on Time},
  author = {Chen, Joyce L. and Penhune, Virginia B. and Zatorre, Robert J.},
  date = {2008-02},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {J Cogn Neurosci},
  volume = {20},
  pages = {226--239},
  issn = {0898-929X},
  doi = {10.1162/jocn.2008.20018},
  abstract = {Much is known about the motor system and its role in simple movement execution. However, little is understood about the neural systems underlying auditory-motor integration in the context of musical rhythm, or the enhanced ability of musicians to execute precisely timed sequences. Using functional magnetic resonance imaging, we investigated how performance and neural activity were modulated as musicians and nonmusicians tapped in synchrony with progressively more complex and less metrically structured auditory rhythms. A functionally connected network was implicated in extracting higher-order features of a rhythm's temporal structure, with the dorsal premotor cortex mediating these auditory-motor interactions. In contrast to past studies, musicians recruited the prefrontal cortex to a greater degree than nonmusicians, whereas secondary motor regions were recruited to the same extent. We argue that the superior ability of musicians to deconstruct and organize a rhythm's temporal structure relates to the greater involvement of the prefrontal cortex mediating working memory.},
  eprint = {18275331},
  eprinttype = {pmid},
  keywords = {Adult,Auditory Perception,Brain Mapping,Cerebral Cortex,Female,Humans,Magnetic Resonance Imaging,Male,Music,Neural Pathways,Pattern Recognition; Physiological,Practice; Psychological,Psychomotor Performance,Reference Values,Time Perception},
  langid = {english},
  number = {2}
}

@artwork{chiltonBrainOutline2020,
  title = {Brain Outline},
  author = {Chilton, John},
  date = {2020-07-01},
  doi = {10.5281/zenodo.3925989},
  url = {https://zenodo.org/record/3925989#.X9SDIthKiUk},
  urldate = {2020-12-12},
  abstract = {Drawing uploaded to scidraw.io on: 30 March 2020},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LP85CRBW\\3925989.html}
}

@book{christiansenCreatingLanguageIntegrating2016,
  title = {Creating {{Language}}: {{Integrating Evolution}}, {{Acquisition}}, and {{Processing}}},
  author = {Christiansen, M. H. and Chater, Nick and Culicover, P.},
  date = {2016},
  publisher = {{MIT Press}},
  location = {{Massachusetts}}
}

@article{christiansenNoworNeverBottleneckFundamental2016,
  title = {The {{Now}}-or-{{Never}} Bottleneck: {{A}} Fundamental Constraint on Language},
  shorttitle = {The {{Now}}-or-{{Never}} Bottleneck},
  author = {Christiansen, M. H. and Chater, N.},
  date = {2016},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {39},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X1500031X},
  url = {https://www.cambridge.org/core/product/identifier/S0140525X1500031X/type/journal_article},
  urldate = {2020-03-16},
  abstract = {Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this “Now-or-Never” bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This observation has strong implications for the nature of language processing: (1) the language system must “eagerly” recode and compress linguistic input; (2) as the bottleneck recurs at each new representational level, the language system must build a multilevel linguistic representation; and (3) the language system must deploy all available information predictively to ensure that local linguistic ambiguities are dealt with “Right-First-Time”; once the original input is lost, there is no way for the language system to recover. This is “Chunk-and-Pass” processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Moreover, this perspective provides a cognitive foundation for grammaticalization and other aspects of language change. Chunk-and-Pass processing also helps explain a variety of core properties of language, including its multilevel representational structure and duality of patterning. This approach promises to create a direct relationship between psycholinguistics and linguistic theory. More generally, we outline a framework within which to integrate often disconnected inquiries into language processing, language acquisition, and language change and evolution.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\A86QWSVA\\Christiansen and Chater - 2016 - The Now-or-Never bottleneck A fundamental constra.pdf},
  langid = {english}
}

@article{chuSynchronizationSpeechGesture2014,
  title = {Synchronization of Speech and Gesture: {{Evidence}} for Interaction in Action},
  author = {Chu, M. and Hagoort, P.},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {143},
  doi = {10.1037/a0036281},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3V5X83P7\\Chu and Hagoort - Synchronization of Speech and Gesture Evidence fo.pdf},
  langid = {english},
  number = {3}
}

@article{chuSynchronizationSpeechGesture2014a,
  title = {Synchronization of Speech and Gesture: {{Evidence}} for Interaction in Action},
  shorttitle = {Synchronization of Speech and Gesture},
  author = {Chu, Mingyuan and Hagoort, Peter},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {143},
  pages = {1726--1741},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/a0036281},
  abstract = {Language and action systems are highly interlinked. A critical piece of evidence is that speech and its accompanying gestures are tightly synchronized. Five experiments were conducted to test 2 hypotheses about the synchronization of speech and gesture. According to the interactive view, there is continuous information exchange between the gesture and speech systems, during both their planning and execution phases. According to the ballistic view, information exchange occurs only during the planning phases of gesture and speech, but the 2 systems become independent once their execution has been initiated. In all experiments, participants were required to point to and/or name a light that had just lit up. Virtual reality and motion tracking technologies were used to disrupt their gesture or speech execution. Participants delayed their speech onset when their gesture was disrupted. They did so even when their gesture was disrupted at its late phase and even when they received only the kinesthetic feedback of their gesture. Also, participants prolonged their gestures when their speech was disrupted. These findings support the interactive view and add new constraints on models of speech and gesture production. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZT399LTA\\Chu and Hagoort - 2014 - Synchronization of speech and gesture Evidence fo.pdf;C\:\\Users\\u668173\\Zotero\\storage\\TF5QW5CL\\2014-09045-001.html},
  keywords = {Articulation (Speech),Gestures,Models,Virtual Reality},
  number = {4}
}

@article{claidiereCulturalEvolutionSystematically2014,
  title = {Cultural Evolution of Systematically Structured Behaviour in a Non-Human Primate},
  author = {Claidière, Nicolas and Smith, Kenny and Kirby, Simon and Fagot, Joël},
  date = {2014-12-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {281},
  pages = {20141541},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2014.1541},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2014.1541},
  urldate = {2020-03-16},
  abstract = {Culture pervades human life and is at the origin of the success of our species. A wide range of other animals have culture too, but often in a limited form that does not complexify through the gradual accumulation of innovations. We developed a new paradigm to study cultural evolution in primates in order to better evaluate our closest relatives' cultural capacities. Previous studies using transmission chain experimental paradigms, in which the behavioural output of one individual becomes the target behaviour for the next individual in the chain, show that cultural transmission can lead to the progressive emergence of systematically structured behaviours in humans. Inspired by this work, we combined a pattern reproduction task on touch screens with an iterated learning procedure to develop transmission chains of baboons (Papio papio). Using this procedure, we show that baboons can exhibit three fundamental aspects of human cultural evolution: a progressive increase in performance, the emergence of systematic structure and the presence of lineage specificity. Our results shed new light on human uniqueness: we share with our closest relatives essential capacities to produce human-like cultural evolution.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CAJK2GLH\\Claidière et al. - 2014 - Cultural evolution of systematically structured be.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZFLGUFAV\\rspb.2014.html},
  number = {1797}
}

@software{clarkeGgbeeswarmCategoricalScatter2017,
  title = {Ggbeeswarm: {{Categorical Scatter}} ({{Violin Point}}) {{Plots}}},
  shorttitle = {Ggbeeswarm},
  author = {Clarke, Erik and Sherrill-Mix, Scott},
  date = {2017-08-07},
  url = {https://CRAN.R-project.org/package=ggbeeswarm},
  urldate = {2019-09-03},
  abstract = {Provides two methods of plotting categorical scatter plots such that the arrangement of points within a category reflects the density of data at that region, and avoids over-plotting.},
  version = {0.6.0}
}

@article{clarkReferringCollaborativeProcess1986,
  title = {Referring as a Collaborative Process},
  author = {Clark, Herbert H. and Wilkes-Gibbs, Deanna},
  date = {1986-02-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {22},
  pages = {1--39},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(86)90010-7},
  url = {https://www.sciencedirect.com/science/article/pii/0010027786900107},
  urldate = {2021-03-09},
  abstract = {In conversation, speakers and addressees work together in the making of a definite reference. In the model we propose, the speaker initiates the process by presenting or inviting a noun phrase. Before going on to the next contribution, the participants, if necessary, repair, expand on, or replace the noun phrase in an iterative process until they reach a version they mutually accept. In doing so they try to minimize their joint effort. The preferred procedure is for the speaker to present a simple noun phrase and for the addressee to accept it by allowing the next contribution to begin. We describe a communication task in which pairs of people conversed about arranging complex figures and show how the proposed model accounts for many features of the references they produced. The model follows, we suggest, from the mutual responsibility that participants in conversation bear toward the understanding of each utterance. Résumé Au cours d' une conversation, les interlocuteurs travaillent ensemble pour construire une référence définie. Dans le modèle proposè, le locuteur initie le processus en présentant un syntagme nominal. Avant de passer à la contribution suivante, les participants, si cela est nécessaire, corrigent, développent ou remplacent ce syntagme nominal au cours d'un processus itératif jusqu'a ce que soit atteinte une version que tout deux acceptent. En faisant cela ils essaient de minimiser l'effort conjoint. La procedure préférée consiste pour le locuteur à présenter un syntagme nominal simple et pour l'allocuteur d'accepter ce syntagme en donnant le feu vert pour l'échange suivant. Nous décrivons une tache de communication an cours de laquelle deux personnes discutent l'agencement de figures complexes et nous montrons comment le modele proposé rend compte de nombreux traits des références produites. Le modéle découle, selon notre suggestion, de la responsabilité mutuelle que les participants prennent pour que soit compris chaque énoncé durant la conversation.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XK3XZ88M\\0010027786900107.html},
  langid = {english},
  number = {1}
}

@article{clarkValidityReliabilityNintendo2010,
  title = {Validity and Reliability of the {{Nintendo Wii Balance Board}} for Assessment of Standing Balance},
  author = {Clark, Ross A. and Bryant, Adam L. and Pua, Yonghao and McCrory, Paul and Bennell, Kim and Hunt, Michael},
  date = {2010-03},
  journaltitle = {Gait \& Posture},
  shortjournal = {Gait Posture},
  volume = {31},
  pages = {307--310},
  issn = {1879-2219},
  doi = {10.1016/j.gaitpost.2009.11.012},
  abstract = {Impaired standing balance has a detrimental effect on a person's functional ability and increases their risk of falling. There is currently no validated system which can precisely quantify center of pressure (COP), an important component of standing balance, while being inexpensive, portable and widely available. The Wii Balance Board (WBB) fits these criteria, and we examined its validity in comparison with the 'gold standard'-a laboratory-grade force platform (FP). Thirty subjects without lower limb pathology performed a combination of single and double leg standing balance tests with eyes open or closed on two separate occasions. Data from the WBB were acquired using a laptop computer. The test-retest reliability for COP path length for each of the testing devices, including a comparison of the WBB and FP data, was examined using intraclass correlation coefficients (ICC), Bland-Altman plots (BAP) and minimum detectable change (MDC). Both devices exhibited good to excellent COP path length test-retest reliability within-device (ICC=0.66-0.94) and between-device (ICC=0.77-0.89) on all testing protocols. Examination of the BAP revealed no relationship between the difference and the mean in any test, however the MDC values for the WBB did exceed those of the FP in three of the four tests. These findings suggest that the WBB is a valid tool for assessing standing balance. Given that the WBB is portable, widely available and a fraction of the cost of a FP, it could provide the average clinician with a standing balance assessment tool suitable for the clinical setting.},
  eprint = {20005112},
  eprinttype = {pmid},
  keywords = {Biomechanical Phenomena,Female,Humans,Male,Microcomputers,Postural Balance,Pressure,Reproducibility of Results,Video Games,Young Adult},
  langid = {english},
  number = {3}
}

@article{claytonTimeGestureAttention2007,
  title = {Time, {{Gesture}} and {{Attention}} in a "{{Khyāl}}" {{Performance}}},
  author = {Clayton, Martin},
  date = {2007},
  journaltitle = {Asian Music},
  volume = {38},
  pages = {71--96},
  publisher = {{University of Texas Press}},
  issn = {0044-9202},
  eprint = {4497056},
  eprinttype = {jstor},
  number = {2}
}

@article{cliffAssessingSignificanceDirected2021,
  title = {Assessing the Significance of Directed and Multivariate Measures of Linear Dependence between Time Series},
  author = {Cliff, Oliver M. and Novelli, Leonardo and Fulcher, Ben D. and Shine, James M. and Lizier, Joseph T.},
  date = {2021-02-12},
  journaltitle = {Physical Review Research},
  shortjournal = {Phys. Rev. Research},
  volume = {3},
  pages = {013145},
  issn = {2643-1564},
  doi = {10.1103/PhysRevResearch.3.013145},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.3.013145},
  urldate = {2021-02-17},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UTP3VHKC\\Cliff et al. - 2021 - Assessing the significance of directed and multiva.pdf},
  langid = {english},
  number = {1}
}

@article{cobo-lewisRelationsMotorVocal1996,
  title = {Relations of Motor and Vocal Milestones in Typically Developing Infants and Infants with {{Down}} Syndrome},
  author = {Cobo-Lewis, A. B. and Oller, D. K. and Lynch, M. P. and Levine, S. L.},
  date = {1996-03},
  journaltitle = {American journal of mental retardation: AJMR},
  shortjournal = {Am J Ment Retard},
  volume = {100},
  pages = {456--467},
  issn = {0895-8017},
  abstract = {We measured the ages at which typically developing infants and infants with Down syndrome achieved an important vocal milestone (canonical babbling), a rhythmic motor milestone (hand-banging), and six other motor milestones. The interrelations of the milestone onsets, and their relations with Down syndrome, were assessed quantitatively. Hand-banging and canonical babbling were associated and were somewhat delayed by Down syndrome. Stepping, standing, sitting, and creeping/crawling were associated and were severely delayed by Down syndrome. Rolling and reaching were also delayed by Down syndrome, though they were not strongly associated with other milestones or with one another. These results suggest that the rhythmic behaviors (canonical babbling and hand-banging) may be internally linked by common neuromuscular underpinnings and that the postural behaviors may be similarly linked.},
  eprint = {8852298},
  eprinttype = {pmid},
  keywords = {Child Development,Down Syndrome,Humans,Infant,Motor Skills,Verbal Learning},
  langid = {english},
  number = {5}
}

@article{coffmanCerebellarVermisTarget2011,
  title = {Cerebellar Vermis Is a Target of Projections from the Motor Areas in the Cerebral Cortex},
  author = {Coffman, Keith A. and Dum, Richard P. and Strick, Peter L.},
  date = {2011-09-20},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {108},
  pages = {16068--16073},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1107904108},
  url = {https://www.pnas.org/content/108/38/16068},
  urldate = {2020-07-10},
  abstract = {The cerebellum has a medial, cortico-nuclear zone consisting of the cerebellar vermis and the fastigial nucleus. Functionally, this zone is concerned with whole-body posture and locomotion. The vermis classically is thought to be included within the “spinocerebellum” and to receive somatic sensory input from ascending spinal pathways. In contrast, the lateral zone of the cerebellum is included in the “cerebro-cerebellum” because it is densely interconnected with the cerebral cortex. Here we report the surprising result that a portion of the vermis receives dense input from the cerebral cortex. We injected rabies virus into lobules VB–VIIIB of the vermis and used retrograde transneuronal transport of the virus to define disynaptic inputs to it. We found that large numbers of neurons in the primary motor cortex and in several motor areas on the medial wall of the hemisphere project to the vermis. Thus, our results challenge the classical view of the vermis and indicate that it no longer should be considered as entirely isolated from the cerebral cortex. Instead, lobules VB–VIIIB represent a site where the cortical motor areas can influence descending control systems involved in the regulation of whole-body posture and locomotion. We argue that the projection from the cerebral cortex to the vermis is part of the neural substrate for anticipatory postural adjustments and speculate that dysfunction of this system may underlie some forms of dystonia.},
  eprint = {21911381},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HT6FUC9I\\Coffman et al. - 2011 - Cerebellar vermis is a target of projections from .pdf;C\:\\Users\\u668173\\Zotero\\storage\\Y69498JG\\16068.html},
  keywords = {motor system,virus tracing},
  langid = {english},
  number = {38}
}

@article{coleEvokedPotentialsMan1991,
  title = {Evoked Potentials in a Man with a Complete Large Myelinated Fibre Sensory Neuropathy below the Neck},
  author = {Cole, J. D. and Katifi, H. A.},
  date = {1991-03-01},
  journaltitle = {Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section},
  shortjournal = {Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section},
  volume = {80},
  pages = {103--107},
  issn = {0168-5597},
  doi = {10.1016/0168-5597(91)90147-P},
  url = {http://www.sciencedirect.com/science/article/pii/016855979190147P},
  urldate = {2019-04-02},
  abstract = {Cortical somatosensory evoked potentials (SEPs) were recorded from a man with a severe neuropathy without touch and proprioception below the neck. Peripheral neurophysiological tests showed a complete large myelinated fibre sensory neuropathy. Sensory threshold to electrical stimulation of the median nerve was 15 mA (normal 2–4 mA). With a stimulus of 39 mA, duration 400 μsec, applied at the wrist a cortical SEP was recorded with a latency of 84 msec, giving a propagation velocity of 11.9 m/sec. At stimulation rates of above 3.3 Hz the SEP was absent. It is concluded that the SEPs recorded were conducted along Aδ peripheral fibres.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TA76HKNA\\016855979190147P.html},
  keywords = {Aδ SEPs,Electrical stimuli,Sensory neuropathy},
  number = {2}
}

@article{coleEvokedPotentialsSubject1995,
  title = {Evoked Potentials in a Subject with a Large-Fibre Sensory Neuropathy below the Neck},
  author = {Cole, J. D. and Merton, W. L. and Barrett, G. and Katifi, H. A. and Treede, R.-D.},
  date = {1995-02-01},
  journaltitle = {Canadian Journal of Physiology and Pharmacology},
  shortjournal = {Can. J. Physiol. Pharmacol.},
  volume = {73},
  pages = {234--245},
  issn = {0008-4212},
  doi = {10.1139/y95-034},
  url = {https://www.nrcresearchpress.com/doi/abs/10.1139/y95-034},
  urldate = {2019-04-02},
  abstract = {The results from experiments in various modalities of evoked potentials are described in a subject with a complete large peripheral neuropathy below the neck. He has no tactile or position sensitivity below that level, but has retained fatigue, pain, and temperature sensation. Percutaneous electrical stimulation of peripheral nerves led to scalp recorded evoked potentials with thresholds and propagation velocities compatible with conduction along A-δ peripheral pathways. CO2 laser evoked potentials were similar to those seen in controls, further support for intact A-δ peripheral fibres. Movement-related cortical potentials (MRCPs) were recorded associated with active and passive movement of the middle finger. The former were normal, evidence that the termination of the MRCP is not dependent on peripheral feedback. By comparing passive MRCPs between controls and the subject it was possible to establish which parts of the potentials are visual and which are proprioceptive and to gain evidence of central reo..., On décrit les résultats d'expériences effectuées en utilisant divers protocoles de potentiels évoqués (PÉ) chez un sujet souffrant d'une neuropathie périphérique des fibres de grand diamètre de toute la partie du corps située au-dessous du cou. Ce sujet n'a ni sensibilité posturale ni sensibilité tactile sous ce niveau, mais ressent encore la douleur due à la fatigue et présente une sensibilité thermique. Une stimulation électrique percutanée des nerfs périphériques a permis d'enregistrer au niveau crânien des PÉ dont les seuils et les vitesses de propagation s'accordaient avec une conduction le long des voies périphériques A-δ. Les PÉ au laser CO2 ont été similaires à ceux observés chez les témoins, confirmant ainsi l'existence de fibres périphériques A-δ intactes. On a enregistré des potentiels corticaux liés au mouvement (PCLM) lors de mouvements actifs et passifs du médius. Les premiers étaient normaux, ce qui indique que l'interruption du PCLM ne dépend pas d'une boucle de rétroaction périphérique. L...},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4D4VU529\\y95-034.html},
  number = {2}
}

@article{coleGestureFollowingDeafferentation2002,
  title = {Gesture Following Deafferentation: A Phenomenologically Informed Experimental Study},
  shorttitle = {Gesture Following Deafferentation},
  author = {Cole, Jonathan and Gallagher, Shaun and McNeill, David},
  date = {2002-03-01},
  journaltitle = {Phenomenology and the Cognitive Sciences},
  shortjournal = {Phenomenology and the Cognitive Sciences},
  volume = {1},
  pages = {49--67},
  issn = {1572-8676},
  doi = {10.1023/A:1015572619184},
  url = {https://doi.org/10.1023/A:1015572619184},
  urldate = {2019-04-16},
  abstract = {Empirical studies of gesture in a subject who has lost proprioception and the sense of touch from the neck down show that specific aspects of gesture remain normal despite abnormal motor processes for instrumental movement. The experiments suggest that gesture, as a linguistic phenomenon, is not reducible to instrumental movement. They also support and extend claims made by Merleau-Ponty concerning the relationship between language and cognition. Gesture, as language, contributes to the accomplishment of thought.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WMQEJIJ8\\Cole et al. - 2002 - Gesture following deafferentation a phenomenologi.pdf},
  keywords = {Artificial Intelligence,Empirical Study,Experimental Study,Motor Process,Specific Aspect},
  langid = {english},
  number = {1}
}

@book{coleLosingTouchMan2016,
  title = {Losing {{Touch}}: {{A}} Man without His Body},
  shorttitle = {Losing {{Touch}}},
  author = {Cole, Jonathan},
  date = {2016-09-01},
  edition = {1 edition},
  publisher = {{Oxford University Press}},
  location = {{Oxford, United Kingdom ; New York, NY, United States of America}},
  abstract = {What is like to live without touch or movement/position sense (proprioception)? The only way to understand the importance of these senses, so familiar we cannot imagine their absence, is to ask someone in that position. Ian Waterman lost them below the neck over forty years ago, though pain and temperature perception and his peripheral movement nerves were unaffected. Without proprioceptive feedback and touch the movement brain was disabled. Completely unable to move, he felt disembodied and frightened. Then, slowly, he taught himself to dress, eat and walk by thinking about each movement and with visual supervision. In Losing Touch, the narrative moves between biography and scientific research, theatre, documentary and zero gravity. He has been married three times, and built up successful careers in disability access audit, using his impairment to his advantage, and in rare turkey breeding and journalism. The neuroscience has led to data on movement without feedback, the pleasantness of touch, gesture, pain and body orientation in space. The account shows how the science was actually done but also reveals Ian's journey from passive subject to informed critic of science and scientists and that the science has given him both more understanding but also greater confidence personally. His unique response to such a rare condition has also led to a BBC documentary, theatrical portrayals and a weightless flight with NASA.As a young man he sought triumph over his impairment; now, nearly 65, he has more mature reflections on living with such an extraordinary loss, the limits it has imposed and the opportunities it has enabled. He gives his views on scientists and on others he has met including Oliver Sacks and Peter Brook. In an Afterword those from science, the arts and philosophy give an appreciation of his contribution. The book is the result of nearly 30 years close collaboration between author and subject.},
  isbn = {978-0-19-877887-5},
  langid = {english},
  pagetotal = {224}
}

@book{coleLosingTouchMan2016a,
  title = {Losing {{Touch}}: {{A}} Man without His Body},
  shorttitle = {Losing {{Touch}}},
  author = {Cole, J. D.},
  date = {2016-10-28},
  publisher = {{Oxford University Press}},
  abstract = {What is like to live without touch or movement/position sense (proprioception)? The only way to understand the importance of these senses, so familiar we cannot imagine their absence, is to ask someone in that position. Ian Waterman lost them below the neck over forty years ago, though pain and temperature perception and his peripheral movement nerves were unaffected. Without proprioceptive feedback and touch the movement brain was disabled. Completely unable to move, he felt disembodied and frightened. Then, slowly, he taught himself to dress, eat and walk by thinking about each movement and with visual supervision. In Losing Touch, the narrative moves between biography and scientific research, theatre, documentary and zero gravity. He has been married three times, and built up successful careers in disability access audit, using his impairment to his advantage, and in rare turkey breeding and journalism. The neuroscience has led to data on movement without feedback, the pleasantness of touch, gesture, pain and body orientation in space. The account shows how the science was actually done but also reveals Ian's journey from passive subject to informed critic of science and scientists and that the science has given him both more understanding but also greater confidence personally. His unique response to such a rare condition has also led to a BBC documentary, theatrical portrayals and a weightless flight with NASA. As a young man he sought triumph over his impairment; now, nearly 65, he has more mature reflections on living with such an extraordinary loss, the limits it has imposed and the opportunities it has enabled. He gives his views on scientists and on others he has met including Oliver Sacks and Peter Brook. In an Afterword those from science, the arts and philosophy give an appreciation of his contribution. The book is the result of nearly 30 years close collaboration between author and subject.},
  eprint = {V6akDAAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-19-108769-1},
  keywords = {Medical / Neurology,Philosophy / Mind & Body,Psychology / Cognitive Psychology & Cognition,Psychology / Physiological Psychology,Science / General,Science / Life Sciences / Neuroscience},
  langid = {english},
  pagetotal = {201}
}

@book{colePrideDailyMarathon1995,
  title = {Pride and a {{Daily Marathon}}},
  author = {Cole, Jonathan},
  date = {1995-07-11},
  edition = {New Ed edition},
  publisher = {{A Bradford Book}},
  location = {{Cambridge, Mass}},
  abstract = {At the age of 19, Ian Waterman was suddenly struck down at work by a rare neurological illness that deprived him of all sensation below the neck. He fell on the floor in a heap, unable to stand or control his limbs, having lost the sense of joint position and proprioception, of that "sixth sense" of his body in space, which we all take for granted. After months in a neurological ward he was judged incurable and condemned to a life of wheelchair dependence. This is the first U.S. publication of a remarkable book by his physician, Jonathan Cole. It tells the compelling story, including a clear clinical description of a rare condition, of how Waterman reclaimed a life of full mobility against all expectations, by mental effort and sheer courage. Cole describes how Waterman gradually adapted to his strange condition. As the doctors had predicted, there was no neurological recovery. He had to monitor every movement by sight to work out where his limbs were, since he had no feedback from his peripheral nerves. But with astonishing persistence Waterman developed elaborate tricks and strategies to control his movements, enabling him to cope not only with the day-to-day problems of living, but even with the challenges of work, love, and marriage.},
  isbn = {978-0-262-53136-8},
  langid = {english},
  pagetotal = {216}
}

@article{colleyInfluenceVisualCues2018,
  title = {The Influence of Visual Cues on Temporal Anticipation and Movement Synchronization with Musical Sequences},
  author = {Colley, Ian D. and Varlet, Manuel and MacRitchie, Jennifer and Keller, Peter E.},
  date = {2018-11-01},
  journaltitle = {Acta Psychologica},
  shortjournal = {Acta Psychologica},
  volume = {191},
  pages = {190--200},
  issn = {0001-6918},
  doi = {10.1016/j.actpsy.2018.09.014},
  url = {http://www.sciencedirect.com/science/article/pii/S0001691818300921},
  urldate = {2020-12-07},
  abstract = {Music presents a complex case of movement timing, as one to several dozen musicians coordinate their actions at short time-scales. This process is often directed by a conductor who provides a visual beat and guides the ensemble through tempo changes. The current experiment tested the ways in which audio-motor coordination is influenced by visual cues from a conductor's gestures, and how this influence might manifest in two ways: movements used to produce sound related to the music, and movements of the upper-body that do not directly affect sound output. We designed a virtual conductor that was derived from morphed motion capture recordings of human conductors. Two groups of participants (29 musicians and 28 nonmusicians, to test the generalizability of visuo-motor synchronization to non-experts) were shown the virtual conductor, a simple visual metronome, or a stationary circle while completing a drumming task that required synchronization with tempo-changing musical sequences. We measured asynchronies and temporal anticipation in the drumming task, as well as participants' upper-body movement using motion capture. Drumming results suggest the conductor generally improves synchronization by facilitating anticipation of tempo changes in the music. Motion capture results showed that the conductor visual cue elicited more structured head movements than the other two visual cues for nonmusicians only. Multiple regression analysis showed that the nonmusicians with less rigid movement and high anticipation had lower asynchronies. Thus, the visual cues provided by a conductor might serve to facilitate temporal anticipation and more synchronous movement in the general population, but might also cause rigid ancillary movements in some non-experts.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GMX3FNSD\\S0001691818300921.html},
  keywords = {Detrended fluctuation analysis,Sensorimotor synchronization,Temporal prediction,Visuo-motor coordination},
  langid = {english}
}

@article{colnaghiBodySwayIncreases2017,
  title = {Body {{Sway Increases After Functional Inactivation}} of the {{Cerebellar Vermis}} by {{cTBS}}},
  author = {Colnaghi, Silvia and Honeine, Jean-Louis and Sozzi, Stefania and Schieppati, Marco},
  date = {2017-02},
  journaltitle = {The Cerebellum},
  volume = {16},
  pages = {1--14},
  issn = {1473-4222, 1473-4230},
  doi = {10.1007/s12311-015-0758-5},
  url = {http://link.springer.com/10.1007/s12311-015-0758-5},
  urldate = {2020-07-10},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I2GJZDQY\\Colnaghi et al. - 2017 - Body Sway Increases After Functional Inactivation .pdf},
  langid = {english},
  number = {1}
}

@book{colombettiFeelingBodyAffective2014,
  title = {The Feeling Body: {{Affective}} Science Meets the Enactive Mind},
  author = {Colombetti, G.},
  date = {2014},
  publisher = {{MIT press}},
  location = {{Cambridge, MA}}
}

@article{colonnesiRelationPointingLanguage2010,
  title = {The Relation between Pointing and Language Development: {{A}} Meta-Analysis},
  shorttitle = {The Relation between Pointing and Language Development},
  author = {Colonnesi, Cristina and Stams, Geert Jan J. M. and Koster, Irene and Noom, Marc J.},
  date = {2010-12-01},
  journaltitle = {Developmental Review},
  shortjournal = {Developmental Review},
  volume = {30},
  pages = {352--366},
  issn = {0273-2297},
  doi = {10.1016/j.dr.2010.10.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0273229710000377},
  urldate = {2020-12-05},
  abstract = {The use of the pointing gesture is one of the first ways to communicate with the world. This gesture emerges before the second year of life and it is assumed to be the first form of intentional communication. This meta-analysis examined the concurrent and longitudinal relation between pointing and the emergence of language. Twenty-five studies were included into the meta-analysis, including 734 children. The role of several moderators was examined: pointing modality, pointing motive, age at which the pointing was measured, the assessment method of the pointing gesture and language development, the modality of language, SES, and country. The results showed both a concurrent (r=.52) and a longitudinal (r=.35) relation between pointing and language development. The relation between pointing and language development became stronger with age, and was found for pointing with a declarative and general motive, but not for pointing with an imperative motive. It is concluded that the pointing gesture is a key joint-attention behavior involved in the acquisition of language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4LKV8GFM\\S0273229710000377.html},
  keywords = {Communication,Declarative,Imperative,Joint attention,Language,Pointing},
  langid = {english},
  number = {4}
}

@article{comstockSensorimotorSynchronizationAuditory2018,
  title = {Sensorimotor {{Synchronization With Auditory}} and {{Visual Modalities}}: {{Behavioral}} and {{Neural Differences}}},
  shorttitle = {Sensorimotor {{Synchronization With Auditory}} and {{Visual Modalities}}},
  author = {Comstock, Daniel C. and Hove, Michael J. and Balasubramaniam, Ramesh},
  date = {2018-07-18},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front Comput Neurosci},
  volume = {12},
  issn = {1662-5188},
  doi = {10.3389/fncom.2018.00053},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6058047/},
  urldate = {2019-10-18},
  abstract = {It has long been known that the auditory system is better suited to guide temporally precise behaviors like sensorimotor synchronization (SMS) than the visual system. Although this phenomenon has been studied for many years, the underlying neural and computational mechanisms remain unclear. Growing consensus suggests the existence of multiple, interacting, context-dependent systems, and that reduced precision in visuo-motor timing might be due to the way experimental tasks have been conceived. Indeed, the appropriateness of the stimulus for a given task greatly influences timing performance. In this review, we examine timing differences for sensorimotor synchronization and error correction with auditory and visual sequences, to inspect the underlying neural mechanisms that contribute to modality differences in timing. The disparity between auditory and visual timing likely relates to differences in the processing specialization between auditory and visual modalities (temporal vs. spatial). We propose this difference could offer potential explanation for the differing temporal abilities between modalities. We also offer suggestions as to how these sensory systems interface with motor and timing systems.},
  eprint = {30072885},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SV43Z9DY\\Comstock et al. - 2018 - Sensorimotor Synchronization With Auditory and Vis.pdf},
  pmcid = {PMC6058047}
}

@article{comstockSensorimotorSynchronizationAuditory2018a,
  title = {Sensorimotor {{Synchronization With Auditory}} and {{Visual Modalities}}: {{Behavioral}} and {{Neural Differences}}},
  shorttitle = {Sensorimotor {{Synchronization With Auditory}} and {{Visual Modalities}}},
  author = {Comstock, Daniel C. and Hove, Michael J. and Balasubramaniam, Ramesh},
  date = {2018},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  volume = {12},
  publisher = {{Frontiers}},
  issn = {1662-5188},
  doi = {10.3389/fncom.2018.00053},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2018.00053/full},
  urldate = {2020-09-21},
  abstract = {It has long been known that the auditory system is better suited to guide temporally precise behaviors like sensorimotor synchronization (SMS) than the visual system. Although this phenomenon has been studied for many years, the underlying neural and computational mechanisms remain unclear. Growing consensus suggests the existence of multiple, interacting, context-dependent systems, and that reduced precision in visuo-motor timing might be due to the way experimental tasks have been conceived. Indeed, the appropriateness of the stimulus for a given task greatly influences timing performance. In this review, we examine timing differences for sensorimotor synchronization and error correction with auditory and visual sequences, to inspect the underlying neural mechanisms that contribute to modality differences in timing. The disparity between auditory and visual timing likely relates to differences in the processing specialization between auditory and visual modalities (temporal vs spatial). We propose this difference could offer potential explanation for the differing temporal abilities between modalities. We also offer suggestions as to how these sensory systems interface with motor and timing systems.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UDESLK59\\Comstock et al. - 2018 - Sensorimotor Synchronization With Auditory and Vis.pdf},
  keywords = {Auditory Perception,Rhythm,sensorimotor synchronization,timing,Visual Perception},
  langid = {english}
}

@article{conklinMultipleViewpointSystems1995,
  title = {Multiple Viewpoint Systems for Music Prediction},
  author = {Conklin, Darrell and Witten, Ian H.},
  date = {1995-03-01},
  journaltitle = {Journal of New Music Research},
  volume = {24},
  pages = {51--73},
  publisher = {{Routledge}},
  issn = {0929-8215},
  doi = {10.1080/09298219508570672},
  url = {https://doi.org/10.1080/09298219508570672},
  urldate = {2020-12-03},
  abstract = {This paper examines the prediction and generation of music using a multiple viewpoint system, a collection of independent views of the musical surface each of which models a specific type of musical phenomena. Both the general style and a particular piece are modeled using dual short‐term and long‐term theories, and the model is created using machine learning techniques on a corpus of musical examples. The models are used for analysis and prediction, and we conjecture that highly predictive theories will also generate original, acceptable, works. Although the quality of the works generated is hard to quantify objectively, the predictive power of models can be measured by the notion of entropy, or unpredictability. Highly predictive theories will produce low‐entropy estimates of a musical language. The methods developed are applied to the Bach chorale melodies. Multiple‐viewpoint systems are learned from a sample of 95 chorales, estimates of entropy are produced, and a predictive theory is used to generate new, unseen pieces.},
  annotation = {\_eprint: https://doi.org/10.1080/09298219508570672},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7PJXEWGH\\Conklin and Witten - 1995 - Multiple viewpoint systems for music prediction.pdf;C\:\\Users\\u668173\\Zotero\\storage\\BX4CW5PL\\09298219508570672.html;C\:\\Users\\u668173\\Zotero\\storage\\HYB5LCEG\\09298219508570672.html},
  number = {1}
}

@article{connaghanImpactDeicticGesture2017,
  title = {The Impact of Deictic Gesture on Vowel Acoustics in Childhood Apraxia of Speech},
  author = {Connaghan, K. P. and Rusiewicz, H. L.},
  date = {2017-05-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {141},
  pages = {3840--3840},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.4988550},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.4988550},
  urldate = {2020-09-27},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YSMJM5U8\\1.html},
  number = {5}
}

@article{connaghanImpactDeicticGesture2017a,
  title = {The Impact of Deictic Gesture on Vowel Acoustics in Childhood Apraxia of Speech},
  author = {Connaghan, Kathryn and Rusiewicz, Heather},
  date = {2017-05-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {141},
  pages = {3840--3840},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.4988550},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.4988550},
  urldate = {2020-12-05},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RTW5SQX7\\1.html},
  number = {5}
}

@article{connaghanRespiratoryKinematicsVocalization2004,
  title = {Respiratory {{Kinematics During Vocalization}} and {{Nonspeech Respiration}} in {{Children From}} 9 to 48 {{Months}}},
  author = {Connaghan, K. P. and Moore, C. A. and Higashakawa, M.},
  date = {2004-02},
  journaltitle = {Journal of speech, language, and hearing research : JSLHR},
  shortjournal = {J Speech Lang Hear Res},
  volume = {47},
  pages = {70--84},
  issn = {1092-4388},
  doi = {10.1044/1092-4388(2004/007)},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3984457/},
  urldate = {2020-09-27},
  abstract = {The development of respiratory drive for vocalization was studied by observing chest wall kinematics longitudinally in 4 typically developing children from the age of 9 to 48 months. Measurements of the relative contribution of rib cage and abdominal movement during vocalization (i.e., babbling and true words) and rest breathing were obtained every 3 months using respiratory plethysmography (Respitrace™). Extending earlier findings in 15-month-olds, 2 methods of analysis of rib cage and abdominal movement were used: (a) a dynamic index of the strength of coupling between the rib cage and abdomen, and (b) a classification scheme describing the moment-by-moment changes in each of the 2 components (C. A. ). The developmental course of relative chest wall kinematics differed between vocalization and rest breathing. The coupling of rib cage and abdomen during vocalization weakened significantly with development, whereas it remained consistently strong for rest breathing throughout the observed period. The developmental changes in frequency of occurrence of relative moment-by-moment changes varied across movement type. The results support previous findings that speech breathing is distinct from rest breathing based on the relative contributions of the rib cage and abdomen. Longitudinal changes are likely responsive to anatomic development, including changes to rib cage shape and compliance.},
  eprint = {15072529},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3BATLMMW\\Connaghan et al. - 2004 - Respiratory Kinematics During Vocalization and Non.pdf},
  number = {1},
  pmcid = {PMC3984457}
}

@online{ConversationalGesturesAutism,
  title = {Conversational Gestures in Autism Spectrum Disorders: {{Asynchrony}} but Not Decreased Frequency - de {{Marchena}} - 2010 - {{Autism Research}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/aur.159?casa_token=mR8s8HhLJ-wAAAAA:YDn6WZNz0NJB1KygloMRwkT5_5E2x9v3qvGSQSVfOOo4vfS7ED1CzE7a3HvHtetb5H8_vf8vRyZppYbY},
  urldate = {2019-08-31},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5J538ALY\\aur.html}
}

@article{cookEmbodiedCommunicationSpeakers2009,
  title = {Embodied Communication: Speakers' Gestures Affect Listeners' Actions},
  shorttitle = {Embodied Communication},
  author = {Cook, Susan Wagner and Tanenhaus, Michael K.},
  date = {2009-10},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {113},
  pages = {98--104},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2009.06.006},
  abstract = {We explored how speakers and listeners use hand gestures as a source of perceptual-motor information during naturalistic communication. After solving the Tower of Hanoi task either with real objects or on a computer, speakers explained the task to listeners. Speakers' hand gestures, but not their speech, reflected properties of the particular objects and the actions that they had previously used to solve the task. Speakers who solved the problem with real objects used more grasping handshapes and produced more curved trajectories during the explanation. Listeners who observed explanations from speakers who had previously solved the problem with real objects subsequently treated computer objects more like real objects; their mouse trajectories revealed that they lifted the objects in conjunction with moving them sideways, and this behavior was related to the particular gestures that were observed. These findings demonstrate that hand gestures are a reliable source of perceptual-motor information during human communication.},
  eprint = {19682672},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9AQ99SZN\\Cook and Tanenhaus - 2009 - Embodied communication speakers' gestures affect .pdf},
  keywords = {Acoustic Stimulation,Adult,Cognition,Gestures,Humans,Problem Solving,Psychomotor Performance,Speech,Speech Perception},
  langid = {english},
  number = {1},
  pmcid = {PMC2763957}
}

@article{cookPerceptionLargeScaleTonal1987,
  title = {The {{Perception}} of {{Large}}-{{Scale Tonal Closure}}},
  author = {Cook, Nicholas},
  date = {1987-12-01},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  volume = {5},
  pages = {197--205},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.2307/40285392},
  url = {/mp/article/5/2/197/93875/The-Perception-of-Large-Scale-Tonal-Closure},
  urldate = {2020-12-03},
  abstract = {Music of the tonal period generally begins and ends in the same key, although passing through other keys in the course of a movement. Theorists of music generally ascribe great significance to such large-scale tonal closure. In order to test the effect of such closure upon aesthetic response, listeners were required to evaluate a number of compositions in two versions, one of which was in each case tonally closed while the other was not. The results indicate that the direct influence of tonal closure on listeners' responses is relatively weak and is restricted to fairly short time spansmuch shorter than the duration of most tonal compositions. Although large-scale tonal structure may not in itself be perceptible, it plays an important role as a means of compositional organization, and it is argued that the theory of tonal music is more usefully regarded as a means of understanding such organization than as a means of making empirically verifiable predictions regarding the effects of music upon listeners.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YQ3HMZ7N\\The-Perception-of-Large-Scale-Tonal-Closure.html},
  langid = {english},
  number = {2}
}

@article{cooperMultimodalSignalsEnhancement2004,
  title = {Multimodal Signals: Enhancement and Constraint of Song Motor Patterns by Visual Display},
  shorttitle = {Multimodal Signals},
  author = {Cooper, Brenton G. and Goller, Franz},
  date = {2004-01-23},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {303},
  pages = {544--546},
  issn = {1095-9203},
  doi = {10.1126/science.1091099},
  abstract = {Many birds perform visual signals during their learned songs, but little is known about the interrelationship between visual and vocal displays. We show here that male brown-headed cowbirds (Molothrus ater) synchronize the most elaborate wing movements of their display with atypically long silent periods in their song, potentially avoiding adverse biomechanical effects on sound production. Furthermore, expiratory effort for song is significantly reduced when cowbirds perform their wing display. These results show a close integration between vocal and visual displays and suggest that constraints and synergistic interactions between the motor patterns of multimodal signals influence the evolution of birdsong.},
  eprint = {14739462},
  eprinttype = {pmid},
  keywords = {Abdominal Muscles,Air Sacs,Animals,Electromyography,Male,Motor Activity,Movement,Posture,Pressure,Pulmonary Ventilation,Respiration,Respiratory Muscles,Songbirds,Video Recording,Vocalization; Animal,Wings; Animal},
  langid = {english},
  number = {5657}
}

@article{cooperriderForegroundGestureBackground2019,
  title = {Foreground Gesture, Background Gesture},
  author = {Cooperrider, K.},
  date = {2019},
  journaltitle = {Gesture},
  volume = {16},
  pages = {176--202},
  doi = {10.1075/gest.16.2.02coo},
  url = {https://benjamins.com/catalog/gest.16.2.02coo},
  urldate = {2020-01-26},
  abstract = {Do speakers intend their gestures to communicate? Central as this question is to the study of gesture, researchers cannot seem to agree on the answer. According to one common framing, gestures are an “unwitting” window into the mind (McNeill, 1992); but, according to another common framing, they are designed along with speech to form “composite utterances” (Enfield, 2009). These two framings correspond to two cultures within gesture studies~– the first cognitive and the second interactive in orientation~– and they appear to make incompatible claims. In this article I attempt to bridge the cultures by developing a distinction between foreground gestures and background gestures. Foreground gestures are designed in their particulars to communicate a critical part of the speaker’s message; background gestures are not designed in this way. These are two fundamentally different kinds of gesture, not two different ways of framing the same monolithic behavior. Foreground gestures can often be identified by one or more of the following hallmarks: they are produced along with demonstratives; they are produced in the absence of speech; they are co-organized with speaker gaze; and they are produced with conspicuous effort. The distinction between foreground and background gestures helps dissolve the apparent tension between the two cultures: interactional researchers have focused on foreground gestures and elevated them to the status of a prototype, whereas cognitive researchers have done the same with background gestures. The distinction also generates a number of testable predictions about gesture production and understanding, and it opens up new lines of inquiry into gesture across child development and across cultures.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Q6GLFS9Y\\gest.16.2.html},
  langid = {english},
  number = {2}
}

@book{corballisHandMouthOrigins2002,
  title = {From Hand to Mouth: {{The}} Origins of Language},
  author = {Corballis, M. C.},
  date = {2002},
  publisher = {{Princeton University Press}},
  location = {{Princeton, NJ.}}
}

@book{corballisTruthLanguageWhat2017,
  title = {The {{Truth}} about {{Language}}: {{What It Is}} and {{Where It Came From}}},
  shorttitle = {The {{Truth}} about {{Language}}},
  author = {Corballis, M. C.},
  date = {2017-03-29},
  publisher = {{University of Chicago Press}},
  abstract = {Evolutionary science has long viewed language as, basically, a fortunate accident—a crossing of wires that happened to be extraordinarily useful, setting humans apart from other animals and onto a trajectory that would see their brains (and the products of those brains) become increasingly complex.  But as Michael C. Corballis shows in The Truth about Language, it’s time to reconsider those assumptions. Language, he argues, is not the product of some “big bang” 60,000 years ago, but rather the result of a typically slow process of evolution with roots in elements of grammatical language found much farther back in our evolutionary history. Language, Corballis explains, evolved as a way to share thoughts—and, crucially for human development, to connect our own “mental time travel,” our imagining of events and people that are not right in front of us, to that of other people. We share that ability with other animals, but it was the development of language that made it powerful: it led to our ability to imagine other perspectives, to imagine ourselves in the minds of others, a development that, by easing social interaction, proved to be an extraordinary evolutionary advantage.   Even as his thesis challenges such giants as Chomsky and Stephen Jay Gould, Corballis writes accessibly and wittily, filling his account with unforgettable anecdotes and fascinating historical examples. The result is a book that’s perfect both for deep engagement and as brilliant fodder for that lightest of all forms of language, cocktail party chatter.},
  eprint = {cPcmDgAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-226-28722-5},
  keywords = {Language Arts & Disciplines / General,Language Arts & Disciplines / Linguistics / General,Psychology / Cognitive Psychology & Cognition,Science / Cognitive Science,Science / Life Sciences / Evolution},
  langid = {english},
  pagetotal = {273}
}

@article{cordoPropertiesPosturalAdjustments1982,
  title = {Properties of Postural Adjustments Associated with Rapid Arm Movements},
  author = {Cordo, P. J. and Nashner, L. M.},
  date = {1982-02},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {J. Neurophysiol.},
  volume = {47},
  pages = {287--302},
  issn = {0022-3077},
  doi = {10.1152/jn.1982.47.2.287},
  abstract = {1. We have examined rapid postural adjustments associated with a class of voluntary movements that disturb postural equilibrium. In the text that follows, these motor activities are termed associated postural adjustments and voluntary focal movements, respectively. Standing human subjects performed a variety of movement tasks on a hand-held manipulandum, resulting in disturbances to their postural equilibrium. The experimental use of movements that interact with the subject's environment in a relatively simple was permitted a more precise comparison of the postural adjustments with their associated focal movements. 2. Subjects either pulled or pushed on a stiff interface (the handle) or they responded in a predetermined way to handle perturbations. These activities were carried out with various degrees of steady-state postural stability. Prior to and during these movements, support surface and handle forces, electromyographic (EMG) signals, and body sway were monitored. 3. In addition to previously shown postural adjustments associated with reaction-time armed movements, we have demonstrated these postural activities occur in concept with segmental stretch reflexes and self-initiated (untriggered) movements. Postural adjustments were initiated shortly before all focal movements tested except the short-latency component of the biceps stretch reflex (25- to 30-ms latency). However, this reflex component was rarely elicited by handle perturbations in free-standing subjects; therefore, postural adjustments usually preceded any biceps activity under this condition. 4. By varying the degree of steady-state postural equilibrium, a reciprocal gain/threshold relationship between postural and focal components was documented, i.e., when stability was high, postural activity was reduced or absent and focal activity enhanced. Conversely, the biceps stretch reflex was difficult to elicit under any condition where the subjects was not fully supported in the direction of movement and reaction times of focal movements were prolonged. 5. Postural activities associated with focal movements were found to share a number of organizational properties with automatic postural adjustments to support surface movements. Specifically, the postural muscle synergies were equivalent in muscle composition, relative activation magnitudes, and relative temporal sequencing. Furthermore, both types of postural adjustments were highly specific in locus and magnitude to the quality of steady-state postural equilibrium (e.g., postural "set"). 6. A conceptual model is proposed that suggests one simple way in which the reciprocal influence of postural set on postural and focal movement components and their temporal sequencing might be accomplished. Furthermore, we propose in this model a common central organization of postural adjustments associated with focal movements and those elicited by support-surface movements.},
  eprint = {7062101},
  eprinttype = {pmid},
  keywords = {Afferent Pathways,Arm,Feedback,Humans,Models; Neurological,Motor Activity,Movement,Muscle Contraction,Muscles,Organ Specificity,Posture},
  langid = {english},
  number = {2}
}

@article{cornishSequenceMemoryConstraints2017,
  title = {Sequence {{Memory Constraints Give Rise}} to {{Language}}-{{Like Structure}} through {{Iterated Learning}}},
  author = {Cornish, Hannah and Dale, Rick and Kirby, Simon and Christiansen, Morten H.},
  editor = {Berwick, Robert C},
  date = {2017-01-24},
  journaltitle = {PLoS ONE},
  volume = {12},
  pages = {e0168532},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0168532},
  url = {https://dx.plos.org/10.1371/journal.pone.0168532},
  urldate = {2020-03-24},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6B9XFGD9\\Cornish et al. - 2017 - Sequence Memory Constraints Give Rise to Language-.pdf},
  langid = {english},
  number = {1}
}

@article{corpsCoordinatingUtterancesTurnTaking2018,
  title = {Coordinating {{Utterances During Turn}}-{{Taking}}: {{The Role}} of {{Prediction}}, {{Response Preparation}}, and {{Articulation}}},
  shorttitle = {Coordinating {{Utterances During Turn}}-{{Taking}}},
  author = {Corps, Ruth E. and Gambi, Chiara and Pickering, Martin J.},
  date = {2018-02-17},
  journaltitle = {Discourse Processes},
  volume = {55},
  pages = {230--240},
  publisher = {{Routledge}},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2017.1330031},
  url = {https://doi.org/10.1080/0163853X.2017.1330031},
  urldate = {2020-05-28},
  abstract = {During conversation, interlocutors rapidly switch between speaker and listener roles and take turns at talk. How do they achieve such fine coordination? Most research has concentrated on the role of prediction, but listeners must also prepare a response in advance (assuming they wish to respond) and articulate this response at the appropriate moment. Such mechanisms may overlap with the processes of comprehending the speaker’s incoming turn and predicting its end. However, little is known about the stages of response preparation and production. We discuss three questions pertaining to such stages: (1) Do listeners prepare their own response in advance?, (2) Can listeners buffer their prepared response?, and (3) Does buffering lead to interference with concurrent comprehension? We argue that fine coordination requires more than just an accurate prediction of the interlocutor’s incoming turn: Listeners must also simultaneously prepare their own response.},
  annotation = {\_eprint: https://doi.org/10.1080/0163853X.2017.1330031},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CSDSJ97A\\Corps et al. - 2018 - Coordinating Utterances During Turn-Taking The Ro.pdf;C\:\\Users\\u668173\\Zotero\\storage\\MFUZWG2L\\0163853X.2017.html},
  number = {2}
}

@article{corpsEarlyPreparationTurntaking2018,
  title = {Early Preparation during Turn-Taking: {{Listeners}} Use Content Predictions to Determine What to Say but Not When to Say It},
  shorttitle = {Early Preparation during Turn-Taking},
  author = {Corps, Ruth E. and Crossley, Abigail and Gambi, Chiara and Pickering, Martin J.},
  date = {2018-06-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {175},
  pages = {77--95},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2018.01.015},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027718300234},
  urldate = {2020-10-07},
  abstract = {During conversation, there is often little gap between interlocutors’ utterances. In two pairs of experiments, we manipulated the content predictability of yes/no questions to investigate whether listeners achieve such coordination by (i) preparing a response as early as possible or (ii) predicting the end of the speaker’s turn. To assess these two mechanisms, we varied the participants’ task: They either pressed a button when they thought the question was about to end (Experiments 1a and 2a), or verbally answered the questions with either yes or no (Experiments 1b and 2b). Predictability effects were present when participants had to prepare a verbal response, but not when they had to predict the turn-end. These findings suggest content prediction facilitates turn-taking because it allows listeners to prepare their own response early, rather than because it helps them predict when the speaker will reach the end of their turn.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\J9AT7HMZ\\Corps et al. - 2018 - Early preparation during turn-taking Listeners us.pdf;C\:\\Users\\u668173\\Zotero\\storage\\6S6HLEGG\\S0010027718300234.html},
  keywords = {Conversation,Dialogue,Prediction,Question-answering,Response preparation,Turn-taking},
  langid = {english}
}

@article{corpsEarlyPreparationTurntaking2018a,
  title = {Early Preparation during Turn-Taking: {{Listeners}} Use Content Predictions to Determine What to Say but Not When to Say It},
  shorttitle = {Early Preparation during Turn-Taking},
  author = {Corps, Ruth E. and Crossley, Abigail and Gambi, Chiara and Pickering, Martin J.},
  date = {2018-06},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {175},
  pages = {77--95},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2018.01.015},
  abstract = {During conversation, there is often little gap between interlocutors' utterances. In two pairs of experiments, we manipulated the content predictability of yes/no questions to investigate whether listeners achieve such coordination by (i) preparing a response as early as possible or (ii) predicting the end of the speaker's turn. To assess these two mechanisms, we varied the participants' task: They either pressed a button when they thought the question was about to end (Experiments 1a and 2a), or verbally answered the questions with either yes or no (Experiments 1b and 2b). Predictability effects were present when participants had to prepare a verbal response, but not when they had to predict the turn-end. These findings suggest content prediction facilitates turn-taking because it allows listeners to prepare their own response early, rather than because it helps them predict when the speaker will reach the end of their turn.},
  eprint = {29477750},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\65MW596X\\Corps et al. - 2018 - Early preparation during turn-taking Listeners us.pdf},
  keywords = {Auditory Perception,Communication,Conversation,Dialogue,Female,Humans,Male,Prediction,Question-answering,Reaction Time,Response preparation,Speech,Turn-taking,Young Adult},
  langid = {english}
}

@article{couper-kuhlenRelatednessTimingTalkinInteraction2009,
  title = {Relatedness and {{Timing}} in {{Talk}}-in-{{Interaction}}},
  author = {Couper-Kuhlen, Elizabeth},
  date = {2009-01-01},
  journaltitle = {Where Prosody Meets Pragmatics},
  pages = {257--276},
  publisher = {{Brill}},
  doi = {10.1163/9789004253223_012},
  url = {https://brill.com/view/book/edcoll/9789004253223/B9789004253223-s012.xml},
  urldate = {2020-12-08},
  abstract = {"Relatedness and Timing in Talk-in-Interaction" published on 01 Jan 2009 by Brill.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AF445NH4\\B9789004253223-s012.html},
  langid = {english}
}

@incollection{cox2016,
  booktitle = {Chromatic and {{Anisotropic Cross}}-{{Recurrence Quantification Analysis}} of {{Interpersonal Behavior}} | {{SpringerLink}}},
  author = {Cox, R. F. A. and van der Steen, S. and Guevara, M. and De Jonge-Hoekstra, L. and van Dijk, M.},
  date = {2016},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-29922-8_11},
  urldate = {2019-12-08},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VCUXJUG5\\978-3-319-29922-8_11.html},
  options = {useprefix=true},
  series = {Recurrence {{Plots}} and {{Their Quanitifactions}}}
}

@article{crasbornCombiningVideoNumeric,
  title = {Combining Video and Numeric Data in the Analysis of Sign Languages within the {{ELAN}} Annotation Software},
  author = {Crasborn, Onno and Sloetjes, Han and Auer, Eric and Wittenburg, Peter},
  pages = {7},
  abstract = {This paper describes hardware and software that can be used for the phonetic study of sign languages. The field of sign language phonetics is characterised, and the hardware that is currently in use is described. The paper focuses on the software that was developed to enable the recording of finger and hand movement data, and the additions to the ELAN annotation software that facilitate the further visualisation and analysis of the data.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QLUDTLMV\\Crasborn et al. - Combining video and numeric data in the analysis o.pdf},
  langid = {english}
}

@article{cravottaEffectsEncouragingUse2019,
  title = {Effects of {{Encouraging}} the {{Use}} of {{Gestures}} on {{Speech}}},
  author = {Cravotta, A. and Busà, M. G. and Prieto, P.},
  date = {2019},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  volume = {62},
  pages = {3204--3219},
  doi = {10.21437/SpeechProsody.2018-42},
  number = {9}
}

@article{cravottaRestrainingEncouragingUse2018,
  title = {Restraining and Encouraging the Use of Hand Gestures: Effects on Speech},
  shorttitle = {Restraining and Encouraging the Use of Hand Gestures},
  author = {Cravotta, A. and Grazia, B. M. and Prieto, P.},
  date = {2018},
  issn = {2333-2042},
  doi = {http://dx.doi.org/10.21437/SpeechProsody.2018-42},
  url = {http://repositori.upf.edu/handle/10230/35125},
  urldate = {2019-05-04},
  abstract = {Previous studies have investigated the effects of the inability to make hand gestures on speakers’ fluency; however, the question of whether encouraging speakers to gesture affects their fluency has received little attention. This study investigates the effect of restraining (Experiment 1) and encouraging (Experiment 2) hand gestures on the following correlates of speech: speech discourse length (number of words and discourse length in seconds), disfluencies (filled pauses, self-corrections, repetitions, insertions, interruptions, silent pauses), and acoustic properties (speech rate, measures of intensity and pitch). In two experiments, 10 native speakers of Italian took part in a narration task where they were asked to describe comic strips. Each experiment compared two conditions. In Experiment 1, subjects first received no instructions as to how to behave when narrating. Then they were told to sit on their hands while speaking. In Experiment 2, subjects first received no instructions and were then actively encouraged to use hand gestures. The results showed that restraining gestures leads to quieter and slower paced speech, while encouraging gestures triggers longer speech discourse, faster speech rate and more fluent and louder speech. Thus, both restraining and encouraging hand gestures seem to clearly affect prosodic properties of speech, particularly speech fluency.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YKLIAL3A\\Prieto Vives et al. - 2018 - Restraining and encouraging the use of hand gestur.pdf;C\:\\Users\\u668173\\Zotero\\storage\\64PB283C\\35125.html},
  langid = {english}
}

@article{crevecoeurOptimalIntegrationGravity2009,
  title = {Optimal Integration of Gravity in Trajectory Planning of Vertical Pointing Movements},
  author = {Crevecoeur, Frédéric and Thonnard, Jean-Louis and Lefèvre, Philippe},
  date = {2009-08},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {J. Neurophysiol.},
  volume = {102},
  pages = {786--796},
  issn = {0022-3077},
  doi = {10.1152/jn.00113.2009},
  abstract = {The planning and control of motor actions requires knowledge of the dynamics of the controlled limb to generate the appropriate muscular commands and achieve the desired goal. Such planning and control imply that the CNS must be able to deal with forces and constraints acting on the limb, such as the omnipresent force of gravity. The present study investigates the effect of hypergravity induced by parabolic flights on the trajectory of vertical pointing movements to test the hypothesis that motor commands are optimized with respect to the effect of gravity on the limb. Subjects performed vertical pointing movements in normal gravity and hypergravity. We use a model based on optimal control to identify the role played by gravity in the optimal arm trajectory with minimal motor costs. First, the simulations in normal gravity reproduce the asymmetry in the velocity profiles (the velocity reaches its maximum before half of the movement duration), which typically characterizes the vertical pointing movements performed on Earth, whereas the horizontal movements present symmetrical velocity profiles. Second, according to the simulations, the optimal trajectory in hypergravity should present an increase in the peak acceleration and peak velocity despite the increase in the arm weight. In agreement with these predictions, the subjects performed faster movements in hypergravity with significant increases in the peak acceleration and peak velocity, which were accompanied by a significant decrease in the movement duration. This suggests that movement kinematics change in response to an increase in gravity, which is consistent with the hypothesis that motor commands are optimized and the action of gravity on the limb is taken into account. The results provide evidence for an internal representation of gravity in the central planning process and further suggest that an adaptation to altered dynamics can be understood as a reoptimization process.},
  eprint = {19458149},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KRY3V7BI\\Crevecoeur et al. - 2009 - Optimal integration of gravity in trajectory plann.pdf},
  keywords = {Adult,Algorithms,Arm,Biomechanical Phenomena,Computer Simulation,Female,Gravitation,Humans,Hypergravity,Learning,Male,Mental Processes,Middle Aged,Models; Neurological,Motor Activity,Time Factors},
  langid = {english},
  number = {2}
}

@software{csardiPackageIgraphNetwork2019,
  title = {Package 'igraph' {{Network Analysis}} and {{Visualization}}},
  author = {Csárdi, G.},
  date = {2019},
  url = {http://bioconductor.statistik.tu-dortmund.de/cran/web/packages/igraph/igraph.pdf},
  version = {1.2.4.1}
}

@inproceedings{cumminsRhythmicCommonalitiesHand1992,
  title = {Rhythmic Commonalities between Hand Gestures and Speech},
  booktitle = {Proceedings of the 18th {{Meeting}} of {{Cognitiive Science Society}}},
  author = {Cummins, F. and Port, R. F.},
  date = {1992},
  pages = {415--419},
  publisher = {{Erlbbaum}},
  location = {{Hillsdale}}
}

@incollection{cumminsRhythmSpeech2015,
  title = {Rhythm and {{Speech}}},
  booktitle = {The {{Handbook}} of {{Speech Production}}},
  author = {Cummins, Fred},
  editor = {Redford, Melissa A.},
  date = {2015-04-24},
  pages = {158--177},
  publisher = {{John Wiley \& Sons, Inc}},
  location = {{Hoboken, NJ}},
  doi = {10.1002/9781118584156.ch8},
  url = {http://doi.wiley.com/10.1002/9781118584156.ch8},
  urldate = {2019-09-27},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5SS5Q277\\Cummins - 2015 - Rhythm and Speech.pdf},
  isbn = {978-1-118-58415-6 978-0-470-65993-9},
  langid = {english}
}

@book{cwiekHandMouthCoordinationPointing2021,
  title = {Hand-{{Mouth Coordination}} in a {{Pointing Task Requiring Manual Precision}}},
  author = {Ćwiek, Aleksandra and Fuchs, Susanne},
  date = {2021-02-01},
  abstract = {In daily life, articulatory movements and pointing gestures are tightly coupled. Nevertheless, the two motor systems governing the movements of the articulators and hands differ in their dynamics: the articulators are fast and much lighter than the limbs, which are slower due to their mass. We investigated the timely coordination of those motor systems in a pointing task requiring manual precision. In our experiment, the initial segment was always [p], allowing the participants for early articulatory preparation. Most importantly, we found that the hand gesture onset precedes the onset of the articulatory gesture. We also found that some speakers begin the articulatory movement only after reaching the hand gesture target. Overall, our data reveal that when the articulatory movement is not audible, as it is the case of [p], speakers are very flexible in the coordination between hand and mouth.}
}

@inproceedings{cwiekIconicProsodyRooted2019,
  title = {Iconic {{Prosody}} Is Rooted in Sensori-Motor Properties: {{Fundamental}} Frequency and the Vertical Space},
  booktitle = {41st {{Annual Meeting}} of the {{Cognitive Science SocietyAt}}},
  author = {Cwiek, A. and Fuchs, S.},
  date = {2019},
  location = {{Montreal, Canada}},
  eventtitle = {{{CogSci}} 2019}
}

@article{daleHowHumansMake2018,
  title = {“{{How}} Do Humans Make Sense?” Multiscale Dynamics and Emergent Meaning},
  shorttitle = {“{{How}} Do Humans Make Sense?},
  author = {Dale, Rick and Kello, Christopher T.},
  date = {2018-08-01},
  journaltitle = {New Ideas in Psychology},
  shortjournal = {New Ideas in Psychology},
  volume = {50},
  pages = {61--72},
  issn = {0732-118X},
  doi = {10.1016/j.newideapsych.2017.09.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0732118X1730020X},
  urldate = {2019-08-15},
  abstract = {The challenges posed by the composite nature of sense-making encourage us to study how that composite is dynamically assembled. In this paper, we consider the computational underpinnings that drive the composite nature of interaction. We look to the dynamic properties of recurrent neural networks. What kind of dynamic system inherently integrates multiple signals across different levels and modalities? We argue below that three fundamental properties are needed: dynamic memory, timescale integration, and multimodal integration. We argue that a growing area of investigation in neural networks, reservoir computing, has all these properties (Jaeger, 2001). A simple version of this model is then created to demonstrate “emergent meaning,” using a simplified model communication system.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ANKZFS9Q\\S0732118X1730020X.html}
}

@article{daleyImpactLoadingLocomotorRespiratory2013,
  title = {Impact {{Loading}} and {{Locomotor}}-{{Respiratory Coordination Significantly Influence Breathing Dynamics}} in {{Running Humans}}},
  author = {Daley, Monica A. and Bramble, Dennis M. and Carrier, David R.},
  date = {2013-08-12},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  pages = {e70752},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0070752},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0070752},
  urldate = {2020-09-19},
  abstract = {Locomotor-respiratory coupling (LRC), phase-locking between breathing and stepping rhythms, occurs in many vertebrates. When quadrupedal mammals gallop, 1∶1 stride per breath coupling is necessitated by pronounced mechanical interactions between locomotion and ventilation. Humans show more flexibility in breathing patterns during locomotion, using LRC ratios of 2∶1, 2.5∶1, 3∶1, or 4∶1 and sometimes no coupling. Previous studies provide conflicting evidence on the mechanical significance of LRC in running humans. Some studies suggest LRC improves breathing efficiency, but others suggest LRC is mechanically insignificant because ‘step-driven flows’ (ventilatory flows attributable to step-induced forces) contribute a negligible fraction of tidal volume. Yet, although step-driven flows are brief, they cause large fluctuations in ventilatory flow. Here we test the hypothesis that running humans use LRC to minimize antagonistic effects of step-driven flows on breathing. We measured locomotor-ventilatory dynamics in 14 subjects running at a self-selected speed (2.6±0.1 ms−1) and compared breathing dynamics in their naturally ‘preferred’ and ‘avoided’ entrainment patterns. Step-driven flows occurred at 1-2X step frequency with peak magnitudes of 0.97±0.45 Ls−1 (mean ±S.D). Step-driven flows varied depending on ventilatory state (high versus low lung volume), suggesting state-dependent changes in compliance and damping of thoraco-abdominal tissues. Subjects naturally preferred LRC patterns that minimized antagonistic interactions and aligned ventilatory transitions with assistive phases of the step. Ventilatory transitions initiated in ‘preferred’ phases within the step cycle occurred 2x faster than those in ‘avoided’ phases. We hypothesize that humans coordinate breathing and locomotion to minimize antagonistic loading of respiratory muscles, reduce work of breathing and minimize rate of fatigue. Future work could address the potential consequences of locomotor-ventilatory interactions for elite endurance athletes and individuals who are overweight or obese, populations in which respiratory muscle fatigue can be limiting.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EJXVTS2V\\Daley et al. - 2013 - Impact Loading and Locomotor-Respiratory Coordinat.pdf;C\:\\Users\\u668173\\Zotero\\storage\\2XYPVD6R\\article.html},
  keywords = {Abdominal muscles,Biological locomotion,Breathing,Material fatigue,Respiratory physiology,Running,Signal filtering,Tidal volume},
  langid = {english},
  number = {8}
}

@incollection{dallava-santucciRespiratoryInductivePlethysmography1991,
  title = {Respiratory {{Inductive Plethysmography}}},
  booktitle = {Pulmonary {{Function}} in {{Mechanically Ventilated Patients}}},
  author = {DalľAva-Santucci, J. and Armanganidis, A.},
  editor = {Benito, Salvador and Net, Alvar},
  date = {1991},
  pages = {121--142},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-84209-2_11},
  url = {https://doi.org/10.1007/978-3-642-84209-2_11},
  urldate = {2021-02-26},
  abstract = {In 1977, at the International Symposium of Ambulatory Monitoring, Cohn et al. [1] presented a new transducer for non-invasive monitoring of respiration: the respiratory inductive Plethysmograph (RIP), consisting essentially of two coils, one wrapped around the chest and the other around the abdomen. During the following 12 years considerable interest was focused on this technique not only for monitoring of ventilation but also for thoracoabdominal partitioning of tidal volume and monitoring of end-expiratory volume and mechanical studies. Normal subjects have been studied during spontaneous breathing and during exercise and sleep. Sleep studies included infants, often with the objective of gaining better understanding of the sudden death syndrome. Patients with asthma, chronic obstructive lung disease or sleep apnea syndrome and ventilated patients have also been studied.},
  isbn = {978-3-642-84209-2},
  keywords = {Ambulatory Monitoring,Breathing Pattern,Sudden Death Syndrome,Tidal Volume,Volume Motion},
  langid = {english},
  series = {Update in {{Intensive Care}} and {{Emergency Medicine}}}
}

@article{dalziellDanceChoreographyCoordinated2013,
  title = {Dance {{Choreography Is Coordinated}} with {{Song Repertoire}} in a {{Complex Avian Display}}},
  author = {Dalziell, Anastasia H. and Peters, Richard A. and Cockburn, Andrew and Dorland, Alexandra D. and Maisey, Alex C. and Magrath, Robert D.},
  date = {2013-06-17},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {23},
  pages = {1132--1135},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2013.05.018},
  url = {https://www.cell.com/current-biology/abstract/S0960-9822(13)00581-2},
  urldate = {2019-10-17},
  eprint = {23746637},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EHU86PNY\\Dalziell et al. - 2013 - Dance Choreography Is Coordinated with Song Repert.pdf},
  langid = {english},
  number = {12}
}

@article{dammWhyWeMove2020,
  title = {Why Do We Move to the Beat? {{A}} Multi-Scale Approach, from Physical Principles to Brain Dynamics},
  shorttitle = {Why Do We Move to the Beat?},
  author = {Damm, Loïc and Varoqui, Déborah and De Cock, Valérie Cochen and Dalla Bella, Simone and Bardy, Benoît},
  date = {2020-05-01},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  shortjournal = {Neuroscience \& Biobehavioral Reviews},
  volume = {112},
  pages = {553--584},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2019.12.024},
  url = {http://www.sciencedirect.com/science/article/pii/S0149763419302118},
  urldate = {2020-10-29},
  abstract = {Humans’ ability to synchronize movement with auditory rhythms relies on motor networks, such as cortical areas, basal ganglia and the cerebellum, which also participate in rhythm perception and movement production. Current research has provided insights into the dependence of this action-perception coupling upon the entrainment of neuronal activity by external rhythms. At a physical level, advances on wearable robotics have enriched our understanding of the dynamical properties of the locomotor system showing evidence of mechanical entrainment. Here we defend the view that modelling brain and locomotor oscillatory activities as dynamical systems, at both neural and physical levels, provides a unified theoretical framework for the understanding of externally driven rhythmic entrainment of biological systems. To better understand the underlying mechanisms of this multi-level entrainment during locomotion, we review in a common framework the core questions related to the dynamic properties of biological oscillators and the neural bases of auditory-motor synchronization. Illustrations of our approach, using personalized auditory stimulation, to gait rehabilitation in Parkinson disease and to manipulation of runners’ kinematics are presented.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9IW92H8Y\\Damm et al. - 2020 - Why do we move to the beat A multi-scale approach.pdf;C\:\\Users\\u668173\\Zotero\\storage\\JBF6L6B2\\S0149763419302118.html},
  keywords = {Auditory cueing,Beat,Cadence,Dynamical systems,Music,Oscillators,Prediction,Rhythm,Running,Synchronization,Walking},
  langid = {english}
}

@thesis{dannerEffectsSpeechContext2017,
  title = {Effects of {{Speech Context On Charactersitics}} of {{Manual Gesture}}},
  author = {Danner, S. G.},
  date = {2017},
  institution = {{University of Southern California}}
}

@article{dannerQuantitativeAnalysisMultimodal2018,
  title = {Quantitative Analysis of Multimodal Speech Data},
  author = {Danner, Samantha Gordon and Barbosa, Adriano Vilela and Goldstein, Louis},
  date = {2018-11-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {71},
  pages = {268--283},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2018.09.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447017302280},
  urldate = {2019-05-04},
  abstract = {This study presents techniques for quantitatively analyzing coordination and kinematics in multimodal speech using video, audio and electromagnetic articulography (EMA) data. Multimodal speech research has flourished due to recent improvements in technology, yet gesture detection/annotation strategies vary widely, leading to difficulty in generalizing across studies and in advancing this field of research. We describe how FlowAnalyzer software can be used to extract kinematic signals from basic video recordings; and we apply a technique, derived from speech kinematic research, to detect bodily gestures in these kinematic signals. We investigate whether kinematic characteristics of multimodal speech differ dependent on communicative context, and we find that these contexts can be distinguished quantitatively, suggesting a way to improve and standardize existing gesture identification/annotation strategy. We also discuss a method, Correlation Map Analysis (CMA), for quantifying the relationship between speech and bodily gesture kinematics over time. We describe potential applications of CMA to multimodal speech research, such as describing characteristics of speech-gesture coordination in different communicative contexts. The use of the techniques presented here can improve and advance multimodal speech and gesture research by applying quantitative methods in the detection and description of multimodal speech.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\U3QL5MLF\\Danner et al. - 2018 - Quantitative analysis of multimodal speech data.pdf;C\:\\Users\\u668173\\Zotero\\storage\\H4ZUTUED\\S0095447017302280.html},
  keywords = {Bodily gesture,Communicative context,Correlation Map Analysis,FlowAnalyzer,Multimodal speech,Time-varying coordination}
}

@article{debeerHowMuchInformation2017,
  title = {How {{Much Information Do People With Aphasia Convey}} via {{Gesture}}?},
  author = {de Beer, Carola and Carragher, Marcella and van Nispen, Karin and Hogrefe, Katharina and de Ruiter, Jan P. and Rose, Miranda L.},
  date = {2017-05-17},
  journaltitle = {American Journal of Speech-Language Pathology},
  shortjournal = {Am J Speech Lang Pathol},
  volume = {26},
  pages = {483--497},
  issn = {1558-9110},
  doi = {10.1044/2016_AJSLP-15-0027},
  abstract = {Purpose : People with aphasia (PWA) face significant challenges in verbally expressing their communicative intentions. Different types of gestures are produced spontaneously by PWA, and a potentially compensatory function of these gestures has been discussed. The current study aimed to investigate how much information PWA communicate through 3 types of gesture and the communicative effectiveness of such gestures. Method: Listeners without language impairment rated the information content of short video clips taken from PWA in conversation. Listeners were asked to rate communication within a speech-only condition and a gesture + speech condition. Results: The results revealed that the participants' interpretations of the communicative intentions expressed in the clips of PWA were significantly more accurate in the gesture + speech condition for all tested gesture types. Conclusion: It was concluded that all 3 gesture types under investigation contributed to the expression of semantic meaning communicated by PWA. Gestures are an important communicative means for PWA and should be regarded as such by their interlocutors. Gestures have been shown to enhance listeners' interpretation of PWA's overall communication.},
  eprint = {28492911},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XBQBXT77\\de Beer et al. - 2017 - How Much Information Do People With Aphasia Convey.pdf},
  keywords = {Adult,Aged,Aphasia,Communication Methods; Total,Comprehension,Female,Gestures,Humans,Interpersonal Relations,Male,Manual Communication,Middle Aged,Semantics,Speech Production Measurement},
  langid = {english},
  number = {2},
  options = {useprefix=true}
}

@article{debreslioskaDiscourseReferenceBimodal2019,
  title = {Discourse {{Reference Is Bimodal}}: {{How Information Status}} in {{Speech Interacts}} with {{Presence}} and {{Viewpoint}} of {{Gestures}}},
  shorttitle = {Discourse {{Reference Is Bimodal}}},
  author = {Debreslioska, Sandra and Gullberg, Marianne},
  date = {2019-01-02},
  journaltitle = {Discourse Processes},
  volume = {56},
  pages = {41--60},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2017.1351909},
  url = {https://doi.org/10.1080/0163853X.2017.1351909},
  urldate = {2019-09-20},
  abstract = {Speakers use speech and gestures to represent referents in discourse. Depending on referents’ information status, in speech speakers will vary richness of expression (e.g., lexical noun phrase [NP]/pronoun), nominal definiteness (indefinite/definite), and grammatical role (subject/object). This study tested whether these three linguistic markers of information status interact with presence of gestures and gestural viewpoint (observer/character). The results show that gestures are more frequent with less accessible referents expressed with richer spoken forms but that richness of expression does not interact with viewpoint. In contrast, nominal definiteness and grammatical role interact with both presence and viewpoint of gestures. Gestures occur mainly with indefinite lexical NPs and objects. Character viewpoint gestures occur mainly with indefinite lexical NPs and objects plus predicates. The results shed light on when and how speakers use gestures in connected discourse and specifically highlight the discursive function of gestural viewpoint.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZCX2ASCE\\0163853X.2017.html},
  keywords = {bimodal discourse,gestures,information status,mode of representation,referential expressions},
  number = {1}
}

@article{decarvalhoEnactiveEcologicalApproachInformation2020,
  title = {An {{Enactive}}-{{Ecological Approach}} to {{Information}} and {{Uncertainty}}},
  author = {de Carvalho, Eros Moreira and Rolla, Giovanni},
  date = {2020-04-21},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  volume = {11},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.00588},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7186374/},
  urldate = {2020-09-04},
  abstract = {Information is a central notion for cognitive sciences and neurosciences, but there is no agreement on what it means for a cognitive system to acquire information about its surroundings. In this paper, we approximate three influential views on information: the one at play in ecological psychology, which is sometimes called information for action; the notion of information as covariance as developed by some enactivists, and the idea of information as a minimization of uncertainty as presented by Shannon. Our main thesis is that information for action can be construed as covariant information, and that learning to perceive covariant information is a matter of minimizing uncertainty through skilled performance. We argue that the agent’s cognitive system conveys information for acting in an environment by minimizing uncertainty about how to achieve intended goals in that environment. We conclude by reviewing empirical findings that support our view by showing how direct learning, seen as an instance of ecological rationality at work, is how mere possibilities for action are turned into embodied know-how. Finally, we indicate the affinity between direct learning and sense-making activity.},
  eprint = {32373006},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5L4272J9\\de Carvalho and Rolla - 2020 - An Enactive-Ecological Approach to Information and.pdf},
  options = {useprefix=true},
  pmcid = {PMC7186374}
}

@article{dejaegherParticipatorySensemaking2007,
  title = {Participatory Sense-Making},
  author = {De Jaegher, Hanne and Di Paolo, Ezequiel},
  date = {2007-12-01},
  journaltitle = {Phenomenology and the Cognitive Sciences},
  shortjournal = {Phenom Cogn Sci},
  volume = {6},
  pages = {485--507},
  issn = {1572-8676},
  doi = {10.1007/s11097-007-9076-9},
  url = {https://doi.org/10.1007/s11097-007-9076-9},
  urldate = {2021-02-18},
  abstract = {As yet, there is no enactive account of social cognition. This paper extends the enactive concept of sense-making into the social domain. It takes as its departure point the process of interaction between individuals in a social encounter. It is a well-established finding that individuals can and generally do coordinate their movements and utterances in such situations. We argue that the interaction process can take on a form of autonomy. This allows us to reframe the problem of social cognition as that of how meaning is generated and transformed in the interplay between the unfolding interaction process and the individuals engaged in it. The notion of sense-making in this realm becomes participatory sense-making. The onus of social understanding thus moves away from strictly the individual only.},
  langid = {english},
  number = {4}
}

@article{dejongCorrelationPcenterAdjustments1994,
  title = {The Correlation of {{P}}-Center Adjustments with Articulatory and Acoustic Events},
  author = {De Jong, Kenneth J.},
  date = {1994-07-01},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  volume = {56},
  pages = {447--460},
  issn = {1532-5962},
  doi = {10.3758/BF03206736},
  url = {https://doi.org/10.3758/BF03206736},
  urldate = {2019-09-28},
  abstract = {To evaluate articulatory models of perceptual center (P-center) location, listeners performed perceptual adjustments on stimuli which were extracted from a corpus of articulatory data. To avoid streaming effects, the stimuli were not edited to obtain temporal variation; instead, they varied in stress and segmental content. Adjustments were evaluated as to their simultaneity with acoustic and articulatory events. The first experiment yielded various articulatory and acoustic correlates of P-center location; the second yielded different articulatory predictors and no acoustic effective predictors. Multiple correlation analyses showed a variation from P-center locations predicted by the articulatory events that were associated with other predictors. Thus, P-center locations do not correspond to any particular kinematic articulatory event, but rather to a complex of events taken from throughout the stimuli. These results are discussed in terms of their relevance to a model of P-centers as indices of underlying gestural timing.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8JYH24TV\\De Jong - 1994 - The correlation of P-center adjustments with artic.pdf},
  keywords = {Acoustic Event,Tongue Dorsum,Voice Onset Time,Vowel Duration,Word Pair},
  langid = {english},
  number = {4}
}

@article{delacruz-paviaFindingPhrasesInterplay2019,
  title = {Finding {{Phrases}}: {{The Interplay}} of {{Word Frequency}}, {{Phrasal Prosody}} and {{Co}}-Speech {{Visual Information}} in {{Chunking Speech}} by {{Monolingual}} and {{Bilingual Adults}}},
  shorttitle = {Finding {{Phrases}}},
  author = {de la Cruz-Pavía, Irene and Werker, Janet F. and Vatikiotis-Bateson, Eric and Gervain, Judit},
  date = {2019-04-19},
  journaltitle = {Language and Speech},
  shortjournal = {Lang Speech},
  pages = {0023830919842353},
  issn = {0023-8309},
  doi = {10.1177/0023830919842353},
  url = {https://doi.org/10.1177/0023830919842353},
  urldate = {2020-01-06},
  abstract = {The audiovisual speech signal contains multimodal information to phrase boundaries. In three artificial language learning studies with 12 groups of adult participants we investigated whether English monolinguals and bilingual speakers of English and a language with opposite basic word order (i.e., in which objects precede verbs) can use word frequency, phrasal prosody and co-speech (facial) visual information, namely head nods, to parse unknown languages into phrase-like units. We showed that monolinguals and bilinguals used the auditory and visual sources of information to chunk “phrases” from the input. These results suggest that speech segmentation is a bimodal process, though the influence of co-speech facial gestures is rather limited and linked to the presence of auditory prosody. Importantly, a pragmatic factor, namely the language of the context, seems to determine the bilinguals’ segmentation, overriding the auditory and visual cues and revealing a factor that begs further exploration.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HUVB9ZII\\de la Cruz-Pavía et al. - 2019 - Finding Phrases The Interplay of Word Frequency, .pdf},
  keywords = {artificial grammar learning,bilingualism,co-speech visual information,frequency-based information,phrase segmentation,prosody},
  langid = {english},
  options = {useprefix=true}
}

@article{delnegroBreathingMatters2018,
  title = {Breathing Matters},
  author = {Del Negro, Christopher A. and Funk, Gregory D. and Feldman, Jack L.},
  date = {2018-06},
  journaltitle = {Nature Reviews. Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {19},
  pages = {351--367},
  issn = {1471-0048},
  doi = {10.1038/s41583-018-0003-6},
  abstract = {Breathing is a well-described, vital and surprisingly complex behaviour, with behavioural and physiological outputs that are easy to directly measure. Key neural elements for generating breathing pattern are distinct, compact and form a network amenable to detailed interrogation, promising the imminent discovery of molecular, cellular, synaptic and network mechanisms that give rise to the behaviour. Coupled oscillatory microcircuits make up the rhythmic core of the breathing network. Primary among these is the preBötzinger Complex (preBötC), which is composed of excitatory rhythmogenic interneurons and excitatory and inhibitory pattern-forming interneurons that together produce the essential periodic drive for inspiration. The preBötC coordinates all phases of the breathing cycle, coordinates breathing with orofacial behaviours and strongly influences, and is influenced by, emotion and cognition. Here, we review progress towards cracking the inner workings of this vital core.},
  eprint = {29740175},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6D9IW4T2\\Del Negro et al. - 2018 - Breathing matters.pdf},
  keywords = {Animals,Brain,Central Pattern Generators,Cranial Nerves,Humans,Interneurons,Lung,Muscle Contraction,Muscle; Skeletal,Neural Pathways,Respiration},
  langid = {english},
  number = {6},
  pmcid = {PMC6636643}
}

@article{delpOpenSimOpensourceSoftware2007,
  title = {{{OpenSim}}: {{Open}}-Source Software to Create and Analyze Dynamic Simulations of Movement},
  shorttitle = {{{OpenSim}}},
  author = {Delp, Scott L. and Anderson, Frank C. and Arnold, Allison S. and Loan, Peter and Habib, Ayman and John, T. and Guendelman, Eran and Thelen, Darryl G.},
  date = {2007},
  journaltitle = {IEEE Transactions on Biomedical Engineering},
  pages = {1940--1950},
  abstract = {Abstract—Dynamic simulations of movement allow one to study neuromuscular coordination, analyze athletic performance, and estimate internal loading of the musculoskeletal system. Simu-lations can also be used to identify the sources of pathological movement and establish a scientific basis for treatment planning. We have developed a freely available, open-source software system (OpenSim) that lets users develop models of musculoskeletal structures and create dynamic simulations of a wide variety of movements. We are using this system to simulate the dynamics of individuals with pathological gait and to explore the biomechan-ical effects of treatments. OpenSim provides a platform on which the biomechanics community can build a library of simulations that can be exchanged, tested, analyzed, and improved through a multi-institutional collaboration. Developing software that enables a concerted effort from many investigators poses technical and sociological challenges. Meeting those challenges will accelerate the discovery of principles that govern movement control and improve treatments for individuals with movement pathologies. Index Terms—Computed muscle control, forward dynamic sim-ulation, musculoskeletal modeling, open-source software. I.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GF2GC4QF\\Delp et al. - 2007 - OpenSim Open-source software to create and analyz.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5FREMQNQ\\download.html;C\:\\Users\\u668173\\Zotero\\storage\\5GFDCTL5\\download.html}
}

@article{dennisPrivacyOpenScience2019,
  title = {Privacy versus Open Science},
  author = {Dennis, Simon and Garrett, Paul and Yim, Hyungwook and Hamm, Jihun and Osth, Adam F. and Sreekumar, Vishnu and Stone, Ben},
  date = {2019-08-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {51},
  pages = {1839--1848},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01259-5},
  url = {https://doi.org/10.3758/s13428-019-01259-5},
  urldate = {2019-11-30},
  abstract = {Pervasive internet and sensor technologies promise to revolutionize psychological science. However, the data collected using these technologies are often very personal—indeed, the value of the data is often directly related to how personal they are. At the same time, driven by the replication crisis, there is a sustained push to publish data to open repositories. These movements are in fundamental conflict. In this article, we propose a way to navigate this issue. We argue that there are significant advantages to be gained by ceding the ownership of data to the participants who generate the data. We then provide desiderata for a privacy-preserving platform. In particular, we suggest that researchers should use an interface to perform experiments and run analyses, rather than observing the stimuli themselves. We argue that this method not only improves privacy but will also encourage greater compliance with good research practices than is possible through open repositories.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\V4WH7AAN\\Dennis et al. - 2019 - Privacy versus open science.pdf},
  keywords = {Differential privacy,Open repositories,Open science,Privacy},
  langid = {english},
  number = {4}
}

@incollection{deruiterAsymmetricRedundancyGesture2017,
  title = {The Asymmetric Redundancy of Gesture and Speech.},
  booktitle = {Why Gesture? {{How}} the Hands Function in Speaking, Thinking and Communicating},
  author = {De Ruiter, J.P.},
  editor = {Church, R.B. and Alibali, M.W. and Kelly, S.D.},
  date = {2017},
  publisher = {{John Benjamins Publishing Company}},
  location = {{Amsterdam}}
}

@book{deutscherUnfoldingLanguageEvolutionary2005,
  title = {The {{Unfolding}} of {{Language}}: {{An Evolutionary}} of {{Mankind}}'s {{Greatest Invention}}},
  author = {Deutscher, G.},
  date = {2005},
  publisher = {{Metropolitan Books}},
  location = {{New York}}
}

@article{devosTurntimingSignedConversations2015,
  title = {Turn-Timing in Signed Conversations: Coordinating Stroke-to-Stroke Turn Boundaries},
  shorttitle = {Turn-Timing in Signed Conversations},
  author = {de Vos, Connie and Torreira, Francisco and Levinson, Stephen C.},
  date = {2015},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00268},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00268/full},
  urldate = {2020-05-19},
  abstract = {In spoken interactions, interlocutors carefully plan and time their utterances, minimising gaps and overlaps between consecutive turns. Cross-linguistic comparison has indicated that spoken languages vary only minimally in terms of turn-timing, and language acquisition research has shown pre-linguistic vocal turn-taking in the first half year of life. These observations suggest that the turn-taking system may provide a fundamental basis for our linguistic capacities. The question remains however to what extent our capacity for rapid turn-taking is determined by modality constraints. The avoidance of overlapping turns could be motivated by the difficulty of hearing and speaking at the same time. If so, turn-taking in sign might show greater toleration for overlap. Alternatively, signed conversations may show a similar distribution of turn-timing as spoken languages, thus avoiding both gaps and overlaps. To address this question we look at turn-timing in question-answer sequences in spontaneous conversations of Sign Language of the Netherlands. The findings indicate that although there is considerable overlap in two or more signers' articulators in conversation, when proper allowance is made for onset preparation, post-utterance retraction and the intentional holding of signs for response, turn-taking latencies in sign look remarkably like those reported for spoken language. This is consistent with the possibility that, at least with regard to responses to questions, speakers and signers follow similar time courses in planning and producing their utterances in on-going conversation. This suggests that turn-taking systems may well be a shared cognitive infrastructure underlying all modern human languages, both spoken and signed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VJFH38W5\\de Vos et al. - 2015 - Turn-timing in signed conversations coordinating .pdf},
  keywords = {conversation analysis,sign language,sign phonetics,turn-taking,turn-timing,visual-gestural modality},
  langid = {english},
  options = {useprefix=true}
}

@article{dewitIntroducingNEMOLowlandsIconic2020,
  title = {Introducing the {{NEMO}}-{{Lowlands}} Iconic Gesture Dataset, Collected through a Gameful Human–Robot Interaction},
  author = {de Wit, Jan and Krahmer, Emiel and Vogt, Paul},
  date = {2020-10-19},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  issn = {1554-3528},
  doi = {10.3758/s13428-020-01487-0},
  url = {https://doi.org/10.3758/s13428-020-01487-0},
  urldate = {2020-11-19},
  abstract = {This paper describes a novel dataset of iconic gestures, together with a publicly available robot-based elicitation method to record these gestures, which consists of playing a game of charades with a humanoid robot. The game was deployed at a science museum (NEMO) and a large popular music festival (Lowlands) in the Netherlands. This resulted in recordings of 428 participants, both adults and children, performing 3715 silent iconic gestures for 35 different objects in a naturalistic setting. Our dataset adds to existing collections of iconic gesture recordings in two important ways. First, participants were free to choose how they represented the broad concepts using gestures, and they were asked to perform a second attempt if the robot did not recognize their gesture the first time. This provides insight into potential repair strategies that might be used. Second, by making the interactive game available we enable other researchers to collect additional recordings, for different concepts, and in diverse cultures or contexts. This can be done in a consistent manner because a robot is used as a confederate in the elicitation procedure, which ensures that every data collection session plays out in the same way. The current dataset can be used for research into human gesturing behavior, and as input for the gesture recognition and production capabilities of robots and virtual agents.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VGBHGTIZ\\de Wit et al. - 2020 - Introducing the NEMO-Lowlands iconic gesture datas.pdf},
  langid = {english},
  options = {useprefix=true}
}

@online{DimensionsGuideDatabase,
  title = {Dimensions.{{Guide}} | {{Database}} of {{Dimensioned Drawings}}},
  url = {https://www.dimensions.guide},
  urldate = {2019-05-01},
  abstract = {A comprehensive reference database of dimensioned drawings documenting the standard measurements and sizes of the everyday objects and spaces that make up our world.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NZQXJZAC\\www.dimensions.guide.html},
  langid = {english}
}

@article{dimitrovaBeatThatWord2016,
  title = {Beat That {{Word}}: {{How Listeners Integrate Beat Gesture}} and {{Focus}} in {{Multimodal Speech Discourse}}},
  shorttitle = {Beat That {{Word}}},
  author = {Dimitrova, Diana and Chu, Mingyuan and Wang, Lin and Özyürek, Asli and Hagoort, Peter},
  date = {2016-09},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {J Cogn Neurosci},
  volume = {28},
  pages = {1255--1269},
  issn = {1530-8898},
  doi = {10.1162/jocn_a_00963},
  abstract = {Communication is facilitated when listeners allocate their attention to important information (focus) in the message, a process called "information structure." Linguistic cues like the preceding context and pitch accent help listeners to identify focused information. In multimodal communication, relevant information can be emphasized by nonverbal cues like beat gestures, which represent rhythmic nonmeaningful hand movements. Recent studies have found that linguistic and nonverbal attention cues are integrated independently in single sentences. However, it is possible that these two cues interact when information is embedded in context, because context allows listeners to predict what information is important. In an ERP study, we tested this hypothesis and asked listeners to view videos capturing a dialogue. In the critical sentence, focused and nonfocused words were accompanied by beat gestures, grooming hand movements, or no gestures. ERP results showed that focused words are processed more attentively than nonfocused words as reflected in an N1 and P300 component. Hand movements also captured attention and elicited a P300 component. Importantly, beat gesture and focus interacted in a late time window of 600-900 msec relative to target word onset, giving rise to a late positivity when nonfocused words were accompanied by beat gestures. Our results show that listeners integrate beat gesture with the focus of the message and that integration costs arise when beat gesture falls on nonfocused information. This suggests that beat gestures fulfill a unique focusing function in multimodal discourse processing and that they have to be integrated with the information structure of the message.},
  eprint = {27027421},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\35T49C6D\\Dimitrova et al. - 2016 - Beat that Word How Listeners Integrate Beat Gestu.pdf},
  keywords = {Adolescent,Adult,Cues,Electroencephalography,Evoked Potentials,Female,Gestures,Humans,Male,Motion Perception,Neuropsychological Tests,Speech Perception,Young Adult},
  langid = {english},
  number = {9}
}

@article{dingemanseArbitrarinessIconicitySystematicity2015,
  title = {Arbitrariness, {{Iconicity}}, and {{Systematicity}} in {{Language}}},
  author = {Dingemanse, Mark and Blasi, Damián E. and Lupyan, Gary and Christiansen, Morten H. and Monaghan, Padraic},
  date = {2015-10-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {19},
  pages = {603--615},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2015.07.013},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661315001771},
  urldate = {2020-03-10},
  abstract = {The notion that the form of a word bears an arbitrary relation to its meaning accounts only partly for the attested relations between form and meaning in the languages of the world. Recent research suggests a more textured view of vocabulary structure, in which arbitrariness is complemented by iconicity (aspects of form resemble aspects of meaning) and systematicity (statistical regularities in forms predict function). Experimental evidence suggests these form-to-meaning correspondences serve different functions in language processing, development, and communication: systematicity facilitates category learning by means of phonological cues, iconicity facilitates word learning and communication by means of perceptuomotor analogies, and arbitrariness facilitates meaning individuation through distinctive forms. Processes of cultural evolution help to explain how these competing motivations shape vocabulary structure.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PHKPL4HL\\Dingemanse et al. - 2015 - Arbitrariness, Iconicity, and Systematicity in Lan.pdf;C\:\\Users\\u668173\\Zotero\\storage\\WAYCL3R5\\S1364661315001771.html},
  keywords = {arbitrariness,Iconicity,lexicon,sound-symbolism,systematicity,vocabulary},
  langid = {english},
  number = {10}
}

@article{dingemanseConstrualsIconicityExperimental2020,
  title = {Construals of Iconicity: Experimental Approaches to Form–Meaning Resemblances in Language},
  shorttitle = {Construals of Iconicity},
  author = {Dingemanse, Mark and Perlman, Marcus and Perniss, Pamela},
  date = {2020-03},
  journaltitle = {Language and Cognition},
  volume = {12},
  pages = {1--14},
  publisher = {{Cambridge University Press}},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2019.48},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/construals-of-iconicity-experimental-approaches-to-formmeaning-resemblances-in-language/1E2DF517E530A8D5C4B489A0AD76AFA7},
  urldate = {2020-03-09},
  abstract = {While speculations on form–meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well equipped to contribute to a comprehensive science of language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\F3B3U98R\\Dingemanse et al. - 2020 - Construals of iconicity experimental approaches t.pdf;C\:\\Users\\u668173\\Zotero\\storage\\YVZNZJAS\\1E2DF517E530A8D5C4B489A0AD76AFA7.html},
  keywords = {conceptual foundations,experimental linguistics,iconicity,linguistic theory},
  langid = {english},
  number = {1}
}

@article{dingemanseUniversalPrinciplesRepair2015,
  title = {Universal {{Principles}} in the {{Repair}} of {{Communication Problems}}},
  author = {Dingemanse, Mark and Roberts, Seán G. and Baranova, Julija and Blythe, Joe and Drew, Paul and Floyd, Simeon and Gisladottir, Rosa S. and Kendrick, Kobin H. and Levinson, Stephen C. and Manrique, Elizabeth and Rossi, Giovanni and Enfield, N. J.},
  date = {2015-09-16},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {10},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0136100},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4573759/},
  urldate = {2020-03-18},
  abstract = {There would be little adaptive value in a complex communication system like human language if there were no ways to detect and correct problems. A systematic comparison of conversation in a broad sample of the world’s languages reveals a universal system for the real-time resolution of frequent breakdowns in communication. In a sample of 12 languages of 8 language families of varied typological profiles we find a system of ‘other-initiated repair’, where the recipient of an unclear message can signal trouble and the sender can repair the original message. We find that this system is frequently used (on average about once per 1.4 minutes in any language), and that it has detailed common properties, contrary to assumptions of radical cultural variation. Unrelated languages share the same three functionally distinct types of repair initiator for signalling problems and use them in the same kinds of contexts. People prefer to choose the type that is the most specific possible, a principle that minimizes cost both for the sender being asked to fix the problem and for the dyad as a social unit. Disruption to the conversation is kept to a minimum, with the two-utterance repair sequence being on average no longer that the single utterance which is being fixed. The findings, controlled for historical relationships, situation types and other dependencies, reveal the fundamentally cooperative nature of human communication and offer support for the pragmatic universals hypothesis: while languages may vary in the organization of grammar and meaning, key systems of language use may be largely similar across cultural groups. They also provide a fresh perspective on controversies about the core properties of language, by revealing a common infrastructure for social interaction which may be the universal bedrock upon which linguistic diversity rests.},
  eprint = {26375483},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\S4WAC66J\\Dingemanse et al. - 2015 - Universal Principles in the Repair of Communicatio.pdf},
  number = {9},
  pmcid = {PMC4573759}
}

@article{dingTemporalModulationsSpeech2017,
  title = {Temporal Modulations in Speech and Music},
  author = {Ding, Nai and Patel, Aniruddh D. and Chen, Lin and Butler, Henry and Luo, Cheng and Poeppel, David},
  date = {2017-10-01},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  shortjournal = {Neuroscience \& Biobehavioral Reviews},
  volume = {81},
  pages = {181--187},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2017.02.011},
  url = {http://www.sciencedirect.com/science/article/pii/S0149763416305668},
  urldate = {2020-11-03},
  abstract = {Speech and music have structured rhythms. Here we discuss a major acoustic correlate of spoken and musical rhythms, the slow (0.25–32Hz) temporal modulations in sound intensity and compare the modulation properties of speech and music. We analyze these modulations using over 25h of speech and over 39h of recordings of Western music. We show that the speech modulation spectrum is highly consistent across 9 languages (including languages with typologically different rhythmic characteristics). A different, but similarly consistent modulation spectrum is observed for music, including classical music played by single instruments of different types, symphonic, jazz, and rock. The temporal modulations of speech and music show broad but well-separated peaks around 5 and 2Hz, respectively. These acoustically dominant time scales may be intrinsic features of speech and music, a possibility which should be investigated using more culturally diverse samples in each domain. Distinct modulation timescales for speech and music could facilitate their perceptual analysis and its neural processing.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8A3YIHM6\\Ding et al. - 2017 - Temporal modulations in speech and music.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5DVVIG86\\S0149763416305668.html},
  keywords = {Modulation spectrum,Music,Rhythm,Speech,Temporal modulations},
  langid = {english},
  series = {The {{Biology}} of {{Language}}}
}

@inproceedings{dohenCoproductionSpeechPointing2017,
  title = {Co-Production of Speech and Pointing Gestures in Clear and Perturbed Interactive Tasks: Multimodal Designation Strategies},
  shorttitle = {Co-Production of Speech and Pointing Gestures in Clear and Perturbed Interactive Tasks},
  booktitle = {Interspeech 2017},
  author = {Dohen, Marion and Roustan, Benjamin},
  date = {2017-08},
  location = {{Stockholm, Sweden}},
  url = {https://hal.archives-ouvertes.fr/hal-02367749},
  urldate = {2020-10-20},
  abstract = {Designation consists in attracting an interlocutor's attention on a specific object and/or location. It is most often achieved using both speech (e.g., demonstratives) and gestures (e.g., manual pointing). This study aims at analyzing how speech and pointing gestures are co-produced in a semi-directed interactive task involving designation. 20 native speakers of French were involved in a cooperative task in which they provided instructions to a partner for her to reproduce a model she could not see on a grid both of them saw. They had to use only sentences of the form 'The [target word] goes there.'. They did this in two conditions: silence and noise. Their speech and articulatory/hand movements (motion capture) were recorded. The analyses show that the participants' speech features were modified in noise (Lombard effect). They also spoke slower and made more pauses and errors. Their pointing gestures lasted longer and started later showing an adaptation of gesture production to speech. The condition did not influence speech/gesture coordination. The apex (part of the gesture that shows) mainly occurred at the same time as the target word and not as the demonstrative showing that speakers group speech and gesture carrying complementary rather than redundant information.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KSF9DQ5Y\\Dohen and Roustan - 2017 - Co-production of speech and pointing gestures in c.pdf},
  keywords = {interaction,multimodality,noise,perturbed communication,pointing,speech/gesture coordination}
}

@inproceedings{dohenSpeechEarEye2009,
  title = {Speech through the {{Ear}}, the {{Eye}}, the {{Mouth}} and the {{Hand}}},
  booktitle = {Multimodal {{Signals}}: {{Cognitive}} and {{Algorithmic Issues}}},
  author = {Dohen, Marion},
  editor = {Esposito, Anna and Hussain, Amir and Marinaro, Maria and Martone, Raffaele},
  date = {2009},
  pages = {24--39},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-00525-1_2},
  abstract = {This chapter aims at describing how speech is multimodal not only in its perception but also in its production. It first focuses on multimodal perception of speech segments and speech prosody. It describes how multimodal perception is linked to speech production and explains why we consider speech perception as a sensory-motor process. It then analyses some aspects of hand-mouth coordination in spoken communication. Input from evolutionary perspectives, ontogenesis and behavioral studies on infants and adults are combined to detail how and why this coordination is crucial in speech.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VMJTG5HI\\Dohen - 2009 - Speech through the Ear, the Eye, the Mouth and the.pdf},
  isbn = {978-3-642-00525-1},
  keywords = {auditory-visual perception,hand-mouth coordination,multimodal speech,pointing,prosody,sensori-motor,speech and language development},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{Doi101016,
  title = {Doi:10.1016/{{S1364}}-6613(03)00136-0 | {{Elsevier Enhanced Reader}}},
  url = {https://reader.elsevier.com/reader/sd/pii/S1364661303001360?token=641BF18A47FF7B99F9BFC808B67B13B78B765E129697D3B02D2B89C46823CF0850CD50831C636F4BD44E539F83A46F4B},
  urldate = {2020-02-21},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9G5YHC7F\\S1364661303001360.html}
}

@book{donaldOriginsModernMind1991,
  title = {Origins of the Modern Mind: {{Three}} Stages in the Evolution of Culture and Cognition},
  author = {Donald, M.},
  date = {1991},
  publisher = {{Boston: Harvard University Press}}
}

@software{donaldsonTsneTDistributedStochastic2016,
  title = {Tsne: {{T}}-{{Distributed Stochastic Neighbor Embedding}} for {{R}} (t-{{SNE}})},
  shorttitle = {Tsne},
  author = {Donaldson, Justin},
  date = {2016-07-15},
  url = {https://CRAN.R-project.org/package=tsne},
  urldate = {2020-12-29},
  abstract = {A "pure R" implementation of the t-SNE algorithm.},
  version = {0.1-3}
}

@article{drijversAlphaBetaOscillations2018,
  title = {Alpha and Beta Oscillations Index Semantic Congruency between Speech and Gestures in Clear and Degraded Speech},
  author = {Drijvers, Linda and Özyürek, Asli and Jensen, Ole},
  date = {2018},
  journaltitle = {Journal of Cognitive Neuroscience},
  volume = {30},
  pages = {1086--1097},
  publisher = {{MIT Press}},
  location = {{US}},
  issn = {1530-8898(Electronic),0898-929X(Print)},
  doi = {10.1162/jocn_a_01301},
  abstract = {Previous work revealed that visual semantic information conveyed by gestures can enhance degraded speech comprehension, but the mechanisms underlying these integration processes under adverse listening conditions remain poorly understood. We used MEG to investigate how oscillatory dynamics support speech–gesture integration when integration load is manipulated by auditory (e.g., speech degradation) and visual semantic (e.g., gesture congruency) factors. Participants were presented with videos of an actress uttering an action verb in clear or degraded speech, accompanied by a matching (mixing gesture + “mixing”) or mismatching (drinking gesture + “walking”) gesture. In clear speech, alpha/beta power was more suppressed in the left inferior frontal gyrus and motor and visual cortices when integration load increased in response to mismatching versus matching gestures. In degraded speech, beta power was less suppressed over posterior STS and medial temporal lobe for mismatching compared with matching gestures, showing that integration load was lowest when speech was degraded and mismatching gestures could not be integrated and disambiguate the degraded signal. Our results thus provide novel insights on how low-frequency oscillatory modulations in different parts of the cortex support the semantic audiovisual integration of gestures in clear and degraded speech: When speech is clear, the left inferior frontal gyrus and motor and visual cortices engage because higher-level semantic information increases semantic integration load. When speech is degraded, posterior STS/middle temporal gyrus and medial temporal lobe are less engaged because integration load is lowest when visual semantic information does not aid lexical retrieval and speech and gestures cannot be integrated. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\69AGPL8B\\Drijvers et al. - 2018 - Alpha and beta oscillations index semantic congrue.pdf;C\:\\Users\\u668173\\Zotero\\storage\\2NANEHLX\\2018-32817-004.html;C\:\\Users\\u668173\\Zotero\\storage\\E8JSZ9RY\\2018-32817-004.html},
  keywords = {Alpha Rhythm,Beta Rhythm,Gestures,Oral Communication,Oscillatory Network},
  number = {8}
}

@article{drijversRapidInvisibleFrequency2020,
  title = {Rapid Invisible Frequency Tagging Reveals Nonlinear Integration of Auditory and Visual Semantic Information},
  author = {Drijvers, Linda and Spaak, Eelke and Jensen, Ole},
  date = {2020-04-30},
  journaltitle = {bioRxiv},
  pages = {2020.04.29.067454},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.04.29.067454},
  url = {https://www.biorxiv.org/content/10.1101/2020.04.29.067454v1},
  urldate = {2020-10-26},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}During communication in real-life settings, the brain integrates information from auditory and visual modalities to form a unified percept of our environment. In the current magnetoencephalography (MEG) study, we used rapid invisible frequency tagging (RIFT) to generate steady-state evoked fields and investigated the integration of audiovisual information in a semantic context. We presented participants with videos of an actress uttering action verbs (auditory; tagged at 61 Hz) accompanied by a gesture (visual; tagged at 68 Hz, using a projector with a 1440 Hz refresh rate). Integration ease was manipulated by auditory factors (clear/degraded speech) and visual factors (congruent/incongruent gesture). We identified MEG spectral peaks at the individual (61/68 Hz) tagging frequencies. We furthermore observed a peak at the intermodulation frequency of the auditory and visually tagged signals (f\textsubscript{visual} – f\textsubscript{auditory} = 7 Hz), specifically when integration was easiest (i.e., when speech was clear and accompanied by a congruent gesture). This intermodulation peak is a signature of nonlinear audiovisual integration, and was strongest in left inferior frontal gyrus and left temporal regions; areas known to be involved in speech-gesture integration. The enhanced power at the intermodulation frequency thus reflects the ease of integration and demonstrates that speech-gesture information interacts in higher-order language areas. Furthermore, we provide a proof-of-principle of the use of RIFT to study the integration of audiovisual stimuli, in relation to, for instance, semantic context.{$<$}/p{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\E7SQ9B8M\\Drijvers et al. - 2020 - Rapid invisible frequency tagging reveals nonlinea.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZHXFHHNV\\2020.04.29.html},
  langid = {english}
}

@article{drijversRapidInvisibleFrequency2020a,
  title = {Rapid Invisible Frequency Tagging Reveals Nonlinear Integration of Auditory and Visual Information},
  author = {Drijvers, Linda and Jensen, Ole and Spaak, Eelke},
  date = {2020-10-22},
  journaltitle = {bioRxiv},
  pages = {2020.04.29.067454},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.04.29.067454},
  url = {https://www.biorxiv.org/content/10.1101/2020.04.29.067454v2},
  urldate = {2020-12-05},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}During communication in real-life settings, the brain integrates information from auditory and visual modalities to form a unified percept of our environment. In the current magnetoencephalography (MEG) study, we used rapid invisible frequency tagging (RIFT) to generate steady-state evoked fields and investigated the integration of audiovisual information in a semantic context. We presented participants with videos of an actress uttering action verbs (auditory; tagged at 61 Hz) accompanied by a gesture (visual; tagged at 68 Hz, using a projector with a 1440 Hz refresh rate). Integration difficulty was manipulated by lower-order auditory factors (clear/degraded speech) and higher-order visual factors (congruent/incongruent gesture). We identified MEG spectral peaks at the individual (61/68 Hz) tagging frequencies. We furthermore observed a peak at the intermodulation frequency of the auditory and visually tagged signals (f\textsubscript{visual} - f\textsubscript{auditory} = 7 Hz), specifically when lower-order integration was easiest because signal quality was optimal. This intermodulation peak is a signature of nonlinear audiovisual integration, and was strongest in left inferior frontal gyrus and left temporal regions; areas known to be involved in speech-gesture integration. The enhanced power at the intermodulation frequency thus reflects the ease of lower-order audiovisual integration and demonstrates that speech-gesture information interacts in higher-order language areas. Furthermore, we provide a proof-of-principle of the use of RIFT to study the integration of audiovisual stimuli, in relation to, for instance, semantic context.{$<$}/p{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WXHVSRU7\\Drijvers et al. - 2020 - Rapid invisible frequency tagging reveals nonlinea.pdf;C\:\\Users\\u668173\\Zotero\\storage\\95JP8X57\\2020.04.29.html},
  langid = {english}
}

@article{drijversVisualContextEnhanced2017,
  title = {Visual {{Context Enhanced}}: {{The Joint Contribution}} of {{Iconic Gestures}} and {{Visible Speech}} to {{Degraded Speech Comprehension}}},
  shorttitle = {Visual {{Context Enhanced}}},
  author = {Drijvers, Linda and Özyürek, Asli},
  date = {2017-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  volume = {60},
  pages = {212--222},
  issn = {1092-4388, 1558-9102},
  doi = {10.1044/2016_JSLHR-H-16-0101},
  url = {http://pubs.asha.org/doi/10.1044/2016_JSLHR-H-16-0101},
  urldate = {2019-09-18},
  abstract = {Purpose: This study investigated whether and to what extent iconic co-speech gestures 7 8 contribute to information from visible speech to enhance degraded speech comprehension at 9 10 11 different levels of noise-vocoding. Previously, the contributions of these two visual 12 13 articulators to speech comprehension have only been studied separately. 14 15 16 Method: Twenty participants watched videos of an actress uttering an action verb and 17 18 completed a free-recall task. The videos were presented in three speech (2-band; 6-band For Peer Review 19 20 noise-vocoding; clear), three multimodal (Speech+Lips blurred; Speech+VisibleSpeech; 21 22 Speech+VisibleSpeech+Gesture) and two visual only conditions (VisibleSpeech; 23 24 25 VisibleSpeech+Gesture). 26 27 28 Results: Accuracy levels were higher when both visual articulators were present compared to 29 30 one or none. The enhancement effects of a) visible speech, b) gestural information on top of 31 32 visible speech and c) both visible speech and iconic gestures were larger in 6-band than 233 34 band noise-vocoding or visual only conditions. Gestural enhancement in 2-band noise35 36 37 vocoding did not differ from gestural enhancement in visual only conditions. 38 39 40 Conclusions: When perceiving degraded speech in a visual context, listeners benefit more 41 42},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UTLHPE5D\\Drijvers and Özyürek - 2017 - Visual Context Enhanced The Joint Contribution of.pdf},
  langid = {english},
  number = {1}
}

@article{dueckerNoEvidenceEntrainment2020,
  title = {No Evidence for Entrainment: Endogenous Gamma Oscillations and Rhythmic Flicker Responses Coexist in Visual Cortex},
  shorttitle = {No Evidence for Entrainment},
  author = {Duecker, Katharina and Gutteling, Tjerk P. and Herrmann, Christoph S. and Jensen, Ole},
  date = {2020-09-26},
  journaltitle = {bioRxiv},
  pages = {2020.09.02.279497},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.09.02.279497},
  url = {https://www.biorxiv.org/content/10.1101/2020.09.02.279497v2},
  urldate = {2020-12-05},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Motivated by the plethora of studies associating gamma oscillations (\emph{∼}30-100 Hz) with various neuronal processes, including inter-regional communication and neuroprotection, we asked if endogenous gamma oscillations in the human brain can be entrained by rhythmic photic stimulation. The photic drive produced a robust Magnetoencephalography (MEG) response in visual cortex up to frequencies of about 80 Hz. Strong, endogenous gamma oscillations were induced using moving grating stimuli as repeatedly shown in previous research. When superimposing the flicker and the gratings, there was no evidence for phase or frequency entrainment of the endogenous gamma oscillations by the photic drive. Rather – as supported by source modelling – our results show that the flicker response and the endogenous gamma oscillations coexist and are generated by different neuronal populations in visual cortex. Our findings challenge the notion that neuronal entrainment by visual stimulation generalises to cortical gamma oscillations.{$<$}/p{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TLNV3LF8\\Duecker et al. - 2020 - No evidence for entrainment endogenous gamma osci.pdf;C\:\\Users\\u668173\\Zotero\\storage\\2879TFTR\\2020.09.02.html},
  langid = {english}
}

@article{duffyARMGESTURESOUR,
  title = {{{ARM GESTURES}}, {{OUR GESTURES}}: {{THE ROLE OF CONDUCTING IN COORDINATING BREATHING PATTERNS AMONG SINGERS}}},
  author = {Duffy, William},
  pages = {3},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DCVVTWXE\\Duffy - ARM GESTURES, OUR GESTURES THE ROLE OF CONDUCTING.pdf},
  langid = {english}
}

@article{dumasInterBrainSynchronizationSocial2010,
  title = {Inter-{{Brain Synchronization}} during {{Social Interaction}}},
  author = {Dumas, Guillaume and Nadel, Jacqueline and Soussignan, Robert and Martinerie, Jacques and Garnero, Line},
  date = {2010-08-17},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {5},
  pages = {e12166},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0012166},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0012166},
  urldate = {2020-09-27},
  abstract = {During social interaction, both participants are continuously active, each modifying their own actions in response to the continuously changing actions of the partner. This continuous mutual adaptation results in interactional synchrony to which both members contribute. Freely exchanging the role of imitator and model is a well-framed example of interactional synchrony resulting from a mutual behavioral negotiation. How the participants' brain activity underlies this process is currently a question that hyperscanning recordings allow us to explore. In particular, it remains largely unknown to what extent oscillatory synchronization could emerge between two brains during social interaction. To explore this issue, 18 participants paired as 9 dyads were recorded with dual-video and dual-EEG setups while they were engaged in spontaneous imitation of hand movements. We measured interactional synchrony and the turn-taking between model and imitator. We discovered by the use of nonlinear techniques that states of interactional synchrony correlate with the emergence of an interbrain synchronizing network in the alpha-mu band between the right centroparietal regions. These regions have been suggested to play a pivotal role in social interaction. Here, they acted symmetrically as key functional hubs in the interindividual brainweb. Additionally, neural synchronization became asymmetrical in the higher frequency bands possibly reflecting a top-down modulation of the roles of model and imitator in the ongoing interaction.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4PJM3WM8\\Dumas et al. - 2010 - Inter-Brain Synchronization during Social Interact.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ICMUPZ7T\\article.html},
  keywords = {Behavior,Diencephalon,Electroencephalography,Functional magnetic resonance imaging,Hands,Imitation,Principal component analysis,Signal filtering},
  langid = {english},
  number = {8}
}

@book{dunbarHumanEvolutionOur2016,
  title = {Human {{Evolution}}: {{Our Brains}} and {{Behavior}}},
  author = {Dunbar, R.},
  date = {2016},
  publisher = {{Oxford University Press}},
  location = {{Oxford}}
}

@online{DynamicsMultipleSignalling,
  title = {Dynamics of Multiple Signalling Systems: Animal Communication in a World in Flux | {{Elsevier Enhanced Reader}}},
  shorttitle = {Dynamics of Multiple Signalling Systems},
  doi = {10.1016/j.tree.2009.11.003},
  url = {https://reader.elsevier.com/reader/sd/pii/S0169534709003450?token=5025236735D1260CACE1396EEDD537C42FB865B2301A20CC868105583859347FDC86D5B96C5CE1B9DEFE2046778DC01A},
  urldate = {2019-11-15},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4HW3XZPQ\\S0169534709003450.html},
  langid = {english}
}

@online{DynamicsMultipleSignallinga,
  title = {Dynamics of Multiple Signalling Systems: Animal Communication in a World in Flux | {{Elsevier Enhanced Reader}}},
  url = {https://reader.elsevier.com/reader/sd/pii/S0169534709003450?token=5025236735D1260CACE1396EEDD537C42FB865B2301A20CC868105583859347FDC86D5B96C5CE1B9DEFE2046778DC01A},
  urldate = {2019-11-15},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\28XCG4GC\\S0169534709003450.html}
}

@article{eagleNetworkDiversityEconomic2010,
  title = {Network {{Diversity}} and {{Economic Development}}},
  author = {Eagle, Nathan and Macy, Michael and Claxton, Rob},
  date = {2010-05-21},
  journaltitle = {Science},
  volume = {328},
  pages = {1029--1031},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1186605},
  url = {https://science.sciencemag.org/content/328/5981/1029},
  urldate = {2021-01-03},
  abstract = {Social diversity is associated with economic development. Social diversity is associated with economic development.},
  eprint = {20489022},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AYGVPQWQ\\tab-pdf.html},
  langid = {english},
  number = {5981}
}

@article{ebertCoordinationBreathingForearm2000,
  title = {Coordination between Breathing and Forearm Movements during Sinusoidal Tracking},
  author = {Ebert, Dietrich and Raßler, Beate and Hefter, Harald},
  date = {2000-01-01},
  journaltitle = {European Journal of Applied Physiology},
  shortjournal = {Eur J Appl Physiol},
  volume = {81},
  pages = {288--296},
  issn = {1439-6327},
  doi = {10.1007/s004210050045},
  url = {https://doi.org/10.1007/s004210050045},
  urldate = {2020-07-23},
  abstract = {Time relationships (coordination) between breathing and rhythmical limb movements were analyzed during sinusoidal tracking movements of the forearm in 11 healthy subjects. The tracking rate was varied systematically between 0.1 and 1.0 Hz in 0.1-Hz steps. The aim of the study was to elucidate whether rhythmical tracking movements can entrain breathing, and whether this entrainment depends upon the movement rate. Subjects exhibited coordination between tracking movements and breathing at various rate ratios (1:1, 1:2, 1:3). At tracking rates of between 0.2 and 0.6\,Hz, 1:1 coordination occurred with a maximum at 0.3\,Hz; this rate range was called the 1:1 entrainment band. Coordination of 1:2 occurred at between 0.5 and 1.0\,Hz (the 1:2 coordination band) with a maximum at 0.7\,Hz. Coordination of 1:3 could be detected at between 0.5 and 1.0\,Hz. Different subjects showed 1:n entrainment bands at similar locations but different widths of the rate range studied. The breathing rate during tracking was significantly higher than at rest, and it was correlated positively with tracking rate. This correlation, however, depended upon the width of the entrainment bands. Breathing rates varied between 0.2 and 0.6\,Hz for all coordination patterns. We conclude that the occurrence of fixed time relationships is an expression of the strength of central nervous system coupling between the two processes. The frequency of coordination between breathing and rhythmical tracking movements depends critically upon the movement rate.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\589WH4F8\\Ebert et al. - 2000 - Coordination between breathing and forearm movemen.pdf},
  langid = {english},
  number = {4}
}

@article{edelmanDegeneracyComplexityBiological2001,
  title = {Degeneracy and Complexity in Biological Systems},
  author = {Edelman, Gerald M. and Gally, Joseph A.},
  date = {2001-11-20},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {98},
  pages = {13763--13768},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.231499798},
  url = {https://www.pnas.org/content/98/24/13763},
  urldate = {2020-10-21},
  abstract = {Degeneracy, the ability of elements that are structurally different to perform the same function or yield the same output, is a well known characteristic of the genetic code and immune systems. Here, we point out that degeneracy is a ubiquitous biological property and argue that it is a feature of complexity at genetic, cellular, system, and population levels. Furthermore, it is both necessary for, and an inevitable outcome of, natural selection.},
  eprint = {11698650},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8EYKXEKP\\Edelman and Gally - 2001 - Degeneracy and complexity in biological systems.pdf;C\:\\Users\\u668173\\Zotero\\storage\\KCCJ2EAN\\13763.html},
  langid = {english},
  number = {24}
}

@incollection{edelsbrunnerPersistentHomologyTheory2013,
  title = {Persistent {{Homology}}: {{Theory}} and {{Practice}}},
  shorttitle = {Persistent {{Homology}}},
  booktitle = {European {{Congress}} of {{Mathematics Kraków}}, 2 – 7 {{July}}, 2012},
  author = {Edelsbrunner, Herbert and Morozov, Dmitriy},
  editor = {Latała, Rafał and Ruciński, Andrzej and Strzelecki, Paweł and Świątkowski, Jacek and Wrzosek, Dariusz and Zakrzewski, Piotr},
  date = {2013-11-30},
  pages = {31--50},
  publisher = {{European Mathematical Society Publishing House}},
  location = {{Zuerich, Switzerland}},
  doi = {10.4171/120-1/3},
  url = {http://www.ems-ph.org/doi/10.4171/120-1/3},
  urldate = {2020-03-11},
  abstract = {Persistent homology is a recent grandchild of homology that has found use in science and engineering as well as in mathematics. This paper surveys the method as well as the applications, neglecting completeness in favor of highlighting ideas and directions.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZW8JN79S\\Edelsbrunner and Morozov - 2013 - Persistent Homology Theory and Practice.pdf},
  isbn = {978-3-03719-120-0},
  langid = {english}
}

@article{edelsbrunnerTopologicalPersistenceSimplification2002,
  title = {Topological {{Persistence}} and {{Simplification}}},
  author = {{Edelsbrunner} and {Letscher} and {Zomorodian}},
  date = {2002-11},
  journaltitle = {Discrete \& Computational Geometry},
  volume = {28},
  pages = {511--533},
  issn = {0179-5376, 1432-0444},
  doi = {10.1007/s00454-002-2885-2},
  url = {http://link.springer.com/10.1007/s00454-002-2885-2},
  urldate = {2020-01-06},
  abstract = {We formalize a notion of topological simplification within the framework of a filtration, which is the history of a growing complex. We classify a topological change that happens during growth as either a feature or noise depending on its lifetime or persistence within the filtration. We give fast algorithms for computing persistence and experimental evidence for their speed and utility.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9Z9IY3DU\\Edelsbrunner et al. - 2002 - Topological Persistence and Simplification.pdf},
  langid = {english},
  number = {4}
}

@article{eerolaSharedPeriodicPerformer,
  title = {Shared Periodic Performer Movements Coordinate Interactions in Duo Improvisations},
  author = {Eerola, Tuomas and Jakubowski, Kelly and Moran, Nikki and Keller, Peter E. and Clayton, Martin},
  journaltitle = {Royal Society Open Science},
  shortjournal = {Royal Society Open Science},
  volume = {5},
  pages = {171520},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.171520},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.171520},
  urldate = {2021-03-16},
  abstract = {Human interaction involves the exchange of temporally coordinated, multimodal cues. Our work focused on interaction in the visual domain, using music performance as a case for analysis due to its temporally diverse and hierarchical structures. We made use of two improvising duo datasets—(i) performances of a jazz standard with a regular pulse and (ii) non-pulsed, free improvizations—to investigate whether human judgements of moments of interaction between co-performers are influenced by body movement coordination at multiple timescales. Bouts of interaction in the performances were manually annotated by experts and the performers’ movements were quantified using computer vision techniques. The annotated interaction bouts were then predicted using several quantitative movement and audio features. Over 80\% of the interaction bouts were successfully predicted by a broadband measure of the energy of the cross-wavelet transform of the co-performers’ movements in non-pulsed duos. A more complex model, with multiple predictors that captured more specific, interacting features of the movements, was needed to explain a significant amount of variance in the pulsed duos. The methods developed here have key implications for future work on measuring visual coordination in musical ensemble performances, and can be easily adapted to other musical contexts, ensemble types and traditions.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6KLZN5KR\\Eerola et al. - Shared periodic performer movements coordinate int.pdf},
  number = {2}
}

@online{EffectSyllableArticulation,
  title = {Effect of {{Syllable Articulation}} on {{Precision}} and {{Power Grip Performance}}},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0053061},
  urldate = {2020-02-24},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\X3RY27NN\\article.html}
}

@book{efronGestureRaceCulture1972,
  title = {Gesture, {{Race}} and {{Culture}}: {{A Tentative Study}} of the {{Spatio}}-Temporal and "Linguistic" {{Aspects}} of the {{Gestural Behavior}} of {{Eastern Jews}} and {{Southern Italians}} in {{New York City}}, {{Living Under Similar}} as {{Well}} as {{Different Environmental Conditions}}},
  shorttitle = {Gesture, {{Race}} and {{Culture}}},
  author = {Efron, David and Efron, John M. and Veen, Stuyvesant Van},
  date = {1972},
  publisher = {{Mouton}},
  eprint = {dsaNAAAAMAAJ},
  eprinttype = {googlebooks},
  langid = {english},
  pagetotal = {236}
}

@article{egermannProbabilisticModelsExpectation2013,
  title = {Probabilistic Models of Expectation Violation Predict Psychophysiological Emotional Responses to Live Concert Music},
  author = {Egermann, Hauke and Pearce, Marcus T. and Wiggins, Geraint A. and McAdams, Stephen},
  date = {2013-09},
  journaltitle = {Cognitive, Affective \& Behavioral Neuroscience},
  shortjournal = {Cogn Affect Behav Neurosci},
  volume = {13},
  pages = {533--553},
  issn = {1531-135X},
  doi = {10.3758/s13415-013-0161-y},
  abstract = {We present the results of a study testing the often-theorized role of musical expectations in inducing listeners' emotions in a live flute concert experiment with 50 participants. Using an audience response system developed for this purpose, we measured subjective experience and peripheral psychophysiological changes continuously. To confirm the existence of the link between expectation and emotion, we used a threefold approach. (1) On the basis of an information-theoretic cognitive model, melodic pitch expectations were predicted by analyzing the musical stimuli used (six pieces of solo flute music). (2) A continuous rating scale was used by half of the audience to measure their experience of unexpectedness toward the music heard. (3) Emotional reactions were measured using a multicomponent approach: subjective feeling (valence and arousal rated continuously by the other half of the audience members), expressive behavior (facial EMG), and peripheral arousal (the latter two being measured in all 50 participants). Results confirmed the predicted relationship between high-information-content musical events, the violation of musical expectations (in corresponding ratings), and emotional reactions (psychologically and physiologically). Musical structures leading to expectation reactions were manifested in emotional reactions at different emotion component levels (increases in subjective arousal and autonomic nervous system activations). These results emphasize the role of musical structure in emotion induction, leading to a further understanding of the frequently experienced emotional effects of music.},
  eprint = {23605956},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Adolescent,Adult,Arousal,Auditory Perception,Autonomic Nervous System,Emotions,Female,Humans,Male,Models; Statistical,Music,Psychophysiology,Young Adult},
  langid = {english},
  number = {3}
}

@article{egnorTrackingSilenceAdjusting2007,
  title = {Tracking Silence: Adjusting Vocal Production to Avoid Acoustic Interference},
  shorttitle = {Tracking Silence},
  author = {Egnor, S. E. Roian and Wickelgren, Jeanette Graham and Hauser, Marc D.},
  date = {2007-04-01},
  journaltitle = {Journal of Comparative Physiology A},
  shortjournal = {J Comp Physiol A},
  volume = {193},
  pages = {477--483},
  issn = {1432-1351},
  doi = {10.1007/s00359-006-0205-7},
  url = {https://doi.org/10.1007/s00359-006-0205-7},
  urldate = {2020-12-12},
  abstract = {Organisms that use vocal signals to communicate often modulate their vocalizations to avoid being masked by other sounds in the environment. Although some environmental noise is continuous, both biotic and abiotic noise can be intermittent, or even periodic. Interference from intermittent noise can be avoided if calls are timed to coincide with periods of silence, a capacity that is unambiguously present in insects, amphibians, birds, and humans. Surprisingly, we know virtually nothing about this fundamental capacity in nonhuman primates. Here we show that a New World monkey, the cotton-top tamarin (Saguinus oedipus), can restrict calls to periodic silent intervals in loud white noise. In addition, calls produced during these silent intervals were significantly louder than calls recorded in silent baseline sessions. Finally, average call duration dropped across sessions, indicating that experience with temporally patterned noise caused tamarins to compress their calls. Taken together, these results show that in the presence of a predictable, intermittent environmental noise, cotton-top tamarins are able to modify the duration, timing, and amplitude of their calls to avoid acoustic interference.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8ZV69TSV\\Egnor et al. - 2007 - Tracking silence adjusting vocal production to av.pdf},
  langid = {english},
  number = {4}
}

@article{eilersRolePrematuritySocioeconomic1993,
  title = {The Role of Prematurity and Socioeconomic Status in the Onset of Canonical Babbling in Infants},
  author = {Eilers, Rebecca E. and Oller, D. Kimbrough and Levine, Sharyse and Basinger, Devorah and Lynch, Michael P. and Urbano, Richard},
  date = {1993-07-01},
  journaltitle = {Infant Behavior and Development},
  shortjournal = {Infant Behavior and Development},
  volume = {16},
  pages = {297--315},
  issn = {0163-6383},
  doi = {10.1016/0163-6383(93)80037-9},
  url = {http://www.sciencedirect.com/science/article/pii/0163638393800379},
  urldate = {2020-12-02},
  abstract = {The onset of canonical babbling (implying production of well-formed syllables) is a landmark event in the development of the capacity for speech, capping a series of vocal stages of the infant's first year of life. Infants who are handicapped with regard to linguistic development are, in some cases, delayed in the onset of speech-like sounds such as canonical syllables. The age of onset of canonical babbling in infants born at risk, either due to prematurity or due to low socioeconomic status (SES) has not been extensively studied. This research, based on a longitudinal investigation of babbling and other motor milestones in term and preterm infants of middle and low SES, indicates that the onset of canonical babbling is robust with regard to such risk factors. Neither preterm infants whose ages were corrected for gestational age, nor infants of low SES were delayed in the onset of canonical babbling. In fact, at corrected ages, the preterm infants appeared to begin canonical babbling earlier than their full-term counterparts. It is suggested that the greater auditory experience of the preterms in this study may account for the early appearance of canonical babbling and hand banging, both of which can be viewed as rhythmic stereotypies that may require auditory feedback for normal development. Other motor milestones studied showed neither delay nor acceleration of onset in the same infants.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\W2FQQDGP\\0163638393800379.html},
  keywords = {canonical babbling,prelinguistic,prematurity,risk factors,vocal development,vocalizations},
  langid = {english},
  number = {3}
}

@article{eitanBeethovenLastPiano2010,
  title = {Beethoven's Last Piano Sonata and Those Who Follow Crocodiles: Cross-Domain Mappings of Auditory Pitch in a Musical Context},
  shorttitle = {Beethoven's Last Piano Sonata and Those Who Follow Crocodiles},
  author = {Eitan, Zohar and Timmers, Renee},
  date = {2010-03},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {114},
  pages = {405--422},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2009.10.013},
  abstract = {Though auditory pitch is customarily mapped in Western cultures onto spatial verticality (high-low), both anthropological reports and cognitive studies suggest that pitch may be mapped onto a wide variety of other domains. We collected a total number of 35 pitch mappings and investigated in four experiments how these mappings are used and structured. In particular, we inquired (1) how Western subjects apply Western and non-Western metaphors to "high" and "low" pitches, (2) whether mappings applied in an abstract conceptual task are similarly applied by listeners to actual music, (3) how mappings of spatial height relate to these pitch mappings, and (4) how mappings of "high" and "low" pitch associate with other dimensions, in particular quantity, size, intensity and valence. The results show strong agreement among Western participants in applying familiar and unfamiliar metaphors for pitch, in both an abstract, conceptual task (Exp. 1) and in a music listening task (Exp. 2), indicating that diverse cross-domain mappings for pitch exist latently besides the common verticality metaphor. Furthermore, limited overlap between mappings of spatial height and pitch height was found, suggesting that, the ubiquity of the verticality metaphor in Western usage notwithstanding, cross-domain pitch mappings are largely independent of that metaphor, and seem to be based upon other underlying dimensions. Part of the discrepancy between spatial height and pitch height is that, for pitch, "up" is not necessarily "more," nor is it necessarily "good." High pitch is only "more" for height, intensity and brightness. It is "less" for mass, size and quantity. We discuss implications of these findings for music and speech prosody, and their relevance to notions of embodied cognition and of cross-domain magnitude representation.},
  eprint = {20036356},
  eprinttype = {pmid},
  keywords = {Adult,Female,Humans,Male,Music,Pitch Perception,Speech Perception},
  langid = {english},
  number = {3}
}

@article{eitanHowMusicMoves2006,
  title = {How {{Music Moves}}: {{Musical Parameters}} and {{Listeners}}' {{Images}} of {{Motion}}},
  shorttitle = {How {{Music Moves}}},
  author = {Eitan, Zohar and Granot, Roni Y.},
  date = {2006},
  journaltitle = {Music Perception},
  volume = {23},
  pages = {221--247},
  publisher = {{University of California Press}},
  location = {{US}},
  issn = {1533-8312(Electronic),0730-7829(Print)},
  doi = {10.1525/mp.2006.23.3.221},
  abstract = {This article presents an empirical investigation of the ways listeners associate changes in musical parameters with physical space and bodily motion. In the experiments reported, participants were asked to associate melodic stimuli with imagined motions of a human character and to specify the type, direction, and pace-change of these motions, as well as the forces affecting them. The stimuli consisted of pairs of brief figures, one member of a pair presenting an "intensification" in a specific musical parameter, the other an "abatement" (e.g., crescendo vs. diminuendo, accelerando vs. ritardando). Musical parameters manipulated included dynamics, pitch contour, pitch intervals, attack rate, and articulation. Results indicate that most musical parameters significantly affect several dimensions of motion imagery. For instance, pitch contour affected imagined motion along all three spatial axes (not only verticality), as well as velocity and "energy." A surprising finding of this study is that musical-spatial analogies are often asymmetrical, as a musical change in one direction evokes a significantly stronger spatial analogy than its opposite. Such asymmetries include even the entrenched association of pitch change and spatial verticality, which applies mostly to pitch falls, but only weakly to rises. In general, musical abatements are strongly associated with spatial descents, while musical intensifications are generally associated with increasing speed rather than ascent. The implications of these results for notions of perceived musical space and for accounts of expressive musical gesture are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KS9246GT\\2006-03948-003.html},
  keywords = {Auditory Perception,Imagery,Motor Processes,Music,Music Perception,Stimulus Parameters},
  number = {3}
}

@article{eitanMusicalParametersListeners2006,
  title = {Musical {{Parameters}} and {{Listeners Images}} of {{Motion}}: : {{Musical Parameters}} and {{Listeners Images}} of {{Motion}}},
  author = {Eitan, Zohar and Granot, Roni Y.},
  date = {2006},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  volume = {23},
  pages = {221--248},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.23.3.221},
  eprint = {10.1525/mp.2006.23.3.221},
  eprinttype = {jstor},
  number = {3}
}

@article{ejiriCooccurencesPreverbalVocal2001,
  title = {Co-Occurences of Preverbal Vocal Behavior and Motor Action in Early Infancy},
  author = {Ejiri, Keiko and Masataka, Nobuo},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {40--48},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00147},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00147},
  urldate = {2020-01-26},
  abstract = {This study reports on co-occurrence of vocal behaviors and motor actions in infants in the prelinguistic stage. Four Japanese infants were studied longitudinally from the age of 6 months to 11 months. For all the infants, a 40 min sample was coded for each monthly period. The vocalizations produced by the infants co-occurred with their rhythmic actions with high frequency, particularly in the period preceding the onset of canonical babbling. Acoustical analysis was conducted on the vocalizations recorded before and after the period when co-occurrence took place most frequently. Among the vocalizations recorded in the period when co-occurrence appeared most frequently, those that co-occurred with rhythmic action had significantly shorter syllable duration and shorter formant-frequency transition duration compared with those that did not co-occur with rhythmic action. The rapid transitions and short syllables were similar to patterns of duration found in mature speech. The acoustic features remained even after co-occurrence disappeared. These findings suggest that co-occurrence of rhythmic action and vocal behavior may contribute to the infant’s acquisition of the ability to perform the rapid glottal and articulatory movements that are indispensable for spoken language acquisition.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7EB3ZUMA\\1467-7687.html},
  langid = {english},
  number = {1}
}

@article{ejiriCooccurencesPreverbalVocal2001a,
  title = {Co-Occurences of Preverbal Vocal Behavior and Motor Action in Early Infancy},
  author = {Ejiri, Keiko and Masataka, Nobuo},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {40--48},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00147},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00147},
  urldate = {2020-12-01},
  abstract = {This study reports on co-occurrence of vocal behaviors and motor actions in infants in the prelinguistic stage. Four Japanese infants were studied longitudinally from the age of 6 months to 11 months. For all the infants, a 40 min sample was coded for each monthly period. The vocalizations produced by the infants co-occurred with their rhythmic actions with high frequency, particularly in the period preceding the onset of canonical babbling. Acoustical analysis was conducted on the vocalizations recorded before and after the period when co-occurrence took place most frequently. Among the vocalizations recorded in the period when co-occurrence appeared most frequently, those that co-occurred with rhythmic action had significantly shorter syllable duration and shorter formant-frequency transition duration compared with those that did not co-occur with rhythmic action. The rapid transitions and short syllables were similar to patterns of duration found in mature speech. The acoustic features remained even after co-occurrence disappeared. These findings suggest that co-occurrence of rhythmic action and vocal behavior may contribute to the infant’s acquisition of the ability to perform the rapid glottal and articulatory movements that are indispensable for spoken language acquisition.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-7687.00147},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7XX7UFM9\\Ejiri and Masataka - 2001 - Co-occurences of preverbal vocal behavior and moto.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5YPGXKV3\\1467-7687.html},
  langid = {english},
  number = {1}
}

@article{ejiriRelationshipRhythmicBehavior1998,
  title = {Relationship between {{Rhythmic Behavior}} and {{Canonical Babbling}} in {{Infant Vocal Development}}},
  author = {Ejiri, Keiko},
  date = {1998},
  journaltitle = {Phonetica},
  volume = {55},
  pages = {226--237},
  issn = {1423-0321, 0031-8388},
  doi = {10.1159/000028434},
  url = {https://www.karger.com/Article/FullText/28434},
  urldate = {2020-01-26},
  abstract = {The onset of canonical babbling (CB) is a landmark event in infants’ vocal development for spoken language. Previous research has suggested that the onset of CB coincides with the peak period of rhythmic activities. To examine this phenomenon in detail, 28 Japanese infants (14 girls, 14 boys) were observed longitudinally from the age of 5 to 9 months. In the experimental sessions, an audible or an inaudible rattle was placed into a hand of each tested infant. Then the number of times that the infant shook the rattle was counted. In the observational sessions, infants’ spontaneous rhythmic activities under natural conditions were observed. The result shows that rhythmic activities reached their peak around the onset of CB. When the infants began to babble, they shook whichever rattle was in their hand, regardless of its audibility. After this period, they shook the audible rattles more frequently than the inaudible ones. These findings suggest that, around the onset of CB, infants learn to control their motor activities based on auditory feedback.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\N5V7PDX5\\Ejiri - 1998 - Relationship between Rhythmic Behavior and Canonic.pdf},
  langid = {english},
  number = {4}
}

@software{eklundBeeswarmBeeSwarm2016,
  title = {Beeswarm: {{The Bee Swarm Plot}}, an {{Alternative}} to {{Stripchart}}},
  shorttitle = {Beeswarm},
  author = {Eklund, Aron},
  date = {2016-04-25},
  url = {https://CRAN.R-project.org/package=beeswarm},
  urldate = {2019-04-23},
  abstract = {The bee swarm plot is a one-dimensional scatter plot like "stripchart", but with closely-packed, non-overlapping points.},
  version = {0.2.3}
}

@online{EmergenceCombinatorialStructure,
  title = {Emergence of Combinatorial Structure and Economy through Iterated Learning with Continuous Acoustic Signals | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.wocn.2014.02.005},
  url = {https://reader.elsevier.com/reader/sd/pii/S0095447014000205?token=269ED5275540ECD10962036E68A36E9B8E5FA4F5C3C613CD1A2C61CFF7FDE03A3AB6E9E71D87DDD3BFC3D7933ED769A6},
  urldate = {2020-03-09},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9BQHNLMM\\S0095447014000205.html},
  langid = {english}
}

@online{EmergenceDynamicalOrder,
  title = {Emergence of {{Dynamical Order}}: {{Synchronization Phenomena}} in {{Complex Systems}} - {{Susanna C}}. {{Manrubia}}, {{Alexander S}}. {{Mikhailov}}, {{Dam}}¡an {{H}}. {{Zannette}} - {{Google Books}}},
  url = {https://books.google.nl/books?id=w6RpDQAAQBAJ&pg=PA340&lpg=PA340&dq=pikovsky+emergence&source=bl&ots=uKWbVFXqRZ&sig=ACfU3U1zs5_xWCrVeyw3FXY_clGXvKTRCA&hl=en&sa=X&ved=2ahUKEwj5l6S516zjAhUrIMUKHW7QBCYQ6AEwAHoECAkQAQ#v=onepage&q=pikovsky%20emergence&f=false},
  urldate = {2019-07-11},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QRZ95AGX\\books.html}
}

@book{enfieldAnatomyMeaningSpeech2009,
  title = {The Anatomy of Meaning: {{Speech}}, Gesture, and Composite Utterances},
  shorttitle = {The {{Anatomy}} of {{Meaning}}},
  author = {Enfield, N. J.},
  date = {2009},
  publisher = {{Cambridge University Press}},
  abstract = {How do we understand what others are trying to say? The answer cannot be found in language alone. Words are linked to hand gestures and other visible phenomena to create unified 'composite utterances'. In this book N. J. Enfield presents original case studies of speech-with-gesture based on fieldwork carried out with speakers of Lao (a language of Southeast Asia). He examines pointing gestures (including lip and finger-pointing) and illustrative gestures (examples include depicting fish traps and tracing kinship relations). His detailed analyses focus on the 'semiotic unification' problem, that is, how to make a single interpretation when multiple signs occur together. Enfield's arguments have implications for all branches of science with a stake in meaning and its place in human social life. The book will appeal to all researchers interested in the study of meaning, including linguists, anthropologists, and psychologists.},
  isbn = {978-0-521-88064-0},
  keywords = {Language Arts & Disciplines / Linguistics / General,Language Arts & Disciplines / Linguistics / Psycholinguistics,Language Arts & Disciplines / Linguistics / Semantics,Language Arts & Disciplines / Linguistics / Sociolinguistics,Social Science / Anthropology / General},
  langid = {english},
  pagetotal = {267}
}

@book{enfieldNaturalCausesLanguage2014,
  title = {Natural Causes of Language: {{Frames}}, Biases, and Cultural Transmission},
  shorttitle = {Natural Causes of Language},
  author = {Enfield, N. J.},
  date = {2014-11-21},
  publisher = {{Language Science Press}},
  abstract = {What causes a language to be the way it is? Some features are universal, some are inherited, others are borrowed, and yet others are internally innovated. But no matter where a bit of language is from, it will only exist if it has been diffused and kept in circulation through social interaction in the history of a community. This book makes the case that a proper understanding of the ontology of language systems has to be grounded in the causal mechanisms by which linguistic items are socially transmitted, in communicative contexts. A \textbackslash textit\{biased transmission\} model provides a basis for understanding why certain things and not others are likely to develop, spread, and stick in languages. Because bits of language are always parts of systems, we also need to show how it is that items of knowledge and behavior become structured wholes. The book argues that to achieve this, we need to see how causal processes apply in multiple frames or ``time scales\&\#39;\&\#39; simultaneously, and we need to understand and address each and all of these frames in our work on language. This forces us to confront implications that are not always comfortable: for example, that ``a language\&\#39;\&\#39; is not a real thing but a convenient fiction, that language-internal and language-external processes have a lot in common, and that tree diagrams are poor conceptual tools for understanding the history of languages. By exploring avenues for clear solutions to these problems, this book suggests a conceptual framework for ultimately explaining, in causal terms, what languages are like and why they are like that.\vphantom\{\}},
  eprint = {RIabBQAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-3-944675-50-3},
  keywords = {Language Arts & Disciplines / Linguistics / General},
  langid = {english},
  pagetotal = {97}
}

@book{enfieldNaturalCausesLanguage2016,
  title = {Natural causes of language: Frames, biases, and cultural transmission},
  shorttitle = {Natural causes of language},
  author = {Enfield, N.J.},
  date = {2016-12-12},
  publisher = {{Language Science Press}},
  location = {{Berlin}},
  url = {https://langsci-press.org/catalog/book/48},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SW5ZP9CB\\48.html},
  langid = {en\_},
  series = {Conceptual Foundations of Language Science}
}

@article{enfieldRelationshipThinkingHuman2009,
  title = {Relationship Thinking and Human Pragmatics},
  author = {Enfield, N. J.},
  date = {2009-01-01},
  journaltitle = {Journal of Pragmatics},
  shortjournal = {Journal of Pragmatics},
  volume = {41},
  pages = {60--78},
  issn = {0378-2166},
  doi = {10.1016/j.pragma.2008.09.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0378216608002014},
  urldate = {2021-03-09},
  abstract = {The approach to pragmatics explored in this article focuses on elements of social interaction which are of universal relevance, and which may provide bases for a comparative approach. The discussion is anchored by reference to a fragment of conversation from a video-recording of Lao speakers during a home visit in rural Laos. The following points are discussed. First, an understanding of the full richness of context is indispensable for a proper understanding of any interaction. Second, human relationships are a primary locus of social organization, and as such constitute a key focus for pragmatics. Third, human social intelligence forms a universal cognitive under-carriage for interaction, and requires careful cross-cultural study. Fourth, a neo-Peircean framework for a general understanding of semiotic processes gives us a way of stepping away from language as our basic analytical frame. It is argued that in order to get a grip on pragmatics across human groups, we need to take a comparative approach in the biological sense—i.e. with reference to other species as well. From this perspective, human pragmatics is about using semiotic resources to try to meet goals in the realm of social relationships.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\F5GEAYUC\\Enfield - 2009 - Relationship thinking and human pragmatics.pdf;C\:\\Users\\u668173\\Zotero\\storage\\SPJJ9K68\\S0378216608002014.html},
  keywords = {Comparative pragmatics,Conversation,Lao language,Relationships,Semiotics,Social intelligence},
  langid = {english},
  number = {1},
  series = {Towards an {{Emancipatory Pragmatics}}}
}

@article{engesserCombinatorialityVocalSystems2019,
  title = {Combinatoriality in the Vocal Systems of Nonhuman Animals},
  author = {Engesser, Sabrina and Townsend, Simon W.},
  date = {2019},
  journaltitle = {WIREs Cognitive Science},
  volume = {10},
  pages = {e1493},
  issn = {1939-5086},
  doi = {10.1002/wcs.1493},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1493},
  urldate = {2020-03-25},
  abstract = {A key challenge in the field of human language evolution is the identification of the selective conditions that gave rise to language's generative nature. Comparative data on nonhuman animals provides a powerful tool to investigate similarities and differences among nonhuman and human communication systems and to reveal convergent evolutionary mechanisms. In this article, we provide an overview of the current evidence for combinatorial structures found in the vocal system of diverse species. We show that considerable structural diversity exits across and within species in the forms of combinatorial structures used. Based on this we suggest that a fine-grained classification and differentiation of combinatoriality is a useful approach permitting systematic comparisons across animals. Specifically, this will help to identify factors that might promote the emergence of combinatoriality and, crucially, whether differences in combinatorial mechanisms might be driven by variations in social and ecological conditions or cognitive capacities. This article is categorized under: Cognitive Biology {$>$} Evolutionary Roots of Cognition Linguistics {$>$} Evolution of Language},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.1493},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VH6GGANA\\Engesser and Townsend - 2019 - Combinatoriality in the vocal systems of nonhuman .pdf;C\:\\Users\\u668173\\Zotero\\storage\\7NH859KG\\wcs.html},
  keywords = {animal communication,combinatoriality,language evolution},
  langid = {english},
  number = {4}
}

@article{erikssonCoordinatedMandibularHeadneck2000,
  title = {Co-Ordinated Mandibular and Head-Neck Movements during Rhythmic Jaw Activities in Man},
  author = {Eriksson, P. O. and Häggman-Henrikson, B. and Nordh, E. and Zafar, H.},
  date = {2000-06},
  journaltitle = {Journal of Dental Research},
  volume = {79},
  pages = {1378--1384},
  issn = {0022-0345},
  doi = {10.1177/00220345000790060501},
  abstract = {Recent observations in man of concomitant mandibular and head movements during single maximal jaw-opening/-closing tasks suggest a close functional relationship between the mandibular and the head-neck motor systems. This study was aimed at further testing of the hypothesis of a functional integration between the human jaw and neck regions. Spatiotemporal characteristics of mandibular and associated head movements were evaluated for 3 different modes of rhythmic jaw activities: self-paced continuous maximal jaw-opening/-closing movements, paced continuous maximal jaw-opening/-closing movements at 50 cycles/minute, and unilateral chewing. Mandibular and head-neck movements were simultaneously recorded in 12 healthy young adults, by means of a wireless opto-electronic system for 3-D movement recordings, with retro-reflective markers attached to the lower (mandible) and upper (head) incisors. The results showed that rhythmic mandibular movements were paralleled by head movements. An initial change in head position (head extension) was seen at the start of the first jaw-movement cycle, and this adjusted head position was retained during the following cycles. In addition to this prevailing head extension, the maximal jaw-opening/-closing cycles were paralleled by head extension-flexion movements, and in general the start of these head movements preceded the start of the mandibular movements. The results support the idea of a functional relationship between the temporomandibular and the cranio-cervical neuromuscular systems. We therefore suggest a new concept for human jaw function, in which "functional jaw movements" are the result of activation of jaw as well as neck muscles, leading to simultaneous movements in the temporomandibular, atlanto-occipital, and cervical spine joints.},
  eprint = {10890716},
  eprinttype = {pmid},
  keywords = {Adult,Atlanto-Occipital Joint,Cervical Vertebrae,Dental Occlusion,Electronics,Female,Head Movements,Humans,Image Processing; Computer-Assisted,Male,Mandible,Mastication,Masticatory Muscles,Middle Aged,Monitoring; Physiologic,Movement,Neck,Neck Muscles,Neuromuscular Junction,Optics and Photonics,Periodicity,Statistics as Topic,Temporomandibular Joint,Time Factors},
  langid = {english},
  number = {6}
}

@article{escoffierAuditoryRhythmsEntrain2015,
  title = {Auditory Rhythms Entrain Visual Processes in the Human Brain: {{Evidence}} from Evoked Oscillations and Event-Related Potentials},
  shorttitle = {Auditory Rhythms Entrain Visual Processes in the Human Brain},
  author = {Escoffier, Nicolas and Herrmann, Christoph S. and Schirmer, Annett},
  date = {2015-05-01},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {111},
  pages = {267--276},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2015.02.024},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811915001251},
  urldate = {2020-12-12},
  abstract = {Temporal regularities in the environment are thought to guide the allocation of attention in time. Here, we explored whether entrainment of neuronal oscillations underpins this phenomenon. Participants viewed a regular stream of images in silence, or in-synchrony or out-of-synchrony with an unmarked beat position of a slow (1.3Hz) auditory rhythm. Focusing on occipital recordings, we analyzed evoked oscillations shortly before and event-related potentials (ERPs) shortly after image onset. The phase of beta-band oscillations in the in-synchrony condition differed from that in the out-of-synchrony and silence conditions. Additionally, ERPs revealed rhythm effects for a stimulus onset potential (SOP) and the N1. Both were more negative for the in-synchrony as compared to the out-of-synchrony and silence conditions and their amplitudes positively correlated with the beta phase effects. Taken together, these findings indicate that rhythmic expectations are supported by a reorganization of neural oscillations that seems to benefit stimulus processing at expected time points. Importantly, this reorganization emerges from global rhythmic cues, across modalities, and for frequencies significantly higher than the external rhythm. As such, our findings support the idea that entrainment of neuronal oscillations represents a general mechanism through which the brain uses predictive elements in the environment to optimize attention and stimulus perception.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PPHJTCS6\\S1053811915001251.html},
  keywords = {Dynamic attending,Temporal preparation,Time perception,Timing},
  langid = {english}
}

@article{esteve-gibertInfantsTemporallyCoordinate2014,
  title = {Infants Temporally Coordinate Gesture-Speech Combinations before They Produce Their First Words},
  author = {Esteve-Gibert, Núria and Prieto, Pilar},
  date = {2014-02-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {57},
  pages = {301--316},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2013.06.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639313000721},
  urldate = {2020-12-05},
  abstract = {This study explores the patterns of gesture and speech combinations from the babbling period to the one-word stage and the temporal alignment between the two modalities. The communicative acts of four Catalan children at 0;11, 1;1, 1;3, 1;5, and 1;7 were gesturally and acoustically analyzed. Results from the analysis of a total of 4,507 communicative acts extracted from approximately 24h of at-home recordings showed that (1) from the early single-word period onwards gesture starts being produced mainly in combination with speech rather than as a gesture-only act; (2) in these early gesture-speech combinations most of the gestures are deictic gestures (pointing and reaching gestures) with a declarative communicative purpose; and (3) there is evidence of temporal coordination between gesture and speech already at the babbling stage because gestures start before the vocalizations associated with them, the stroke onset coincides with the onset of the prominent syllable in speech, and the gesture apex is produced before the end of the accented syllable. These results suggest that during the transition between the babbling stage and single-word period infants start combining deictic gestures and speech and, when combined, the two modalities are temporally coordinated.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9RHKE6MN\\Esteve-Gibert and Prieto - 2014 - Infants temporally coordinate gesture-speech combi.pdf;C\:\\Users\\u668173\\Zotero\\storage\\HNA75C5Z\\S0167639313000721.html},
  keywords = {Early acquisition of multimodality,Early gesture-speech temporal coordination,Early gestures},
  langid = {english}
}

@article{esteve-gibertProsodicStructureShapes2013,
  title = {Prosodic Structure Shapes the Temporal {{Realization}} of Intonation and Manual Gesture Movements},
  author = {Esteve-Gibert, N. and Prieto, P.},
  date = {2013-06-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {56},
  pages = {850--864},
  doi = {10.1044/1092-4388(2012/12-0049)},
  url = {https://jslhr.pubs.asha.org/doi/full/10.1044/1092-4388%282012/12-0049%29},
  urldate = {2019-04-16},
  abstract = {Purpose       Previous work on the temporal coordination between gesture and speech found that the          prominence in gesture coordinates with speech prominence. In this study, the authors          investigated the anchoring regions in speech and pointing gesture that align with          each other. The authors hypothesized that (a) in contrastive focus conditions, the          gesture apex is anchored in the intonation peak and (b) the upcoming prosodic boundary          influences the timing of gesture and intonation movements.                     Method       Fifteen Catalan speakers pointed at a screen while pronouncing a target word with          different metrical patterns in a contrastive focus condition and followed by a phrase          boundary. A total of 702 co-speech deictic gestures were acoustically and gesturally          analyzed.                     Results       Intonation peaks and gesture apexes showed parallel behavior with respect to their          position within the accented syllable: They occurred at the end of the accented syllable          in non–phrase-final position, whereas they occurred well before the end of the accented          syllable in phrase-final position. Crucially, the position of intonation peaks and          gesture apexes was correlated and was bound by prosodic structure.                     Conclusions       The results refine the phonological synchronization rule (McNeill, 1992), showing          that gesture apexes are anchored in intonation peaks and that gesture and prosodic          movements are bound by prosodic phrasing.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5K5WWP56\\Esteve-Gibert Núria and Prieto Pilar - 2013 - Prosodic Structure Shapes the Temporal Realization.pdf;C\:\\Users\\u668173\\Zotero\\storage\\QM79RK99\\12-0049).html},
  number = {3}
}

@article{esteve-gibertProsodyAuditoryVisual2018,
  title = {Prosody in the {{Auditory}} and {{Visual Domains}}: {{A Developmental Perspective}}},
  author = {Esteve-Gibert, N. and Guellai, B.},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  volume = {9},
  pages = {1--10},
  doi = {10.3389/fpsyg.2018.00338},
  url = {http://www.readcube.com/articles/10.3389/fpsyg.2018.00338},
  urldate = {2019-08-09},
  number = {338}
}

@article{everettClimateVocalFolds2015,
  title = {Climate, Vocal Folds, and Tonal Languages: {{Connecting}} the Physiological and Geographic Dots},
  shorttitle = {Climate, Vocal Folds, and Tonal Languages},
  author = {Everett, Caleb and Blasi, Damián E. and Roberts, Seán G.},
  date = {2015-02-03},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {112},
  pages = {1322--1327},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1417413112},
  url = {https://www.pnas.org/content/112/5/1322},
  urldate = {2020-08-27},
  abstract = {We summarize a number of findings in laryngology demonstrating that perturbations of phonation, including increased jitter and shimmer, are associated with desiccated ambient air. We predict that, given the relative imprecision of vocal fold vibration in desiccated versus humid contexts, arid and cold ecologies should be less amenable, when contrasted to warm and humid ecologies, to the development of languages with phonemic tone, especially complex tone. This prediction is supported by data from two large independently coded databases representing 3,700+ languages. Languages with complex tonality have generally not developed in very cold or otherwise desiccated climates, in accordance with the physiologically based predictions. The predicted global geographic–linguistic association is shown to operate within continents, within major language families, and across language isolates. Our results offer evidence that human sound systems are influenced by environmental factors.},
  eprint = {25605876},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\A76MHWIN\\Everett et al. - 2015 - Climate, vocal folds, and tonal languages Connect.pdf;C\:\\Users\\u668173\\Zotero\\storage\\QLBM5QV2\\1322.html},
  keywords = {adaptation,climate,language,tone},
  langid = {english},
  number = {5}
}

@online{EvolutionaryDynamicsDispersal,
  title = {Evolutionary Dynamics in the Dispersal of Sign Languages | {{Royal Society Open Science}}},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.191100},
  urldate = {2020-02-20},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3CXKSNQZ\\rsos.html}
}

@article{fahlmanRespiratoryFunctionMechanics2017,
  title = {Respiratory Function and Mechanics in Pinnipeds and Cetaceans},
  author = {Fahlman, Andreas and Moore, Michael J. and Garcia-Parraga, Daniel},
  date = {2017-05-15},
  journaltitle = {Journal of Experimental Biology},
  volume = {220},
  pages = {1761--1773},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.126870},
  url = {https://jeb.biologists.org/content/220/10/1761},
  urldate = {2020-09-19},
  abstract = {Skip to Next Section In this Review, we focus on the functional properties of the respiratory system of pinnipeds and cetaceans, and briefly summarize the underlying anatomy; in doing so, we provide an overview of what is currently known about their respiratory physiology and mechanics. While exposure to high pressure is a common challenge among breath-hold divers, there is a large variation in respiratory anatomy, function and capacity between species – how are these traits adapted to allow the animals to withstand the physiological challenges faced during dives? The ultra-deep diving feats of some marine mammals defy our current understanding of respiratory physiology and lung mechanics. These animals cope daily with lung compression, alveolar collapse, transient hyperoxia and extreme hypoxia. By improving our understanding of respiratory physiology under these conditions, we will be better able to define the physiological constraints imposed on these animals, and how these limitations may affect the survival of marine mammals in a changing environment. Many of the respiratory traits to survive exposure to an extreme environment may inspire novel treatments for a variety of respiratory problems in humans.},
  eprint = {28515170},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\X3VD5AVX\\Fahlman et al. - 2017 - Respiratory function and mechanics in pinnipeds an.pdf;C\:\\Users\\u668173\\Zotero\\storage\\GG7DWZL2\\1761.html},
  langid = {english},
  number = {10}
}

@report{fairhurstReciprocityAlignmentQuantifying2019,
  title = {Reciprocity and Alignment: Quantifying Coupling in Dynamic Interactions},
  shorttitle = {Reciprocity and Alignment},
  author = {Fairhurst, Merle Theresa and Dumas, Guillaume},
  date = {2019-03-25},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/nmg4x},
  url = {https://osf.io/nmg4x},
  urldate = {2020-04-10},
  abstract = {Recent accounts of social cognition focus on how we do things together suggesting that becoming aligned relies on a reciprocal exchange of information. The next step is to develop richer computational methods that quantify the degree of coupling and describe the nature of the information exchange. We put forward a definition of coupling comparing it to related terminology and detail available computational methods and the level of organisation to which they pertain, presenting them as a hierarchy from weakest to richest forms of coupling. The rationale is that a temporally coherent link between two dynamical systems at the lowest level of organisation sustains mutual adaptation and alignment at the highest level. Postulating that when we do things together, we do so dynamically over time, we argue that to determine and measure instances of true reciprocity in social exchanges is key. Along with this computationally rich definition of coupling, we present challenges for the field to be tackled by a diverse community working towards a dynamic account of social cognition.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IG85C3SH\\Fairhurst and Dumas - 2019 - Reciprocity and alignment quantifying coupling in.pdf},
  langid = {english},
  type = {preprint}
}

@article{faisalNoiseNervousSystem2008,
  title = {Noise in the Nervous System},
  author = {Faisal, A. Aldo and Selen, Luc P. J. and Wolpert, Daniel M.},
  date = {2008-04},
  journaltitle = {Nature Reviews. Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {9},
  pages = {292--303},
  issn = {1471-0048},
  doi = {10.1038/nrn2258},
  abstract = {Noise--random disturbances of signals--poses a fundamental problem for information processing and affects all aspects of nervous-system function. However, the nature, amount and impact of noise in the nervous system have only recently been addressed in a quantitative manner. Experimental and computational methods have shown that multiple noise sources contribute to cellular and behavioural trial-to-trial variability. We review the sources of noise in the nervous system, from the molecular to the behavioural level, and show how noise contributes to trial-to-trial variability. We highlight how noise affects neuronal networks and the principles the nervous system applies to counter detrimental effects of noise, and briefly discuss noise's potential benefits.},
  eprint = {18319728},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\D73GUUIV\\Faisal et al. - 2008 - Noise in the nervous system.pdf},
  keywords = {Animals,Humans,Information Theory,Models; Neurological,Nerve Net,Nervous System,Nervous System Physiological Phenomena,Neural Networks; Computer,Neural Pathways,Noise},
  langid = {english},
  number = {4},
  pmcid = {PMC2631351}
}

@article{falandaysInteractionismLanguageNeural2018,
  title = {Interactionism in Language: {{From}} Neural Networks to Bodies to Dyads},
  shorttitle = {Interactionism in Language},
  author = {Falandays, J. Benjamin and Batzloff, Brandon J. and Spevack, Samuel C. and Spivey, Michael J.},
  date = {2018},
  journaltitle = {Language, Cognition and Neuroscience},
  pages = {No Pagination Specified-No Pagination Specified},
  issn = {2327-3801(Electronic),2327-3798(Print)},
  doi = {10.1080/23273798.2018.1501501},
  abstract = {In a science of language, it can be useful to partition different formats of linguistic information into different categories, such as phonetics, phonology, semantics, and syntax. However, when the actual phenomena of language processing cross those boundaries and blur those lines, it can become difficult to understand how these different formats of linguistic information maintain their integrity while engaging in complex interactions with one another. For example, if the function of a cortical network that is known to process phonetics is immediately taking into account contextual influences from a cortical network that is known to process semantics, then it seems clear that this “phonetic cortical network” is doing more than just phonetics. In the neuroscience and cognitive science of language, the scope of analysis where different formats of linguistic information are seen to interact reveals a wide array of context effects in almost every possible direction. When one expands the scope of analysis to include nonlinguistic sensory modalities, such as vision and action, research is showing that even those lines are getting blurred. Visual perception and motor movement appear to influence various aspects of language processing in real time. As this scope of analysis expands further still, research is showing that two human brains and bodies exhibit various forms of synchrony or coordination with one another during natural conversation. Interactionism at all these levels of analysis poses a challenge to traditional frameworks that treat different components of language, perception, and action as operating via domain specific computations. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7GGTYQQG\\2018-36198-001.html}
}

@article{falkCharacterizationAtypicalVocal2012,
  title = {Characterization of Atypical Vocal Source Excitation, Temporal Dynamics and Prosody for Objective Measurement of Dysarthric Word Intelligibility},
  author = {Falk, Tiago H. and Chan, Wai-Yip and Shein, Fraser},
  date = {2012-06-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {54},
  pages = {622--631},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2011.03.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639311000513},
  urldate = {2020-11-06},
  abstract = {Objective measurement of dysarthric speech intelligibility can assist clinicians in the diagnosis of speech disorder severity as well as in the evaluation of dysarthria treatments. In this paper, several objective measures are proposed and tested as correlates of subjective intelligibility. More specifically, the kurtosis of the linear prediction residual is proposed as a measure of vocal source excitation oddity. Additionally, temporal perturbations resultant from imprecise articulation and atypical speech rates are characterized by short- and long-term temporal dynamics measures, which in turn, are based on log-energy dynamics and on an auditory-inspired modulation spectral signal representation, respectively. Motivated by recent insights in the communication disorders literature, a composite measure is developed based on linearly combining a salient subset of the proposed measures with conventional prosodic parameters. Experiments with the publicly-available ‘Universal Access’ database of spastic dysarthric speech (10 patient speakers; 300 words spoken in isolation, per speaker) show that the proposed composite measure can achieve correlation with subjective intelligibility ratings as high as 0.97; thus the measure can serve as an accurate indicator of dysarthric speech intelligibility.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5RY4ST7N\\Falk et al. - 2012 - Characterization of atypical vocal source excitati.pdf;C\:\\Users\\u668173\\Zotero\\storage\\SX5I623U\\S0167639311000513.html},
  keywords = {Dysarthria,Intelligibility,Linear prediction,Temporal dynamics,Vocal source excitation},
  langid = {english},
  number = {5},
  series = {Advanced {{Voice Function Assessment}}}
}

@article{falkHierarchicalOrganizationTemporal2017,
  title = {Hierarchical Organization in the Temporal Structure of Infant-Direct Speech and Song},
  author = {Falk, Simone and Kello, Christopher T.},
  date = {2017-06-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {163},
  pages = {80--86},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2017.02.017},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027717300574},
  urldate = {2020-12-13},
  abstract = {Caregivers alter the temporal structure of their utterances when talking and singing to infants compared with adult communication. The present study tested whether temporal variability in infant-directed registers serves to emphasize the hierarchical temporal structure of speech. Fifteen German-speaking mothers sang a play song and told a story to their 6-months-old infants, or to an adult. Recordings were analyzed using a recently developed method that determines the degree of nested clustering of temporal events in speech. Events were defined as peaks in the amplitude envelope, and clusters of various sizes related to periods of acoustic speech energy at varying timescales. Infant-directed speech and song clearly showed greater event clustering compared with adult-directed registers, at multiple timescales of hundreds of milliseconds to tens of seconds. We discuss the relation of this newly discovered acoustic property to temporal variability in linguistic units and its potential implications for parent-infant communication and infants learning the hierarchical structures of speech and language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TLBYSMKG\\S0010027717300574.html},
  keywords = {Hierarchical nested clustering,Infant-directed communication,Language development,Speech and song,Temporal variability},
  langid = {english}
}

@article{falkItBetterWhen2016,
  title = {It Is Better When Expected: Aligning Speech and Motor Rhythms Enhances Verbal Processing},
  shorttitle = {It Is Better When Expected},
  author = {Falk, Simone and Bella, Simone Dalla},
  date = {2016-05-27},
  journaltitle = {Language, Cognition and Neuroscience},
  volume = {31},
  pages = {699--708},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2016.1144892},
  url = {https://doi.org/10.1080/23273798.2016.1144892},
  urldate = {2020-11-02},
  abstract = {Rhythm is a powerful way to shape the processing of complex sounds such as speech or music by generating temporal expectancies in the listener. Here, we investigated if multisensory expectancies generated by aligning speech and motor rhythms may enhance verbal processing. Participants listened to rhythmically regular German sentences and detected word changes occurring on stressed or unstressed syllables. Participants were cued to produce finger taps simultaneously with the auditory speech rhythm. Finger taps were aligned or misaligned with stressed syllables. Detection of word changes was facilitated when manual movements were temporally aligned with the auditory speech rhythm. Moreover, motor alignment enhanced sensitivity to detect changes on stressed syllables compared to a perceptual control condition. Thus, rhythmic speech structure reinforced by concurrent movement in multisensory contexts has beneficial effects on verbal processing. This finding lends support to models of expectancy-driven speech processing.},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2016.1144892},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4DP49B3I\\Falk and Bella - 2016 - It is better when expected aligning speech and mo.pdf;C\:\\Users\\u668173\\Zotero\\storage\\TZXQBMYP\\23273798.2016.html},
  keywords = {multisensory rhythm,Speech perception,temporal expectancies,temporal prediction,verbal processing},
  number = {5}
}

@article{farmerVentilationGasExchange2000,
  title = {Ventilation and Gas Exchange during Treadmill Locomotion in the {{American}} Alligator ({{Alligator}} Mississippiensis)},
  author = {Farmer, C. G. and Carrier, D. R.},
  date = {2000-06},
  journaltitle = {The Journal of Experimental Biology},
  shortjournal = {J. Exp. Biol.},
  volume = {203},
  pages = {1671--1678},
  issn = {0022-0949},
  abstract = {A number of anatomical characters of crocodilians appear to be inconsistent with their lifestyle as sit-and-wait predators. To address this paradoxical association of characters further, we measured lung ventilation and respiratory gas exchange during walking in American alligators (Alligator mississippiensis). During exercise, ventilation consisted of low-frequency, large-volume breaths. The alligators hyperventilated severely during walking with respect to their metabolic demands. Air convection requirements were among the highest and estimates of lung P(CO2) were among the lowest known in air-breathing vertebrates. Air convection requirements dropped immediately with cessation of exercise. These observations indicate that the ventilation of alligators is not limited by their locomotor movements. We suggest that the highly specialized ventilatory system of modern crocodilians represents a legacy from cursorial ancestors rather than an adaptation to a lifestyle as amphibious sit-and-wait predators.},
  eprint = {10804157},
  eprinttype = {pmid},
  issue = {Pt 11},
  keywords = {Alligators and Crocodiles,Animals,Motor Activity,Oxygen Consumption,Physical Exertion,Pulmonary Gas Exchange,Pulmonary Ventilation},
  langid = {english}
}

@article{fayInteractiveEvolutionHuman2010,
  title = {The Interactive Evolution of Human Communication Systems},
  author = {Fay, Nicolas and Garrod, Simon and Roberts, Leo and Swoboda, Nik},
  date = {2010-04},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  volume = {34},
  pages = {351--386},
  issn = {1551-6709},
  doi = {10.1111/j.1551-6709.2009.01090.x},
  abstract = {This paper compares two explanations of the process by which human communication systems evolve: iterated learning and social collaboration. It then reports an experiment testing the social collaboration account. Participants engaged in a graphical communication task either as a member of a community, where they interacted with seven different partners drawn from the same pool, or as a member of an isolated pair, where they interacted with the same partner across the same number of games. Participants' horizontal, pair-wise interactions led "bottom up" to the creation of an effective and efficient shared sign system in the community condition. Furthermore, the community-evolved sign systems were as effective and efficient as the local sign systems developed by isolated pairs. Finally, and as predicted by a social collaboration account, and not by an iterated learning account, interaction was critical to the creation of shared sign systems, with different isolated pairs establishing different local sign systems and different communities establishing different global sign systems.},
  eprint = {21564217},
  eprinttype = {pmid},
  langid = {english},
  number = {3}
}

@article{ferrericanchoPatternsSyntacticDependency2004,
  title = {Patterns in Syntactic Dependency Networks},
  author = {Ferrer i Cancho, Ramon and Solé, Ricard V. and Köhler, Reinhard},
  date = {2004-05-26},
  journaltitle = {Physical Review E},
  volume = {69},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.69.051915},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.051915},
  urldate = {2020-01-21},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XC7Y9XLI\\Ferrer i Cancho et al. - 2004 - Patterns in syntactic dependency networks.pdf},
  langid = {english},
  number = {5}
}

@inproceedings{ferstlUnderstandingPredictabilityGesture2020,
  title = {Understanding the {{Predictability}} of {{Gesture Parameters}} from {{Speech}} and Their {{Perceptual Importance}}},
  booktitle = {Proceedings of the 20th {{ACM}} International {{Conference}} on {{Intelligent Virtual Agents}}},
  author = {Ferstl, Ylva and Neff, Michael and McDonnell, Rachel},
  date = {2020-10-19},
  pages = {1--8},
  publisher = {{ACM New York}},
  location = {{Scotland, UK}},
  doi = {10.1145/3383652.3423882},
  url = {http://arxiv.org/abs/2010.00995},
  urldate = {2020-10-11},
  abstract = {Gesture behavior is a natural part of human conversation. Much work has focused on removing the need for tedious hand-animation to create embodied conversational agents by designing speechdriven gesture generators. However, these generators often work in a black-box manner, assuming a general relationship between input speech and output motion. As their success remains limited, we investigate in more detail how speech may relate to different aspects of gesture motion. We determine a number of parameters characterizing gesture, such as speed and gesture size, and explore their relationship to the speech signal in a two-fold manner. First, we train multiple recurrent networks to predict the gesture parameters from speech to understand how well gesture attributes can be modeled from speech alone. We find that gesture parameters can be partially predicted from speech, and some parameters, such as path length, being predicted more accurately than others, like velocity. Second, we design a perceptual study to assess the importance of each gesture parameter for producing motion that people perceive as appropriate for the speech. Results show that a degradation in any parameter was viewed negatively, but some changes, such as hand shape, are more impactful than others. A video summarization can be found at https://youtu.be/aw6-\_5kmLjY.},
  archiveprefix = {arXiv},
  eprint = {2010.00995},
  eprinttype = {arxiv},
  eventtitle = {{{IVA}} '20},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EZMSWXM6\\Ferstl et al. - 2020 - Understanding the Predictability of Gesture Parame.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  langid = {english}
}

@book{feyereisenCognitivePsychologySpeechRelated2017,
  title = {The {{Cognitive Psychology}} of {{Speech}}-{{Related Gesture}}},
  author = {Feyereisen, P.},
  date = {2017-07-28},
  publisher = {{Routledge}},
  location = {{New York}},
  abstract = {Why do we gesture when we speak? The Cognitive Psychology of Speech-Related Gesture offers answers to this question while introducing readers to the huge interdisciplinary field of gesture. Drawing on ideas from cognitive psychology, this book highlights key debates in gesture research alongside advocating new approaches to conventional thinking.  Beginning with the definition of the notion of communication, this book explores experimental approaches to gesture production and comprehension, the possible gestural origin of language and its implication for brain organization, and the development of gestural communication from infancy to childhood. Through these discussions the author presents the idea that speech-related gestures are not just peripheral phenomena, but rather a key function of the cognitive architecture, and should consequently be studied alongside traditional concepts in cognitive psychology.  The Cognitive Psychology of Speech Related Gesture offers a broad overview which will be essential reading for all students of gesture research and language, as well as speech therapists, teachers and communication practitioners. It will also be of interest to anybody who is curious about why we move our bodies when we talk.},
  eprint = {nJguDwAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-1-351-78827-4},
  keywords = {Psychology / Cognitive Psychology & Cognition,Psychology / General},
  langid = {english},
  pagetotal = {353}
}

@article{filippiEmotionalVoiceIntonation2020,
  title = {Emotional {{Voice Intonation}}: {{A Communication Code}} at the {{Origins}} of {{Speech Processing}} and {{Word}}-{{Meaning Associations}}?},
  shorttitle = {Emotional {{Voice Intonation}}},
  author = {Filippi, Piera},
  date = {2020-12-01},
  journaltitle = {Journal of Nonverbal Behavior},
  shortjournal = {J Nonverbal Behav},
  volume = {44},
  pages = {395--417},
  issn = {1573-3653},
  doi = {10.1007/s10919-020-00337-z},
  url = {https://doi.org/10.1007/s10919-020-00337-z},
  urldate = {2021-01-07},
  abstract = {The aim of the present work is to investigate the facilitating effect of vocal emotional intonation on the evolution of the following processes involved in language: (a) identifying and producing phonemes, (b) processing compositional rules underlying vocal utterances, and (c) associating vocal utterances with meanings. To this end, firstly, I examine research on the presence of these abilities in animals, and the biologically ancient nature of emotional vocalizations. Secondly, I review research attesting to the facilitating effect of emotional voice intonation on these abilities in humans. Thirdly, building on these studies in animals and humans, and through taking an evolutionary perspective, I provide insights for future empirical work on the facilitating effect of emotional intonation on these three processes in animals and preverbal humans. In this work, I highlight the importance of a comparative approach to investigate language evolution empirically. This review supports Darwin’s hypothesis, according to which the ability to express emotions through voice modulation was a key step in the evolution of spoken language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3I6J3AXQ\\Filippi - 2020 - Emotional Voice Intonation A Communication Code a.pdf},
  langid = {english},
  number = {4}
}

@article{filippiTemporalModulationSpeech2019,
  title = {Temporal Modulation in Speech, Music, and Animal Vocal Communication: Evidence of Conserved Function},
  shorttitle = {Temporal Modulation in Speech, Music, and Animal Vocal Communication},
  author = {Filippi, Piera and Hoeschele, Marisa and Spierings, Michelle and Bowling, Daniel L.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  volume = {1453},
  pages = {99--113},
  issn = {1749-6632},
  doi = {10.1111/nyas.14228},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14228},
  urldate = {2019-10-22},
  abstract = {Speech is a distinctive feature of our species. It is the default channel for language and constitutes our primary mode of social communication. Determining the evolutionary origins of speech is a challenging prospect, in large part because it appears to be unique in the animal kingdom. However, direct comparisons between speech and other forms of acoustic communication, both in humans (music) and animals (vocalization), suggest that important components of speech are shared across domains and species. In this review, we focus on a single aspect of speech—temporal patterning—examining similarities and differences across speech, music, and animal vocalization. Additional structure is provided by focusing on three specific functions of temporal patterning across domains: (1) emotional expression, (2) social interaction, and (3) unit identification. We hypothesize an evolutionary trajectory wherein the ability to identify units within a continuous stream of vocal sounds derives from social vocal interaction, which, in turn, derives from vocal emotional communication. This hypothesis implies that unit identification has parallels in music and precursors in animal vocal communication. Accordingly, we demonstrate the potential of comparisons between fundamental domains of biological acoustic communication to provide insight into the evolution of language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PWE2GTN9\\nyas.html},
  keywords = {emotion expression,language evolution,social interaction,temporal patterns,unit identification},
  langid = {english},
  number = {1}
}

@article{filippiTemporalModulationSpeech2019a,
  title = {Temporal Modulation in Speech, Music, and Animal Vocal Communication: Evidence of Conserved Function},
  shorttitle = {Temporal Modulation in Speech, Music, and Animal Vocal Communication},
  author = {Filippi, Piera and Hoeschele, Marisa and Spierings, Michelle and Bowling, Daniel L.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  volume = {1453},
  pages = {99--113},
  issn = {1749-6632},
  doi = {10.1111/nyas.14228},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14228},
  urldate = {2020-07-13},
  abstract = {Speech is a distinctive feature of our species. It is the default channel for language and constitutes our primary mode of social communication. Determining the evolutionary origins of speech is a challenging prospect, in large part because it appears to be unique in the animal kingdom. However, direct comparisons between speech and other forms of acoustic communication, both in humans (music) and animals (vocalization), suggest that important components of speech are shared across domains and species. In this review, we focus on a single aspect of speech—temporal patterning—examining similarities and differences across speech, music, and animal vocalization. Additional structure is provided by focusing on three specific functions of temporal patterning across domains: (1) emotional expression, (2) social interaction, and (3) unit identification. We hypothesize an evolutionary trajectory wherein the ability to identify units within a continuous stream of vocal sounds derives from social vocal interaction, which, in turn, derives from vocal emotional communication. This hypothesis implies that unit identification has parallels in music and precursors in animal vocal communication. Accordingly, we demonstrate the potential of comparisons between fundamental domains of biological acoustic communication to provide insight into the evolution of language.},
  annotation = {\_eprint: https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/nyas.14228},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Q5UYJ47T\\Filippi et al. - 2019 - Temporal modulation in speech, music, and animal v.pdf;C\:\\Users\\u668173\\Zotero\\storage\\D8VHUXKT\\nyas.html},
  keywords = {emotion expression,language evolution,social interaction,temporal patterns,unit identification},
  langid = {english},
  number = {1}
}

@article{finneganModulationsRespiratoryLaryngeal2000,
  title = {Modulations in Respiratory and Laryngeal Activity Associated with Changes in Vocal Intensity during Speech},
  author = {Finnegan, E. M. and Luschei, E. S. and Hoffman, H. T.},
  date = {2000-08},
  journaltitle = {Journal of speech, language, and hearing research: JSLHR},
  shortjournal = {J. Speech Lang. Hear. Res.},
  volume = {43},
  pages = {934--950},
  issn = {1092-4388},
  doi = {10.1044/jslhr.4304.934},
  abstract = {We tested the hypothesis that different strategies are used to alter tracheal pressure (Pt) during sustained and transient increases in intensity. It has been suggested that the respiratory system plays the primary role in Pt changes associated with alteration in overall intensity, whereas laryngeal adjustment is primary for transient change in Pt related to emphasis. Tracheal pressure, obtained via tracheal puncture, airflow (U), and laryngeal electromyography from the thyroarytenoid muscle (TA EMG) were collected from 6 subjects during sentence production at different intensity levels and with various stress patterns. Using a technique described in a previous study, we computed lower airway resistance (Rlaw) from measures of Pt and U obtained during a sudden change in upper airway resistance. We used this resistance value, together with direct measures of Pt and U during speech, to derive a time-varying measure of alveolar pressure (Pa), the pressure created by respiratory muscle activity and elastic recoil of the lungs. Pa provided a measure of respiratory drive that was unaffected by laryngeal activity. Laryngeal airway resistance (Rlx) and TA EMG provided measures of laryngeal activity. The results of this study indicated that, contrary to the outcome predicted by the hypothesis, there was no difference in the strategies used to alter Pt during sustained and transient increases in intensity. Although changes in both Pa and Rlx contributed to increase in Pt, the contribution of Pa was substantially greater. On average, Pa contributed to 94\% and Rlx to 6\% of the increase in Pt associated with vocal intensity. A secondary purpose of the study was to determine the extent to which laryngeal muscle activity was related to Rlx during speech. We found TA EMG activity increased with intensity but was not well correlated with Rlx, suggesting that when it contracts, the TA muscle may affect intensity by loosening the cover, which allows for greater amplitude of vocal fold vibration, without necessarily increasing laryngeal airway resistance.},
  eprint = {11386480},
  eprinttype = {pmid},
  keywords = {Adult,Electromyography,Female,Humans,Larynx,Male,Reproducibility of Results,Respiration,Speech,Trachea,Voice},
  langid = {english},
  number = {4}
}

@incollection{fitchAcousticCommunication2002,
  title = {Acoustic {{Communication}}},
  booktitle = {Unpacking "{{Honesty}}": {{Vertebrate Vocal Production}} and the {{Evolution}} of {{Acoustic Signals}}},
  author = {Fitch, T. and Hauser, M. D.},
  editor = {Simmons, A. and Fay, R. R. and Popper, A. N.},
  date = {2002},
  publisher = {{Springer}},
  location = {{New York}},
  abstract = {this paper we have argued that the study of the proximate mechanisms underlying vocal behavior, both physiological and cognitive, is a necessary part of the study of the evolution of communication, and in particular for analyzing honesty in communication. In the first part we surveyed basic principles of vocal production in terrestrial vertebrates, and the morphological diversity of their production systems. We then provided some examples of the interactions between acoustics and anatomy that can enforce honesty, or subvert it. In the second part we examined the evidence for cognitive mechanisms that allow animals to produce deceptive calls, as well as "retaliatory" perceptual mechanisms that allow perceivers to accurately identify and ignore (and in some cases even punish) the deceivers. Both vocal production mechanisms, and cognitive mechanisms controlling vocalization, play a crucial role in determining what is possible or impossible in a particular species' communication system. A better understanding of these mechanisms can lead to rich insights into the evolution of acoustic communication.  Fitch \& Hauser p. 38  For the reader already interested in mechanism, the chapter also provided illustrations of the value of an ultimate evolutionary viewpoint. An evolutionary perspective proves valuable both for identifying functional problems that are solved by communicators, and for using phylogenies and the comparative method as tools to identify and understand widespread selective pressures and functional constraints. The species we observe today are the outcome of a long dynamic process of coevolution and interaction. Signalers' ability to avoid, repel or attract predators, competitors and potential mates has played a critical role in the evolution of their acoustic si...},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\59FSS74Q\\Fitch and Hauser - Unpacking Honesty Vertebrate Vocal Production a.pdf;C\:\\Users\\u668173\\Zotero\\storage\\7N4CR8GF\\summary.html},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@book{fitchEvolutionLanguage2010,
  title = {The {{Evolution}} of {{Language}}},
  author = {Fitch, W. Tecumseh},
  date = {2010-04},
  publisher = {{Cambridge University Press}},
  abstract = {Language, more than anything else, is what makes us human. It appears that no communication system of equivalent power exists elsewhere in the animal kingdom. Any normal human child will learn a language based on rather sparse data in the surrounding world, while even the brightest chimpanzee, exposed to the same environment, will not. Why not? How, and why, did language evolve in our species and not in others? Since Darwin's theory of evolution, questions about the origin of language have generated a rapidly-growing scientific literature, stretched across a number of disciplines, much of it directed at specialist audiences. The diversity of perspectives - from linguistics, anthropology, speech science, genetics, neuroscience and evolutionary biology - can be bewildering. Tecumseh Fitch cuts through this vast literature, bringing together its most important insights to explore one of the biggest unsolved puzzles of human history.},
  eprint = {RProTk_Ag7gC},
  eprinttype = {googlebooks},
  isbn = {978-0-521-85993-6},
  keywords = {Language Arts & Disciplines / Linguistics / General,Language Arts & Disciplines / Linguistics / Historical & Comparative,Science / Life Sciences / Evolution,Social Science / Anthropology / General},
  langid = {english},
  pagetotal = {625}
}

@article{fitchFourPrinciplesBiomusicology2015,
  title = {Four Principles of Bio-Musicology},
  author = {Fitch, W. Tecumseh},
  date = {2015-03-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {370},
  pages = {20140091},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2014.0091},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2014.0091},
  urldate = {2020-06-30},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QLBBJ427\\Fitch - 2015 - Four principles of bio-musicology.pdf},
  langid = {english},
  number = {1664}
}

@article{fitchMonkeyVocalTracts2016,
  title = {Monkey Vocal Tracts Are Speech-Ready},
  author = {Fitch, W. Tecumseh and de Boer, Bart and Mathur, Neil and Ghazanfar, Asif A.},
  date = {2016-12-01},
  journaltitle = {Science Advances},
  volume = {2},
  pages = {e1600723},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.1600723},
  url = {https://advances.sciencemag.org/content/2/12/e1600723},
  urldate = {2020-03-06},
  abstract = {For four decades, the inability of nonhuman primates to produce human speech sounds has been claimed to stem from limitations in their vocal tract anatomy, a conclusion based on plaster casts made from the vocal tract of a monkey cadaver. We used x-ray videos to quantify vocal tract dynamics in living macaques during vocalization, facial displays, and feeding. We demonstrate that the macaque vocal tract could easily produce an adequate range of speech sounds to support spoken language, showing that previous techniques based on postmortem samples drastically underestimated primate vocal capabilities. Our findings imply that the evolution of human speech capabilities required neural changes rather than modifications of vocal anatomy. Macaques have a speech-ready vocal tract but lack a speech-ready brain to control it. X-ray analyses of macaque vocal tract movements show that monkeys’ inability to speak is not due to limitations of peripheral anatomy. X-ray analyses of macaque vocal tract movements show that monkeys’ inability to speak is not due to limitations of peripheral anatomy.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9DPG5VVG\\Fitch et al. - 2016 - Monkey vocal tracts are speech-ready.pdf;C\:\\Users\\u668173\\Zotero\\storage\\SN5IVXBR\\e1600723.html},
  langid = {english},
  number = {12}
}

@article{fitchRhesusMacaquesSpontaneously2006,
  title = {Rhesus Macaques Spontaneously Perceive Formants in Conspecific Vocalizations},
  author = {Fitch, W. Tecumseh and Fritz, Jonathan B.},
  date = {2006-10-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {120},
  pages = {2132--2141},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.2258499},
  url = {https://asa.scitation.org/doi/full/10.1121/1.2258499},
  urldate = {2020-12-03},
  abstract = {We provide a direct demonstration that nonhuman primates spontaneously perceive changes in formant frequencies in their own species-typical vocalizations, without training or reinforcement. Formants are vocal tract resonances leading to distinctive spectral prominences in the vocal signal, and provide the acoustic determinant of many key phonetic distinctions in human languages. We developed algorithms for manipulating formants in rhesus macaque calls. Using the resulting computer-manipulated calls in a habituation/dishabituation paradigm, with blind video scoring, we show that rhesus macaques spontaneously respond to a change in formant frequencies within the normal macaque vocal range. Lack of dishabituation to a “synthetic replica” signal demonstrates that dishabituation was not due to an artificial quality of synthetic calls, but to the formant shift itself. These results indicate that formant perception, a significant component of human voice and speech perception, is a perceptual ability shared with other primates.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\L4RGI53A\\Fitch and Fritz - 2006 - Rhesus macaques spontaneously perceive formants in.pdf;C\:\\Users\\u668173\\Zotero\\storage\\RBRMGDHG\\1.html},
  number = {4}
}

@incollection{fitchVertebrateBioacousticsProspects2016,
  title = {Vertebrate {{Bioacoustics}}: {{Prospects}} and {{Open Problems}}},
  shorttitle = {Vertebrate {{Bioacoustics}}},
  booktitle = {Vertebrate {{Sound Production}} and {{Acoustic Communication}}},
  author = {Fitch, W. Tecumseh},
  editor = {Suthers, Roderick A. and Fitch, W. Tecumseh and Fay, Richard R. and Popper, Arthur N.},
  date = {2016},
  pages = {297--328},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-27721-9_10},
  url = {https://doi.org/10.1007/978-3-319-27721-9_10},
  urldate = {2020-09-19},
  abstract = {Vertebrate bioacoustics has made great gains in the last two decades in terms of increased understanding of the functional morphology of the vocal tract: how sounds are produced by the larynx (or syrinx in birds) and then filtered in the vocal tract. Despite this fundamental progress, many unusual features seen in the vocal anatomy of particular vertebrates remain poorly understood. This results mainly from the fact that these potential vocal adaptations were described by classical comparative anatomists more than a century ago, long before a good understanding of the physics and physiology of vocal production was in place. Adding to this difficulty, many of the descriptions of anatomical peculiarities were published in non-English languages and often in hard-to-access journals. This chapter starts with a short review of the rise and fall of comparative anatomy as a leading branch of biology, focusing especially on vocal anatomy. It then provides a brief overview of the many known anatomical peculiarities that, although poorly understood, are thought to play some role in vocal production. Both morphology and possible function are considered, and any available empirical research is reviewed. The chapter covers most known vocal peculiarities including air sacs, vocal fold modifications, the syringeal bulla present in most ducks, or the elongated trachea seen in many bird species. Such unusual modifications of vocal anatomy will provide a rich and rewarding topic of future research in bioacoustics.},
  isbn = {978-3-319-27721-9},
  keywords = {Animal bioacoustics,Animal communication,Comparative anatomy,Functional morphology,Laryngeal air sacs,Larynx,Syrinx,Tracheal elongation,Vocal tract},
  langid = {english},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@article{fitchVocalProductionNonhuman1995,
  title = {Vocal Production in Nonhuman Primates: {{Acoustics}}, Physiology, and Functional Constraints on "Honest" Advertisement},
  shorttitle = {Vocal Production in Nonhuman Primates},
  author = {Fitch, W. Tecumseh and Hauser, Marc D.},
  date = {1995},
  journaltitle = {American Journal of Primatology},
  shortjournal = {Am. J. Primatol.},
  volume = {37},
  pages = {191--219},
  issn = {1098-2345},
  doi = {10.1002/ajp.1350370303},
  abstract = {The physiological mechanisms and acoustic principles underlying sound production in primates are important for analyzing and synthesizing primate vocalizations, for determining the range of calls that are physically producible, and for understanding primate communication in the broader comparative context of what is known about communication in other vertebrates. In this paper we discuss what is known about vocal production in nonhuman primates, relying heavily on models from speech and musical acoustics. We first describe the role of the lungs and larynx in generating the sound source, and then discuss the effects of the supralaryngeal vocal tract in modifying this source. We conclude that more research is needed to resolve several important questions about the acoustics of primate calls, including the nature of the vocal tract's contribution to call production. Nonetheless, enough is known to explore the implications of call acoustics for the evolution of primate communication. In particular, we discuss how anatomy and physiology may provide constraints resulting in "honest" acoustic indicators of body size. © 1995 Wiley-Liss, Inc.},
  eprint = {31936952},
  eprinttype = {pmid},
  keywords = {body size,evolution,larynx,primate communication,vocal tract},
  langid = {english},
  number = {3}
}

@article{fitchVocalProductionNonhuman1995a,
  title = {Vocal Production in Nonhuman Primates: {{Acoustics}}, Physiology, and Functional Constraints on “Honest” Advertisement},
  shorttitle = {Vocal Production in Nonhuman Primates},
  author = {Fitch, W. Tecumseh and Hauser, Marc D.},
  date = {1995},
  journaltitle = {American Journal of Primatology},
  volume = {37},
  pages = {191--219},
  issn = {1098-2345},
  doi = {10.1002/ajp.1350370303},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ajp.1350370303},
  urldate = {2020-09-18},
  abstract = {The physiological mechanisms and acoustic principles underlying sound production in primates are important for analyzing and synthesizing primate vocalizations, for determining the range of calls that are physically producible, and for understanding primate communication in the broader comparative context of what is known about communication in other vertebrates. In this paper we discuss what is known about vocal production in nonhuman primates, relying heavily on models from speech and musical acoustics. We first describe the role of the lungs and larynx in generating the sound source, and then discuss the effects of the supralaryngeal vocal tract in modifying this source. We conclude that more research is needed to resolve several important questions about the acoustics of primate calls, including the nature of the vocal tract's contribution to call production. Nonetheless, enough is known to explore the implications of call acoustics for the evolution of primate communication. In particular, we discuss how anatomy and physiology may provide constraints resulting in “honest” acoustic indicators of body size. © 1995 Wiley-Liss, Inc.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ajp.1350370303},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DPP9V7DC\\ajp.html},
  keywords = {body size,evolution,larynx,primate communication,vocal tract},
  langid = {english},
  number = {3}
}

@article{fitchVocalTractLength1997,
  title = {Vocal Tract Length and Formant Frequency Dispersion Correlate with Body Size in Rhesus Macaques},
  author = {Fitch, W. T.},
  date = {1997-08},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J Acoust Soc Am},
  volume = {102},
  pages = {1213--1222},
  issn = {0001-4966},
  doi = {10.1121/1.421048},
  abstract = {Body weight, length, and vocal tract length were measured for 23 rhesus macaques (Macaca mulatta) of various sizes using radiographs and computer graphic techniques. linear predictive coding analysis of tape-recorded threat vocalizations were used to determine vocal tract resonance frequencies ("formants") for the same animals. A new acoustic variable is proposed, "formant dispersion," which should theoretically depend upon vocal tract length. Formant dispersion is the averaged difference between successive formant frequencies, and was found to be closely tied to both vocal tract length and body size. Despite the common claim that voice fundamental frequency (F0) provides an acoustic indication of body size, repeated investigations have failed to support such a relationship in many vertebrate species including humans. Formant dispersion, unlike voice pitch, is proposed to be a reliable predictor of body size in macaques, and probably many other species.},
  eprint = {9265764},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YTVN5XN9\\Fitch - 1997 - Vocal tract length and formant frequency dispersio.pdf},
  issue = {2 Pt 1},
  keywords = {Acoustics,Animals,Body Constitution,Humans,Macaca mulatta,Sound Spectrography,Vocal Cords,Vocalization; Animal},
  langid = {english}
}

@article{fittsInformationCapacityHuman1954,
  title = {The Information Capacity of the Human Motor System in Controlling the Amplitude of Movement},
  author = {Fitts, Paul M.},
  date = {1954},
  journaltitle = {Journal of Experimental Psychology},
  volume = {47},
  pages = {381--391},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {0022-1015(Print)},
  doi = {10.1037/h0055392},
  abstract = {Reports of 3 experiments testing the hypothesis that the average duration of responses is directly proportional to the minimum average amount of information per response. The results show that the rate of performance is approximately constant over a wide range of movement amplitude and tolerance limits. This supports the thesis that "the performance capacity of the human motor system plus its associated visual and proprioceptive feedback mechanisms, when measured in information units, is relatively constant over a considerable range of task conditions." 25 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MI869Z5H\\Fitts - 1954 - The information capacity of the human motor system.pdf;C\:\\Users\\u668173\\Zotero\\storage\\KKNFS6WY\\1955-02059-001.html},
  keywords = {Motor Processes,Perceptual Motor Processes,Response Amplitude,Response Duration,Testing},
  number = {6}
}

@article{flashCoordinationArmMovements1985,
  title = {The Coordination of Arm Movements: An Experimentally Confirmed Mathematical Model},
  shorttitle = {The Coordination of Arm Movements},
  author = {Flash, T. and Hogan, N.},
  date = {1985-07-01},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {5},
  pages = {1688--1703},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.05-07-01688.1985},
  url = {https://www.jneurosci.org/content/5/7/1688},
  urldate = {2021-02-24},
  abstract = {This paper presents studies of the coordination of voluntary human arm movements. A mathematical model is formulated which is shown to predict both the qualitative features and the quantitative details observed experimentally in planar, multijoint arm movements. Coordination is modeled mathematically by defining an objective function, a measure of performance for any possible movement. The unique trajectory which yields the best performance is determined using dynamic optimization theory. In the work presented here, the objective function is the square of the magnitude of jerk (rate of change of acceleration) of the hand integrated over the entire movement. This is equivalent to assuming that a major goal of motor coordination is the production of the smoothest possible movement of the hand. Experimental observations of human subjects performing voluntary unconstrained movements in a horizontal plane are presented. They confirm the following predictions of the mathematical model: unconstrained point-to-point motions are approximately straight with bell-shaped tangential velocity profiles; curved motions (through an intermediate point or around an obstacle) have portions of low curvature joined by portions of high curvature; at points of high curvature, the tangential velocity is reduced; the durations of the low-curvature portions are approximately equal. The theoretical analysis is based solely on the kinematics of movement independent of the dynamics of the musculoskeletal system and is successful only when formulated in terms of the motion of the hand in extracorporal space. The implications with respect to movement organization are discussed.},
  eprint = {4020415},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KHCCF5T7\\Flash and Hogan - 1985 - The coordination of arm movements an experimental.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5I9KSHZ9\\1688.html},
  langid = {english},
  number = {7}
}

@article{flashCoordinationArmMovements1985a,
  title = {The Coordination of Arm Movements: An Experimentally Confirmed Mathematical Model},
  shorttitle = {The Coordination of Arm Movements},
  author = {Flash, T and Hogan, N},
  date = {1985-07-01},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {5},
  pages = {1688--1703},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.05-07-01688.1985},
  url = {http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.05-07-01688.1985},
  urldate = {2021-02-24},
  abstract = {This paper presents studies of the coordination of voluntary human arm movements. A mathematical model is formulated which is shown to predict both the qualitative features and the quantitative details observed experimentally in planar, multijoint arm movements. Coordination is modeled mathematically by defining an objective function, a measure of performance for any possible movement. The unique trajectory which yields the best performance is determined using dynamic optimization theory. In the work presented here, the objective function is the square of the magnitude of jerk (rate of change of acceleration) of the hand integrated over the entire movement. This is equivalent to assuming that a major goal of motor coordination is the production of the smoothest possible movement of the hand. Experimental observations of human subjects performing voluntary unconstrained movements in a horizontal plane are presented. They confirm the following predictions of the mathematical model: unconstrained point-to-point motions are approximately straight with bell-shaped tangential velocity profiles; curved motions (through an intermediate point or around an obstacle) have portions of low curvature joined by portions of high curvature; at points of high curvature, the tangential velocity is reduced; the durations of the lowcurvature portions are approximately equal.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8BVJWFW6\\Flash and Hogan - 1985 - The coordination of arm movements an experimental.pdf},
  langid = {english},
  number = {7}
}

@incollection{fletcherProsodySpeechTiming2010,
  title = {The {{Prosody}} of {{Speech}}: {{Timing}} and {{Rhythm}}},
  shorttitle = {The {{Prosody}} of {{Speech}}},
  booktitle = {The {{Handbook}} of {{Phonetic Sciences}}},
  author = {Fletcher, Janet},
  date = {2010},
  pages = {521--602},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781444317251.ch15},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781444317251.ch15},
  urldate = {2020-03-17},
  abstract = {This chapter contains sections titled: Introduction Lengthenings and Shortenings: The Temporal Signatures of Prosody Speech Timing: A Rhythmic Dimension Tempo and Pausing Concluding Comments References},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781444317251.ch15},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FVIAETUK\\9781444317251.html},
  isbn = {978-1-4443-1725-1},
  keywords = {influential interactive segment duration model - rule system by Klatt,intermediate phrase,intonational phrase,isochrony - perceptual phenomenon in spoken English and “stress-timed” languages in general,lengthenings and shortenings - temporal signatures of prosody,measuring tempo - monitoring speaking tempo,prosody of speech - timing and rhythm,segmental and syllable timing patterns – prosodic word,speaking rate and articulation rate,speech - activity in unfolding time,speech rhythms - successions and alternations of events with specific temporal paradigm,syllable duration - factors contributing to syllable timing versus stress timing,syllable structure,utterance,vowel reduction - maximizing difference between stressed and unstressed syllables},
  langid = {english}
}

@article{florAssessmentStressrelatedPsychophysiological1985,
  title = {Assessment of Stress-Related Psychophysiological Reactions in Chronic Back Pain Patients},
  author = {Flor, H. and Turk, D. C. and Birbaumer, N.},
  date = {1985-06},
  journaltitle = {Journal of Consulting and Clinical Psychology},
  shortjournal = {J Consult Clin Psychol},
  volume = {53},
  pages = {354--364},
  issn = {0022-006X},
  doi = {10.1037//0022-006x.53.3.354},
  eprint = {3159767},
  eprinttype = {pmid},
  keywords = {Adult,Aged,Arousal,Back Pain,Electromyography,Female,Humans,Male,Middle Aged,Stress; Psychological},
  langid = {english},
  number = {3}
}

@article{forbusExtendingSMEHandle2017,
  title = {Extending {{SME}} to {{Handle Large}}-{{Scale Cognitive Modeling}}},
  author = {Forbus, Kenneth D. and Ferguson, Ronald W. and Lovett, Andrew and Gentner, Dedre},
  date = {2017-07},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  volume = {41},
  pages = {1152--1201},
  issn = {1551-6709},
  doi = {10.1111/cogs.12377},
  abstract = {Analogy and similarity are central phenomena in human cognition, involved in processes ranging from visual perception to conceptual change. To capture this centrality requires that a model of comparison must be able to integrate with other processes and handle the size and complexity of the representations required by the tasks being modeled. This paper describes extensions to Structure-Mapping Engine (SME) since its inception in 1986 that have increased its scope of operation. We first review the basic SME algorithm, describe psychological evidence for SME as a process model, and summarize its role in simulating similarity-based retrieval and generalization. Then we describe five techniques now incorporated into the SME that have enabled it to tackle large-scale modeling tasks: (a) Greedy merging rapidly constructs one or more best interpretations of a match in polynomial time: O(n2 log(n)); (b) Incremental operation enables mappings to be extended as new information is retrieved or derived about the base or target, to model situations where information in a task is updated over time; (c) Ubiquitous predicates model the varying degrees to which items may suggest alignment; (d) Structural evaluation of analogical inferences models aspects of plausibility judgments; (e) Match filters enable large-scale task models to communicate constraints to SME to influence the mapping process. We illustrate via examples from published studies how these enable it to capture a broader range of psychological phenomena than before.},
  eprint = {27322750},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CIFPPLCW\\Forbus et al. - 2017 - Extending SME to Handle Large-Scale Cognitive Mode.pdf},
  keywords = {Algorithms,Analogical learning,Analogical reasoning,Analogy,Artificial intelligence,Brain,Cognition,Cognitive psychology,Cognitive simulation,Humans,Models; Psychological,Similarity,Symbolic modeling},
  langid = {english},
  number = {5}
}

@article{formisanoWhoSayingWhat2008,
  title = {"{{Who}}" {{Is Saying}} "{{What}}"? {{Brain}}-{{Based Decoding}} of {{Human Voice}} and {{Speech}}},
  shorttitle = {"{{Who}}" {{Is Saying}} "{{What}}"?},
  author = {Formisano, E. and De Martino, F. and Bonte, M. and Goebel, R.},
  date = {2008-11-07},
  journaltitle = {Science},
  volume = {322},
  pages = {970--973},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1164318},
  url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1164318},
  urldate = {2020-09-14},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\H6I6CC2G\\Formisano et al. - 2008 - Who Is Saying What Brain-Based Decoding of Hu.pdf},
  langid = {english},
  number = {5903}
}

@article{fowlerEmbodiedEmbeddedLanguage2010,
  title = {Embodied, {{Embedded Language Use}}},
  author = {Fowler, Carol A.},
  date = {2010-10-01},
  journaltitle = {Ecological psychology : a publication of the International Society for Ecological Psychology},
  volume = {22},
  pages = {286},
  issn = {10.1080/10407413.2010.517115},
  doi = {10.1080/10407413.2010.517115},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3020794/},
  urldate = {2019-05-04},
  abstract = {Language use has a public face that is as important to study as the private faces under intensive psycholinguistic study. In the domain of phonology, public use of speech must meet an interpersonal “parity” constraint if it is to serve ...},
  eprint = {21243080},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IL765TVU\\Fowler - 2010 - Embodied, Embedded Language Use.pdf;C\:\\Users\\u668173\\Zotero\\storage\\WLRZ6HFX\\PMC3020794.html},
  langid = {english},
  number = {4}
}

@article{fowlerEmbodiedEmbeddedLanguage2010a,
  title = {Embodied, {{Embedded Language Use}}},
  author = {Fowler, Carol A.},
  date = {2010-10-01},
  journaltitle = {Ecological psychology : a publication of the International Society for Ecological Psychology},
  shortjournal = {Ecol Psychol},
  volume = {22},
  pages = {286--303},
  issn = {1040-7413},
  doi = {10.1080/10407413.2010.517115},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3020794/},
  urldate = {2020-03-08},
  abstract = {Language use has a public face that is as important to study as the private faces under intensive psycholinguistic study. In the domain of phonology, public use of speech must meet an interpersonal “parity” constraint if it is to serve to communicate. That is, spoken language forms must reliably be identified by listeners. To that end, language forms are embodied, at the lowest level of description, as phonetic gestures of the vocal tract that lawfully structure informational media such as air and light. Over time, under the parity constraint, sound inventories emerge over communicative exchanges that have the property of sufficient identifiability., Communicative activities involve more than vocal tract actions. Talkers gesture and use facial expressions and eye gaze to communicate. Listeners embody their language understandings, exhibiting dispositions to behave in ways related to language understanding. Moreover, linguistic interchanges are embedded in the larger context of language use. Talkers recruit the environment in their communicative activities, for example, in using deictic points. Moreover, in using language as a “coordination device,” interlocutors mutually entrain.},
  eprint = {21243080},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3PWVMW4Z\\Fowler - 2010 - Embodied, Embedded Language Use.pdf},
  number = {4},
  pmcid = {PMC3020794}
}

@article{fowlerEmbodiedEmbeddedLanguage2010b,
  title = {Embodied, {{Embedded Language Use}}},
  author = {Fowler, Carol A.},
  date = {2010-10-29},
  journaltitle = {Ecological Psychology},
  volume = {22},
  pages = {286--303},
  publisher = {{Routledge}},
  issn = {1040-7413},
  doi = {10.1080/10407413.2010.517115},
  url = {https://doi.org/10.1080/10407413.2010.517115},
  urldate = {2021-01-23},
  abstract = {Language use has a public face that is as important to study as the private faces under intensive psycholinguistic study. In the domain of phonology, public use of speech must meet an interpersonal “parity” constraint if it is to serve to communicate. That is, spoken language forms must reliably be identified by listeners. To that end, language forms are embodied, at the lowest level of description, as phonetic gestures of the vocal tract that lawfully structure informational media such as air and light. Over time, under the parity constraint, sound inventories emerge over communicative exchanges that have the property of sufficient identifiability. Communicative activities involve more than vocal tract actions. Talkers gesture and use facial expressions and eye gaze to communicate. Listeners embody their language understandings, exhibiting dispositions to behave in ways related to language understanding. Moreover, linguistic interchanges are embedded in the larger context of language use. Talkers recruit the environment in their communicative activities, for example, in using deictic points. Moreover, in using language as a “coordination device,” interlocutors mutually entrain.},
  annotation = {\_eprint: https://doi.org/10.1080/10407413.2010.517115},
  eprint = {21243080},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RDBJDMLG\\Fowler - 2010 - Embodied, Embedded Language Use.pdf;C\:\\Users\\u668173\\Zotero\\storage\\BQD92XZD\\10407413.2010.html},
  number = {4}
}

@article{fowlerEventApproachStudy1986,
  title = {An Event Approach to the Study of Speech Perception from a Direct-Realist Perspective},
  author = {Fowler, Carol A.},
  date = {1986},
  journaltitle = {Journal of Phonetics},
  volume = {14},
  pages = {3--28},
  issn = {1095-8576(Electronic),0095-4470(Print)},
  abstract = {Proposes an event approach to a theory of speech perception and speech production, focusing on the perception of speech events (i.e., a talker's phonetically structured articulations). In defining a speech event interchangeably from the perspectives of talkers and listeners, the author adopts a "direct realist" perspective: Perception is assumed to recover events in the real world. To do this, perception must be direct and unmediated by cognitive processes of inference or hypothesis testing. Barriers to the theory are outlined and evidence presented to refute them. Support for direct perception of local, short-term events and longer ones is discussed. Attention is also given to the way in which perception of a linguistic message guides a listener's behavior. Differences in the reliability of the information conveyed between direct and indirect perception and the implications for the relation between an utterance and what it signified are examined. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YUI62C42\\1987-23991-001.html},
  keywords = {Articulation (Speech),Phonetics,Speech Perception,Theories},
  number = {1}
}

@article{fowlerListeningEyeHand1991,
  title = {Listening with Eye and Hand: {{Cross}}-Modal Contributions to Speech Perception},
  shorttitle = {Listening with Eye and Hand},
  author = {Fowler, Carol A. and Dekle, Dawn J.},
  date = {1991},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {17},
  pages = {816--828},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.17.3.816},
  abstract = {Three experiments investigated the "McGurk effect" whereby optically specified syllables experienced synchronously with acoustically specified syllables integrate in perception to determine a listener's auditory perceptual experience. Experiments contrasted the cross-modal effect of orthographic on acoustic syllables presumed to be associated in experience and memory with that of haptically experienced and acoustic syllables presumed not to be associated. The latter pairing gave rise to cross-modal influences when Ss were informed that cross-modal syllables were paired independently. Mouthed syllables affected reports of simultaneously heard syllables (and vice versa). These effects were absent when syllables were simultaneously seen (spelled) and heard. The McGurk effect does not arise from association in memory but from conjoint near specification of the same casual source in the environment—in speech, the moving vocal tract producing phonetic gestures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NRS9DDFB\\1992-00274-001.html},
  keywords = {Auditory Perception,Auditory Stimulation,Orthography,Speech Perception},
  number = {3}
}

@article{foxeCaseFeedforwardMultisensory2005,
  title = {The Case for Feedforward Multisensory Convergence during Early Cortical Processing},
  author = {Foxe, John J. and Schroeder, Charles E.},
  date = {2005-04-04},
  journaltitle = {Neuroreport},
  shortjournal = {Neuroreport},
  volume = {16},
  pages = {419--423},
  issn = {0959-4965},
  doi = {10.1097/00001756-200504040-00001},
  abstract = {The prevailing hierarchical model of sensory processing in the brain holds that different modalities of sensory information emanating from a single object are analyzed extensively during passage through their respective unisensory processing streams before they are combined in higher-order 'multisensory' regions of the cortex. Because of this view, multisensory interactions that have been found at early, putatively 'unisensory' cortical processing stages during hemodynamic imaging studies have been assumed to reflect feedback modulations that occur subsequent to multisensory processing in the higher-order multisensory areas. In this paper, we consider findings that challenge an exclusively feedback interpretation of early multisensory integration effects. First, high-density electrical mapping studies in humans have shown that multisensory convergence and integration effects can occur so early in the time course of sensory processing that purely feedback mediation becomes extremely unlikely. Second, direct neural recordings in monkeys show that, in some cases, convergent inputs at early cortical stages have physiological profiles characteristic of feedforward rather than feedback inputs. Third, damage to higher-order integrative regions in humans often spares the ability to integrate across sensory modalities. Finally, recent anatomic tracer studies have reported direct anatomical connections between primary visual and auditory cortex. These findings make it clear that multisensory convergence at early stages of cortical processing results from feedforward as well as feedback and lateral connections, thus using the full range of anatomical connections available in brain circuitry.},
  eprint = {15770144},
  eprinttype = {pmid},
  keywords = {Animals,Brain Mapping,Cerebral Cortex,Feedback,Humans,Neural Pathways},
  langid = {english},
  number = {5}
}

@article{fritzMultimodalLanguageProcessing2021,
  title = {Multimodal Language Processing: {{How}} Preceding Discourse Constrains Gesture Interpretation and Affects Gesture Integration When Gestures Do Not Synchronise with Semantic Affiliates},
  shorttitle = {Multimodal Language Processing},
  author = {Fritz, Isabella and Kita, Sotaro and Littlemore, Jeannette and Krott, Andrea},
  date = {2021-04-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {117},
  pages = {104191},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104191},
  url = {http://www.sciencedirect.com/science/article/pii/S0749596X20301054},
  urldate = {2020-12-31},
  abstract = {Previous studies have suggested that a co-speech gesture needs to be synchronous with semantically related speech (semantic affiliates) for its successful semantic integration into a discourse model because co-speech gestures are often highly ambiguous on their own. But not all gestures synchronise with semantic affiliates, some precede them. The current study tested whether the interpretation of a gesture that does not synchronise with its semantic affiliate can be constrained by preceding verbal discourse and integrated into a recipient’s discourse model. A behavioural experiment (Experiment 1) showed that related discourse information can indeed constrain recipients’ interpretations of such gestures. Results from an ERP experiment (Experiment 2) confirmed that synchronisation between gesture and semantic affiliate is not essential in order for the gesture to become part of a discourse model, but only if the preceding context constrains the gesture’s meaning. In this case, we found evidence for post-semantic integration (P600, time-locked to the gesture’s semantic affiliate).},
  keywords = {Discourse,ERP,Gesture,Synchrony},
  langid = {english}
}

@article{frohlichMultimodalCommunicationLanguage2019,
  title = {Multimodal Communication and Language Origins: Integrating Gestures and Vocalizations},
  shorttitle = {Multimodal Communication and Language Origins},
  author = {Fröhlich, Marlen and Sievers, Christine and Townsend, Simon W. and Gruber, Thibaud and van Schaik, Carel P.},
  date = {2019},
  journaltitle = {Biological Reviews},
  volume = {94},
  pages = {1809--1829},
  issn = {1469-185X},
  doi = {10.1111/brv.12535},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12535},
  urldate = {2019-12-09},
  abstract = {The presence of divergent and independent research traditions in the gestural and vocal domains of primate communication has resulted in major discrepancies in the definition and operationalization of cognitive concepts. However, in recent years, accumulating evidence from behavioural and neurobiological research has shown that both human and non-human primate communication is inherently multimodal. It is therefore timely to integrate the study of gestural and vocal communication. Herein, we review evidence demonstrating that there is no clear difference between primate gestures and vocalizations in the extent to which they show evidence for the presence of key language properties: intentionality, reference, iconicity and turn-taking. We also find high overlap in the neurobiological mechanisms producing primate gestures and vocalizations, as well as in ontogenetic flexibility. These findings confirm that human language had multimodal origins. Nonetheless, we note that in great apes, gestures seem to fulfil a carrying (i.e. predominantly informative) role in close-range communication, whereas the opposite holds for face-to-face interactions of humans. This suggests an evolutionary shift in the carrying role from the gestural to the vocal stream, and we explore this transition in the carrying modality. Finally, we suggest that future studies should focus on the links between complex communication, sociality and cooperative tendency to strengthen the study of language origins.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QP3HIK3A\\Fröhlich et al. - 2019 - Multimodal communication and language origins int.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8H8XMILJ\\brv.html},
  keywords = {cognition,comparative approach,evolution of language,gestural origins,learning,multimodality,ontogeny,primates,vocal origins},
  langid = {english},
  number = {5}
}

@article{frohlichMustAllSignals2020,
  title = {Must All Signals Be Evolved? {{A}} Proposal for a New Classification of Communicative Acts},
  shorttitle = {Must All Signals Be Evolved?},
  author = {Fröhlich, Marlen and van Schaik, Carel P.},
  date = {2020-03-16},
  journaltitle = {WIREs Cognitive Science},
  issn = {1939-5078, 1939-5086},
  doi = {10.1002/wcs.1527},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1527},
  urldate = {2020-03-24},
  abstract = {While signals in evolutionary biology are usually defined as “acts or traits that have evolved because of their effect on others”, work on gestures and vocalizations in various animal taxa have revealed population- or even individual-specific meanings of social signals. These results strongly suggest that communicative acts that are like signals with regard to both form and function (meaning) can also be acquired ontogenetically, and we discuss direct evidence for such plasticity in captive settings with rich opportunities for repeated social interactions with the same individuals. Therefore, in addition to evolved signals, we can recognize invented signals that are acquired during ontogeny (either through ontogenetic ritualization or social transmission). Thus, both gestures and vocalizations can be inventions or innate adaptations. We therefore propose to introduce innate versus invented signals as major distinct categories, with invented signals subdivided into dyad-specific and cultural signals. We suggest that elements of some signals may have mixed origins, and propose criteria to recognize acquired features of signals. We also suggest that invented signals may be most common in species with intentional communication, consistent with their ubiquity in humans, and that the ability to produce them was a necessary condition for the evolution of language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\V5THSFP7\\Fröhlich and van Schaik - 2020 - Must all signals be evolved A proposal for a new .pdf},
  langid = {english},
  options = {useprefix=true}
}

@article{frohlichUnpeelingLayersLanguage2016,
  title = {Unpeeling the Layers of Language: {{Bonobos}} and Chimpanzees Engage in Cooperative Turn-Taking Sequences},
  shorttitle = {Unpeeling the Layers of Language},
  author = {Fröhlich, Marlen and Kuchenbuch, Paul and Müller, Gudrun and Fruth, Barbara and Furuichi, Takeshi and Wittig, Roman M. and Pika, Simone},
  date = {2016-05-23},
  journaltitle = {Scientific Reports},
  volume = {6},
  pages = {25887},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep25887},
  url = {https://www.nature.com/articles/srep25887},
  urldate = {2020-12-12},
  abstract = {Human language is a fundamentally cooperative enterprise, embodying fast-paced and extended social interactions. It has been suggested that it evolved as part of a larger adaptation of humans’ species-unique forms of cooperation. Although our closest living relatives, bonobos and chimpanzees, show general cooperative abilities, their communicative interactions seem to lack the cooperative nature of human conversation. Here, we revisited this claim by conducting the first systematic comparison of communicative interactions in mother-infant dyads living in two different communities of bonobos (LuiKotale, DRC; Wamba, DRC) and chimpanzees (Taï South, Côte d’Ivoire; Kanyawara, Uganda) in the wild. Focusing on the communicative function of joint-travel-initiation, we applied parameters of conversation analysis to gestural exchanges between mothers and infants. Results showed that communicative exchanges in both species resemble cooperative turn-taking sequences in human conversation. While bonobos consistently addressed the recipient via gaze before signal initiation and used so-called overlapping responses, chimpanzees engaged in more extended negotiations, involving frequent response waiting and gestural sequences. Our results thus strengthen the hypothesis that interactional intelligence paved the way to the cooperative endeavour of human language and suggest that social matrices highly impact upon communication styles.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RTVRCHUE\\Fröhlich et al. - 2016 - Unpeeling the layers of language Bonobos and chim.pdf;C\:\\Users\\u668173\\Zotero\\storage\\DG65TCJL\\srep25887.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{frohlichUnpeelingLayersLanguage2016a,
  title = {Unpeeling the Layers of Language: {{Bonobos}} and Chimpanzees Engage in Cooperative Turn-Taking Sequences},
  shorttitle = {Unpeeling the Layers of Language},
  author = {Fröhlich, Marlen and Kuchenbuch, Paul and Müller, Gudrun and Fruth, Barbara and Furuichi, Takeshi and Wittig, Roman M. and Pika, Simone},
  date = {2016-05-23},
  journaltitle = {Scientific Reports},
  volume = {6},
  pages = {25887},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep25887},
  url = {https://www.nature.com/articles/srep25887},
  urldate = {2020-12-17},
  abstract = {Human language is a fundamentally cooperative enterprise, embodying fast-paced and extended social interactions. It has been suggested that it evolved as part of a larger adaptation of humans’ species-unique forms of cooperation. Although our closest living relatives, bonobos and chimpanzees, show general cooperative abilities, their communicative interactions seem to lack the cooperative nature of human conversation. Here, we revisited this claim by conducting the first systematic comparison of communicative interactions in mother-infant dyads living in two different communities of bonobos (LuiKotale, DRC; Wamba, DRC) and chimpanzees (Taï South, Côte d’Ivoire; Kanyawara, Uganda) in the wild. Focusing on the communicative function of joint-travel-initiation, we applied parameters of conversation analysis to gestural exchanges between mothers and infants. Results showed that communicative exchanges in both species resemble cooperative turn-taking sequences in human conversation. While bonobos consistently addressed the recipient via gaze before signal initiation and used so-called overlapping responses, chimpanzees engaged in more extended negotiations, involving frequent response waiting and gestural sequences. Our results thus strengthen the hypothesis that interactional intelligence paved the way to the cooperative endeavour of human language and suggest that social matrices highly impact upon communication styles.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CCKCC6DE\\Fröhlich et al. - 2016 - Unpeeling the layers of language Bonobos and chim.pdf;C\:\\Users\\u668173\\Zotero\\storage\\4G4DR4LA\\srep25887.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{fuchsAssessingRespiratoryContributions2015,
  title = {Assessing Respiratory Contributions to F0 Declination in {{German}} across Varying Speech Tasks and Respiratory Demands},
  author = {Fuchs, S. and Petrone, C. and Rochet-Capellan, A. and Reichel, W. D. and Koenig, L. L.},
  date = {2015},
  journaltitle = {Journal of Phonetics},
  volume = {52},
  pages = {35--45},
  doi = {10.1016/j.wocn.2015.04.002},
  url = {https://hal.archives-ouvertes.fr/hal-01164773},
  urldate = {2019-08-08},
  abstract = {Many past studies have sought to determine the factors that affect f0 declination, and the physiological underpinnings of the phenomenon.  This study assessed the relation between respiration and f0 declination by means of simultaneous acoustic and respiratory recordings from read and spontaneous speech from speakers of German. Within the respective Intonational Phrase unit, we analysed the effect of the number of syllables and voiceless obstruents. Both factors could influence the slope of either f0 declination or rib cage movement. If respiration and f0 declination are related physiologically, their relationship might also be modulated by either one or both factors. Our results show consistently for both speech tasks that the slope of the rib cage movement is not related with f0 declination when length and consonant content vary. Furthermore f0 slopes are generally shallower in spontaneous than in read speech. Finally, although a higher number of voiceless obstruents yielded a greater rib cage compression, it did not affect f0 declination. These results suggest that although f0 declination occurs in many languages, it might not have a purely physiological origin in breathing, but rather reflects cognitive processing which allows speakers to look ahead when planning their utterances.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\49ZD9E2Z\\Fuchs et al. - 2015 - Assessing respiratory contributions to f0 declinat.pdf},
  keywords = {f0 declination,number of syllables,reading,respiration,speech tasks,spontaneous speech,voiceless obstruents}
}

@article{fuchsExploringSourceShortterm2019,
  title = {Exploring the Source of Short-Term Variations in Respiratory Data},
  author = {Fuchs, Susanne and Koenig, Laura L. and Petrone, Caterina},
  date = {2019-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {145},
  pages = {EL66-EL71},
  issn = {0001-4966},
  doi = {10.1121/1.5087272},
  url = {https://asa.scitation.org/doi/10.1121/1.5087272},
  urldate = {2019-08-08},
  abstract = {This study explores short-term respiratory volume changes in German oral and nasal stops and discusses to what extent these changes may be explained by laryngeal-oral coordination. It is expected that respiratory volumes decrease more rapidly when the glottis and the vocal tract are open after the release of voiceless aspirated stops. Two experiments were performed using Inductance Plethysmography and acoustics, varying consonantal properties, loudness, and prosodic focus. Results show consistent differences in respiratory slopes between voiceless vs voiced and nasal stops, which are more extreme in a loud or focused position. Thus, respiratory changes can even occur at a local level.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FZBWF5AZ\\Fuchs et al. - 2019 - Exploring the source of short-term variations in r.pdf;C\:\\Users\\u668173\\Zotero\\storage\\7J4V352F\\1.html},
  number = {1}
}

@article{fuchsExploringSourceShortterm2019a,
  title = {Exploring the Source of Short-Term Variations in Respiratory Data},
  author = {Fuchs, Susanne and Koenig, Laura L. and Petrone, Caterina},
  date = {2019-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {145},
  pages = {EL66-EL71},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.5087272},
  url = {https://asa.scitation.org/doi/full/10.1121/1.5087272},
  urldate = {2020-09-10},
  abstract = {This study explores short-term respiratory volume changes in German oral and nasal stops and discusses to what extent these changes may be explained by laryngeal-oral coordination. It is expected that respiratory volumes decrease more rapidly when the glottis and the vocal tract are open after the release of voiceless aspirated stops. Two experiments were performed using Inductance Plethysmography and acoustics, varying consonantal properties, loudness, and prosodic focus. Results show consistent differences in respiratory slopes between voiceless vs voiced and nasal stops, which are more extreme in a loud or focused position. Thus, respiratory changes can even occur at a local level.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BLW6R7JN\\Fuchs et al. - 2019 - Exploring the source of short-term variations in r.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZXGYGZIK\\1.html},
  number = {1}
}

@article{fuchsRespiratoryFoundationsSpoken2021,
  title = {The {{Respiratory Foundations}} of {{Spoken Language}}},
  author = {Fuchs, Susanne and Rochet-Capellan, Amélie},
  date = {2021},
  journaltitle = {Annual Review of Linguistics},
  volume = {7},
  pages = {null},
  doi = {10.1146/annurev-linguistics-031720-103907},
  url = {https://doi.org/10.1146/annurev-linguistics-031720-103907},
  urldate = {2020-10-27},
  abstract = {Why is breathing relevant in linguistics? In this review, we approach this question from different perspectives. The most popular view is that breathing adapts to speech because respiratory behavior has astonishing flexibility. We review research that shows that breathing pauses occur mostly at meaningful places, that breathing adapts to cognitive load during speech perception, and that breathing adapts to communicative needs in dialogue. However, speech may also adapt to breathing (e.g., the larynx can compensate for air loss, breathing can partially affect f0 declination). Enhanced breathing control may have played a role in vocalization and language evolution. These views are not mutually exclusive but, rather, reveal that speech production and breathing have an interwoven relationship that depends on communicative and physical constraints. We suggest that breathing should become an important topic for different linguistic areas and that future work should investigate the interaction between breathing and speech in different situational contexts. Expected final online publication date for the Annual Review of Linguistics, Volume 7 is January 14, 2021. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-linguistics-031720-103907},
  number = {1}
}

@article{fujiwaraRhythmicFeaturesMovement2020,
  title = {Rhythmic {{Features}} of {{Movement Synchrony}} for {{Bonding Individuals}} in {{Dyadic Interaction}}},
  author = {Fujiwara, Ken and Kimura, Masanori and Daibo, Ikuo},
  date = {2020-03-01},
  journaltitle = {Journal of Nonverbal Behavior},
  shortjournal = {J Nonverbal Behav},
  volume = {44},
  pages = {173--193},
  issn = {1573-3653},
  doi = {10.1007/s10919-019-00315-0},
  url = {https://doi.org/10.1007/s10919-019-00315-0},
  urldate = {2020-07-20},
  abstract = {This study examined ways in which rhythmic features of movement contribute to bonding between individuals. Though previous studies have described synchrony as a form of social glue, this research extends those findings to consider the impact of fast versus slow tempo on movement synchrony. This two-part experiment examined dyadic interactions as they occurred between same-sex strangers (Study 1) and friends (Study 2). Participants were video-recorded as they engaged in 5- or 6-min chats, and synchrony was evaluated using wavelet transform via calculations of cross-wavelet coherence. Study 1 employed regression commonality analysis and hierarchical linear modeling and found that among various frequency bands, rapport between individuals was positively associated with synchrony under 0.025~Hz (i.e., slower than once every 40~s) and 0.5–1.5~Hz (i.e., once every 0.67–2~s). On the contrary, Study 2 determined that synchrony of 0.5–1.5~Hz was not impactful among friend dyads and only predictive of the motivation to cultivate a friendly relationship during interactions with strangers. These results indicate the existence of a distinctive rhythm for bonding individuals, and the role of pre-existing friendship as a moderator of the bonding effect of synchrony. However, the role of relative phase (i.e., timing of movement; same versus opposite timing) remains unclear, as the ratio of in- and anti-phase patterning had no significant influence on perceived rapport and motivation to develop relationships. On the basis of the research results, a theoretical contribution is proposed to the study of interpersonal coordination.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WDNW72HR\\Fujiwara et al. - 2020 - Rhythmic Features of Movement Synchrony for Bondin.pdf},
  langid = {english},
  number = {1}
}

@article{fusaroliDialogInterpersonalSynergy2014,
  title = {Dialog as Interpersonal Synergy},
  author = {Fusaroli, Riccardo and Rączaszek-Leonardi, Joanna and Tylén, Kristian},
  date = {2014-01-01},
  journaltitle = {New Ideas in Psychology},
  shortjournal = {New Ideas in Psychology},
  volume = {32},
  pages = {147--157},
  issn = {0732-118X},
  doi = {10.1016/j.newideapsych.2013.03.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0732118X13000342},
  urldate = {2020-05-14},
  abstract = {What is the proper unit of analysis in the psycholinguistics of dialog? While classical approaches are largely based on models of individual linguistic processing, recent advances stress the social coordinative nature of dialog. In the influential interactive alignment model, dialogue is thus approached as the progressive entrainment of interlocutors' linguistic behaviors toward the alignment of situation models. Still, the driving mechanisms are attributed to individual cognition in the form of automatic structural priming. Challenging these ideas, we outline a dynamical framework for studying dialog based on the notion of interpersonal synergy. Crucial to this synergetic model is the emphasis on dialog as an emergent, self-organizing, interpersonal system capable of functional coordination. A consequence of this model is that linguistic processes cannot be reduced to the workings of individual cognitive systems but must be approached also at the interpersonal level. From the synergy model follows a number of new predictions: beyond simple synchrony, good dialog affords complementary dynamics, constrained by contextual sensitivity and functional specificity. We substantiate our arguments by reference to recent empirical studies supporting the idea of dialog as interpersonal synergy.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JAXFLHUQ\\Fusaroli et al. - 2014 - Dialog as interpersonal synergy.pdf;C\:\\Users\\u668173\\Zotero\\storage\\26XWHSYL\\S0732118X13000342.html},
  keywords = {Alignment,Complementarity,Interpersonal coordination,Linguistic coordination,Social interaction,Synergy},
  langid = {english}
}

@book{gallagherHowBodyShapes2005,
  title = {How the {{Body Shapes}} the {{Mind}}},
  author = {Gallagher, Shaun},
  date = {2005-01-27},
  publisher = {{Oxford University Press}},
  url = {http://www.oxfordscholarship.com/view/10.1093/0199271941.001.0001/acprof-9780199271948},
  urldate = {2019-04-16},
  abstract = {This book contributes to the idea that to have an understanding of the mind, consciousness, or cognition, a detailed scientific and phenomenological understanding of the body is essential. There is still a need to develop a common vocabulary that is capable of integrating discussions of brain mechanisms in neuroscience, behavioral expressions in psychology, design concerns in artificial intelligence and robotics, and debates about embodied experience in the phenomenology and philosophy of mind. This book helps to formulate this common vocabulary by developing a conceptual framework that avoids both the overly reductionistic approaches that explain everything in terms of bottom-up neuronal mechanisms, and the inflationistic approaches that explain everything in terms of Cartesian, top-down cognitive states. Through discussions of neonate imitation, the Molyneux problem, gesture, self-awareness, free will, social cognition and intersubjectivity, as well as pathologies such as deafferentation, unilateral neglect, phantom limb, autism and schizophrenia, the book proposes to remap the conceptual landscape by revitalizing the concepts of body image and body schema, proprioception, ecological experience, intermodal perception, and enactive concepts of ownership and agency for action. Informed by both philosophical theory and scientific evidence, it addresses two basic sets of questions that concern the structure of embodied experience. First, questions about the phenomenal aspects of that structure, specifically the relatively regular and constant phenomenal features found in the content of experience. Second, questions about aspects of the structure of consciousness that are more hidden, those that may be more difficult to get at because they happen before one knows it, and do not normally enter into the phenomenal content of experience in an explicit way.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Y5QC7YKC\\acprof-9780199271948.html},
  isbn = {978-0-19-160311-2},
  langid = {american}
}

@article{galModulationRespiratoryNetwork2020,
  title = {Modulation of Respiratory Network Activity by Forelimb and Hindlimb Locomotor Generators},
  author = {Gal, Jean-Patrick Le and Colnot, Eloïse and Cardoit, Laura and Bacqué-Cazenave, Julien and Thoby‐Brisson, Muriel and Juvin, Laurent and Morin, Didier},
  date = {2020},
  journaltitle = {European Journal of Neuroscience},
  volume = {52},
  pages = {3181--3195},
  issn = {1460-9568},
  doi = {10.1111/ejn.14717},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.14717},
  urldate = {2020-10-29},
  abstract = {Early at the onset of exercise, breathing rate accelerates in order to anticipate the increasing metabolic demand resulting from the extra effort produced. Accordingly, the respiratory neural networks are the target of various input signals originating either centrally or peripherally. For example, during locomotion, the activation of muscle sensory afferents is able to entrain and thereby increase the frequency of spontaneous respiratory rhythmogenesis. Moreover, the lumbar spinal networks engaged in generating hindlimb locomotor rhythms are also capable of activating the medullary respiratory generators through an ascending excitatory command. However, in the context of quadrupedal locomotion, the influence of other spinal cord regions, such as cervical and thoracic segments, remains unknown. Using isolated brainstem-spinal cord preparations from neonatal rats and mice, we show that cervicothoracic circuitry may also contribute to locomotion-induced acceleration of respiratory cycle frequency. As previously observed for the hindlimb CPGs, the pharmacological activation of forelimb locomotor networks produces episodes of fictive locomotion that in turn increase the ongoing respiratory rhythm. Thoracic neuronal circuitry may also participate indirectly in this modulation via the activation of both cervical and lumbar CPG neurons. Furthermore, using light stimulation of CHR2-expressing glutamatergic neurons, we found that the modulation of the respiratory rate during locomotion involves lumbar glutamatergic circuitry. Our results demonstrate that during locomotion, the respiratory rhythm-generating networks receive excitatory ascending inputs from the spinal circuits responsible for generating and coordinating fore- and hindlimb movements. This constitutes a distributed central mechanism that contributes to matching breathing rate to the speed of locomotion.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.14717},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CZ75XQSG\\Gal et al. - 2020 - Modulation of respiratory network activity by fore.pdf;C\:\\Users\\u668173\\Zotero\\storage\\SYT96PS5\\ejn.html},
  keywords = {locomotion,lumbar glutamatergic neurons,neonatal rodent,neural network interactions,respiration},
  langid = {english},
  number = {4}
}

@article{galModulationRespiratoryNetwork2020a,
  title = {Modulation of Respiratory Network Activity by Forelimb and Hindlimb Locomotor Generators},
  author = {Gal, Jean-Patrick Le and Colnot, Eloïse and Cardoit, Laura and Bacqué-Cazenave, Julien and Thoby‐Brisson, Muriel and Juvin, Laurent and Morin, Didier},
  date = {2020},
  journaltitle = {European Journal of Neuroscience},
  volume = {52},
  pages = {3181--3195},
  issn = {1460-9568},
  doi = {10.1111/ejn.14717},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.14717},
  urldate = {2020-10-29},
  abstract = {Early at the onset of exercise, breathing rate accelerates in order to anticipate the increasing metabolic demand resulting from the extra effort produced. Accordingly, the respiratory neural networks are the target of various input signals originating either centrally or peripherally. For example, during locomotion, the activation of muscle sensory afferents is able to entrain and thereby increase the frequency of spontaneous respiratory rhythmogenesis. Moreover, the lumbar spinal networks engaged in generating hindlimb locomotor rhythms are also capable of activating the medullary respiratory generators through an ascending excitatory command. However, in the context of quadrupedal locomotion, the influence of other spinal cord regions, such as cervical and thoracic segments, remains unknown. Using isolated brainstem-spinal cord preparations from neonatal rats and mice, we show that cervicothoracic circuitry may also contribute to locomotion-induced acceleration of respiratory cycle frequency. As previously observed for the hindlimb CPGs, the pharmacological activation of forelimb locomotor networks produces episodes of fictive locomotion that in turn increase the ongoing respiratory rhythm. Thoracic neuronal circuitry may also participate indirectly in this modulation via the activation of both cervical and lumbar CPG neurons. Furthermore, using light stimulation of CHR2-expressing glutamatergic neurons, we found that the modulation of the respiratory rate during locomotion involves lumbar glutamatergic circuitry. Our results demonstrate that during locomotion, the respiratory rhythm-generating networks receive excitatory ascending inputs from the spinal circuits responsible for generating and coordinating fore- and hindlimb movements. This constitutes a distributed central mechanism that contributes to matching breathing rate to the speed of locomotion.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.14717},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\G3BVDB2U\\Gal et al. - 2020 - Modulation of respiratory network activity by fore.pdf;C\:\\Users\\u668173\\Zotero\\storage\\9CT49RJS\\ejn.html},
  keywords = {locomotion,lumbar glutamatergic neurons,neonatal rodent,neural network interactions,respiration},
  langid = {english},
  number = {4}
}

@article{gambaIndrisHaveGot2016,
  title = {The {{Indris}} Have Got Rhythm! {{Timing}} and Pitch Variation of a Primate Song Examined between Sexes and Age Classes},
  author = {Gamba, Marco and Torti, Valeria and Estienne, Vittoria and Randrianarison, Rose M. and Valente, Daria and Rovara, Paolo and Bonadonna, Giovanna and Friard, Olivier and Giacoma, Cristina},
  date = {2016},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {10},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2016.00249},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2016.00249/full},
  urldate = {2020-12-12},
  abstract = {A crucial, common feature of speech and music is that they show non-random structures over time. It is an open question which of the other species share rhythmic abilities with humans, but in most cases the lack of knowledge about their behavioural displays prevents further studies. Indris are the only lemurs who sing. They produce loud howling cries that can be heard at several kilometres, in which all members of a group usually sing. We tested whether overlapping and turn-taking during the songs followed a precise pattern by analysing the temporal structure of the individuals' contribution to the song. We found that both dominants (males and females) and non-dominants influenced the onset timing one another. We have found that the dominant male and the dominant female in a group overlapped each other more frequently than they did with the non-dominants. We then focused on the temporal and frequency structure of particular phrases occurring during the song. Our results show that males and females have dimorphic inter-onset intervals during the phrases. Moreover, median frequencies of the unit emitted in the phrases also differ between the sexes, with males showing higher frequencies when compared to females. We have not found an effect of age on the temporal and spectral structure of the phrases. These results indicate that singing in indris has a high behavioural flexibility and varies according to social and individual factors. The flexible spectral structure of the phrases given during the song may underlie perceptual abilities that are currently unknown in other nonhuman primates, such as the ability to recognize particular pitch patterns.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NP8RHPCW\\Gamba et al. - 2016 - The Indris Have Got Rhythm! Timing and Pitch Varia.pdf},
  keywords = {gender differences,Lemurs,musical abilities,pitch pattern recognition,singing primates},
  langid = {english}
}

@article{gamezPredictiveRhythmicTapping2018,
  title = {Predictive Rhythmic Tapping to Isochronous and Tempo Changing Metronomes in the Nonhuman Primate},
  author = {Gámez, Jorge and Yc, Karyna and Ayala, Yaneri A. and Dotov, Dobromir and Prado, Luis and Merchant, Hugo},
  date = {2018-04-30},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann N Y Acad Sci},
  issn = {1749-6632},
  doi = {10.1111/nyas.13671},
  abstract = {Beat entrainment is the ability to entrain one's movements to a perceived periodic stimulus, such as a metronome or a pulse in music. Humans have a capacity to predictively respond to a periodic pulse and to dynamically adjust their movement timing to match the varying music tempos. Previous studies have shown that monkeys share some of the human capabilities for rhythmic entrainment, such as tapping regularly at the period of isochronous stimuli. However, it is still unknown whether monkeys can predictively entrain to dynamic tempo changes like humans. To address this question, we trained monkeys in three tapping tasks and compared their rhythmic entrainment abilities with those of humans. We found that, when immediate feedback about the timing of each movement is provided, monkeys can predictively entrain to an isochronous beat, generating tapping movements in anticipation of the metronome pulse. This ability also generalized to a novel untrained tempo. Notably, macaques can modify their tapping tempo by predicting the beat changes of accelerating and decelerating visual metronomes in a manner similar to humans. Our findings support the notion that nonhuman primates share with humans the ability of temporal anticipation during tapping to isochronous and smoothly changing sequences of stimuli.},
  eprint = {29707785},
  eprinttype = {pmid},
  keywords = {beat entrainment,monkey,predictive timing,rhythm perception and production,synchronization task},
  langid = {english}
}

@article{gamezPredictiveRhythmicTapping2018a,
  title = {Predictive Rhythmic Tapping to Isochronous and Tempo Changing Metronomes in the Nonhuman Primate},
  author = {Gámez, Jorge and Yc, Karyna and Ayala, Yaneri A. and Dotov, Dobromir and Prado, Luis and Merchant, Hugo},
  date = {2018-04-30},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann N Y Acad Sci},
  issn = {1749-6632},
  doi = {10.1111/nyas.13671},
  abstract = {Beat entrainment is the ability to entrain one's movements to a perceived periodic stimulus, such as a metronome or a pulse in music. Humans have a capacity to predictively respond to a periodic pulse and to dynamically adjust their movement timing to match the varying music tempos. Previous studies have shown that monkeys share some of the human capabilities for rhythmic entrainment, such as tapping regularly at the period of isochronous stimuli. However, it is still unknown whether monkeys can predictively entrain to dynamic tempo changes like humans. To address this question, we trained monkeys in three tapping tasks and compared their rhythmic entrainment abilities with those of humans. We found that, when immediate feedback about the timing of each movement is provided, monkeys can predictively entrain to an isochronous beat, generating tapping movements in anticipation of the metronome pulse. This ability also generalized to a novel untrained tempo. Notably, macaques can modify their tapping tempo by predicting the beat changes of accelerating and decelerating visual metronomes in a manner similar to humans. Our findings support the notion that nonhuman primates share with humans the ability of temporal anticipation during tapping to isochronous and smoothly changing sequences of stimuli.},
  eprint = {29707785},
  eprinttype = {pmid},
  keywords = {beat entrainment,monkey,predictive timing,rhythm perception and production,synchronization task},
  langid = {english}
}

@article{garcia-martinezEarlyDevelopmentNeanderthal2020,
  title = {Early Development of the {{Neanderthal}} Ribcage Reveals a Different Body Shape at Birth Compared to Modern Humans},
  author = {García-Martínez, Daniel and Bastir, Markus and Gómez-Olivencia, Asier and Maureille, Bruno and Golovanova, Liubov and Doronichev, Vladimir and Akazawa, Takeru and Kondo, Osamu and Ishida, Hajime and Gascho, Dominic and Zollikofer, Christoph P. E. and de León, Marcia Ponce and Heuzé, Yann},
  date = {2020-10-01},
  journaltitle = {Science Advances},
  volume = {6},
  pages = {eabb4377},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abb4377},
  url = {https://advances.sciencemag.org/content/6/41/eabb4377},
  urldate = {2020-10-08},
  abstract = {Ontogenetic studies provide clues for understanding important paleobiological aspects of extinct species. When compared to that of modern humans, the adult Neanderthal thorax was shorter, deeper, and wider. This is related to the wide Neanderthal body and is consistent with their hypothetical large requirements for energy and oxygen. Whether these differences were already established at birth or appeared later during development is unknown. To delve into this question, we use virtual reconstruction tools and geometric morphometrics to recover the 3D morphology of the ribcages of four Neanderthal individuals from birth to around 3 years old: Mezmaiskaya 1, Le Moustier 2, Dederiyeh 1, and Roc de Marsal. Our results indicate that the comparatively deep and short ribcage of the Neanderthals was already present at birth, as were other skeletal species-specific traits. This morphology possibly represents the plesiomorphic condition shared with Homo erectus, and it is likely linked to large energetic requirements. Neanderthal infants had a short and deep ribcage that was genetically determined and able to sustain the high metabolism of their massive bodies. Neanderthal infants had a short and deep ribcage that was genetically determined and able to sustain the high metabolism of their massive bodies.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JHCSTHN3\\García-Martínez et al. - 2020 - Early development of the Neanderthal ribcage revea.pdf;C\:\\Users\\u668173\\Zotero\\storage\\KNCLARQ5\\tab-pdf.html},
  langid = {english},
  number = {41}
}

@article{garciaAcousticAllometryVocal2020,
  title = {Acoustic Allometry and Vocal Learning in Mammals},
  author = {Garcia, Maxime and Ravignani, Andrea},
  date = {2020-07-29},
  journaltitle = {Biology Letters},
  shortjournal = {Biology Letters},
  volume = {16},
  pages = {20200081},
  publisher = {{Royal Society}},
  doi = {10.1098/rsbl.2020.0081},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsbl.2020.0081},
  urldate = {2020-12-04},
  abstract = {Acoustic allometry is the study of how animal vocalizations reflect their body size. A key aim of this research is to identify outliers to acoustic allometry principles and pinpoint the evolutionary origins of such outliers. A parallel strand of research investigates species capable of vocal learning, the experience-driven ability to produce novel vocal signals through imitation or modification of existing vocalizations. Modification of vocalizations is a common feature found when studying both acoustic allometry and vocal learning. Yet, these two fields have only been investigated separately to date. Here, we review and connect acoustic allometry and vocal learning across mammalian clades, combining perspectives from bioacoustics, anatomy and evolutionary biology. Based on this, we hypothesize that, as a precursor to vocal learning, some species might have evolved the capacity for volitional vocal modulation via sexual selection for ‘dishonest' signalling. We provide preliminary support for our hypothesis by showing significant associations between allometric deviation and vocal learning in a dataset of 164 mammals. Our work offers a testable framework for future empirical research linking allometric principles with the evolution of vocal learning.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SNCQCIUR\\Garcia and Ravignani - 2020 - Acoustic allometry and vocal learning in mammals.pdf},
  number = {7}
}

@article{garciaBoundSpecificSounds2020,
  title = {Bound for {{Specific Sounds}}: {{Vocal Predisposition}} in {{Animal Communication}}},
  shorttitle = {Bound for {{Specific Sounds}}},
  author = {Garcia, Maxime and Manser, Marta},
  date = {2020-09},
  journaltitle = {Trends in Cognitive Sciences},
  volume = {24},
  pages = {690--693},
  issn = {13646613},
  doi = {10.1016/j.tics.2020.05.013},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661320301376},
  urldate = {2020-08-26},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AYSH9A6X\\Garcia and Manser - 2020 - Bound for Specific Sounds Vocal Predisposition in.pdf},
  langid = {english},
  number = {9}
}

@article{garciaBoundSpecificSounds2020a,
  title = {Bound for {{Specific Sounds}}: {{Vocal Predisposition}} in {{Animal Communication}}},
  shorttitle = {Bound for {{Specific Sounds}}},
  author = {Garcia, Maxime and Manser, Marta},
  date = {2020-06-25},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2020.05.013},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661320301376},
  urldate = {2020-07-15},
  abstract = {Mechanical constraints imposed by anatomical adaptations are a ubiquitous feature of animal sound production. They can give rise to ‘vocal predispositions’ (i.e., acoustic structures strictly determined by vocal anatomy). Such predispositions are crucial to the investigation of the cognitive and evolutionary processes underlying acoustic communication in vertebrates, including human speech.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6NUUNHMJ\\S1364661320301376.html},
  keywords = {animal sound production,bioacoustics,human speech,vocal adaptation,vocal anatomy,vocal control,vocal learning},
  langid = {english}
}

@article{garciaBoundSpecificSounds2020b,
  title = {Bound for {{Specific Sounds}}: {{Vocal Predisposition}} in {{Animal Communication}}},
  shorttitle = {Bound for {{Specific Sounds}}},
  author = {Garcia, Maxime and Manser, Marta},
  date = {2020-09},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {24},
  pages = {690--693},
  issn = {13646613},
  doi = {10.1016/j.tics.2020.05.013},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661320301376},
  urldate = {2021-02-27},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9I5Z9G9E\\Garcia and Manser - 2020 - Bound for Specific Sounds Vocal Predisposition in.pdf},
  langid = {english},
  number = {9}
}

@article{gardenforsDemonstrationPantomimeEvolution2017,
  title = {Demonstration and {{Pantomime}} in the {{Evolution}} of {{Teaching}}},
  author = {Gärdenfors, Peter},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {8},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00415},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00415/full},
  urldate = {2020-01-26},
  abstract = {Donald proposes that early Homo evolved mimesis as a new form of cognition. This article investigates the mimesis hypothesis in relation to the evolution of teaching. The capacities that distinguish hominin teaching from that of other animals are demonstration and pantomime. A conceptual analysis of the instructional and communicative functions of demonstration and pantomime is presented. Archaeological evidence that demonstration was used for transmitting the Oldowan technology is summarized. It is argued that pantomime develops out of demonstration so that the primary objective of pantomime is that the onlooker learns the motoric patterns shown in the pantomime. The communicative use of pantomime is judged to be secondary. This use of pantomime is also contrasted with other forms of gestures. A key feature of the analysis is that the meaning of a pantomime is characterized by the force patterns of the movements. These force patterns form the core of a model of the cognitive mechanism behind pantomime. Finally, the role of pantomime in the evolution of language is also discussed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\R5WJ35IA\\Gärdenfors - 2017 - Demonstration and Pantomime in the Evolution of Te.pdf},
  keywords = {Demonstration,evolution of language,Gesture,mental simulation,Mimesis,pantomime,Teaching},
  langid = {english}
}

@article{garrodFoundationsRepresentationWhere2007,
  title = {Foundations of Representation: Where Might Graphical Symbol Systems Come From?},
  shorttitle = {Foundations of Representation},
  author = {Garrod, Simon and Fay, Nicolas and Lee, John and Oberlander, Jon and Macleod, Tracy},
  date = {2007-11-12},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  volume = {31},
  pages = {961--987},
  issn = {0364-0213},
  doi = {10.1080/03640210701703659},
  abstract = {It has been suggested that iconic graphical signs evolve into symbolic graphical signs through repeated usage. This article reports a series of interactive graphical communication experiments using a 'pictionary' task to establish the conditions under which the evolution might occur. Experiment 1 rules out a simple repetition based account in favor of an account that requires feedback and interaction between communicators. Experiment 2 shows how the degree of interaction affects the evolution of signs according to a process of grounding. Experiment 3 confirms the prediction that those not involved directly in the interaction have trouble interpreting the graphical signs produced in Experiment 1. On the basis of these results, this article argues that icons evolve into symbols as a consequence of the systematic shift in the locus of information from the sign to the users' memory of the sign's usage supported by an interactive grounding process.},
  eprint = {21635324},
  eprinttype = {pmid},
  langid = {english},
  number = {6}
}

@article{garrodJointActionInteractive2009,
  title = {Joint {{Action}}, {{Interactive Alignment}}, and {{Dialog}}},
  author = {Garrod, Simon and Pickering, Martin J.},
  date = {2009},
  journaltitle = {Topics in Cognitive Science},
  volume = {1},
  pages = {292--304},
  issn = {1756-8765},
  doi = {10.1111/j.1756-8765.2009.01020.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-8765.2009.01020.x},
  urldate = {2019-06-24},
  abstract = {Dialog is a joint action at different levels. At the highest level, the goal of interlocutors is to align their mental representations. This emerges from joint activity at lower levels, both concerned with linguistic decisions (e.g., choice of words) and nonlinguistic processes (e.g., alignment of posture or speech rate). Because of the high-level goal, the interlocutors are particularly concerned with close coupling at these lower levels. As we illustrate with examples, this means that imitation and entrainment are particularly pronounced during interactive communication. We then argue that the mechanisms underlying such processes involve covert imitation of interlocutors’ communicative behavior, leading to emulation of their expected behavior. In other words, communication provides a very good example of predictive emulation, in a way that leads to successful joint activity.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EQUU2PFQ\\Garrod and Pickering - 2009 - Joint Action, Interactive Alignment, and Dialog.pdf;C\:\\Users\\u668173\\Zotero\\storage\\UK5T6L9Y\\j.1756-8765.2009.01020.html},
  keywords = {Dialog,Emulation,Interactive alignment,Joint action,Prediction},
  langid = {english},
  number = {2}
}

@article{gaukerZeroTolerancePragmatics2008,
  title = {Zero Tolerance for Pragmatics},
  author = {Gauker, Christopher},
  date = {2008-12},
  journaltitle = {Synthese},
  volume = {165},
  pages = {359--371},
  issn = {0039-7857, 1573-0964},
  doi = {10.1007/s11229-007-9189-2},
  url = {http://link.springer.com/10.1007/s11229-007-9189-2},
  urldate = {2020-09-02},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WZGXI6BQ\\Gauker - 2008 - Zero tolerance for pragmatics.pdf},
  langid = {english},
  number = {3}
}

@article{gaveauTemporalStructureVertical2011,
  title = {The {{Temporal Structure}} of {{Vertical Arm Movements}}},
  author = {Gaveau, Jérémie and Papaxanthis, Charalambos},
  date = {2011-07-12},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {6},
  pages = {e22045},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0022045},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0022045},
  urldate = {2020-04-29},
  abstract = {The present study investigates how the CNS deals with the omnipresent force of gravity during arm motor planning. Previous studies have reported direction-dependent kinematic differences in the vertical plane; notably, acceleration duration was greater during a downward than an upward arm movement. Although the analysis of acceleration and deceleration phases has permitted to explore the integration of gravity force, further investigation is necessary to conclude whether feedforward or feedback control processes are at the origin of this incorporation. We considered that a more detailed analysis of the temporal features of vertical arm movements could provide additional information about gravity force integration into the motor planning. Eight subjects performed single joint vertical arm movements (45° rotation around the shoulder joint) in two opposite directions (upwards and downwards) and at three different speeds (slow, natural and fast). We calculated different parameters of hand acceleration profiles: movement duration (MD), duration to peak acceleration (D PA), duration from peak acceleration to peak velocity (D PA-PV), duration from peak velocity to peak deceleration (D PV-PD), duration from peak deceleration to the movement end (D PD-End), acceleration duration (AD), deceleration duration (DD), peak acceleration (PA), peak velocity (PV), and peak deceleration (PD). While movement durations and amplitudes were similar for upward and downward movements, the temporal structure of acceleration profiles differed between the two directions. More specifically, subjects performed upward movements faster than downward movements; these direction-dependent asymmetries appeared early in the movement (i.e., before PA) and lasted until the moment of PD. Additionally, PA and PV were greater for upward than downward movements. Movement speed also changed the temporal structure of acceleration profiles. The effect of speed and direction on the form of acceleration profiles is consistent with the premise that the CNS optimises motor commands with respect to both gravitational and inertial constraints.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AHHDCHBD\\Gaveau and Papaxanthis - 2011 - The Temporal Structure of Vertical Arm Movements.pdf;C\:\\Users\\u668173\\Zotero\\storage\\P8QUXH2F\\article.html},
  keywords = {Acceleration,Arms,Deceleration,Gravitation,Kinematics,Musculoskeletal system,Shoulders,Velocity},
  langid = {english},
  number = {7}
}

@article{geissmannRelationshipDuetSongs2000,
  title = {The Relationship between Duet Songs and Pair Bonds in Siamangs, {{Hylobates}} Syndactylus},
  author = {Geissmann, Thomas and Orgeldinger, Mathias},
  date = {2000-12-01},
  journaltitle = {Animal Behaviour},
  shortjournal = {Animal Behaviour},
  volume = {60},
  pages = {805--809},
  issn = {0003-3472},
  doi = {10.1006/anbe.2000.1540},
  url = {http://www.sciencedirect.com/science/article/pii/S0003347200915409},
  urldate = {2020-12-12},
  abstract = {One of the most commonly cited functional explanations for animal duet songs is strengthening of the pair bond. However, the evidence to support this view is, at best, limited. This study provides support by documenting a relationship between pair bonds and duet singing in siamangs. As a working hypothesis, we assume that if duetting were related to pair bonding, we might expect to see a relationship between duetting intensity and indicators of pair bond strength. Like most gibbon species, siamang pairs produce loud, long and well-coordinated duet songs. We recorded daily frequency and duration of duetting and three generally accepted indicators of pair bond strength (mutual grooming, behavioural synchronization and distance between mates) in 10 siamang groups in zoos. Duetting activity was positively correlated with grooming activity and behavioural synchronization, and negatively correlated with distance between mates. These results suggest that the production of coordinated duets by siamang pairs is related to pair bonding.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BDCQWNQM\\Geissmann and Orgeldinger - 2000 - The relationship between duet songs and pair bonds.pdf;C\:\\Users\\u668173\\Zotero\\storage\\W8YES5TA\\S0003347200915409.html},
  langid = {english},
  number = {6}
}

@inproceedings{gentnerSnowReallyShovel1999,
  title = {Is {{Snow Really Like}} a {{Shovel}}? {{Distinguishing Similarity}} from {{Thematic Relatedness}}},
  shorttitle = {Is {{Snow Really Like}} a {{Shovel}}?},
  booktitle = {Proceedings of the {{Twenty}}-First {{Annual Meeting}} of the {{Cogntiive Science Society}}},
  author = {Gentner, Dedre and Brem, Sarah K.},
  editor = {Hahn, M. and Stoness, S. C.},
  date = {1999},
  pages = {179--184},
  publisher = {{Mahwa, NJ: Lawrence Erlbaum Associates}},
  abstract = {Traditionally, thematic relatedness (chicken and egg) and similarity (chicken and turkey) have been thought of as distinct phenomena, the former the result of associative processes, and the latter reflecting comparison processes. However, recent studies (Bassok \& Medin, 1996; Wisniewski \& Bassok, 1996) suggest that similarity is a result of both association and comparison. This could call for a radical redefinition of similarity as inherently fused with association. We term this view the integration account. We consider an alternative, the confusability account, under which thematic influences intrude upon assessments of similarity but are not an essential part of the similarity process. We present two experiments supporting the confusability account. The first indicates that comparison and association are independent processes. The second shows that thematic influences rise with increased cognitive load. We believe that while a redefinition of similarity is not warranted, similarity is more vulnerable to error and intrusion than is generally thought. ice;m. tnd vo-Ms nal tlar s. Similarity is central to cognitive science. It plays a role in psychological and computational models of analogical},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NMTLE94J\\Gentner et al. - Is Snow Really Like a Shovel Distinguishing Simil.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5ARTB4DA\\summary.html}
}

@article{gentyGesturalCommunicationGorilla2009,
  title = {Gestural Communication of the Gorilla ({{Gorilla}} Gorilla): Repertoire, Intentionality and Possible Origins},
  shorttitle = {Gestural Communication of the Gorilla ({{Gorilla}} Gorilla)},
  author = {Genty, Emilie and Breuer, Thomas and Hobaiter, Catherine and Byrne, Richard W.},
  date = {2009-05-01},
  journaltitle = {Animal Cognition},
  shortjournal = {Anim Cogn},
  volume = {12},
  pages = {527--546},
  issn = {1435-9456},
  doi = {10.1007/s10071-009-0213-4},
  url = {https://doi.org/10.1007/s10071-009-0213-4},
  urldate = {2020-12-07},
  abstract = {Social groups of gorillas were observed in three captive facilities and one African field site. Cases of potential gesture use, totalling 9,540, were filtered by strict criteria for intentionality, giving a corpus of 5,250 instances of intentional gesture use. This indicated a repertoire of 102 gesture types. Most repertoire differences between individuals and sites were explicable as a consequence of environmental affordances and sampling effects: overall gesture frequency was a good predictor of universality of occurrence. Only one gesture was idiosyncratic to a single individual, and was given only to humans. Indications of cultural learning were few, though not absent. Six gestures appeared to be traditions within single social groups, but overall concordance in repertoires was almost as high between as within social groups. No support was found for the ontogenetic ritualization hypothesis as the chief means of acquisition of gestures. Many gestures whose form ruled out such an origin, i.e. gestures derived from species-typical displays, were used as intentionally and almost as flexibly as gestures whose form was consistent with learning by ritualization. When using both classes of gesture, gorillas paid specific attention to the attentional state of their audience. Thus, it would be unwarranted to divide ape gestural repertoires into ‘innate, species-typical, inflexible reactions’ and ‘individually learned, intentional, flexible communication’. We conclude that gorilla gestural communication is based on a species-typical repertoire, like those of most other mammalian species but very much larger. Gorilla gestures are not, however, inflexible signals but are employed for intentional communication to specific individuals.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IYQ8ZHEQ\\Genty et al. - 2009 - Gestural communication of the gorilla (Gorilla gor.pdf},
  langid = {english},
  number = {3}
}

@article{gentyMultiModalUseSocially2014,
  title = {Multi-{{Modal Use}} of a {{Socially Directed Call}} in {{Bonobos}}},
  author = {Genty, Emilie and Clay, Zanna and Hobaiter, Catherine and Zuberbühler, Klaus},
  date = {2014-01-15},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {9},
  pages = {e84738},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0084738},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0084738},
  urldate = {2020-12-07},
  abstract = {‘Contest hoots’ are acoustically complex vocalisations produced by adult and subadult male bonobos (Pan paniscus). These calls are often directed at specific individuals and regularly combined with gestures and other body signals. The aim of our study was to describe the multi-modal use of this call type and to clarify its communicative and social function. To this end, we observed two large groups of bonobos, which generated a sample of 585 communicative interactions initiated by 10 different males. We found that contest hooting, with or without other associated signals, was produced to challenge and provoke a social reaction in the targeted individual, usually agonistic chase. Interestingly, ‘contest hoots’ were sometimes also used during friendly play. In both contexts, males were highly selective in whom they targeted by preferentially choosing individuals of equal or higher social rank, suggesting that the calls functioned to assert social status. Multi-modal sequences were not more successful in eliciting reactions than contest hoots given alone, but we found a significant difference in the choice of associated gestures between playful and agonistic contexts. During friendly play, contest hoots were significantly more often combined with soft than rough gestures compared to agonistic challenges, while the calls' acoustic structure remained the same. We conclude that contest hoots indicate the signaller's intention to interact socially with important group members, while the gestures provide additional cues concerning the nature of the desired interaction.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DRJIBZG9\\Genty et al. - 2014 - Multi-Modal Use of a Socially Directed Call in Bon.pdf;C\:\\Users\\u668173\\Zotero\\storage\\6T429DC8\\article.html;C\:\\Users\\u668173\\Zotero\\storage\\PKMWCZHP\\article.html},
  keywords = {Acoustic signals,Acoustics,Animal communication,Animal sociality,Arms,Primates,Speech signal processing,Vocalization},
  langid = {english},
  number = {1}
}

@article{gentySpatialReferenceBonobo2014,
  title = {Spatial {{Reference}} in a {{Bonobo Gesture}}},
  author = {Genty, Emilie and Zuberbühler, Klaus},
  date = {2014-07-21},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {24},
  pages = {1601--1605},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2014.05.065},
  url = {http://www.sciencedirect.com/science/article/pii/S0960982214006666},
  urldate = {2020-12-07},
  abstract = {Great apes frequently produce gestures during social interactions to communicate in flexible, goal-directed ways [1, 2, 3], a feature with considerable relevance for the ongoing debate over the evolutionary origins of human language [1, 4]. But despite this shared feature with language, there has been a lack of evidence for semantic content in ape gestures. According to one authoritative view, ape gestures thus do not have any specific referential, iconic, or deictic content, a fundamental difference versus human gestures and spoken language [1, 5] that suggests these features have a more recent origin in human evolution, perhaps caused by a fundamental transition from ape-like individual intentionality to human-like shared intentionality [6]. Here, we revisit this human uniqueness claim with a study of a previously undescribed human-like beckoning gesture in bonobos that has potentially both deictic and iconic character. We analyzed beckoning in two groups of bonobos, kept under near natural environmental and social conditions at the Lola Ya Bonobo sanctuary near Kinshasa, Democratic Republic of Congo, in terms of its linguistic content and underlying communicative intention.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UDLI8A24\\Genty and Zuberbühler - 2014 - Spatial Reference in a Bonobo Gesture.pdf;C\:\\Users\\u668173\\Zotero\\storage\\C6XZVCPP\\S0960982214006666.html},
  langid = {english},
  number = {14}
}

@article{gerwingLinguisticInfluencesGesture2004,
  title = {Linguistic Influences on Gesture’s Form},
  author = {Gerwing, Jennifer and Bavelas, Janet},
  date = {2004-01-01},
  journaltitle = {Gesture},
  volume = {4},
  pages = {157--195},
  publisher = {{John Benjamins}},
  issn = {1568-1475, 1569-9773},
  doi = {10.1075/gest.4.2.04ger},
  url = {https://www.jbe-platform.com/content/journals/10.1075/gest.4.2.04ger},
  urldate = {2020-03-18},
  abstract = {Hand gestures in face-to-face dialogue are symbolic acts, integrated with speech. Little is known about the factors that determine the physical form of these gestures. When the gesture depicts a previous nonsymbolic action, it obviously resembles this action; however, such gestures are not only noticeably different from the original action but, when they occur in a series, are different from each other. This paper presents an experiment with two separate analyses (one quantitative, one qualitative) testing the hypothesis that the immediate communicative function is a determinant of the symbolic form of the gesture. First, we manipulated whether the speaker was describing the previous action to an addressee who had done the same actions and therefore shared common ground or to one who had done different actions and therefore did not share common ground. The common ground gestures were judged to be significantly less complex, precise, or informative than the latter, a finding similar to the effects of common ground on words. In the qualitative analysis, we used the given versus new principle to analyze a series of gestures about the same actions by the same speaker. The speaker emphasized the new information in each gesture by making it larger, clearer, etc. When this information became given, a gesture for the same action became smaller or less precise, which is similar to findings for given versus new information in words. Thus the immediate communicative function (e.g., to convey information that is common ground or that is new) played a major role in determining the physical form of the gestures.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HDMQG72F\\gest.4.2.html},
  langid = {english},
  number = {2}
}

@article{ghazanfarEvolutionHumanVocal2008,
  title = {Evolution of Human Vocal Production},
  author = {Ghazanfar, Asif A. and Rendall, Drew},
  date = {2008-06-03},
  journaltitle = {Current biology: CB},
  shortjournal = {Curr. Biol.},
  volume = {18},
  pages = {R457-460},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2008.03.030},
  eprint = {18522811},
  eprinttype = {pmid},
  keywords = {Animals,Biological Evolution,Humans,Larynx,Neocortex,Primates,Respiratory Mechanics,Speech,Speech Perception,Thorax,Vocalization; Animal},
  langid = {english},
  number = {11}
}

@article{ghazanfarFacialExpressionsLinked2003,
  title = {Facial Expressions Linked to Monkey Calls},
  author = {Ghazanfar, Asif A. and Logothetis, Nikos K.},
  date = {2003},
  journaltitle = {Nature},
  volume = {423},
  pages = {937--938},
  publisher = {{Nature Publishing Group}},
  location = {{United Kingdom}},
  issn = {1476-4687(Electronic),0028-0836(Print)},
  doi = {10.1038/423937a},
  abstract = {The perception of human speech can be enhanced by a combination of auditory and visual signals. Animals sometimes accompany their vocalizations with distinctive body postures and facial expressions, although it is not know whether their interpretation of these signals is unified. Here, the authors use a paradigm in which 'preferential looking' is monitored to show that rhesus monkeys (Macaca mulatta), a species that communicates by means of elaborate facial and vocal expression, are able to recognize the correspondence between the auditory and visual components of their calls. This crossmodal identification of vocal signals by a primate might represent an evolutionary precursor to humans' ability to match spoken words with facial articulation. The authors tested whether rhesus monkeys cold recognize auditory-visual correspondence between their 'coo' and 'threat' calls. Results indicated that rhesus monkeys have an inherent ability to match acoustically presented conspecific vocalizations with the appropriate facial posture. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AU3DS2R8\\2003-06549-002.html},
  keywords = {Animal Social Behavior,Animal Vocalizations,Facial Expressions,Monkeys,Visual Discrimination},
  number = {6943}
}

@article{ghazanfarMultisensoryVocalCommunication2013,
  title = {Multisensory Vocal Communication in Primates and the Evolution of Rhythmic Speech},
  author = {Ghazanfar, Asif A.},
  date = {2013-09-01},
  journaltitle = {Behavioral ecology and sociobiology},
  shortjournal = {Behav Ecol Sociobiol},
  volume = {67},
  issn = {0340-5443},
  doi = {10.1007/s00265-013-1491-z},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3821777/},
  urldate = {2019-10-13},
  abstract = {The integration of the visual and auditory modalities during human speech perception is the default mode of speech processing. That is, visual speech perception is not a capacity that is “piggybacked” on to auditory-only speech perception. Visual information from the mouth and other parts of the face is used by all perceivers to enhance auditory speech. This integration is ubiquitous and automatic and is similar across all individuals across all cultures. The two modalities seem to be integrated even at the earliest stages of human cognitive development. If multisensory speech is the default mode of perception, then this should be reflected in the evolution of vocal communication. The purpose of this review is to describe the data that reveal that human speech is not uniquely multisensory. In fact, the default mode of communication is multisensory in nonhuman primates as well but perhaps emerging with a different developmental trajectory. Speech production, however, exhibits a unique bimodal rhythmic structure in that both the acoustic output and the movements of the mouth are rhythmic and tightly correlated. This structure is absent in most monkey vocalizations. One hypothesis is that the bimodal speech rhythm may have evolved through the rhythmic facial expressions of ancestral primates, as indicated by mounting comparative evidence focusing on the lip-smacking gesture.},
  eprint = {24222931},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IN8HWBHH\\Ghazanfar - 2013 - Multisensory vocal communication in primates and t.pdf},
  number = {9},
  pmcid = {PMC3821777}
}

@article{ghazanfarNeocortexEssentiallyMultisensory2006,
  title = {Is Neocortex Essentially Multisensory?},
  author = {Ghazanfar, Asif A. and Schroeder, Charles E.},
  date = {2006-06-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  pages = {278--285},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.04.008},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661306001045},
  urldate = {2020-12-05},
  abstract = {Although sensory perception and neurobiology are traditionally investigated one modality at a time, real world behaviour and perception are driven by the integration of information from multiple sensory sources. Mounting evidence suggests that the neural underpinnings of multisensory integration extend into early sensory processing. This article examines the notion that neocortical operations are essentially multisensory. We first review what is known about multisensory processing in higher-order association cortices and then discuss recent anatomical and physiological findings in presumptive unimodal sensory areas. The pervasiveness of multisensory influences on all levels of cortical processing compels us to reconsider thinking about neural processing in unisensory terms. Indeed, the multisensory nature of most, possibly all, of the neocortex forces us to abandon the notion that the senses ever operate independently during real-world cognition.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MKR3EZVF\\Ghazanfar and Schroeder - 2006 - Is neocortex essentially multisensory.pdf;C\:\\Users\\u668173\\Zotero\\storage\\R5XQS4NG\\S1364661306001045.html},
  langid = {english},
  number = {6}
}

@article{ghazanfarVocaltractResonancesIndexical2007,
  title = {Vocal-Tract Resonances as Indexical Cues in Rhesus Monkeys},
  author = {Ghazanfar, Asif A. and Turesson, Hjalmar K. and Maier, Joost X. and van Dinther, Ralph and Patterson, Roy D. and Logothetis, Nikos K.},
  date = {2007-03-06},
  journaltitle = {Current Biology},
  shortjournal = {Curr Biol},
  volume = {17},
  pages = {425--430},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2007.01.029},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2361420/},
  urldate = {2019-10-17},
  abstract = {Vocal-tract resonances (or formants) are acoustic signatures in the voice and are related to the shape and length of the vocal tract. Formants play an important role in human communication, helping us not only to distinguish several different speech sounds , but also to extract important information related to the physical characteristics of the speaker, so-called indexical cues. How did formants come to play such an important role in human vocal communication? One hypothesis suggests that the ancestral role of formant perception—a role that might be present in extant nonhuman primates—was to provide indexical cues . Although formants are present in the acoustic structure of vowel-like calls of monkeys  and implicated in the discrimination of call types , it is not known whether they use this feature to extract indexical cues. Here, we investigate whether rhesus monkeys can use the formant structure in their “coo” calls to assess the age-related body size of conspecifics. Using a preferential-looking paradigm  and synthetic coo calls in which formant structure simulated an adult/large- or juvenile/small-sounding individual, we demonstrate that untrained monkeys attend to formant cues and link large-sounding coos to large faces and small-sounding coos to small faces—in essence, they can, like humans , use formants as indicators of age-related body size.},
  eprint = {17320389},
  eprinttype = {pmid},
  number = {5-2},
  options = {useprefix=true},
  pmcid = {PMC2361420}
}

@article{gibsonHowEfficiencyShapes2019,
  title = {How {{Efficiency Shapes Human Language}}},
  author = {Gibson, Edward and Futrell, Richard and Piantadosi, Steven P. and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
  date = {2019-05-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  pages = {389--407},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.02.003},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661319300580},
  urldate = {2020-03-12},
  abstract = {Cognitive science applies diverse tools and perspectives to study human language. Recently, an exciting body of work has examined linguistic phenomena through the lens of efficiency in usage: what otherwise puzzling features of language find explanation in formal accounts of how language might be optimized for communication and learning? Here, we review studies that deploy formal tools from probability and information theory to understand how and why language works the way that it does, focusing on phenomena ranging from the lexicon through syntax. These studies show how a pervasive pressure for efficiency guides the forms of natural language and indicate that a rich future for language research lies in connecting linguistics to cognitive psychology and mathematical theories of communication and inference.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HFCJ2V5L\\Gibson et al. - 2019 - How Efficiency Shapes Human Language.pdf;C\:\\Users\\u668173\\Zotero\\storage\\NXL7JLUW\\S1364661319300580.html},
  keywords = {communication,cross-linguistic universals,language complexity,language efficiency,language evolution,language learnability},
  langid = {english},
  number = {5}
}

@book{gibsonSensesConsideredPerceptual1966,
  title = {The Senses Considered as Perceptual Systems},
  author = {Gibson, J. J.},
  date = {1966},
  publisher = {{Houghton Mifflin}},
  location = {{Boston}}
}

@article{gilmanEffectHeadPosition2017,
  title = {The {{Effect}} of {{Head Position}} and/or {{Stance}} on the {{Self}}-Perception of {{Phonatory Effort}}},
  author = {Gilman, Marina and Johns, Michael M.},
  date = {2017-01-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {31},
  pages = {131.e1-131.e4},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2015.11.024},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199715002817},
  urldate = {2020-10-19},
  abstract = {Background Vocal fatigue is a common but poorly defined complaint of patients presenting with voice disorders. Definitions of vocal fatigue generally include increased self-perceived phonatory effort resulting from references to vocal loading or prolonged voice use resulting in deterioration of function. The present study looks at the role of posture, specifically head position and stance, in self-perceived phonatory effort. Methods Forty-six healthy adults, 13 males and 33 females (mean age was 27.5), with no history of vocal problems/disorders within the past year were recruited. Subjects were asked to sustain the vowel /a/ at a comfortable pitch and loudness for 5–10 seconds in each of six positions: sitting and standing in the manner habitual for each subject, two exaggerated positions of the head (head back and head forward), and two exaggerated positions in standing (standing with knees locked and with knees soft). Each position was repeated three times in randomized order, resulting in 18 trials for each subject. After each repetition of the sustained /a/, subjects were asked to rate their experience of vocal effort using a 100-mm visual analog scale (0–40 least effort, 40–60 habitual effort, and 60–100 increased effort). Results Repeated measures analysis of variance revealed significant difference in the self-perceived phonatory effort levels across positions (P value\,{$<$}\,0.001). The exaggerated forward and back head positions in both sitting and standing positions showed the greatest significance on the Tukey post hoc tests (P\,{$<$}\,0.000). Conclusions Based on the findings, posture may play a more important role in vocal fatigue than previously thought.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\63GNQ2TY\\S0892199715002817.html},
  keywords = {mechanisms,posture,vocal effort,vocal fatigue,voice disorders},
  langid = {english},
  number = {1}
}

@article{gilmoreNeuralBehavioralEvidence2021,
  title = {Neural and {{Behavioral Evidence}} for {{Vibrotactile Beat Perception}} and {{Bimodal Enhancement}}},
  author = {Gilmore, Sean A. and Russo, Frank A.},
  date = {2021-01-21},
  journaltitle = {Journal of Cognitive Neuroscience},
  pages = {1--16},
  publisher = {{MIT Press}},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01673},
  url = {https://doi.org/10.1162/jocn_a_01673},
  urldate = {2021-03-01},
  abstract = {The ability to synchronize movements to a rhythmic stimulus, referred to as sensorimotor synchronization (SMS), is a behavioral measure of beat perception. Although SMS is generally superior when rhythms are presented in the auditory modality, recent research has demonstrated near-equivalent SMS for vibrotactile presentations of isochronous rhythms [Ammirante, P., Patel, A. D., \& Russo, F. A. Synchronizing to auditory and tactile metronomes: A test of the auditory–motor enhancement hypothesis. Psychonomic Bulletin \& Review, 23, 1882–1890, 2016]. The current study aimed to replicate and extend this study by incorporating a neural measure of beat perception. Nonmusicians were asked to tap to rhythms or to listen passively while EEG data were collected. Rhythmic complexity (isochronous, nonisochronous) and presentation modality (auditory, vibrotactile, bimodal) were fully crossed. Tapping data were consistent with those observed by Ammirante et al. (2016), revealing near-equivalent SMS for isochronous rhythms across modality conditions and a drop-off in SMS for nonisochronous rhythms, especially in the vibrotactile condition. EEG data revealed a greater degree of neural entrainment for isochronous compared to nonisochronous trials as well as for auditory and bimodal compared to vibrotactile trials. These findings led us to three main conclusions. First, isochronous rhythms lead to higher levels of beat perception than nonisochronous rhythms across modalities. Second, beat perception is generally enhanced for auditory presentations of rhythm but still possible under vibrotactile presentation conditions. Finally, exploratory analysis of neural entrainment at harmonic frequencies suggests that beat perception may be enhanced for bimodal presentations of rhythm.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MJTJAE7K\\Gilmore and Russo - 2021 - Neural and Behavioral Evidence for Vibrotactile Be.pdf;C\:\\Users\\u668173\\Zotero\\storage\\R7DVFVUW\\jocn_a_01673.html}
}

@article{gingrasLinkingMelodicExpectation2016,
  title = {Linking Melodic Expectation to Expressive Performance Timing and Perceived Musical Tension},
  author = {Gingras, Bruno and Pearce, Marcus T. and Goodchild, Meghan and Dean, Roger T. and Wiggins, Geraint and McAdams, Stephen},
  date = {2016-04},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {42},
  pages = {594--609},
  issn = {1939-1277},
  doi = {10.1037/xhp0000141},
  abstract = {This research explored the relations between the predictability of musical structure, expressive timing in performance, and listeners' perceived musical tension. Studies analyzing the influence of expressive timing on listeners' affective responses have been constrained by the fact that, in most pieces, the notated durations limit performers' interpretive freedom. To circumvent this issue, we focused on the unmeasured prelude, a semi-improvisatory genre without notated durations. In Experiment 1, 12 professional harpsichordists recorded an unmeasured prelude on a harpsichord equipped with a MIDI console. Melodic expectation was assessed using a probabilistic model (IDyOM [Information Dynamics of Music]) whose expectations have been previously shown to match closely those of human listeners. Performance timing information was extracted from the MIDI data using a score-performance matching algorithm. Time-series analyses showed that, in a piece with unspecified note durations, the predictability of melodic structure measurably influenced tempo fluctuations in performance. In Experiment 2, another 10 harpsichordists, 20 nonharpsichordist musicians, and 20 nonmusicians listened to the recordings from Experiment 1 and rated the perceived tension continuously. Granger causality analyses were conducted to investigate predictive relations among melodic expectation, expressive timing, and perceived tension. Although melodic expectation, as modeled by IDyOM, modestly predicted perceived tension for all participant groups, neither of its components, information content or entropy, was Granger causal. In contrast, expressive timing was a strong predictor and was Granger causal. However, because melodic expectation was also predictive of expressive timing, our results outline a complete chain of influence from predictability of melodic structure via expressive performance timing to perceived musical tension. (PsycINFO Database Record},
  eprint = {26594881},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\235Q3CYU\\Gingras et al. - 2016 - Linking melodic expectation to expressive performa.pdf},
  keywords = {Adult,Auditory Perception,Female,Humans,Male,Middle Aged,Music,Time Perception,Young Adult},
  langid = {english},
  number = {4}
}

@inproceedings{ginosarLearningIndividualStyles2019,
  title = {Learning Individual Styles of Conversational Gesture},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ginosar, S. and Bar, A. and Kohavi, G. and Chan, C. and Owens, A. and Malik, J.},
  date = {2019},
  pages = {3497--3506},
  url = {https://arxiv.org/abs/1906.04160},
  urldate = {2019-08-09},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LYKM7A5J\\1906.html}
}

@article{giorginoComputingVisualizingDynamic2009,
  title = {Computing and {{Visualizing Dynamic Time Warping Alignments}} in {{{\emph{R}}}} : {{The}} {\textbf{Dtw}} {{Package}}},
  shorttitle = {Computing and {{Visualizing Dynamic Time Warping Alignments}} in {{{\emph{R}}}}},
  author = {Giorgino, Toni},
  date = {2009},
  journaltitle = {Journal of Statistical Software},
  volume = {31},
  issn = {1548-7660},
  doi = {10.18637/jss.v031.i07},
  url = {http://www.jstatsoft.org/v31/i07/},
  urldate = {2019-08-07},
  abstract = {This introduction to the R package dtw is a (slightly) modified version of Giorgino (2009), published in the Journal of Statistical Software. Dynamic time warping is a popular technique for comparing time series, providing both a distance measure that is insensitive to local compression and stretches and the warping which optimally deforms one of the two input series onto the other. A variety of algorithms and constraints have been discussed in the literature. The dtw package provides an unification of them; it allows R users to compute time series alignments mixing freely a variety of continuity constraints, restriction windows, endpoints, local distance definitions, and so on. The package also provides functions for visualizing alignments and constraints using several classic diagram types.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HBYA9Z4B\\Giorgino - 2009 - Computing and Visualizing Dynamic Time Warping Ali.pdf},
  langid = {english},
  number = {7}
}

@article{giraudCorticalOscillationsSpeech2012,
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  shorttitle = {Cortical Oscillations and Speech Processing},
  author = {Giraud, Anne-Lise and Poeppel, David},
  date = {2012-04},
  journaltitle = {Nature Neuroscience},
  volume = {15},
  pages = {511--517},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3063},
  url = {http://www.nature.com/articles/nn.3063},
  urldate = {2019-08-30},
  abstract = {Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example, by segregating information and organizing spike timing. Recent data show that delta, theta and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. We argue that they are foundational in speech and language processing, ‘packaging’ incoming information into units of the appropriate temporal granularity. Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\B5CYUJLD\\Giraud and Poeppel - 2012 - Cortical oscillations and speech processing emerg.pdf},
  langid = {english},
  number = {4}
}

@article{giraudinIntercostalAbdominalRespiratory2008,
  title = {Intercostal and {{Abdominal Respiratory Motoneurons}} in the {{Neonatal Rat Spinal Cord}}: {{Spatiotemporal Organization}} and {{Responses}} to {{Limb Afferent Stimulation}}},
  shorttitle = {Intercostal and {{Abdominal Respiratory Motoneurons}} in the {{Neonatal Rat Spinal Cord}}},
  author = {Giraudin, Aurore and Cabirol-Pol, Marie-Jeanne and Simmers, John and Morin, Didier},
  date = {2008-05-01},
  journaltitle = {Journal of Neurophysiology},
  volume = {99},
  pages = {2626--2640},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.01298.2007},
  url = {https://journals.physiology.org/doi/full/10.1152/jn.01298.2007},
  urldate = {2020-09-19},
  abstract = {Respiration requires the coordinated rhythmic contractions of diverse muscles to produce ventilatory movements adapted to organismal requirements. During fast locomotion, locomotory and respiratory movements are coordinated to reduce mechanical conflict between these functions. Using semi-isolated and isolated in vitro brain stem-spinal cord preparations from neonatal rats, we have characterized for the first time the respiratory patterns of all spinal intercostal and abdominal motoneurons and explored their functional relationship with limb sensory inputs. Neuroanatomical and electrophysiological procedures were initially used to locate intercostal and abdominal motoneurons in the cord. Intercostal motoneuron somata are distributed rostrocaudally from C7–T13 segments. Abdominal motoneuron somata lie between T8 and L2. In accordance with their soma distributions, inspiratory intercostal motoneurons are recruited in a rostrocaudal sequence during each respiratory cycle. Abdominal motoneurons express expiratory-related discharge that alternates with inspiration. Lesioning experiments confirmed the pontine origin of this expiratory activity, which was abolished by a brain stem transection at the rostral boundary of the VII nucleus, a critical area for respiratory rhythmogenesis. Entrainment of fictive respiratory rhythmicity in intercostal and abdominal motoneurons was elicited by periodic low-threshold dorsal root stimulation at lumbar (L2) or cervical (C7) levels. These effects are mediated by direct ascending fibers to the respiratory centers and a combination of long-projection and polysynaptic descending pathways. Therefore the isolated brain stem-spinal cord in vitro generates a complex pattern of respiratory activity in which alternating inspiratory and expiratory discharge occurs in functionally identified spinal motoneuron pools that are in turn targeted by both forelimb and hindlimb somatic afferents to promote locomotor-respiratory coupling.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IG3P5TJE\\Giraudin et al. - 2008 - Intercostal and Abdominal Respiratory Motoneurons .pdf;C\:\\Users\\u668173\\Zotero\\storage\\I8K25BTQ\\jn.01298.html},
  number = {5}
}

@article{giraudinSpinalPontineRelay2012,
  title = {Spinal and {{Pontine Relay Pathways Mediating Respiratory Rhythm Entrainment}} by {{Limb Proprioceptive Inputs}} in the {{Neonatal Rat}}},
  author = {Giraudin, Aurore and Le Bon-Jégo, Morgane and Cabirol, Marie-Jeanne and Simmers, John and Morin, Didier},
  date = {2012-08-22},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J Neurosci},
  volume = {32},
  pages = {11841--11853},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.0360-12.2012},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6703750/},
  urldate = {2020-09-19},
  abstract = {The coordination of locomotion and respiration is widespread among mammals, although the underlying neural mechanisms are still only partially understood. It was previously found in neonatal rat that cyclic electrical stimulation of spinal cervical and lumbar dorsal roots (DRs) can fully entrain (1:1 coupling) spontaneous respiratory activity expressed by the isolated brainstem/spinal cord. Here, we used a variety of preparations to determine the type of spinal sensory inputs responsible for this respiratory rhythm entrainment, and to establish the extent to which limb movement-activated feedback influences the medullary respiratory networks via direct or relayed ascending pathways. During in vivo overground locomotion, respiratory rhythm slowed and became coupled 1:1 with locomotion. In hindlimb-attached semi-isolated preparations, passive flexion–extension movements applied to a single hindlimb led to entrainment of fictive respiratory rhythmicity recorded in phrenic motoneurons, indicating that the recruitment of limb proprioceptive afferents could participate in the locomotor-respiratory coupling. Furthermore, in correspondence with the regionalization of spinal locomotor rhythm-generating circuitry, the stimulation of DRs at different segmental levels in isolated preparations revealed that cervical and lumbosacral proprioceptive inputs are more effective in this entraining influence than thoracic afferent pathways. Finally, blocking spinal synaptic transmission and using a combination of electrophysiology, calcium imaging and specific brainstem lesioning indicated that the ascending entraining signals from the cervical or lumbar limb afferents are transmitted across first-order synapses, probably monosynaptic, in the spinal cord. They are then conveyed to the brainstem respiratory centers via a brainstem pontine relay located in the parabrachial/Kölliker-Fuse nuclear complex.},
  eprint = {22915125},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZKT5BSRK\\Giraudin et al. - 2012 - Spinal and Pontine Relay Pathways Mediating Respir.pdf},
  number = {34},
  pmcid = {PMC6703750}
}

@incollection{gleitmanVerbsFeatherFlock,
  title = {Verbs of a Feather Flock Together {{II}}: {{The}} Child's Discovery of Words and Their Meanings.},
  booktitle = {The {{Legacy}} of {{Zellig Harris}}: {{Language}} and {{Information Into}} the 21st {{Century}}},
  author = {Gleitman, L. R.},
  editor = {Nevin, B. E.},
  volume = {1},
  pages = {209--229},
  series = {Philosophy of {{Science}}, {{Syntax}}, and {{Semantics}}}
}

@inproceedings{godfreySwitchboardTelephoneSpeech1992,
  title = {Switchboard: Telephone Speech Corpus for Research and Development},
  author = {Godfrey, J. J. and Holliman, E. C. and McDaniel, J.},
  date = {1992},
  volume = {1},
  pages = {517--520},
  location = {{San Francisco, CA}},
  url = {https://catalog.ldc.upenn.edu/LDC97S62},
  urldate = {2020-09-21},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EU3EGB9R\\LDC97S62.html}
}

@article{goldin-meadowGestureSignLanguage2017,
  title = {Gesture, Sign, and Language: {{The}} Coming of Age of Sign Language and Gesture Studies},
  shorttitle = {Gesture, Sign, and Language},
  author = {Goldin-Meadow, Susan and Brentari, Diane},
  date = {2017-01},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  volume = {40},
  pages = {e46},
  issn = {1469-1825},
  doi = {10.1017/S0140525X15001247},
  abstract = {How does sign language compare with gesture, on the one hand, and spoken language on the other? Sign was once viewed as nothing more than a system of pictorial gestures without linguistic structure. More recently, researchers have argued that sign is no different from spoken language, with all of the same linguistic structures. The pendulum is currently swinging back toward the view that sign is gestural, or at least has gestural components. The goal of this review is to elucidate the relationships among sign language, gesture, and spoken language. We do so by taking a close look not only at how sign has been studied over the past 50 years, but also at how the spontaneous gestures that accompany speech have been studied. We conclude that signers gesture just as speakers do. Both produce imagistic gestures along with more categorical signs or words. Because at present it is difficult to tell where sign stops and gesture begins, we suggest that sign should not be compared with speech alone but should be compared with speech-plus-gesture. Although it might be easier (and, in some cases, preferable) to blur the distinction between sign and gesture, we argue that distinguishing between sign (or speech) and gesture is essential to predict certain types of learning and allows us to understand the conditions under which gesture takes on properties of sign, and speech takes on properties of gesture. We end by calling for new technology that may help us better calibrate the borders between sign and gesture.},
  eprint = {26434499},
  eprinttype = {pmid},
  keywords = {categorical,gesture-speech mismatch,gradient,homesign,imagistic,learning,morphology,phonology,syntax},
  langid = {english},
  pmcid = {PMC4821822}
}

@book{goldin-meadowHearingGestureHow2003,
  title = {Hearing {{Gesture}}: {{How Our Hands Help Us Think}}},
  shorttitle = {Hearing {{Gesture}}},
  author = {Goldin-Meadow, Susan},
  date = {2003},
  publisher = {{Harvard University Press}},
  abstract = {Many nonverbal behaviors--smiling, blushing, shrugging--reveal our emotions. One nonverbal behavior, gesturing, exposes our thoughts. This book explores how we move our hands when we talk, and what it means when we do so. Susan Goldin-Meadow begins with an intriguing discovery: when explaining their answer to a task, children sometimes communicate different ideas with their hand gestures than with their spoken words. Moreover, children whose gestures do not match their speech are particularly likely to benefit from instruction in that task. Not only do gestures provide insight into the unspoken thoughts of children (one of Goldin-Meadow's central claims), but gestures reveal a child's readiness to learn, and even suggest which teaching strategies might be most beneficial.In addition, Goldin-Meadow characterizes gesture when it fulfills the entire function of language (as in the case of Sign Languages of the Deaf), when it is reshaped to suit different cultures (American and Chinese), and even when it occurs in children who are blind from birth.Focusing on what we can discover about speakers--adults and children alike--by watching their hands, this book discloses the active role that gesture plays in conversation and, more fundamentally, in thinking. In general, we are unaware of gesture, which occurs as an undercurrent alongside an acknowledged verbal exchange. In this book, Susan Goldin-Meadow makes clear why we must not ignore the background conversation.},
  eprint = {LCJ5eQdsolsC},
  eprinttype = {googlebooks},
  isbn = {978-0-674-01837-2},
  keywords = {Language Arts & Disciplines / Communication Studies,Psychology / Cognitive Psychology & Cognition,Psychology / Developmental / General,Psychology / Physiological Psychology},
  langid = {english},
  pagetotal = {308}
}

@article{goldin-meadowNaturalOrderEvents2008,
  title = {The Natural Order of Events: {{How}} Speakers of Different Languages Represent Events Nonverbally},
  shorttitle = {The Natural Order of Events},
  author = {Goldin-Meadow, S. and So, W. C. and Ozyurek, A. and Mylander, C.},
  date = {2008-07-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {105},
  pages = {9163--9168},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0710060105},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0710060105},
  urldate = {2020-03-24},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ECWLU2PY\\Goldin-Meadow et al. - 2008 - The natural order of events How speakers of diffe.pdf},
  langid = {english},
  number = {27}
}

@article{goldin-meadowNaturalOrderEvents2008a,
  title = {The Natural Order of Events: {{How}} Speakers of Different Languages Represent Events Nonverbally},
  shorttitle = {The Natural Order of Events},
  author = {Goldin-Meadow, Susan and So, Wing Chee and Özyürek, Aslı and Mylander, Carolyn},
  date = {2008-07-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {105},
  pages = {9163--9168},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0710060105},
  url = {https://www.pnas.org/content/105/27/9163},
  urldate = {2019-08-26},
  abstract = {To test whether the language we speak influences our behavior even when we are not speaking, we asked speakers of four languages differing in their predominant word orders (English, Turkish, Spanish, and Chinese) to perform two nonverbal tasks: a communicative task (describing an event by using gesture without speech) and a noncommunicative task (reconstructing an event with pictures). We found that the word orders speakers used in their everyday speech did not influence their nonverbal behavior. Surprisingly, speakers of all four languages used the same order and on both nonverbal tasks. This order, actor–patient–act, is analogous to the subject–object–verb pattern found in many languages of the world and, importantly, in newly developing gestural languages. The findings provide evidence for a natural order that we impose on events when describing and reconstructing them nonverbally and exploit when constructing language anew.},
  eprint = {18599445},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\L9ZXNEJ5\\Goldin-Meadow et al. - 2008 - The natural order of events How speakers of diffe.pdf;C\:\\Users\\u668173\\Zotero\\storage\\54FTX72G\\9163.html},
  keywords = {gesture,language genesis,sign language,word order},
  langid = {english},
  number = {27}
}

@article{goldsteinInfluenceClusteringCoefficient2014,
  title = {The Influence of Clustering Coefficient on Word-Learning: How Groups of Similar Sounding Words Facilitate Acquisition},
  shorttitle = {The Influence of Clustering Coefficient on Word-Learning},
  author = {Goldstein, Rutherford and Vitevitch, Michael S.},
  date = {2014},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.01307},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.01307/full},
  urldate = {2021-01-23},
  abstract = {Clustering coefficient, C, measures the extent to which neighbors of a word are also neighbors of each other, and has been shown to influence speech production, speech perception, and several memory-related processes. In this study we examined how C influences word-learning. Participants were trained over three sessions at one-week intervals, and tested with a picture-naming task on nonword-nonobject pairs. We found an advantage for novel words with high C (the neighbors of this novel word are likely to be neighbors with each other), but only after the one-week retention period with no additional exposures to the stimuli. The results are consistent with the spreading-activation network-model of the lexicon proposed by Chan \& Vitevitch (2009). The influence of C on various language-related processes suggests that characteristics of the individual word are not the only things that influence processing; rather, lexical processing may also be influenced by the relationships that exist among words in the lexicon.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RSJFTSCR\\Goldstein and Vitevitch - 2014 - The influence of clustering coefficient on word-le.pdf},
  keywords = {clustering coefficient,neighborhood density,Network Science,psycholinguistic,word-learning},
  langid = {english}
}

@article{gollerPeripheralMotorDynamics2004,
  title = {Peripheral Motor Dynamics of Song Production in the Zebra Finch},
  author = {Goller, Franz and Cooper, Brenton G.},
  date = {2004-06},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann N Y Acad Sci},
  volume = {1016},
  pages = {130--152},
  issn = {0077-8923},
  doi = {10.1196/annals.1298.009},
  abstract = {Singing behavior in songbirds is a model system for motor control of learned behavior. The target organs of its central motor programs are the various muscle systems involved in sound generation. Investigation of these peripheral motor mechanisms of song production is the first step toward an understanding of how different motor systems are coordinated. Here we review physiological studies of all major motor systems that are involved in song production and modification in the zebra finch (Taeniopygia guttata). Acoustic syllables of zebra finch song are produced by a characteristic air sac pressure pattern. Electromyographic (EMG) and sonomicrometric recording of expiratory muscle activity reveals that respiratory motor control is tightly coordinated with syringeal gating of airflow. Recordings of bronchial airflow demonstrate that most of the song syllables are composed of simultaneous independent contributions from the two sides of the syrinx. Sounds generated in the syrinx can be modified by the resonance properties of the upper vocal tract. Tracheal length affects resonance, but dynamic changes of tracheal length are unlikely to make a substantial contribution to sound modification. However, beak movements during song contribute to sound modification and, possibly, affect the vibratory behavior of the labia. Rapid beak aperture changes were associated with nonlinear transitions in the acoustic structure of individual syllables. The synergy between respiratory and syringeal motor systems, and the unique bilateral, simultaneous, and independent sound production, combined with dynamic modification of the acoustic structure of song, make the zebra finch an excellent model system for exploring mechanisms of sensorimotor integration underlying a complex learned behavior.},
  eprint = {15313773},
  eprinttype = {pmid},
  keywords = {Animals,Exhalation,Functional Laterality,Motor Activity,Neuromuscular Junction,Phonation,Respiratory Mechanics,Songbirds,Vocalization; Animal},
  langid = {english}
}

@article{gollerPeripheralMotorDynamics2004a,
  title = {Peripheral Motor Dynamics of Song Production in the Zebra Finch},
  author = {Goller, Franz and Cooper, Brenton G.},
  date = {2004-06},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann N Y Acad Sci},
  volume = {1016},
  pages = {130--152},
  issn = {0077-8923},
  doi = {10.1196/annals.1298.009},
  abstract = {Singing behavior in songbirds is a model system for motor control of learned behavior. The target organs of its central motor programs are the various muscle systems involved in sound generation. Investigation of these peripheral motor mechanisms of song production is the first step toward an understanding of how different motor systems are coordinated. Here we review physiological studies of all major motor systems that are involved in song production and modification in the zebra finch (Taeniopygia guttata). Acoustic syllables of zebra finch song are produced by a characteristic air sac pressure pattern. Electromyographic (EMG) and sonomicrometric recording of expiratory muscle activity reveals that respiratory motor control is tightly coordinated with syringeal gating of airflow. Recordings of bronchial airflow demonstrate that most of the song syllables are composed of simultaneous independent contributions from the two sides of the syrinx. Sounds generated in the syrinx can be modified by the resonance properties of the upper vocal tract. Tracheal length affects resonance, but dynamic changes of tracheal length are unlikely to make a substantial contribution to sound modification. However, beak movements during song contribute to sound modification and, possibly, affect the vibratory behavior of the labia. Rapid beak aperture changes were associated with nonlinear transitions in the acoustic structure of individual syllables. The synergy between respiratory and syringeal motor systems, and the unique bilateral, simultaneous, and independent sound production, combined with dynamic modification of the acoustic structure of song, make the zebra finch an excellent model system for exploring mechanisms of sensorimotor integration underlying a complex learned behavior.},
  eprint = {15313773},
  eprinttype = {pmid},
  keywords = {Animals,Exhalation,Functional Laterality,Motor Activity,Neuromuscular Junction,Phonation,Respiratory Mechanics,Songbirds,Vocalization; Animal},
  langid = {english}
}

@article{gordonMultimodalCommunicationWolf2011,
  title = {Multimodal Communication of Wolf Spiders on Different Substrates: Evidence for Behavioural Plasticity},
  shorttitle = {Multimodal Communication of Wolf Spiders on Different Substrates},
  author = {Gordon, Shira D. and Uetz, George W.},
  date = {2011-02},
  journaltitle = {Animal Behaviour},
  volume = {81},
  pages = {367--375},
  issn = {00033472},
  doi = {10.1016/j.anbehav.2010.11.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0003347210004409},
  urldate = {2019-09-17},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YUMBQQ6V\\Gordon and Uetz - 2011 - Multimodal communication of wolf spiders on differ.pdf},
  langid = {english},
  number = {2}
}

@article{gordonPhysiologicalBehavioralSynchrony2020,
  title = {Physiological and Behavioral Synchrony Predict Group Cohesion and Performance},
  author = {Gordon, Ilanit and Gilboa, Avi and Cohen, Shai and Milstein, Nir and Haimovich, Nir and Pinhasi, Shay and Siegman, Shahar},
  date = {2020-05-21},
  journaltitle = {Scientific Reports},
  volume = {10},
  pages = {8484},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-65670-1},
  url = {https://www.nature.com/articles/s41598-020-65670-1},
  urldate = {2020-09-20},
  abstract = {Interpersonal synchrony contributes to social functioning in dyads, but it remains unknown how synchrony shapes group experiences and performance. To this end, we designed a novel group drumming task in which participants matched their drumming to either predictable or unpredictable tempos. Fifty-one three-person groups were randomly assigned to one of two conditions: synchronized or asynchronized drumming. Outcome measures included electrocardiograms and self-reports of group cohesion and synchrony. The drumming task elicited an increase in physiological synchrony between group members (specifically their hearts’ interbeat intervals). We also found that physiological synchronization and behavioral synchronization predicted individuals’ experience of group cohesion. Physiological synchrony also predicted performance in a subsequent group task that involved freely drumming together. The findings suggest that the behavioral and physiological consequences of synchronization contribute to the formation of group bonds and coordination. They also confirm that insights from translational social neuroscience can inform our knowledge of the development of cohesive and efficacious groups.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6AWM6FWG\\Gordon et al. - 2020 - Physiological and Behavioral Synchrony Predict Gro.pdf;C\:\\Users\\u668173\\Zotero\\storage\\CRUQ5668\\s41598-020-65670-1.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{gordonPhysiologicalBehavioralSynchrony2020a,
  title = {Physiological and {{Behavioral Synchrony Predict Group Cohesion}} and {{Performance}}},
  author = {Gordon, Ilanit and Gilboa, Avi and Cohen, Shai and Milstein, Nir and Haimovich, Nir and Pinhasi, Shay and Siegman, Shahar},
  date = {2020-05-21},
  journaltitle = {Scientific Reports},
  volume = {10},
  pages = {8484},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-65670-1},
  url = {https://www.nature.com/articles/s41598-020-65670-1},
  urldate = {2020-12-05},
  abstract = {Interpersonal synchrony contributes to social functioning in dyads, but it remains unknown how synchrony shapes group experiences and performance. To this end, we designed a novel group drumming task in which participants matched their drumming to either predictable or unpredictable tempos. Fifty-one three-person groups were randomly assigned to one of two conditions: synchronized or asynchronized drumming. Outcome measures included electrocardiograms and self-reports of group cohesion and synchrony. The drumming task elicited an increase in physiological synchrony between group members (specifically their hearts’ interbeat intervals). We also found that physiological synchronization and behavioral synchronization predicted individuals’ experience of group cohesion. Physiological synchrony also predicted performance in a subsequent group task that involved freely drumming together. The findings suggest that the behavioral and physiological consequences of synchronization contribute to the formation of group bonds and coordination. They also confirm that insights from translational social neuroscience can inform our knowledge of the development of cohesive and efficacious groups.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MM3DPVKJ\\Gordon et al. - 2020 - Physiological and Behavioral Synchrony Predict Gro.pdf;C\:\\Users\\u668173\\Zotero\\storage\\J7KG657M\\s41598-020-65670-1.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{grahamKinematicsKineticsMultijoint2003,
  title = {Kinematics and {{Kinetics}} of {{Multijoint Reaching}} in {{Nonhuman Primates}}},
  author = {Graham, Kirsten M. and Moore, Kimberly D. and Cabel, D. William and Gribble, Paul L. and Cisek, Paul and Scott, Stephen H.},
  date = {2003-05-01},
  journaltitle = {Journal of Neurophysiology},
  volume = {89},
  pages = {2667--2677},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.00742.2002},
  url = {https://journals.physiology.org/doi/full/10.1152/jn.00742.2002},
  urldate = {2020-10-26},
  abstract = {The present study identifies the mechanics of planar reaching movements performed by monkeys (Macaca mulatta) wearing a robotic exoskeleton. This device maintained the limb in the horizontal plane such that hand motion was generated only by flexor and extensor motions at the shoulder and elbow. The study describes the kinematic and kinetic features of the shoulder, elbow, and hand during reaching movements from a central target to peripheral targets located on the circumference of a circle: the center-out task. While subjects made reaching movements with relatively straight smooth hand paths and little variation in peak hand velocity, there were large variations in joint motion, torque, and power for movements in different spatial directions. Unlike single-joint movements, joint kinematics and kinetics were not tightly coupled for these multijoint movements. For most movements, power generation was predominantly generated at only one of the two joints. The present analysis illustrates the complexities inherent in multijoint movements and forms the basis for understanding strategies used by the motor system to control reaching movements and for interpreting the response of neurons in different brain regions during this task.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3UQS39EW\\Graham et al. - 2003 - Kinematics and Kinetics of Multijoint Reaching in .pdf;C\:\\Users\\u668173\\Zotero\\storage\\W6L8WRMV\\jn.00742.html},
  number = {5}
}

@article{grazianoHowReferentialGestures2020,
  title = {How {{Referential Gestures Align With Speech}}: {{Evidence From Monolingual}} and {{Bilingual Speakers}}},
  shorttitle = {How {{Referential Gestures Align With Speech}}},
  author = {Graziano, Maria and Nicoladis, Elena and Marentette, Paula},
  date = {2020},
  journaltitle = {Language Learning},
  volume = {70},
  pages = {266--304},
  issn = {1467-9922},
  doi = {10.1111/lang.12376},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12376},
  urldate = {2020-03-03},
  abstract = {When speaking, people often produce gestures that are closely timed with the speech with which they constitute a semantically coherent unit. Analyzing the temporal patterns between the two modalities may reveal insights about how speakers plan them. Using elicited narratives, we tested English/French monolinguals and bilinguals to check whether bilinguals, known to experience a higher degree of competition in lexical access, show a different pattern of gesture–speech alignment compared to that of monolinguals. Results revealed no difference in the temporal patterns between gestures and co-semantic speech for the two language groups. Synchronous gestures were significantly more frequent than asynchronous ones; asynchronous gestures both preceded and followed the correlated speech, yet preceding gestures tended to occur more often. A qualitative analysis conducted for asynchronous gestures revealed that they may serve a rhetoric function. We argue that the variability in gesture–speech timing results from speakers’ strategic use of gesture.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lang.12376},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9WC9AXIF\\Graziano et al. - 2020 - How Referential Gestures Align With Speech Eviden.pdf;C\:\\Users\\u668173\\Zotero\\storage\\L5M33N6D\\lang.html},
  keywords = {bilinguals,gesture,gesture–speech alignment,monolinguals,speech production,temporal patterns},
  langid = {english},
  number = {1}
}

@article{grossNeuralBasisIntermittent2002,
  title = {The Neural Basis of Intermittent Motor Control in Humans},
  author = {Gross, J. and Timmermann, L. and Kujala, J. and Dirks, M. and Schmitz, F. and Salmelin, R. and Schnitzler, A.},
  date = {2002-02-19},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {99},
  pages = {2299--2302},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.032682099},
  url = {https://www.pnas.org/content/99/4/2299},
  urldate = {2020-10-29},
  abstract = {The basic question of whether the human brain controls continuous movements intermittently is still under debate. Here we show that 6- to 9-Hz pulsatile velocity changes of slow finger movements are directly correlated to oscillatory activity in the motor cortex, which is sustained by cerebellar drive through thalamus and premotor cortex. Our findings suggest that coupling of 6- to 9-Hz oscillatory activity in the cerebello–thalamo–cortical loop represents the neural mechanism for the intermittent control of continuous movements.},
  eprint = {11854526},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WS4TADA7\\Gross et al. - 2002 - The neural basis of intermittent motor control in .pdf;C\:\\Users\\u668173\\Zotero\\storage\\K3GM5ZZK\\2299.html},
  keywords = {oscillations‖functional connectivity‖magnetoencephalography‖ synchronization‖ dynamic imaging of coherent sources},
  langid = {english},
  number = {4}
}

@article{guestHowComputationalModeling2021,
  title = {How {{Computational Modeling Can Force Theory Building}} in {{Psychological Science}}},
  author = {Guest, Olivia and Martin, Andrea E.},
  date = {2021-01-22},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  pages = {1745691620970585},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620970585},
  url = {https://doi.org/10.1177/1745691620970585},
  urldate = {2021-01-27},
  abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined—what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
  keywords = {computational model,open science,scientific inference,theoretical psychology},
  langid = {english}
}

@article{gurvenWEIRDBodiesMismatch2020,
  title = {{{WEIRD}} Bodies: Mismatch, Medicine and Missing Diversity},
  shorttitle = {{{WEIRD}} Bodies},
  author = {Gurven, Michael D. and Lieberman, Daniel E.},
  date = {2020-04-14},
  journaltitle = {Evolution and Human Behavior},
  shortjournal = {Evolution and Human Behavior},
  issn = {1090-5138},
  doi = {10.1016/j.evolhumbehav.2020.04.001},
  url = {http://www.sciencedirect.com/science/article/pii/S1090513820300465},
  urldate = {2020-07-22},
  abstract = {Despite recent rapid advances in medical knowledge that have improved survival, conventional medical science's understanding of human health and disease relies heavily on people of European descent living in contemporary urban industrialized environments. Given that modern conditions in high-income countries differ widely in terms of lifestyle and exposures compared to those experienced by billions of people and all our ancestors over several hundred thousand years, this narrow approach to the human body and health is very limiting. We argue that preventing and treating chronic diseases of aging and other mismatch diseases will require both expanding study design to sample diverse populations and contexts, and fully incorporating evolutionary perspectives. In this paper, we first assess the extent of biased representation of industrialized populations in high profile, international biomedical journals, then compare patterns of morbidity and health across world regions. We also compare demographic rates and the force of selection between subsistence and industrialized populations to reflect on the changes in how selection operates on fertility and survivorship across the lifespan. We argue that, contrary to simplistic misguided solutions like the PaleoDiet, the hypothesis of evolutionary mismatch needs critical consideration of population history, evolutionary biology and evolved reaction norms to prevent and treat diseases. We highlight the critical value of broader sampling by considering the effects of three key exposures that have radically changed over the past century in many parts of the world—pathogen burden, reproductive effort and physical activity—on autoimmune, cardiometabolic and other mismatch diseases.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WRE682TD\\Gurven and Lieberman - 2020 - WEIRD bodies mismatch, medicine and missing diver.pdf;C\:\\Users\\u668173\\Zotero\\storage\\MI9Z87FN\\S1090513820300465.html},
  keywords = {Diversity,Evolutionary medicine,Mismatch,WEIRD},
  langid = {english}
}

@article{guskiAcousticTauEasy1992,
  title = {Acoustic {{Tau}}: {{An Easy Analogue}} to {{Visual Tau}}?},
  shorttitle = {Acoustic {{Tau}}},
  author = {Guski, Rainer},
  date = {1992-09-01},
  journaltitle = {Ecological Psychology},
  volume = {4},
  pages = {189--197},
  issn = {1040-7413},
  doi = {10.1207/s15326969eco0403_4},
  url = {https://doi.org/10.1207/s15326969eco0403_4},
  urldate = {2019-05-04},
  abstract = {In an earlier article in this journal, B. Shaw, McGowan, and Turvey (1991) resented an acoustic variable they supposed would specify the time to contact with a sound-emitting source moving rectilinearly toward an observer at constant speed. Their formulation, t = 2l(dI/dt), follows Lee's (1976) main ideas in his derivation of optical tau for small visual angles. In this article I compare the functions of vision and hearing more generally and consider what information vision and hearing would use in normal circumstances. In this context, one may then ask a more specific question, Which kind of auditory information might be used about impending collisions? I make the following points: (a) The specification analysis of Shaw et al. (1991) is a substantial contribution to the growing field of ecological acoustics, (b) the idea that a more general attack on the acoustic guidance of action (Shaw et al., 1991, p. 254) starts with a proposal for an uncommon case without considering the general functions of seeing and hearing first seems inconsistent, (c) an auditory variable specifying time to turn or time to jump is needed, and (d) I do not believe that the auditory system can use the kind of information proposed by Shaw et al. (1991) in estimating time to contact.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TANWDUB2\\s15326969eco0403_4.html},
  number = {3}
}

@article{gustisonVocalLocomotorCoordination2019,
  title = {Vocal and Locomotor Coordination Develops in Association with the Autonomic Nervous System},
  author = {Gustison, Morgan L and Borjon, Jeremy I and Takahashi, Daniel Y and Ghazanfar, Asif A},
  editor = {Tchernichovski, Ofer and Calabrese, Ronald L and Goller, Franz},
  date = {2019-07-16},
  journaltitle = {eLife},
  volume = {8},
  pages = {e41853},
  issn = {2050-084X},
  doi = {10.7554/eLife.41853},
  url = {https://doi.org/10.7554/eLife.41853},
  urldate = {2019-10-17},
  abstract = {In adult animals, movement and vocalizations are coordinated, sometimes facilitating, and at other times inhibiting, each other. What is missing is how these different domains of motor control become coordinated over the course of development. We investigated how postural-locomotor behaviors may influence vocal development, and the role played by physiological arousal during their interactions. Using infant marmoset monkeys, we densely sampled vocal, postural and locomotor behaviors and estimated arousal fluctuations from electrocardiographic measures of heart rate. We found that vocalizations matured sooner than postural and locomotor skills, and that vocal-locomotor coordination improved with age and during elevated arousal levels. These results suggest that postural-locomotor maturity is not required for vocal development to occur, and that infants gradually improve coordination between vocalizations and body movement through a process that may be facilitated by arousal level changes.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9PA6LWLP\\Gustison et al. - 2019 - Vocal and locomotor coordination develops in assoc.pdf},
  keywords = {energetics,locomotion,marmoset monkey,motor development,vocal ontogeny}
}

@article{haasEffectsPerceivedMusical1986,
  title = {Effects of Perceived Musical Rhythm on Respiratory Pattern},
  author = {Haas, F. and Distenfeld, S. and Axen, K.},
  date = {1986-09-01},
  journaltitle = {Journal of Applied Physiology},
  volume = {61},
  pages = {1185--1191},
  issn = {8750-7587, 1522-1601},
  doi = {10.1152/jappl.1986.61.3.1185},
  url = {https://www.physiology.org/doi/10.1152/jappl.1986.61.3.1185},
  urldate = {2020-06-30},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZMVZBEIC\\Haas et al. - 1986 - Effects of perceived musical rhythm on respiratory.pdf},
  langid = {english},
  number = {3}
}

@article{hagoortNeurobiologyLanguageSingleword2019,
  title = {The Neurobiology of Language beyond Single-Word Processing},
  author = {Hagoort, Peter},
  date = {2019-10-04},
  journaltitle = {Science},
  volume = {366},
  pages = {55--58},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax0289},
  url = {https://science.sciencemag.org/content/366/6461/55},
  urldate = {2019-10-14},
  abstract = {In this Review, I propose a multiple-network view for the neurobiological basis of distinctly human language skills. A much more complex picture of interacting brain areas emerges than in the classical neurobiological model of language. This is because using language is more than single-word processing, and much goes on beyond the information given in the acoustic or orthographic tokens that enter primary sensory cortices. This requires the involvement of multiple networks with functionally nonoverlapping contributions.},
  eprint = {31604301},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IGUFE222\\55.html},
  langid = {english},
  number = {6461}
}

@article{hahnSixmontholdInfantsRecognize,
  title = {Six-Month-Old Infants Recognize Phrases in Song and Speech},
  author = {Hahn, Laura E. and Benders, Titia and Snijders, Tineke M. and Fikkert, Paula},
  journaltitle = {Infancy},
  volume = {n/a},
  issn = {1532-7078},
  doi = {10.1111/infa.12357},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/infa.12357},
  urldate = {2020-08-27},
  abstract = {Infants exploit acoustic boundaries to perceptually organize phrases in speech. This prosodic parsing ability is well-attested and is a cornerstone to the development of speech perception and grammar. However, infants also receive linguistic input in child songs. This study provides evidence that infants parse songs into meaningful phrasal units and replicates previous research for speech. Six-month-old Dutch infants (n = 80) were tested in the song or speech modality in the head-turn preference procedure. First, infants were familiarized to two versions of the same word sequence: One version represented a well-formed unit, and the other contained a phrase boundary halfway through. At test, infants were presented two passages, each containing one version of the familiarized sequence. The results for speech replicated the previously observed preference for the passage containing the well-formed sequence, but only in a more fine-grained analysis. The preference for well-formed phrases was also observed in the song modality, indicating that infants recognize phrase structure in song. There were acoustic differences between stimuli of the current and previous studies, suggesting that infants are flexible in their processing of boundary cues while also providing a possible explanation for differences in effect sizes.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/infa.12357},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FNATDV57\\Hahn et al. - Six-month-old infants recognize phrases in song an.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XZIQRA7P\\infa.html},
  keywords = {head-turn preference paradigm,phrase,prosody,song},
  langid = {english},
  number = {n/a}
}

@article{haimoffVideoAnalysisSiamang1981,
  title = {Video {{Analysis}} of {{Siamang}} ({{Hylobates}} Syndactylus) {{Songs}}},
  author = {Haimoff, Elliott H.},
  date = {1981},
  journaltitle = {Behaviour},
  volume = {76},
  pages = {128--151},
  publisher = {{Brill}},
  issn = {0005-7959},
  abstract = {This paper reports a study of the bioacoustics and locomotor behaviours of siamang (Hylobates syndactylus) during calling. Ten call bouts were videotaped and analysed in detail. The results suggest that the calls are complex duets produced sequentially, and that they are interactively organized by means of co-ordinating cues. The relationship between vocal and non-vocal behaviours, and the spacing of the duetting animals, are also organized in a regular fashion. The advantages and importance of videotaping over other methods in the micro-analysis of these calls are stressed. /// Cette étude porte sur le comportement bio-acoustique et locomoteur des Siamangs (Hylobates syndactylus) pendant leurs chants. Dix reprises de chants émises par animaux captifs ont été enregistrées sur bande vidéo et étudiées en profondeur. Les résultats nous permettent de suggérer que ces chants sont des duos complexes produits en série, et qu'ils sont organisés de façon réciproque par l'opération de signes vocaux coordinatifs. La discussion porte également sur le fait que la correspondance entre comportements vocaux et non-vocaux, ainsi que le déploiement du couple chantant, sont organisés de façon regulière. Nous signalons par ailleurs les avantages de l'enregistrement sur bande vidéo, par rapport aux autres méthodes, pour la micro-analyse de tels chants.},
  eprint = {4534093},
  eprinttype = {jstor},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7YXUMXQ6\\Haimoff - 1981 - Video Analysis of Siamang (Hylobates syndactylus) .pdf},
  number = {1/2}
}

@article{halfwerkRiskyRipplesAllow2014,
  title = {Risky {{Ripples Allow Bats}} and {{Frogs}} to {{Eavesdrop}} on a {{Multisensory Sexual Display}}},
  author = {Halfwerk, W. and Jones, P. L. and Taylor, R. C. and Ryan, M. J. and Page, R. A.},
  date = {2014-01-24},
  journaltitle = {Science},
  volume = {343},
  pages = {413--416},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1244812},
  url = {https://science.sciencemag.org/content/343/6169/413},
  urldate = {2020-10-13},
  abstract = {Animal displays are often perceived by intended and unintended receivers in more than one sensory system. In addition, cues that are an incidental consequence of signal production can also be perceived by different receivers, even when the receivers use different sensory systems to perceive them. Here we show that the vocal responses of male túngara frogs (Physalaemus pustulosus) increase twofold when call-induced water ripples are added to the acoustic component of a rival’s call. Hunting bats (Trachops cirrhosus) can echolocate this signal by-product and prefer to attack model frogs when ripples are added to the acoustic component of the call. This study illustrates how the perception of a signal by-product by intended and unintended receivers through different sensory systems generates both costs and benefits for the signaler. It's Complicated Animals have evolved impressive displays used in mate selection. Although intended for the opposite sex of the same species, the potential for eavesdropping is significant. In cases where the sensory signature is the sexual signal itself (such as a bird call), selection from harmful eavesdroppers could result in a reduction in signal intensity that represents a balance between the cost and benefit of the signal. Halfwerk et al. (p. 413), however, show that the physical by-product of a signal can also act as a cue to both intended and eavesdropping recipients. Ripples in the water made by throat sac expansion in calling túngara frogs signal their presence both to rivals and to predatory bats. This physical signature of the call itself cannot be modified; thus, it represents a cost-benefit ratio to calling that cannot be shifted through selection pressure from either side. Thus, physical by-products of sensory signaling create significant complexity in the evolution of sexual signaling. Calling frogs incidentally produce water ripples that are targeted by rival males and frog-eating bats. Calling frogs incidentally produce water ripples that are targeted by rival males and frog-eating bats.},
  eprint = {24458640},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6NLT6CJM\\Halfwerk et al. - 2014 - Risky Ripples Allow Bats and Frogs to Eavesdrop on.pdf;C\:\\Users\\u668173\\Zotero\\storage\\79K9N6IU\\413.html},
  langid = {english},
  number = {6169}
}

@article{halfwerkTestingMultimodalPerception2019,
  title = {Toward {{Testing}} for {{Multimodal Perception}} of {{Mating Signals}}},
  author = {Halfwerk, Wouter and Varkevisser, Judith and Simon, Ralph and Mendoza, Ezequiel and Scharff, Constance and Riebel, Katharina},
  date = {2019},
  journaltitle = {Frontiers in Ecology and Evolution},
  shortjournal = {Front. Ecol. Evol.},
  volume = {7},
  publisher = {{Frontiers}},
  issn = {2296-701X},
  doi = {10.3389/fevo.2019.00124},
  url = {https://www.frontiersin.org/articles/10.3389/fevo.2019.00124/full},
  urldate = {2020-09-02},
  abstract = {Many mating signals consist of multimodal components that need decoding by several sensory modalities on the receiver’s side. For methodological and conceptual reasons, the communicative functions of these signals are often investigated only one at a time. Likewise, variation of single signal traits are frequently correlated by researchers with senders’ quality or receivers’ behavioural responses. Consequently, the two classic and still dominating hypotheses regarding the communicative meaning of multimodal mating signals postulate that different components either serve as back-up messages or provide multiple meanings. Here we discuss how this conceptual dichotomy might have hampered a more integrative, perception encompassing understanding of multimodal communication: neither the multiple message nor the back-up signal hypotheses address the possibility that multimodal signals are integrated neurally into one percept. Therefore when studying multimodal mating signals, we should be aware that multimodal signals can give rise to multimodal percepts, meaning that receivers can gain access to additional information inherent to only combined signal components (‘the whole is something different than the sum of its parts’). We review the evidence for the importance of multimodal percepts and outline potential avenues for discovery of multimodal percepts in animal communication.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Q3YYLXG2\\Halfwerk et al. - 2019 - Toward Testing for Multimodal Perception of Mating.pdf},
  keywords = {cross-modal integration,multimodal communication,multisensory perception,Non-linear responses,Signal evolution},
  langid = {english}
}

@article{halfwerkTestingMultimodalPerception2019a,
  title = {Toward {{Testing}} for {{Multimodal Perception}} of {{Mating Signals}}},
  author = {Halfwerk, Wouter and Varkevisser, Judith and Simon, Ralph and Mendoza, Ezequiel and Scharff, Constance and Riebel, Katharina},
  date = {2019},
  journaltitle = {Frontiers in Ecology and Evolution},
  volume = {7},
  doi = {10.3389/fevo.2019.00124},
  url = {https://www.readcube.com/articles/10.3389%2Ffevo.2019.00124},
  urldate = {2020-10-13},
  abstract = {Many mating signals consist of multimodal components that need decoding by several sensory modalities on the receiver's side. For methodological and conceptual reasons, the communicative functions of these signals are often investigated only one at a time. Likewise, variation of single signal traits are frequently correlated by researchers with senders' quality or receivers' behavioral responses. Consequently, the two classic and still dominating hypotheses regarding the communicative meaning of multimodal mating signals postulate that different components either serve as back-up messages or provide multiple meanings. Here we discuss how this conceptual dichotomy might have hampered a more integrative, perception encompassing understanding of multimodal communication: neither the multiple message nor the back-up signal hypotheses address the possibility that multimodal signals are integrated neurally into one percept. Therefore, when studying multimodal mating signals, we should be aware that they can give rise to multimodal percepts. This means that receivers can gain access to additional information inherent in combined signal components only (“the whole is something different than the sum of its parts”). We review the evidence for the importance of multimodal percepts and outline potential avenues for discovery of multimodal percepts in animal communication.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BW5PW2BF\\Halfwerk et al. - 2019 - Toward Testing for Multimodal Perception of Mating.pdf;C\:\\Users\\u668173\\Zotero\\storage\\YSMZEXHS\\fevo.2019.html},
  langid = {english}
}

@article{handlingOriginLongHead2010,
  title = {The Origin of the Long Head of the Triceps: {{A}} Cadaveric Study},
  shorttitle = {The Origin of the Long Head of the Triceps},
  author = {Handling, Matthew A. and Curtis, Alan S. and Miller, Suzanne L.},
  date = {2010-01-01},
  journaltitle = {Journal of Shoulder and Elbow Surgery},
  shortjournal = {Journal of Shoulder and Elbow Surgery},
  volume = {19},
  pages = {69--72},
  issn = {1058-2746},
  doi = {10.1016/j.jse.2009.06.008},
  url = {http://www.sciencedirect.com/science/article/pii/S1058274609002869},
  urldate = {2020-09-11},
  abstract = {Hypothesis There is a paucity of literature examining the origin, size, and capsular contribution of the long head of the triceps brachii muscle. We hypothesize that there is a more extensive origin and capsular contribution than previously described. Materials and methods Twenty fresh, frozen cadaveric specimens were dissected from a posterior and anterior approach exposing the long head of the triceps and the inferior capsule. The origin and size of the long head of the triceps and contribution to the capsule was documented. Results The average age of the specimens was 65.8. At the scapula, the tendon width averaged 2.69cm at the insertion and the thickness averaged 0.47cm laterally and 0.29cm medially. The bony origin extended on the lateral border dorsal surface of the scapula in addition to the infraglenoid tubercle. The long head of the triceps gave a capsular contribution in each specimen. This contribution measured 1.43cm from superior to inferior and 1.01cm from anterior to posterior after dissecting the capsule off the glenoid. Discussion We found the origin of the long head of the triceps had a more extensive bony attachment on the scapula then previously described. In addition, the long head of the triceps has a consistent contribution to the inferior shoulder capsule. Conclusion The anatomic origin of the long head of the triceps gives a capsular contribution to the inferior glenohumeral capsule. The triceps may be affected by open and arthroscopic procedures that release or shift the posterior inferior glenohumeral capsule. Level of evidence Cadaveric study.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FXEUJ34G\\S1058274609002869.html},
  keywords = {anatomy,cadaver,capsule,insertion,proximal,Triceps},
  langid = {english},
  number = {1}
}

@article{hansenPredictiveUncertaintyAuditory2014,
  title = {Predictive Uncertainty in Auditory Sequence Processing},
  author = {Hansen, Niels Chr and Pearce, Marcus T.},
  date = {2014},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.01052},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.01052/full},
  urldate = {2020-12-03},
  abstract = {Previous studies of auditory expectation have focused on the expectedness perceived by listeners retrospectively in response to events. In contrast, this research examines predictive uncertainty - a property of listeners’ prospective state of expectation prior to the onset of an event. We examine the information-theoretic concept of Shannon entropy as a model of predictive uncertainty in music cognition. This is motivated by the Statistical Learning Hypothesis, which proposes that schematic expectations reflect probabilistic relationships between sensory events learned implicitly through exposure. Using probability estimates from an unsupervised, variable-order Markov model, 12 melodic contexts high in entropy and 12 melodic contexts low in entropy were selected from two musical repertoires differing in structural complexity (simple and complex). Musicians and non-musicians listened to the stimuli and provided explicit judgments of perceived uncertainty (explicit uncertainty). We also examined an indirect measure of uncertainty computed as the entropy of expectedness distributions obtained using a classical probe-tone paradigm where listeners rated the perceived expectedness of the final note in a melodic sequence (inferred uncertainty). Finally, we simulate listeners’ perception of expectedness and uncertainty using computational models of auditory expectation. A detailed model comparison indicates which model parameters maximize fit to the data and how they compare to existing models in the literature. The results show that listeners experience greater uncertainty in high-entropy musical contexts than low-entropy contexts. This effect is particularly apparent for inferred uncertainty and is stronger in musicians than non-musicians. Consistent with the Statistical Learning Hypothesis, the results suggest that increased domain-relevant training is associated with an increasingly accurate cognitive model of probabilistic structure in music.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZE88JPHA\\Hansen and Pearce - 2014 - Predictive uncertainty in auditory sequence proces.pdf},
  keywords = {Auditory cognition,entropy,expectation,Information Theory,melody,Music,statistical learning},
  langid = {english}
}

@article{hansonEffectsObstruentConsonants2009,
  title = {Effects of Obstruent Consonants on Fundamental Frequency at Vowel Onset in {{English}}},
  author = {Hanson, H. M.},
  date = {2009-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {125},
  pages = {425--441},
  issn = {1520-8524},
  doi = {10.1121/1.3021306},
  abstract = {When a vowel follows an obstruent, the fundamental frequency in the first few tens of milliseconds of the vowel is known to be influenced by the voicing characteristics of the consonant. This influence was re-examined in the study reported here. Stops, fricatives, and the nasal /m/ were paired with the vowels /i,a/ to form CVm syllables. Target syllables were embedded in carrier sentences, and intonation was varied to produce each syllable in either a high, low, or neutral pitch environment. In a high-pitch environment, F0 following voiceless obstruents is significantly increased relative to the baseline /m/, but following voiced obstruents it closely traces the baseline. In a low-pitch environment, F0 is very slightly increased following all obstruents, voiced and unvoiced. It is suggested that for certain pitch environments a conflict can occur between gestures corresponding to the segmental feature [stiff vocal folds] and intonational elements. The results are different acoustic manifestations of [stiff] in different pitch environments. The spreading of the vocal folds that occurs during unvoiced stops in certain contexts in English is an enhancing gesture, which aids the resolution of the gestural conflict by allowing the defining segmental gesture to be weakened without losing perceptual salience.},
  eprint = {19173428},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WIKTF2GU\\Hanson - 2009 - Effects of obstruent consonants on fundamental fre.pdf},
  keywords = {Female,Humans,Linguistics,Male,Phonetics,Pitch Perception},
  langid = {english},
  number = {1},
  pmcid = {PMC2677272}
}

@article{hardusToolUseWild2009,
  title = {Tool Use in Wild Orang-Utans Modifies Sound Production: A Functionally Deceptive Innovation?},
  shorttitle = {Tool Use in Wild Orang-Utans Modifies Sound Production},
  author = {Hardus, M. E. and Lameira, A. R. and Schaik, C. S. and Wich, S. A.},
  date = {2009-10-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {276},
  pages = {3689--3694},
  doi = {10.1098/rspb.2009.1027},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2009.1027},
  urldate = {2019-05-04},
  abstract = {Culture has long been assumed to be uniquely human but recent studies, in particular on great apes, have suggested that cultures also occur in non-human primates. The most apparent cultural behaviours in great apes involve tools in the subsistence context where they are clearly functional to obtain valued food. On the other hand, tool-use to modify acoustic communication has been reported only once and its function has not been investigated. Thus, the question whether this is an adaptive behaviour remains open, even though evidence indicates that it is socially transmitted (i.e. cultural). Here we report on wild orang-utans using tools to modulate the maximum frequency of one of their sounds, the kiss squeak, emitted in distress. In this variant, orang-utans strip leaves off a twig and hold them to their mouth while producing a kiss squeak. Using leaves as a tool lowers the frequency of the call compared to a kiss squeak without leaves or with only a hand to the mouth. If the lowering of the maximum frequency functions in orang-utans as it does in other animals, two predictions follow: (i) kiss squeak frequency is related to body size and (ii) the use of leaves will occur in situations of most acute danger. Supporting these predictions, kiss squeaks without tools decreased with body size and kiss squeaks with leaves were only emitted by highly distressed individuals. Moreover, we found indications that the calls were under volitional control. This finding is significant for at least two reasons. First, although few animal species are known to deceptively lower the maximum frequency of their calls to exaggerate their perceived size to the listener (e.g. vocal tract elongation in male deer) it has never been reported that animals may use tools to achieve this, or that they are primates. Second, it shows that the orang-utan culture extends into the communicative domain, thus challenging the traditional assumption that primate calling behaviour is overall purely emotional.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AMGVVAKC\\Hardus Madeleine E. et al. - 2009 - Tool use in wild orang-utans modifies sound produc.pdf;C\:\\Users\\u668173\\Zotero\\storage\\PSELNL5N\\rspb.2009.html},
  number = {1673}
}

@article{haroldEffectsEnvironmentalStimulation2013,
  title = {Effects of Environmental Stimulation on Infant Vocalizations and Orofacial Dynamics at the Onset of Canonical Babbling},
  author = {Harold, Meredith Poore and Barlow, Steven M.},
  date = {2013-02-01},
  journaltitle = {Infant Behavior and Development},
  shortjournal = {Infant Behavior and Development},
  volume = {36},
  pages = {84--93},
  issn = {0163-6383},
  doi = {10.1016/j.infbeh.2012.10.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0163638312001099},
  urldate = {2020-12-01},
  abstract = {The vocalizations and jaw kinematics of 30 infants aged 6–8 months were recorded using a Motion Analysis System and audiovisual technologies. This study represents the first attempt to determine the effect of play environment on infants’ rate of vocalization and jaw movement. Four play conditions were compared: watching videos, social contingent reinforcement and vocal modeling with an adult, playing alone with small toys, and playing alone with large toys. The fewest vocalizations and spontaneous movement were observed when infants were watching videos or interacting with an adult. Infants vocalized most when playing with large toys. The small toys, which naturally elicited gross motor movement (e.g., waving, banging, shaking), educed fewer vocalizations. This study was also the first to quantify the kinematics of vocalized and non-vocalized jaw movements of 6–8 month-old infants. Jaw kinematics did not differentiate infants who produced canonical syllables from those who did not. All infants produced many jaw movements without vocalization. However, during vocalization, infants were unlikely to move their jaw. This contradicts current theories that infant protophonic vocalizations are jaw-dominant. Results of the current study can inform socio-linguistic and kinematic theories of canonical babbling.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DRM57Q98\\Harold and Barlow - 2013 - Effects of environmental stimulation on infant voc.pdf;C\:\\Users\\u668173\\Zotero\\storage\\YNDE929Q\\S0163638312001099.html},
  keywords = {Canonical babbling,Environment,Kinematics,Speech},
  langid = {english},
  number = {1}
}

@book{harrisonAnatomyPhysiologyMammalian1995,
  title = {The {{Anatomy}} and {{Physiology}} of the {{Mammalian Larynx}}},
  author = {Harrison, D. F. N. and Harrison, Donald Frederick Norris},
  date = {1995-07-27},
  publisher = {{Cambridge University Press}},
  abstract = {In this unique book, Sir Donald Harrison draws on his wide-ranging experience as a surgeon and comparative anatomist to produce an authoritative and detailed account of the anatomy and physiology of the mammalian larynx. His investigation of the larynx has involved the study of over 1200 specimens of mammalian larynges from around the world, as well as using data from his own clinical experiences. The comparative morphology of the larynx is discussed from a developmental and functional perspective, and the involvement of the larynx in respiration, locomotion and vocalization is highlighted. Throughout the book the relationship of structure to function is stressed, and the clinical relevance of features of the human larynx is emphasized.},
  eprint = {bZBbTNdx4HoC},
  eprinttype = {googlebooks},
  isbn = {978-0-521-45321-9},
  keywords = {Medical / Physiology,Nature / Animals / General,Nature / Animals / Mammals,Science / Life Sciences / Zoology / General},
  langid = {english},
  pagetotal = {304}
}

@article{harrisonHorsingSpontaneousFourLegged2009,
  title = {Horsing {{Around}}: {{Spontaneous Four}}-{{Legged Coordination}}},
  shorttitle = {Horsing {{Around}}},
  author = {Harrison, Steven J. and Richardson, Michael J.},
  date = {2009-11-06},
  journaltitle = {Journal of Motor Behavior},
  volume = {41},
  pages = {519--524},
  issn = {0022-2895},
  doi = {10.3200/35-08-014},
  url = {https://doi.org/10.3200/35-08-014},
  urldate = {2019-05-07},
  abstract = {Motivated by previous research suggesting that informational and mechanical interlimb coupling can stabilize rhythmic movement patterns, the authors show that stable 4-legged patterns between 2 individuals, either walking or running, can emerge unintentionally from simple forms of coupling. Specifically, they show that the leg movements of pairs of naive individuals become spontaneously phase locked when visually or mechanically coupled via a foam appendage. Analysis of each of the phase locked trials revealed distinct preferences for particular 4-legged patterns, with interpersonal in- and anti-phase coordination patterns (equitable with quadruped pace and trot, respectively) observed almost exclusively. Preference for either pattern depended on the strength of coupling. The authors discuss these findings in light of previous claims that the patterns of human and animal locomotion—as well as coordinated movements in general—can emerge from lawful coupling relations that exist between the subcomponents of perceptual-motor systems.},
  eprint = {19567365},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AIALRCKZ\\Harrison and Richardson - 2009 - Horsing Around Spontaneous Four-Legged Coordinati.pdf;C\:\\Users\\u668173\\Zotero\\storage\\NWAQA7S9\\35-08-014.html},
  keywords = {coordination dynamics,interpersonal coordination,mechanical coupling,quadruped gait,visual coupling},
  number = {6}
}

@inproceedings{hasegawaEvaluationSpeechtoGestureGeneration2018,
  title = {Evaluation of {{Speech}}-to-{{Gesture Generation Using Bi}}-{{Directional LSTM Network}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Intelligent Virtual Agents}}},
  author = {Hasegawa, Dai and Kaneko, Naoshi and Shirakawa, Shinichi and Sakuta, Hiroshi and Sumi, Kazuhiko},
  date = {2018-11-05},
  pages = {79--86},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3267851.3267878},
  url = {https://doi.org/10.1145/3267851.3267878},
  urldate = {2021-02-26},
  abstract = {We present a novel framework to automatically generate natural gesture motions accompanying speech from audio utterances. Based on a Bi-Directional LSTM Network, our deep network learns speech-gesture relationships with both backward and forward consistencies over a long period of time. Our network regresses a full 3D skeletal pose of a human from perceptual features extracted from the input audio in each time step. Then, we apply combined temporal filters to smooth out the generated pose sequences. We utilize a speech-gesture dataset recorded with a headset and marker-based motion capture to train our network. We validated our approach with a subjective evaluation and compared it against "original" human gestures and "mismatched" human gestures taken from a different utterance. The evaluation result shows that our generated gestures are significantly better than the "mismatched" gestures with respect to time consistency. The generated gesture also shows marginally significant improvement in terms of semantic consistency when compared to "mismatched" gestures.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8EG69WL7\\Hasegawa et al. - 2018 - Evaluation of Speech-to-Gesture Generation Using B.pdf},
  isbn = {978-1-4503-6013-5},
  keywords = {deep learning,gesture generation,long short-term memory,neural networks},
  series = {{{IVA}} '18}
}

@article{hasselmanClassifyingAcousticSignals2015,
  title = {Classifying Acoustic Signals into Phoneme Categories: Average and Dyslexic Readers Make Use of Complex Dynamical Patterns and Multifractal Scaling Properties of the Speech Signal},
  shorttitle = {Classifying Acoustic Signals into Phoneme Categories},
  author = {Hasselman, Fred},
  date = {2015-03-26},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  volume = {3},
  pages = {e837},
  issn = {2167-8359},
  doi = {10.7717/peerj.837},
  url = {https://peerj.com/articles/837},
  urldate = {2019-04-05},
  abstract = {Several competing aetiologies of developmental dyslexia suggest that the problems with acquiring literacy skills are causally entailed by low-level auditory and/or speech perception processes. The purpose of this study is to evaluate the diverging claims about the specific deficient peceptual processes under conditions of strong inference. Theoretically relevant acoustic features were extracted from a set of artificial speech stimuli that lie on a /bAk/-/dAk/ continuum. The features were tested on their ability to enable a simple classifier (Quadratic Discriminant Analysis) to reproduce the observed classification performance of average and dyslexic readers in a speech perception experiment. The ‘classical’ features examined were based on component process accounts of developmental dyslexia such as the supposed deficit in Envelope Rise Time detection and the deficit in the detection of rapid changes in the distribution of energy in the frequency spectrum (formant transitions). Studies examining these temporal processing deficit hypotheses do not employ measures that quantify the temporal dynamics of stimuli. It is shown that measures based on quantification of the dynamics of complex, interaction-dominant systems (Recurrence Quantification Analysis and the multifractal spectrum) enable QDA to classify the stimuli almost identically as observed in dyslexic and average reading participants. It seems unlikely that participants used any of the features that are traditionally associated with accounts of (impaired) speech perception. The nature of the variables quantifying the temporal dynamics of the speech stimuli imply that the classification of speech stimuli cannot be regarded as a linear aggregate of component processes that each parse the acoustic signal independent of one another, as is assumed by the ‘classical’ aetiologies of developmental dyslexia. It is suggested that the results imply that the differences in speech perception performance between average and dyslexic readers represent a scaled continuum rather than being caused by a specific deficient component.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\G5RLG5YL\\Hasselman - 2015 - Classifying acoustic signals into phoneme categori.pdf;C\:\\Users\\u668173\\Zotero\\storage\\CG3LHAFN\\837.html},
  langid = {english}
}

@article{hassonBraintoBrainCouplingMechanism2012,
  title = {Brain-to-{{Brain}} Coupling: {{A}} Mechanism for Creating and Sharing a Social World},
  shorttitle = {Brain-to-{{Brain}} Coupling},
  author = {Hasson, Uri and Ghazanfar, Asif A. and Galantucci, Bruno and Garrod, Simon and Keysers, Christian},
  date = {2012-02},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends Cogn Sci},
  volume = {16},
  pages = {114--121},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2011.12.007},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3269540/},
  urldate = {2020-09-27},
  abstract = {Cognition materializes in an interpersonal space. The emergence of complex behaviors requires the coordination of actions among individuals according to a shared set of rules. Despite the central role of other individuals in shaping our minds, most cognitive studies focus on processes that occur within a single individual. We call for a shift from a single-brain to a multi-brain frame of reference. We argue that in many cases the neural processes in one brain are coupled to the neural processes in another brain via the transmission of a signal through the environment. Brain-to-brain coupling constrains and simplifies the actions of each individual in a social network, leading to complex joint behaviors that could not have emerged in isolation.},
  eprint = {22221820},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HKD5LTI7\\Hasson et al. - 2012 - Brain-to-Brain coupling A mechanism for creating .pdf},
  number = {2},
  pmcid = {PMC3269540}
}

@software{hastieGamGeneralizedAdditive2019,
  title = {Gam: {{Generalized Additive Models}}},
  shorttitle = {Gam},
  author = {Hastie, Trevor},
  date = {2019-07-03},
  url = {https://CRAN.R-project.org/package=gam},
  urldate = {2020-01-27},
  abstract = {Functions for fitting and working with generalized additive models, as described in chapter 7 of "Statistical Models in S" (Chambers and Hastie (eds), 1991), and "Generalized Additive Models" (Hastie and Tibshirani, 1990).},
  keywords = {Econometrics,Environmetrics,SocialSciences},
  version = {1.16.1}
}

@article{hattoriSpontaneousSynchronizedTapping2013,
  title = {Spontaneous Synchronized Tapping to an Auditory Rhythm in a Chimpanzee},
  author = {Hattori, Yuko and Tomonaga, Masaki and Matsuzawa, Tetsuro},
  date = {2013},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {3},
  pages = {1566},
  issn = {2045-2322},
  doi = {10.1038/srep01566},
  abstract = {Humans actively use behavioral synchrony such as dancing and singing when they intend to make affiliative relationships. Such advanced synchronous movement occurs even unconsciously when we hear rhythmically complex music. A foundation for this tendency may be an evolutionary adaptation for group living but evolutionary origins of human synchronous activity is unclear. Here we show the first evidence that a member of our closest living relatives, a chimpanzee, spontaneously synchronizes her movement with an auditory rhythm: After a training to tap illuminated keys on an electric keyboard, one chimpanzee spontaneously aligned her tapping with the sound when she heard an isochronous distractor sound. This result indicates that sensitivity to, and tendency toward synchronous movement with an auditory rhythm exist in chimpanzees, although humans may have expanded it to unique forms of auditory and visual communication during the course of human evolution.},
  eprint = {23535698},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XZJ69MZM\\Hattori et al. - 2013 - Spontaneous synchronized tapping to an auditory rh.pdf},
  keywords = {Acoustic Stimulation,Animals,Auditory Perception,Behavior; Animal,Female,Male,Motor Activity,Pan troglodytes,Psychomotor Performance},
  langid = {english},
  pmcid = {PMC3610097}
}

@article{hauserFacultyLanguageWhat2002,
  title = {The {{Faculty}} of {{Language}}: {{What Is It}}, {{Who Has It}}, and {{How Did It Evolve}}?},
  shorttitle = {The {{Faculty}} of {{Language}}},
  author = {Hauser, M. D. and Chomsky, N. and Fitch, W. T.},
  date = {2002-11-22},
  journaltitle = {Science},
  volume = {298},
  pages = {1569--1579},
  issn = {00368075, 10959203},
  doi = {10.1126/science.298.5598.1569},
  url = {https://www.sciencemag.org/lookup/doi/10.1126/science.298.5598.1569},
  urldate = {2020-03-27},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YQH6ITE5\\Hauser - 2002 - The Faculty of Language What Is It, Who Has It, a.pdf},
  langid = {english},
  number = {5598}
}

@article{hauserRoleArticulationProduction1993,
  title = {The Role of Articulation in the Production of Rhesus Monkey, {{Macaca}} Mulatta, Vocalizations},
  author = {Hauser, M. D. and Evans, C. S. and Marler, P.},
  date = {1993-03-01},
  journaltitle = {Animal Behaviour},
  shortjournal = {Animal Behaviour},
  volume = {45},
  pages = {423--433},
  issn = {0003-3472},
  doi = {10.1006/anbe.1993.1054},
  url = {http://www.sciencedirect.com/science/article/pii/S0003347283710547},
  urldate = {2020-12-03},
  abstract = {. Many non-human primates have large and acoustically diverse vocal repertoires. However, our understanding of the phonatory mechanisms underlying such acoustic variation is poor. Representative exemplars of some of the articulatory gestures used by free-ranging rhesus monkeys while uttering a range of vocalizations are provided. Results reveal that different call types appear to be associated with characteristic lip configurations and mandibular positions. Quantitative analyses of the 'coo' vocalization indicate that changes in the position of the mandible are reliably associated with changes in dominant frequency (i.e. resonance frequency), but not with changes in fundamental frequency. This finding suggests that rhesus monkeys can modify the spectral properties of the signal, independent of the glottal source (i.e. fundamental frequency). Such articulatory manoeuvres contribute to the animal's potential acoustic space, thereby potentially increasing the array of meaningful vocalizations within the repertoire.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PDS4BQFC\\Hauser et al. - 1993 - The role of articulation in the production of rhes.pdf;C\:\\Users\\u668173\\Zotero\\storage\\S64JUQPB\\S0003347283710547.html},
  langid = {english},
  number = {3}
}

@article{havilandEmergingGrammarNouns2013,
  title = {The Emerging Grammar of Nouns in a First Generation Sign Language: {{Specification}}, Iconicity, and Syntax},
  shorttitle = {The Emerging Grammar of Nouns in a First Generation Sign Language},
  author = {Haviland, John B.},
  date = {2013-12-31},
  journaltitle = {Gesture},
  volume = {13},
  pages = {309--353},
  issn = {1568-1475, 1569-9773},
  doi = {10.1075/gest.13.3.04hav},
  url = {http://www.jbe-platform.com/content/journals/10.1075/gest.13.3.04hav},
  urldate = {2020-03-25},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AQ5YEQ97\\Haviland - 2013 - The emerging grammar of nouns in a first generatio.pdf},
  langid = {english},
  number = {3}
}

@article{hayamaAirTrappingArboreal2002,
  title = {Air Trapping and Arboreal Locomotor Adaptation in Primates: A Review of Experiments on Humans},
  shorttitle = {Air Trapping and Arboreal Locomotor Adaptation in Primates},
  author = {Hayama, Sugio and Honda, Kiyoshi and Oka, Hideo and Okada, Morihiko},
  date = {2002-03},
  journaltitle = {Zeitschrift Fur Morphologie Und Anthropologie},
  shortjournal = {Z Morphol Anthropol},
  volume = {83},
  pages = {149--159},
  issn = {0044-314X},
  abstract = {A review was made of experiments on humans in which air trapping by glottis closure during three-dimensional movements were examined in four subjects including former Olympic gymnasts. In brachiation and horizontal bar exercises, the behaviour of the larynx was monitored with a fiberoptic endoscope, and EMG-data were recorded from shoulder muscles. The results revealed that immobilization of the polyaxial connection between the shoulder girdle and the thorax by air trapping occurs in phases of extreme loading of the upper limbs. The closure of the airway by the larynx in humans serves three functions: first, the prevention of errors in deglutition; second, the production of vocal sounds; third, the retention of air inside the thoracic cavity. The latter function, air trapping, allows the immobilization of the rib cage for the muscular fixation of the shoulder blade on the trunk in movements that imply unusually high external forces acting on the upper limbs. This morphological-functional innovation probably has been made when early mammals invaded the three dimensional arboreal habitat, because it gave the tree-dwelling early primates the device to anchor themselves by the arms alone and to avoid falling out of trees. The specific functional characteristic of primates is the hermetic closure of the vocal and vestibular folds by rapidly contracting muscles in the folds. So the closure of the glottis, which in humans seems primarily an adaptation to the production of vocal tones, seems to go back to the adaptation of Tertiary arboreal primates to movements in a three-dimensional environment. Our conclusions are in agreement with the results of other contributions to this volume.},
  eprint = {12050889},
  eprinttype = {pmid},
  keywords = {Adaptation; Physiological,Adult,Electromyography,Exercise,Glottis,Humans,Larynx,Locomotion,Male,Muscle; Skeletal,Shoulder},
  langid = {english},
  number = {2-3}
}

@article{healeyRunningRepairsCoordinating2018,
  title = {Running {{Repairs}}: {{Coordinating Meaning}} in {{Dialogue}}},
  shorttitle = {Running {{Repairs}}},
  author = {Healey, Patrick G. T. and Mills, Gregory J. and Eshghi, Arash and Howes, Christine},
  date = {2018},
  journaltitle = {Topics in Cognitive Science},
  volume = {10},
  pages = {367--388},
  issn = {1756-8765},
  doi = {10.1111/tops.12336},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12336},
  urldate = {2020-01-22},
  abstract = {People give feedback in conversation: both positive signals of understanding, such as nods, and negative signals of misunderstanding, such as frowns. How do signals of understanding and misunderstanding affect the coordination of language use in conversation? Using a chat tool and a maze-based reference task, we test two experimental manipulations that selectively interfere with feedback in live conversation: (a) “Attenuation” that replaces positive signals of understanding such as “right” or “okay” with weaker, more provisional signals such as “errr” or “umm” and (2) “Amplification” that replaces relatively specific signals of misunderstanding from clarification requests such as “on the left?” with generic signals of trouble such as “huh?” or “eh?”. The results show that Amplification promotes rapid convergence on more systematic, abstract ways of describing maze locations while Attenuation has no significant effect. We interpret this as evidence that “running repairs”—the processes of dealing with misunderstandings on the fly—are key drivers of semantic coordination in dialogue. This suggests a new direction for experimental work on conversation and a productive way to connect the empirical accounts of Conversation Analysis with the representational and processing concerns of Formal Semantics and Psycholinguistics.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AS38SYQX\\Healey et al. - 2018 - Running Repairs Coordinating Meaning in Dialogue.pdf;C\:\\Users\\u668173\\Zotero\\storage\\YNE7349C\\tops.html},
  keywords = {Dialogue,Miscommunication,Repair},
  langid = {english},
  number = {2}
}

@article{heAmplitudeEnvelopeKinematics2017,
  title = {Amplitude Envelope Kinematics of Speech: {{Parameter}} Extraction and Applications},
  shorttitle = {Amplitude Envelope Kinematics of Speech},
  author = {He, Lei and Dellwo, Volker},
  date = {2017-05-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {141},
  pages = {3582--3582},
  issn = {0001-4966},
  doi = {10.1121/1.4987638},
  url = {https://asa.scitation.org/doi/10.1121/1.4987638},
  urldate = {2019-05-05},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\96CYPDK7\\1.html},
  number = {5}
}

@inproceedings{heAmplitudeEnvelopeKinematics2017a,
  title = {Amplitude Envelope Kinematics of Speech Signal: Parameter Extraction and Applications},
  booktitle = {Konferenz {{Elektronische Sprachsignalverarbeitung}}},
  date = {2017},
  pages = {107--113},
  location = {{Saarbrücken}},
  editora = {He, L. and Dellwo, V.},
  editoratype = {collaborator}
}

@article{hebetsComplexSignalFunction2005,
  title = {Complex Signal Function: Developing a Framework of Testable Hypotheses},
  shorttitle = {Complex Signal Function},
  author = {Hebets, Eileen A. and Papaj, Daniel R.},
  date = {2005-01-01},
  journaltitle = {Behavioral Ecology and Sociobiology},
  shortjournal = {Behav Ecol Sociobiol},
  volume = {57},
  pages = {197--214},
  issn = {1432-0762},
  doi = {10.1007/s00265-004-0865-7},
  url = {https://doi.org/10.1007/s00265-004-0865-7},
  urldate = {2021-03-08},
  abstract = {The basic building blocks of communication are signals, assembled in various sequences and combinations, and used in virtually all inter- and intra-specific interactions. While signal evolution has long been a focus of study, there has been a recent resurgence of interest and research in the complexity of animal displays. Much past research on signal evolution has focused on sensory specialists, or on single signals in isolation, but many animal displays involve complex signaling, or the combination of more than one signal or related component, often serially and overlapping, frequently across multiple sensory modalities. Here, we build a framework of functional hypotheses of complex signal evolution based on content-driven (ultimate) and efficacy-driven (proximate) selection pressures (sensu Guilford and Dawkins 1991). We point out key predictions for various hypotheses and discuss different approaches to uncovering complex signal function. We also differentiate a category of hypotheses based on inter-signal interactions. Throughout our review, we hope to make three points: (1) a complex signal is a functional unit upon which selection can act, (2) both content and efficacy-driven selection pressures must be considered when studying the evolution of complex signaling, and (3) individual signals or components do not necessarily contribute to complex signal function independently, but may interact in a functional way.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\37DCKR27\\Hebets and Papaj - 2005 - Complex signal function developing a framework of.pdf},
  langid = {english},
  number = {3}
}

@article{hebetsSystemsApproachAnimal2016,
  title = {A Systems Approach to Animal Communication},
  author = {Hebets, Eileen A. and Barron, Andrew B. and Balakrishnan, Christopher N. and Hauber, Mark E. and Mason, Paul H. and Hoke, Kim L.},
  date = {2016-03-16},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {283},
  pages = {20152889},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2015.2889},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2015.2889},
  urldate = {2020-10-13},
  abstract = {Why animal communication displays are so complex and how they have evolved are active foci of research with a long and rich history. Progress towards an evolutionary analysis of signal complexity, however, has been constrained by a lack of hypotheses to explain similarities and/or differences in signalling systems across taxa. To address this, we advocate incorporating a systems approach into studies of animal communication—an approach that includes comprehensive experimental designs and data collection in combination with the implementation of systems concepts and tools. A systems approach evaluates overall display architecture, including how components interact to alter function, and how function varies in different states of the system. We provide a brief overview of the current state of the field, including a focus on select studies that highlight the dynamic nature of animal signalling. We then introduce core concepts from systems biology (redundancy, degeneracy, pluripotentiality, and modularity) and discuss their relationships with system properties (e.g. robustness, flexibility, evolvability). We translate systems concepts into an animal communication framework and accentuate their utility through a case study. Finally, we demonstrate how consideration of the system-level organization of animal communication poses new practical research questions that will aid our understanding of how and why animal displays are so complex.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AWAIK2BE\\Hebets et al. - 2016 - A systems approach to animal communication.pdf},
  number = {1826}
}

@inproceedings{hedgesImprovingPredictionsDerived2016,
  title = {Improving Predictions of Derived Viewpoints in Multiple Viewpoint Systems},
  booktitle = {Proceedings of {{ISMIR}} 2016},
  author = {Hedges, Tw and Wiggins, G. A.},
  date = {2016-05-13},
  url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/13047},
  urldate = {2020-12-03},
  annotation = {Accepted: 2016-06-23T12:34:34Z},
  eventtitle = {{{ISMIR}} 2016},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XJ9F28CF\\Hedges et al. - 2016 - Improving predictions of derived viewpoints in mul.pdf;C\:\\Users\\u668173\\Zotero\\storage\\EI5QUIM7\\13047.html},
  langid = {english}
}

@article{hedgesPredictionMergedAttributes2016,
  title = {The {{Prediction}} of {{Merged Attributes}} with {{Multiple Viewpoint Systems}}},
  author = {Hedges, Thomas and Wiggins, Geraint A.},
  date = {2016-10-01},
  journaltitle = {Journal of New Music Research},
  volume = {45},
  pages = {314--332},
  publisher = {{Routledge}},
  issn = {0929-8215},
  doi = {10.1080/09298215.2016.1205632},
  url = {https://doi.org/10.1080/09298215.2016.1205632},
  urldate = {2020-12-03},
  abstract = {Multiple viewpoint systems find statistical structure in multi-dimensional entities, such as music, by combining Markov-based models together in order to make probabilistic predictions. This paper empirically tests two contrasting techniques for predicting multiple attributes of a musical surface. The first, an established method, predicts each attribute in turn, whilst a second, a proposed alternative, merges attributes into a new representation in order to make predictions simultaneously. A set of optimal smoothing techniques are found for both prediction methods across several harmonic and melodic datasets. Results indicate that when surface attributes are highly correlated, predicting merged attributes outperforms predicting the attributes separately. This can allow viewpoint systems with correlated surface attributes to be optimized, giving a closer fit with the training data as measured by mean information content.},
  annotation = {\_eprint: https://doi.org/10.1080/09298215.2016.1205632},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QAQTJN9Q\\Hedges and Wiggins - 2016 - The Prediction of Merged Attributes with Multiple .pdf;C\:\\Users\\u668173\\Zotero\\storage\\HHJQY95D\\09298215.2016.html},
  keywords = {harmony,information theory,multiple viewpoint},
  number = {4}
}

@article{heldnerDetectionThresholdsGaps2011,
  title = {Detection Thresholds for Gaps, Overlaps, and No-Gap-No-Overlaps},
  author = {Heldner, Mattias},
  date = {2011-07},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J Acoust Soc Am},
  volume = {130},
  pages = {508--513},
  issn = {1520-8524},
  doi = {10.1121/1.3598457},
  abstract = {Detection thresholds for gaps and overlaps, that is acoustic and perceived silences and stretches of overlapping speech in speaker changes, were determined. Subliminal gaps and overlaps were categorized as no-gap-no-overlaps. The established gap and overlap detection thresholds both corresponded to the duration of a long vowel, or about 120 ms. These detection thresholds are valuable for mapping the perceptual speaker change categories gaps, overlaps, and no-gap-no-overlaps into the acoustic domain. Furthermore, the detection thresholds allow generation and understanding of gaps, overlaps, and no-gap-no-overlaps in human-like spoken dialogue systems.},
  eprint = {21786916},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LWEIN9FU\\Heldner - 2011 - Detection thresholds for gaps, overlaps, and no-ga.pdf},
  keywords = {Acoustic Stimulation,Adult,Aged,Audiometry; Speech,Auditory Threshold,Female,Humans,Logistic Models,Male,Middle Aged,Psychoacoustics,Signal Detection; Psychological,Speech Acoustics,Speech Perception,Time Factors},
  langid = {english},
  number = {1}
}

@article{heldnerPausesGapsOverlaps2010,
  title = {Pauses, Gaps and Overlaps in Conversations},
  author = {Heldner, Mattias and Edlund, Jens},
  date = {2010-10},
  journaltitle = {Journal of Phonetics},
  volume = {38},
  pages = {555--568},
  issn = {00954470},
  doi = {10.1016/j.wocn.2010.08.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447010000628},
  urldate = {2020-06-16},
  abstract = {This paper explores durational aspects of pauses, gaps and overlaps in three different conversational corpora with a view to challenge claims about precision timing in turn-taking. Distributions of pause, gap and overlap durations in conversations are presented, and methodological issues regarding the statistical treatment of such distributions are discussed. The results are related to published minimal response times for spoken utterances and thresholds for detection of acoustic silences in speech. It is shown that turn-taking is generally less precise than is often claimed by researchers in the field of conversation analysis or interactional linguistics. These results are discussed in the light of their implications for models of timing in turn-taking, and for interaction control models in speech technology. In particular, it is argued that the proportion of speaker changes that could potentially be triggered by information immediately preceding the speaker change is large enough for reactive interaction controls models to be viable in speech technology.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SBED3H4D\\Heldner and Edlund - 2010 - Pauses, gaps and overlaps in conversations.pdf},
  langid = {english},
  number = {4}
}

@article{hellsingChangesPharyngealAirway1989,
  title = {Changes in the Pharyngeal Airway in Relation to Extension of the Head},
  author = {Hellsing, Eva},
  date = {1989-11-01},
  journaltitle = {European Journal of Orthodontics},
  shortjournal = {Eur J Orthod},
  volume = {11},
  pages = {359--365},
  publisher = {{Oxford Academic}},
  issn = {0141-5387},
  doi = {10.1093/oxfordjournals.ejo.a036007},
  url = {https://academic.oup.com/ejo/article/11/4/359/480020},
  urldate = {2020-10-19},
  abstract = {Abstract.  In a sample of 20 adults the associations between cervical lordosis, craniocervical inclination, position of the hyoid bone and cross-sectional dimen},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\L4B6QRLX\\480020.html},
  langid = {english},
  number = {4}
}

@article{helmuthWhenTwoHands1996,
  title = {When Two Hands Are Better than One: {{Reduced}} Timing Variability during Bimanual Movements},
  shorttitle = {When Two Hands Are Better than One},
  author = {Helmuth, Laura L. and Ivry, Richard B.},
  date = {1996},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {22},
  pages = {278--293},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.22.2.278},
  abstract = {Within-hand variability was reduced on a repetitive tapping task when individuals tapped with 2 hands in comparison to single-handed tapping. When the total variability was decomposed into central timing and peripheral implementation components (A.M. Wing and A.B. Kristofferson; see record 1974-10323-001), the bimanual advantage was attributed to decreased central variability. The improved consistency does not require that the movements involve homologous muscles. However, unlike phase coupling, the bimanual advantage is not found when the 2 movements are produced by different individuals, but rather requires that the 2 movements be produced by 1 individual. It is proposed that separate timing mechanisms are associated with each effector. During bimanual movements, the outputs from these timing mechanisms are integrated prior to movement execution, and it is this integration that results in the bimanual advantage. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WEB873UQ\\Helmuth and Ivry - 1996 - When two hands are better than one Reduced timing.pdf;C\:\\Users\\u668173\\Zotero\\storage\\PY27B3R5\\1996-03036-002.html},
  keywords = {Motor Coordination,Motor Performance,Response Variability},
  number = {2}
}

@article{henrichWeirdestPeopleWorld2010,
  title = {The Weirdest People in the World?},
  author = {Henrich, Joseph and Heine, Steven J. and Norenzayan, Ara},
  date = {2010-06},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {33},
  pages = {61--83},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X0999152X},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/weirdest-people-in-the-world/BF84F7517D56AFF7B7EB58411A554C17},
  urldate = {2020-07-09},
  abstract = {Behavioral scientists routinely publish broad claims about human psychology and behavior in the world's top journals based on samples drawn entirely from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies. Researchers – often implicitly – assume that either there is little variation across human populations, or that these “standard subjects” are as representative of the species as any other population. Are these assumptions justified? Here, our review of the comparative database from across the behavioral sciences suggests both that there is substantial variability in experimental results across populations and that WEIRD subjects are particularly unusual compared with the rest of the species – frequent outliers. The domains reviewed include visual perception, fairness, cooperation, spatial reasoning, categorization and inferential induction, moral reasoning, reasoning styles, self-concepts and related motivations, and the heritability of IQ. The findings suggest that members of WEIRD societies, including young children, are among the least representative populations one could find for generalizing about humans. Many of these findings involve domains that are associated with fundamental aspects of psychology, motivation, and behavior – hence, there are no obvious a priori grounds for claiming that a particular behavioral phenomenon is universal based on sampling from a single subpopulation. Overall, these empirical patterns suggests that we need to be less cavalier in addressing questions of human nature on the basis of data drawn from this particularly thin, and rather unusual, slice of humanity. We close by proposing ways to structurally re-organize the behavioral sciences to best tackle these challenges.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZBE2JCMY\\Henrich et al. - 2010 - The weirdest people in the world.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XWTUN9PP\\BF84F7517D56AFF7B7EB58411A554C17.html},
  keywords = {behavioral economics,cross-cultural research,cultural psychology,culture,evolutionary psychology,experiments,external validity,generalizability,human universals,population variability},
  langid = {english},
  number = {2-3}
}

@article{henterMoGlowProbabilisticControllable2020,
  title = {{{MoGlow}}: Probabilistic and Controllable Motion Synthesis Using Normalising Flows},
  shorttitle = {{{MoGlow}}},
  author = {Henter, Gustav Eje and Alexanderson, Simon and Beskow, Jonas},
  date = {2020-11-26},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  pages = {1--14},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3414685.3417836},
  url = {https://dl.acm.org/doi/10.1145/3414685.3417836},
  urldate = {2021-03-01},
  abstract = {Data-driven modelling and synthesis of motion is an active research area with applications that include animation, games, and social robotics. This paper introduces a new class of probabilistic, generative, and controllable motion-data models based on normalising flows. Models of this kind can describe highly complex distributions, yet can be trained efficiently using exact maximum likelihood, unlike GANs or VAEs. Our proposed model is autoregressive and uses LSTMs to enable arbitrarily long time-dependencies. Importantly, is is also causal, meaning that each pose in the output sequence is generated without access to poses or control inputs from future time steps; this absence of algorithmic latency is important for interactive applications with real-time motion control. The approach can in principle be applied to any type of motion since it does not make restrictive, task-specific assumptions regarding the motion or the character morphology. We evaluate the models on motion-capture datasets of human and quadruped locomotion. Objective and subjective results show that randomly-sampled motion from the proposed method outperforms task-agnostic baselines and attains a motion quality close to recorded motion capture. CCS Concepts: • Computing methodologies → Animation; Neural networks; Motion capture.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7IYB327X\\Henter et al. - 2020 - MoGlow probabilistic and controllable motion synt.pdf},
  langid = {english},
  number = {6}
}

@article{hesslerAttentionalDemandsMotorRespiratory2009,
  title = {Attentional {{Demands}} on {{Motor}}-{{Respiratory Coordination}}},
  author = {Hessler, Eric E. and Amazeen, Polemnia G.},
  date = {2009-09-01},
  journaltitle = {Research Quarterly for Exercise and Sport},
  volume = {80},
  pages = {510--523},
  publisher = {{Routledge}},
  issn = {0270-1367},
  doi = {10.1080/02701367.2009.10599589},
  url = {https://www.tandfonline.com/doi/abs/10.1080/02701367.2009.10599589},
  urldate = {2020-09-01},
  abstract = {Athletic performance requires the pacing of breathing with exercise, known as motor-respiratory coordination (MRC). In this study, we added cognitive and physical constraints while participants intentionally controlled their breathing locations during rhythmic arm movement. This is the first study to examine a cognitive constraint on MRC. Cognitive constraints included either instruction (Experiments 1 and 2) or signal detection (Experiment 1). Physical constraints were nonoptimal movement frequencies (Experiment 2). Instruction shifted breathing locations and both shifted and increased variability in the number of movements produced per breath (frequency ratio). Signal detection had no effect on MRC. Fast movement frequency resulted in higher, more variable frequency ratios. Cognitive and physical constraints can generate unnatural and variable breathing during athletic performance.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/02701367.2009.10599589},
  eprint = {19791637},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7C4AP6AV\\02701367.2009.html},
  keywords = {attention,breathing,dual task,load},
  number = {3}
}

@article{hewittFunctionsLaryngealAir2002,
  title = {The {{Functions}} of {{Laryngeal Air Sacs}} in {{Primates}}: {{A New Hypothesis}}},
  shorttitle = {The {{Functions}} of {{Laryngeal Air Sacs}} in {{Primates}}},
  author = {Hewitt, Gwen and MacLarnon, Ann and Jones, Kate E.},
  date = {2002},
  journaltitle = {Folia Primatologica},
  shortjournal = {FPR},
  volume = {73},
  pages = {70--94},
  publisher = {{Karger Publishers}},
  issn = {0015-5713, 1421-9980},
  doi = {10.1159/000064786},
  url = {https://www.karger.com/Article/FullText/64786},
  urldate = {2020-07-07},
  abstract = {A possible function of laryngeal air sacs in apes and gibbons was investigated by examining the relationships between air sac distribution, call rate, call duration and body weight in a phylogenetic context. The results suggest that lack of sacs in the smaller gibbons and in humans is a derived feature. Call parameters in primates, such as rate and duration, scaled to resting breathing rate (and therefore to body weight) only in species without air sacs, which appear to modify these relationships. Apes and larger gibbons may be able to produce fast extended call sequences without the risk of hyperventilating because they can re-breathe exhaled air from their air sacs. Humans may have lost air sacs during their evolutionary history because they are able to modify their speech breathing patterns and so reduce any tendency to hyperventilate.},
  eprint = {12207055},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\E5CIFC7I\\Hewitt et al. - 2002 - The Functions of Laryngeal Air Sacs in Primates A.pdf;C\:\\Users\\u668173\\Zotero\\storage\\G5YZYWUJ\\64786.html},
  langid = {english},
  number = {2-3}
}

@article{hickokComputationalNeuroanatomySpeech2012,
  title = {Computational Neuroanatomy of Speech Production},
  author = {Hickok, Gregory},
  date = {2012-01-05},
  journaltitle = {Nature Reviews. Neuroscience},
  shortjournal = {Nat. Rev. Neurosci.},
  volume = {13},
  pages = {135--145},
  issn = {1471-0048},
  doi = {10.1038/nrn3158},
  abstract = {Speech production has been studied predominantly from within two traditions, psycholinguistics and motor control. These traditions have rarely interacted, and the resulting chasm between these approaches seems to reflect a level of analysis difference: whereas motor control is concerned with lower-level articulatory control, psycholinguistics focuses on higher-level linguistic processing. However, closer examination of both approaches reveals a substantial convergence of ideas. The goal of this article is to integrate psycholinguistic and motor control approaches to speech production. The result of this synthesis is a neuroanatomically grounded, hierarchical state feedback control model of speech production.},
  eprint = {22218206},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\K5YBUR6A\\Hickok - 2012 - Computational neuroanatomy of speech production.pdf},
  keywords = {Cerebral Cortex,Feedback,Humans,Models; Biological,Neuroanatomy,Psycholinguistics,Speech},
  langid = {english},
  number = {2},
  pmcid = {PMC5367153}
}

@online{HierarchyAutonomousSystems,
  title = {A {{Hierarchy}} of {{Autonomous Systems}} for {{Vocal Production}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.tins.2019.12.006},
  url = {https://reader.elsevier.com/reader/sd/pii/S0166223619302401?token=944D77B8B1ECDE0BE1D2E949C7ED1446036B5D3B1B7C62D1E7F61AE7791DFA5D78C530C05B641392085395272CECB253},
  urldate = {2020-01-22},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TCMUAANF\\S0166223619302401.html},
  langid = {english}
}

@article{hillisCombiningSensoryInformation2002,
  title = {Combining Sensory Information: Mandatory Fusion within, but Not between, Senses},
  shorttitle = {Combining Sensory Information},
  author = {Hillis, J. M. and Ernst, M. O. and Banks, M. S. and Landy, M. S.},
  date = {2002-11-22},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {298},
  pages = {1627--1630},
  issn = {1095-9203},
  doi = {10.1126/science.1075396},
  abstract = {Humans use multiple sources of sensory information to estimate environmental properties. For example, the eyes and hands both provide relevant information about an object's shape. The eyes estimate shape using binocular disparity, perspective projection, etc. The hands supply haptic shape information by means of tactile and proprioceptive cues. Combining information across cues can improve estimation of object properties but may come at a cost: loss of single-cue information. We report that single-cue information is indeed lost when cues from within the same sensory modality (disparity and texture gradients in vision) are combined, but not when different modalities (vision and haptics) are combined.},
  eprint = {12446912},
  eprinttype = {pmid},
  keywords = {Cues,Form Perception,Humans,Mathematics,Stereognosis,Touch,Vision Disparity,Visual Perception},
  langid = {english},
  number = {5598}
}

@article{hilloowalaSpectrographicAnalysisLaryngeal1978,
  title = {Spectrographic Analysis of Laryngeal Air Sac Resonance in Rhesus Monkey},
  author = {Hilloowala, Rumy A. and Lass, Norman J.},
  date = {1978},
  journaltitle = {American Journal of Physical Anthropology},
  volume = {49},
  pages = {129--131},
  issn = {1096-8644},
  doi = {10.1002/ajpa.1330490119},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ajpa.1330490119},
  urldate = {2020-09-19},
  abstract = {Laryngeal air sacs are circular out-pocketings, located in the hyoid bone with their ostium in the midline of the anterior part of the larynx. From previous cadaver studies of the rhesus monkey it was deduced that the function of the air sac is to act as a resonating chamber. The present study was designed to test this hypothesis. Recordings were made of three rhesus monkeys before and after surgical removal of the air sac. Spectrographic analysis of the monkeys' vocalizations indicated that differences in formant frequency characteristics between pre-and post-surgical recordings were negligible. This finding suggests that the laryngeal air sac does not play an important role in the resonant properties of the monkeys' vocal tracts.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ajpa.1330490119},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UE793R47\\ajpa.html},
  keywords = {Laryngeal air sac,Rhesus macaque,Spectrographic analysis},
  langid = {english},
  number = {1}
}

@article{hodgesActivationHumanDiaphragm2000,
  title = {Activation of the Human Diaphragm during a Repetitive Postural Task},
  author = {Hodges, P. W. and Gandevia, S. C.},
  date = {2000-01-01},
  journaltitle = {The Journal of Physiology},
  shortjournal = {J Physiol},
  volume = {522},
  pages = {165--175},
  issn = {0022-3751},
  doi = {10.1111/j.1469-7793.2000.t01-1-00165.xm},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2269747/},
  urldate = {2020-08-28},
  abstract = {The co-ordination between respiratory and postural functions of the diaphragm was investigated during repetitive upper limb movement. It was hypothesised that diaphragm activity would occur either tonically or phasically in association with the forces from each movement and that this activity would combine with phasic respiratory activity.                                         Movements of the upper limb and ribcage were measured while standing subjects performed repetitive upper limb movements ‘as fast as possible’. Electromyographic (EMG) recordings of the costal diaphragm were made using intramuscular electrodes in four subjects. Surface electrodes were placed over the deltoid and erector spinae muscles.                                         In contrast to standing at rest, diaphragm activity was present throughout expiration at 78 ± 17\% (mean ± s.d.) of its peak inspiratory magnitude during repeated upper limb movement.                                         Bursts of deltoid and erector spinae EMG activity occurred at the limb movement frequency (≈2.9 Hz). Although the majority of diaphragm EMG power was at the respiratory frequency (≈0.4 Hz), a peak was also present at the movement frequency. This finding was corroborated by averaged EMG activity triggered from upper limb movement. In addition, diaphragm EMG activity was coherent with ribcage motion at the respiratory frequency and with upper limb movement at the movement frequency.                                         The diaphragm response was similar when movement was performed while sitting. In addition, when subjects moved with increasing frequency the peak upper limb acceleration correlated with diaphragm EMG amplitude. These findings support the argument that diaphragm contraction is related to trunk control.                                         The results indicate that activity of human phrenic motoneurones is organised such that it contributes to both posture and respiration during a task which repetitively challenges trunk posture.},
  eprint = {10618161},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZW45JHRE\\Hodges and Gandevia - 2000 - Activation of the human diaphragm during a repetit.pdf},
  issue = {Pt 1},
  pmcid = {PMC2269747}
}

@article{hodgesChangesIntraabdominalPressure2000,
  title = {Changes in Intra-Abdominal Pressure during Postural and Respiratory Activation of the Human Diaphragm},
  author = {Hodges, P. W. and Gandevia, S. C.},
  date = {2000-09},
  journaltitle = {Journal of Applied Physiology (Bethesda, Md.: 1985)},
  shortjournal = {J. Appl. Physiol.},
  volume = {89},
  pages = {967--976},
  issn = {8750-7587},
  doi = {10.1152/jappl.2000.89.3.967},
  abstract = {In humans, when the stability of the trunk is challenged in a controlled manner by repetitive movement of a limb, activity of the diaphragm becomes tonic but is also modulated at the frequency of limb movement. In addition, the tonic activity is modulated by respiration. This study investigated the mechanical output of these components of diaphragm activity. Recordings were made of costal diaphragm, abdominal, and erector spinae muscle electromyographic activity; intra-abdominal, intrathoracic, and transdiaphragmatic pressures; and motion of the rib cage, abdomen, and arm. During limb movement the diaphragm and transversus abdominis were tonically active with added phasic modulation at the frequencies of both respiration and limb movement. Activity of the other trunk muscles was not modulated by respiration. Intra-abdominal pressure was increased during the period of limb movement in proportion to the reactive forces from the movement. These results show that coactivation of the diaphragm and abdominal muscles causes a sustained increase in intra-abdominal pressure, whereas inspiration and expiration are controlled by opposing activity of the diaphragm and abdominal muscles to vary the shape of the pressurized abdominal cavity.},
  eprint = {10956340},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\J6F8BFSL\\Hodges and Gandevia - 2000 - Changes in intra-abdominal pressure during postura.pdf},
  keywords = {Abdomen,Abdominal Muscles,Adult,Diaphragm,Electromyography,Extremities,Humans,Male,Motion,Movement,Muscle; Skeletal,Posture,Pressure,Respiration,Respiratory Physiological Phenomena,Ribs,Shoulder,Spine},
  langid = {english},
  number = {3}
}

@article{hodgesContractionHumanDiaphragm1997,
  title = {Contraction of the Human Diaphragm during Rapid Postural Adjustments},
  author = {Hodges, P. W. and Butler, J. E. and McKenzie, D. K. and Gandevia, S. C.},
  date = {1997-12-01},
  journaltitle = {The Journal of Physiology},
  volume = {505 ( Pt 2)},
  pages = {539--548},
  issn = {0022-3751},
  doi = {10.1111/j.1469-7793.1997.539bb.x},
  abstract = {1. The response of the diaphragm to the postural perturbation produced by rapid flexion of the shoulder to a visual stimulus was evaluated in standing subjects. Gastric, oesophageal and transdiaphragmatic pressures were measured together with intramuscular and oesophageal recordings of electromyographic activity (EMG) in the diaphragm. To assess the mechanics of contraction of the diaphragm, dynamic changes in the length of the diaphragm were measured with ultrasonography. 2. With rapid flexion of the shoulder in response to a visual stimulus, EMG activity in the costal and crural diaphragm occurred about 20 ms prior to the onset of deltoid EMG. This anticipatory contraction occurred irrespective of the phase of respiration in which arm movement began. The onset of diaphragm EMG coincided with that of transversus abdominis. 3. Gastric and transdiaphragmatic pressures increased in association with the rapid arm flexion by 13.8 +/- 1.9 (mean +/- S.E.M.) and 13.5 +/- 1.8 cmH2O, respectively. The increases occurred 49 +/- 4 ms after the onset of diaphragm EMG, but preceded the onset of movement of the limb by 63 +/- 7 ms. 4. Ultrasonographic measurements revealed that the costal diaphragm shortened and then lengthened progressively during the increase in transdiaphragmatic pressure. 5. This study provides definitive evidence that the human diaphragm is involved in the control of postural stability during sudden voluntary movement of the limbs.},
  eprint = {9423192},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VQD4YC5P\\Hodges et al. - 1997 - Contraction of the human diaphragm during rapid po.pdf},
  keywords = {Adult,Arm,Diaphragm,Electromyography,Female,Humans,Male,Movement,Muscle Contraction,Muscle; Skeletal,Posture,Pressure,Time Factors},
  langid = {english},
  pmcid = {PMC1160083}
}

@article{hodgeSemioticDiversityDoing2019,
  title = {The Semiotic Diversity of Doing Reference in a Deaf Signed Language},
  author = {Hodge, Gabrielle and Ferrara, Lindsay N. and Anible, Benjamin D.},
  date = {2019-04-01},
  journaltitle = {Journal of Pragmatics},
  shortjournal = {Journal of Pragmatics},
  volume = {143},
  pages = {33--53},
  issn = {0378-2166},
  doi = {10.1016/j.pragma.2019.01.025},
  url = {http://www.sciencedirect.com/science/article/pii/S0378216618303734},
  urldate = {2020-12-22},
  abstract = {This article describes how deaf signers of Auslan (a deaf signed language of Australia) coordinate fully conventionalised forms (such as lexical manual signs and English fingerspelling and/or mouthing) with more richly improvised semiotics (such as indicating verbs, pointing signs, depicting signs, visible surrogates and/or invisible surrogates) to identify and talk about referents of varying agency. Using twenty retellings of Frog, Where Are You? and twenty retellings of The Boy Who Cried Wolf archived in the Auslan Corpus, we analysed 4,699 tokens of referring expressions with respect to: (a) activation status; (b) semiotic form; and (c) animacy. Statistical analysis confirmed choice of strategy was most strongly motivated by activation status: new referents were expressed with more conventionalised forms (especially lexical manual signs and English mouthing), whereas maintained and reintroduced referents typically involved fewer and more richly improvised, context-dependent semiotics. However, animacy was also a motivating factor: humans and animals were often depicted via visible surrogates (not pointing signs), whereas inanimate referents favoured depicting signs and invisible surrogates. These findings highlight the role of animacy in signed language discourse and challenge the claim that informativeness decreases as cognitive saliency increases, while demonstrating the ‘pretend world’ indexicality of signed language use and the pluralistic complexity of face-to-face communication.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZASMPWFE\\Hodge et al. - 2019 - The semiotic diversity of doing reference in a dea.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XQ9V8I98\\S0378216618303734.html},
  keywords = {Animacy,Indexicality,Multimodal,Referential cohesion,Semiotics,Sign language},
  langid = {english}
}

@article{hodgesFeedforwardContractionTransversus1997,
  title = {Feedforward Contraction of Transversus Abdominis Is Not Influenced by the Direction of Arm Movement},
  author = {Hodges, P. W. and Richardson, C. A.},
  date = {1997-04},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {114},
  pages = {362--370},
  issn = {0014-4819},
  doi = {10.1007/pl00005644},
  abstract = {Because the structure of the spine is inherently unstable, muscle activation is essential for the maintenance of trunk posture and intervertebral control when the limbs are moved. To investigate how the central nervous system deals with this situation the temporal components of the response of the muscles of the trunk were evaluated during rapid limb movement performed in response to a visual stimulus. Fine-wire electromyography (EMG) electrodes were inserted into transversus abdominis (TrA), obliquus internus abdominis (OI) and obliquus externus abdominis (OE) of 15 subjects under the guidance of real-time ultrasound imaging. Surface electrodes were placed over rectus abdominis (RA), lumbar multifidus (MF) and the three parts of deltoid. In a standing position, ten repetitions of shoulder flexion, abduction and extension were performed by the subjects as fast as possible in response to a visual stimulus. The onset of TrA EMG occurred in advance of deltoid irrespective of the movement direction. The time to onset of EMG activity of OI, OE, RA and MF varied with the movement direction, being activated earliest when the prime action of the muscle opposed the reactive forces associated with the specific limb movement. It is postulated that the non-direction-specific contraction of TrA may be related to the control of trunk stability independent of the requirement for direction-specific control of the centre of gravity in relation to the base of support.},
  eprint = {9166925},
  eprinttype = {pmid},
  keywords = {Adult,Arm,Electromyography,Female,Homeostasis,Humans,Male,Movement,Muscle Contraction,Muscle; Skeletal,Psychomotor Performance,Shoulder Joint},
  langid = {english},
  number = {2}
}

@article{hodgesPosturalRespiratoryFunctions2007,
  title = {Postural and Respiratory Functions of the Pelvic Floor Muscles},
  author = {Hodges, P. W. and Sapsford, R. and Pengel, L. H. M.},
  date = {2007},
  journaltitle = {Neurourology and Urodynamics},
  shortjournal = {Neurourol. Urodyn.},
  volume = {26},
  pages = {362--371},
  issn = {0733-2467},
  doi = {10.1002/nau.20232},
  abstract = {AIMS: Due to their contribution to modulation of intra-abdominal pressure (IAP) and stiffness of the sacroiliac joints, the pelvic floor muscles (PFM) have been argued to provide a contribution to control of the lumbar spine and pelvis. Furthermore, as IAP is modulated during respiration this is likely to be accompanied by changes in PFM activity. METHODS: In order to evaluate the postural and respiratory function of the PFM, recordings of anal and vaginal electromyographic activity (EMG) were made with surface electrodes during single and repetitive arm movements that challenge the stability of the spine. EMG recordings were also made during respiratory tasks: quiet breathing and breathing with increased dead-space to induce hypercapnoea. RESULTS: EMG activity of the PFM was increased in advance of deltoid muscle activity as a component of the pre-programmed anticipatory postural activity. This activity was independent of the direction of arm movement. During repetitive movements, PFM EMG was tonic with phasic bursts at the frequency of arm movement. This activity was related to the peak acceleration of the arm, and therefore the amplitude of the reactive forces imposed on the spine. Respiratory activity was observed for the anal and vaginal EMG and was primarily expiratory. When subjects moved the arm repetitively while breathing, PFM EMG was primarily modulated in association with arm movement with little respiratory modulation. CONCLUSIONS: This study provides evidence that the PFM contribute to both postural and respiratory functions.},
  eprint = {17304528},
  eprinttype = {pmid},
  keywords = {Adult,Anal Canal,Arm,Electromyography,Female,Humans,Hypercapnia,Male,Middle Aged,Motor Activity,Movement,Muscle Contraction,Muscle; Skeletal,Pelvic Floor,Posture,Respiratory Dead Space,Respiratory Mechanics,Vagina},
  langid = {english},
  number = {3}
}

@article{hoehlInteractionalSynchronySignals2020,
  title = {Interactional Synchrony: Signals, Mechanisms and Benefits},
  shorttitle = {Interactional Synchrony},
  author = {Hoehl, Stefanie and Fairhurst, Merle and Schirmer, Annett},
  date = {2020-03-03},
  journaltitle = {Social Cognitive and Affective Neuroscience},
  issn = {1749-5016, 1749-5024},
  doi = {10.1093/scan/nsaa024},
  url = {https://academic.oup.com/scan/advance-article/doi/10.1093/scan/nsaa024/5775599},
  urldate = {2020-09-22},
  abstract = {Many group-living animals, humans included, occasionally synchronize their behavior with that of conspecifics. Social psychology and neuroscience have attempted to explain this phenomenon. Here we sought to integrate results around three themes: the stimuli, the mechanisms and the benefits of interactional synchrony. As regards stimuli, we asked what characteristics, apart from temporal regularity, prompt synchronization and found that stimulus modality and complexity are important. The high temporal resolution of the auditory system and the relevance of socio-emotional information endow auditory, multimodal, emotional and somewhat variable and adaptive sequences with particular synchronizing power. Looking at the mechanisms revealed that traditional perspectives emphasizing beat-based representations of others’ signals conf lict with more recent work investigating the perception of temporal regularity. Timing processes supported by striato-cortical loops represent any kind of repetitive interval sequence fairly automatically. Additionally, socio-emotional processes supported by posterior superior temporal cortex help endow such sequences with value motivating the extent of synchronizing. Synchronizing benefits arise from an increased predictability of incoming signals and include many positive outcomes ranging from basic information processing at the individual level to the bonding of dyads and larger groups.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9W4SPSG4\\Hoehl et al. - 2020 - Interactional synchrony signals, mechanisms and b.pdf},
  langid = {english}
}

@article{hoetjesDoesOurSpeech2014,
  title = {Does Our Speech Change When We Cannot Gesture?},
  author = {Hoetjes, Marieke and Krahmer, Emiel and Swerts, Marc},
  date = {2014-02},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {57},
  pages = {257--267},
  issn = {01676393},
  doi = {10.1016/j.specom.2013.06.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639313000733},
  urldate = {2021-03-03},
  abstract = {Do people speak differently when they cannot use their hands? Previous studies have suggested that speech becomes less fluent and more monotonous when speakers cannot gesture, but the evidence for this claim remains inconclusive. The present study attempts to find support for this claim in a production experiment in which speakers had to give addressees instructions on how to tie a tie; half of the participants had to perform this task while sitting on their hands. Other factors that influence the ease of communication, such as mutual visibility and previous experience, were also taken into account. No evidence was found for the claim that the inability to gesture affects speech fluency or monotony. An additional perception task showed that people were also not able to hear whether someone gestures or not.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BASQRNPC\\Hoetjes et al. - 2014 - Does our speech change when we cannot gesture.pdf},
  langid = {english}
}

@article{hoetjesReductionGestureProduction2015,
  title = {Reduction in Gesture during the Production of Repeated References},
  author = {Hoetjes, Marieke and Koolen, Ruud and Goudbeek, Martijn and Krahmer, Emiel and Swerts, Marc},
  date = {2015-02-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {79-80},
  pages = {1--17},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2014.10.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0749596X14001260},
  urldate = {2020-06-26},
  abstract = {In dialogue, repeated references contain fewer words (which are also acoustically reduced) and fewer gestures than initial ones. In this paper, we describe three experiments studying to what extent gesture reduction is comparable to other forms of linguistic reduction. Since previous studies showed conflicting findings for gesture rate, we systematically compare two measures of gesture rate: gesture rate per word and per semantic attribute (Experiment I). In addition, we ask whether repetition impacts the form of gestures, by manual annotation of a number of features (Experiment I), by studying gradient differences using a judgment test (Experiment II), and by investigating how effective initial and repeated gestures are at communicating information (Experiment III). The results revealed no reduction in terms of gesture rate per word, but a U-shaped reduction pattern for gesture rate per attribute. Gesture annotation showed no reliable effects of repetition on gesture form, yet participants judged gestures from repeated references as less precise than those from initial ones. Despite this gradient reduction, gestures from initial and repeated references were equally successful in communicating information. Besides effects of repetition, we found systematic effects of visibility on gesture production, with more, longer, larger and more communicative gestures when participants could see each other. We discuss the implications of our findings for gesture research and for models of speech and gesture production.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RS4KSUQV\\Hoetjes et al. - 2015 - Reduction in gesture during the production of repe.pdf;C\:\\Users\\u668173\\Zotero\\storage\\7X22G78G\\S0749596X14001260.html},
  keywords = {Gesture,Reduction,Repeated references,Visibility},
  langid = {english}
}

@article{hoeyLapsesHowPeople2015,
  title = {Lapses: {{How People Arrive}} at, and {{Deal With}}, {{Discontinuities}} in {{Talk}}},
  shorttitle = {Lapses},
  author = {Hoey, Elliott M.},
  date = {2015-10-02},
  journaltitle = {Research on Language and Social Interaction},
  volume = {48},
  pages = {430--453},
  publisher = {{Routledge}},
  issn = {0835-1813},
  doi = {10.1080/08351813.2015.1090116},
  url = {https://doi.org/10.1080/08351813.2015.1090116},
  urldate = {2020-10-07},
  abstract = {Interaction includes moments of silence. When all participants forgo the option to speak, the silence can be called a “lapse.” This article builds on existing work on lapses and other kinds of silences (gaps, pauses, and so on) to examine how participants reach a point where lapsing is a possibility and how they orient to the lapse that subsequently develops. Drawing from a wide range of activities and settings, I will show that participants may treat lapses as (a) the relevant cessation of talk, (b) the allowable development of silence, or (c) the conspicuous absence of talk. Data are in American and British English.},
  annotation = {\_eprint: https://doi.org/10.1080/08351813.2015.1090116},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\V576T42Q\\Hoey - 2015 - Lapses How People Arrive at, and Deal With, Disco.pdf;C\:\\Users\\u668173\\Zotero\\storage\\KKMT4MSM\\08351813.2015.html},
  number = {4}
}

@article{hoganOrganizingPrincipleClass1984,
  title = {An Organizing Principle for a Class of Voluntary Movements},
  author = {Hogan, N.},
  date = {1984-11},
  journaltitle = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {4},
  pages = {2745--2754},
  issn = {0270-6474},
  abstract = {This paper presents a mathematical model which predicts both the major qualitative features and, within experimental error, the quantitative details of a class of perturbed and unperturbed large-amplitude, voluntary movements performed at intermediate speed by primates. A feature of the mathematical model is that a concise description of the behavioral organization of the movement has been formulated which is separate and distinct from the description of the dynamics of movement execution. Based on observations of voluntary movements in primates, the organization has been described as though the goal were to make the smoothest movement possible under the circumstances, i.e., to minimize the accelerative transients. This has been formalized by using dynamic optimization theory to determine the movement which minimizes the rate of change of acceleration (jerk) of the limb. Based on observations of muscle mechanics, the concept of a "virtual position" determined by the active states of the muscles is rigorously defined as one of the mechanical consequences of the neural commands to the muscles. This provides insight into the mechanics of perturbed and unperturbed movements and is a useful aid in the separation of the descriptions of movement organization and movement execution.},
  eprint = {6502203},
  eprinttype = {pmid},
  keywords = {Animals,Elbow Joint,Forearm,Macaca mulatta,Mathematics,Models; Biological,Motor Neurons,Movement,Muscles,Posture},
  langid = {english},
  number = {11},
  pmcid = {PMC6564718}
}

@article{hoganSensitivitySmoothnessMeasures2009,
  title = {Sensitivity of {{Smoothness Measures}} to {{Movement Duration}}, {{Amplitude}} and {{Arrests}}},
  author = {Hogan, Neville and Sternad, Dagmar},
  date = {2009-11},
  journaltitle = {Journal of motor behavior},
  shortjournal = {J Mot Behav},
  volume = {41},
  pages = {529--534},
  issn = {0022-2895},
  doi = {10.3200/35-09-004-RC},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3470860/},
  urldate = {2020-01-31},
  abstract = {Studies of sensory-motor performance, including those concerned with changes due to age, disease or therapeutic intervention, often use measures based on jerk, the time-derivative of acceleration, to quantify smoothness and coordination. However, results have been mixed, some studies reporting sensitive discrimination of subtle differences, others failing to find significant differences, even when they are obviously present. One reason for this state of affairs is that different measures have been used with different scaling factors. These measures are sensitive to movement amplitude and/or duration to different degrees. We show that jerk-based measures with dimensions vary counter-intuitively with movement smoothness, whereas a dimensionless jerk-based measure properly quantifies common deviations from smooth, coordinated movement.},
  eprint = {19892658},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BR9XVDTG\\Hogan and Sternad - 2009 - Sensitivity of Smoothness Measures to Movement Dur.pdf},
  number = {6},
  pmcid = {PMC3470860}
}

@article{hoitSpeechBreathingIndividuals1990,
  title = {Speech {{Breathing}} in {{Individuals}} with {{Cervical Spinal Cord Injury}}},
  author = {Hoit, J. D. and Banzett, R. B. and Brown, R. and Loring, S. H.},
  date = {1990-12-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {33},
  pages = {798--807},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/jshr.3304.798},
  url = {https://pubs.asha.org/doi/full/10.1044/jshr.3304.798},
  urldate = {2020-09-10},
  abstract = {Ten men with cervical spinal cord injury were studied using magnetometers to record          surface motions of the chest wall during speech breathing. Individual speech breathing          patterns reflected inspiratory and expiratory muscular sparing. Subjects compensated          for expiratory muscle impairment by speaking at large lung volumes, presumably to          take advantage of the higher recoil pressures available at those volumes. Similarly,          subjects used larger lung volumes to increase loudness. Abnormal chest wall behavior          was attributed in large part to loss of abdominal muscle function. Because of this,          speech breathing in individuals with cervical spinal cord injury may be improved by          the use of abdominal binders.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BXJP5CG8\\Hoit Jeannette D. et al. - 1990 - Speech Breathing in Individuals with Cervical Spin.pdf;C\:\\Users\\u668173\\Zotero\\storage\\RQTVDC3U\\jshr.3304.html},
  number = {4}
}

@article{holderiedEcholocationRangeWingbeat2003,
  title = {Echolocation Range and Wingbeat Period Match in Aerial-Hawking Bats},
  author = {Holderied, M. W. and von Helversen, O.},
  date = {2003-11-07},
  journaltitle = {Proceedings. Biological Sciences},
  shortjournal = {Proc. Biol. Sci.},
  volume = {270},
  pages = {2293--2299},
  issn = {0962-8452},
  doi = {10.1098/rspb.2003.2487},
  abstract = {Aerial-hawking bats searching the sky for prey face the problem that flight and echolocation exert independent and possibly conflicting influences on call intervals. These bats can only exploit their full echolocation range unambiguously if they emit their next call when all echoes from the preceding call would have arrived. However, not every call interval is equally available. The need to reduce the high energetic costs of echolocation forces aerial-hawking bats to couple call emission to their wingbeat. We compared the wingbeat periods of 11 aerial-hawking bat species with the delays of the last-expected echoes. Acoustic flight-path tracking was employed to measure the source levels (SLs) of echolocation calls in the field. SLs were very high, extending the known range to 133 dB peak equivalent sound pressure level. We calculated the maximum detection distances for insects, larger flying objects and background targets. Wingbeat periods were derived from call intervals. Small and medium-sized bats in fact matched their maximum detection range for insects and larger flying targets to their wingbeat period. The tendency to skip calls correlated with the species' detection range for background targets. We argue that a species' call frequency is at such a pitch that the resulting detection range matches their wingbeat period.},
  eprint = {14613617},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TQFN5WZX\\Holderied and von Helversen - 2003 - Echolocation range and wingbeat period match in ae.pdf},
  keywords = {Animals,Auditory Perception,Chiroptera,Echolocation,Flight; Animal,Predatory Behavior,Time Factors,Wings; Animal},
  langid = {english},
  number = {1530},
  options = {useprefix=true},
  pmcid = {PMC1691500}
}

@article{hollerExperimentalInvestigationHow2011,
  title = {An Experimental Investigation of How Addressee Feedback Affects Co-Speech Gestures Accompanying Speakers’ Responses},
  author = {Holler, Judith and Wilkin, Katie},
  date = {2011-11-01},
  journaltitle = {Journal of Pragmatics},
  shortjournal = {Journal of Pragmatics},
  volume = {43},
  pages = {3522--3536},
  issn = {0378-2166},
  doi = {10.1016/j.pragma.2011.08.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0378216611002190},
  urldate = {2020-03-18},
  abstract = {There is evidence that co-speech gestures communicate information to addressees and that they are often communicatively intended. However, we still know comparatively little about the role of gestures in the actual process of communication. The present study offers a systematic investigation of speakers’ gesture use before and after addressee feedback. The findings show that when speakers responded to addressees’ feedback gesture rate remained constant when this feedback encouraged clarification, elaboration or correction. However, speakers gestured proportionally less often after feedback when providing confirmatory responses. That is, speakers may not be drawing on gesture in response to addressee feedback per se, but particularly with responses that enhance addressees’ understanding. Further, the large majority of speakers’ gestures changed in their form. They tended to be more precise, larger, or more visually prominent after feedback. Some changes in gesture viewpoint were also observed. In addition, we found that speakers used deixis in speech and gaze to increase the salience of gestures occurring in response to feedback. Speakers appear to conceive of gesture as a useful modality in redesigning utterances to make them more accessible to addressees. The findings further our understanding of recipient design and co-speech gestures in face-to-face dialogue.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SQEQI6BX\\Holler and Wilkin - 2011 - An experimental investigation of how addressee fee.pdf;C\:\\Users\\u668173\\Zotero\\storage\\PWNYMULC\\S0378216611002190.html},
  keywords = {Addressee feedback,Co-speech gesture,Gaze,Recipient design,Verbal deixis},
  langid = {english},
  number = {14}
}

@article{hollerMultimodalLanguageProcessing2019,
  title = {Multimodal Language Processing in Human Communication},
  author = {Holler, Judith and Levinson, Stephen C.},
  date = {2019-08-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  pages = {639--652},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.05.006},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661319301299},
  urldate = {2019-08-26},
  abstract = {The natural ecology of human language is face-to-face interaction comprising the exchange of a plethora of multimodal signals. Trying to understand the psycholinguistic processing of language in its natural niche raises new issues, first and foremost the binding of multiple, temporally offset signals under tight time constraints posed by a turn-taking system. This might be expected to overload and slow our cognitive system, but the reverse is in fact the case. We propose cognitive mechanisms that may explain this phenomenon and call for a multimodal, situated psycholinguistic framework to unravel the full complexities of human language processing.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2JPUT5J2\\Holler and Levinson - 2019 - Multimodal Language Processing in Human Communicat.pdf;C\:\\Users\\u668173\\Zotero\\storage\\LY3C7FWG\\S1364661319301299.html},
  keywords = {binding,cross-level prediction,multimodal gestalts,multimodal language,segregation},
  number = {8}
}

@article{hollerProcessingLanguageFacetoface2018,
  title = {Processing Language in Face-to-Face Conversation: {{Questions}} with Gestures Get Faster Responses},
  shorttitle = {Processing Language in Face-to-Face Conversation},
  author = {Holler, Judith and Kendrick, Kobin H. and Levinson, Stephen C.},
  date = {2018-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {1900--1908},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1363-z},
  url = {https://doi.org/10.3758/s13423-017-1363-z},
  urldate = {2020-09-21},
  abstract = {The home of human language use is face-to-face interaction, a context in which communicative exchanges are characterised not only by bodily signals accompanying what is being said but also by a pattern of alternating turns at talk. This transition between turns is astonishingly fast—typically a mere 200-ms elapse between a current and a next speaker’s contribution—meaning that comprehending, producing, and coordinating conversational contributions in time is a significant challenge. This begs the question of whether the additional information carried by bodily signals facilitates or hinders language processing in this time-pressured environment. We present analyses of multimodal conversations revealing that bodily signals appear to profoundly influence language processing in interaction: Questions accompanied by gestures lead to shorter turn transition times—that is, to faster responses—than questions without gestures, and responses come earlier when gestures end before compared to after the question turn has ended. These findings hold even after taking into account prosodic patterns and other visual signals, such as gaze. The empirical findings presented here provide a first glimpse of the role of the body in the psycholinguistic processes underpinning human communication.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JAJBYSSE\\Holler et al. - 2018 - Processing language in face-to-face conversation .pdf},
  langid = {english},
  number = {5}
}

@article{holowkaHumanFootFunctions2021,
  title = {The Human Foot Functions like a Spring of Adjustable Stiffness during Running},
  author = {Holowka, Nicholas B. and Richards, Alexander and Sibson, Benjamin E. and Lieberman, Daniel E.},
  date = {2021-01-01},
  journaltitle = {Journal of Experimental Biology},
  volume = {224},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.219667},
  url = {https://jeb.biologists.org/content/224/1/jeb219667},
  urldate = {2021-01-13},
  abstract = {Skip to Next Section Like other animals, humans use their legs like springs to save energy during running. One potential contributor to leg stiffness in humans is the longitudinal arch (LA) of the foot. Studies of cadaveric feet have demonstrated that the LA can function like a spring, but it is unknown whether humans can adjust LA stiffness in coordination with more proximal joints to help control leg stiffness during running. Here, we used 3D motion capture to record 27 adult participants running on a forceplate-instrumented treadmill, and calculated LA stiffness using beam bending and midfoot kinematics models of the foot. Because changing stride frequency causes humans to adjust overall leg stiffness, we had participants run at their preferred frequency and frequencies 35\% above and 20\% below preferred frequency to test for similar adjustments in the LA. Regardless of which foot model we used, we found that participants increased LA quasi-stiffness significantly between low and high frequency runs, mirroring changes at the ankle, knee and leg overall. However, among foot models, we found that the model incorporating triceps surae force into bending force on the foot produced unrealistically high LA work estimates, leading us to discourage this modeling approach. Additionally, we found that there was not a consistent correlation between LA height and quasi-stiffness values among the participants, indicating that static LA height measurements are not good predictors of dynamic function. Overall, our findings support the hypothesis that humans dynamically adjust LA stiffness during running in concert with other structures of the leg.},
  eprint = {33199449},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2ETEXFQK\\jeb219667.html},
  langid = {english},
  number = {1}
}

@article{hondaRoleVerticalLarynx1999,
  title = {Role of Vertical Larynx Movement and Cervical Lordosis in {{F0}} Control},
  author = {Honda, K. and Hirai, H. and Masaki, S. and Shimada, Y.},
  year = {1999 Oct-Dec},
  journaltitle = {Language and Speech},
  volume = {42 ( Pt 4)},
  pages = {401--411},
  issn = {0023-8309},
  doi = {10.1177/00238309990420040301},
  abstract = {The role of vertical larynx movement in vocal frequency (F0) change has attracted the attention of many researchers. Recently, Hirai, Honda, Fujimoto, and Shimada (1994) proposed a mechanism of F0 control by vertical larynx movement based on the measurement of magnetic resonance images (MRI). In F0 changes, the larynx moves vertically along the cervical spine, which displays anterior convexity (lordosis) at the level of the larynx. Therefore, the vertical larynx movement results in the rotation of the cricoid cartilage and vocal fold tension changes. The present study reexamines the above mechanism based on a qualitative analysis of midsagittal MRI data using three male subjects with evident cervical lordosis. Tracings of the jaw, hyoid bone, laryngeal cartilage, and cervical spine were compared in high and low F0 ranges. In the high F0 range, the hyoid bone moved horizontally while the larynx height remained relatively constant. In the low F0 range, the entire larynx moved vertically, and the cricoid cartilage rotated along the cervical lordosis. These results indicate that the vertical movement of the larynx comprises an effective F0 lowering mechanism, and suggest that the human morphologies of low larynx position and spinal curvature contribute to voluntary use of the vocal function.},
  eprint = {10845244},
  eprinttype = {pmid},
  keywords = {Cervical Vertebrae,Humans,Larynx,Magnetic Resonance Imaging,Male,Middle Aged,Movement,Spine,Voice},
  langid = {english}
}

@article{hondaRoleVerticalLarynx2016,
  title = {Role of {{Vertical Larynx Movement}} and {{Cervical Lordosis}} in {{F0 Control}}:},
  shorttitle = {Role of {{Vertical Larynx Movement}} and {{Cervical Lordosis}} in {{F0 Control}}},
  author = {Honda, Kiyoshi and Hirai, Hiroyuki and Masaki, Shinobu and Shimada, Yasuhiro},
  date = {2016-08-18},
  journaltitle = {Language and Speech},
  publisher = {{SAGE PublicationsSage UK: London, England}},
  doi = {10.1177/00238309990420040301},
  url = {https://journals.sagepub.com/doi/10.1177/00238309990420040301},
  urldate = {2020-10-19},
  abstract = {The role of vertical larynx movement in vocal frequency (F0) change has attracted the attention of many researchers. Recently, Hirai, Honda, Fujimoto, and Shima...},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZHPF3CKA\\00238309990420040301.html},
  langid = {english}
}

@book{hongRoutledgeHandbookBiomechanics2008,
  title = {Routledge {{Handbook}} of {{Biomechanics}} and {{Human Movement Science}}},
  editor = {Hong, Y. and Bartlett, R.},
  date = {2008},
  publisher = {{Routledge}},
  location = {{New York}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SY8NVXLN\\Routledge_Handbook_of_Biomechanics_and_H.html}
}

@article{hopkinsHandMouthEvolution2003,
  title = {From Hand to Mouth in the Evolution of Language: {{The}} Influence of Vocal Behavior on Lateralized Hand Use in Manual Gestures by Chimpanzees ({{Pan}} Troglodytes)},
  shorttitle = {From Hand to Mouth in the Evolution of Language},
  author = {Hopkins, William D. and Cantero, Monica},
  date = {2003},
  journaltitle = {Developmental Science},
  volume = {6},
  pages = {55--61},
  publisher = {{Blackwell Publishing}},
  location = {{United Kingdom}},
  issn = {1467-7687(Electronic),1363-755X(Print)},
  doi = {10.1111/1467-7687.00254},
  abstract = {Examined the association between hand use for gestural communication and vocal behavior in chimpanzees. On each test trial, an experimenter approached the S's cage, positioned themselves approximately 1 m from the S's home cage and directly in front of the focal chimpanzee, and offered the S a banana. A 2nd experimenter, standing directly behind the experimenter offering the banana, recorded all communicative behavior of the focal S for a 1-minute sampling period. Each chimpanzee was tested on 10 separate occasions separated by at least 1 day. Results offer evidence that chimpanzees exhibit preferential use of the right hand in gestural communication. Moreover, use of the right hand in gestural communication is significantly enhanced when accompanied by a vocalization, particularly among human-reared chimpanzees. Taken together, the data suggest that the lateralization of manual and speech systems of communication may date back as far as 5 million years ago. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5ZIH58HJ\\2003-04786-013.html},
  keywords = {Animal Vocalizations,Chimpanzees,Gestures,Handedness},
  number = {1}
}

@article{hospelhornMethodAnalyzingGestural2017,
  title = {Method for {{Analyzing Gestural Communication}} in {{Musical Groups}}},
  author = {Hospelhorn, Emma and Radinsky, Josh},
  date = {2017-10-03},
  journaltitle = {Discourse Processes},
  volume = {54},
  pages = {504--523},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2015.1137183},
  url = {https://doi.org/10.1080/0163853X.2015.1137183},
  urldate = {2019-09-20},
  abstract = {Musical performances provide a rich context for studying complex spatial and embodied modes of group learning. This article proposes a framework for analyzing gesture in musical performances to highlight the ways musicians' movements reflect and promote their emerging and changing conceptions of a piece of music. The constructs of expressive musical gesture (at the individual level of analysis) and group expressive musical gesture (at the collective level) are used to analyze a series of five sequential performances of a musical passage by a string quartet during rehearsal. The analysis identifies three functions of embodied gesture for score interpretation: (1) gestures served as a tool for group interpretation in passages that had previously been pointed to by verbal exchanges, (2) gestures served to fine-tune the location and enactment of dynamic markings in the score, and (3) group expressive gestures in the final “take” of the rehearsal incorporated group expressive gestures from other takes, constituting a negotiated set of score interpretations.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8ATUUZ2N\\0163853X.2015.html},
  number = {7}
}

@article{hostetterGestureSimulatedAction2019,
  title = {Gesture as Simulated Action: {{Revisiting}} the Framework},
  shorttitle = {Gesture as Simulated Action},
  author = {Hostetter, Autumn B. and Alibali, Martha W.},
  date = {2019-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {26},
  pages = {721--752},
  issn = {1531-5320},
  doi = {10.3758/s13423-018-1548-0},
  url = {https://doi.org/10.3758/s13423-018-1548-0},
  urldate = {2020-09-10},
  abstract = {The Gesture as Simulated Action (GSA) framework was proposed to explain how gestures arise from embodied simulations of the motor and perceptual states that occur during speaking and thinking (Hostetter \& Alibali, Psychonomic Bulletin \& Review, 15, 495–514, 2008). In this review, we revisit the framework’s six main predictions regarding gesture rates, gesture form, and the cognitive cost of inhibiting gesture. We find that the available evidence largely supports the main predictions of the framework. We also consider several challenges to the framework that have been raised, as well as several of the framework’s limitations as it was originally proposed. We offer additional elaborations of the framework to address those challenges that fall within the framework’s scope, and we conclude by identifying key directions for future work on how gestures arise from an embodied mind.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2MYADTIZ\\Hostetter and Alibali - 2019 - Gesture as simulated action Revisiting the framew.pdf},
  langid = {english},
  number = {3}
}

@article{hostetterVisibleEmbodimentGestures2008,
  title = {Visible Embodiment: {{Gestures}} as Simulated Action},
  shorttitle = {Visible Embodiment},
  author = {Hostetter, Autumn B. and Alibali, Martha W.},
  date = {2008-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {15},
  pages = {495--514},
  issn = {1531-5320},
  doi = {10.3758/PBR.15.3.495},
  url = {https://doi.org/10.3758/PBR.15.3.495},
  urldate = {2019-08-26},
  abstract = {Spontaneous gestures that accompany speech are related to both verbal and spatial processes. We argue that gestures emerge from perceptual and motor simulations that underlie embodied language and mental imagery. We first review current thinking about embodied cognition, embodied language, and embodied mental imagery. We then provide evidence that gestures stem from spatial representations and mental images. We then propose the gestures-as-simulated-action framework to explain how gestures might arise from an embodied cognitive system. Finally, we compare this framework with other current models of gesture production, and we briefly outline predictions that derive from the framework.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4VGTSXDV\\Hostetter and Alibali - 2008 - Visible embodiment Gestures as simulated action.pdf},
  keywords = {Lexical Access,Mental Image,Mental Rotation,Motor Imagery,Speech Production},
  langid = {english},
  number = {3}
}

@article{hoveItAllTiming2009,
  title = {It's {{All}} in the {{Timing}}: {{Interpersonal Synchrony Increases Affiliation}}},
  shorttitle = {It's {{All}} in the {{Timing}}},
  author = {Hove, Michael J. and Risen, Jane L.},
  date = {2009-12-01},
  journaltitle = {Social Cognition},
  shortjournal = {Social Cognition},
  volume = {27},
  pages = {949--960},
  publisher = {{Guilford Publications Inc.}},
  issn = {0278-016X},
  doi = {10.1521/soco.2009.27.6.949},
  url = {https://guilfordjournals.com/doi/10.1521/soco.2009.27.6.949},
  urldate = {2020-09-17},
  abstract = {The tendency to mimic and synchronize with others is well established. Although mimicry has been shown to lead to affiliation between co-actors, the effect of interpersonal synchrony on affiliation remains an open question. The authors investigated the relationship by having participants match finger movements with a visual moving metronome. In Experiment 1, affiliation ratings were examined based on the extent to which participants tapped in synchrony with the experimenter. In Experiment 2, synchrony was manipulated. Affiliation ratings were compared for an experimenter who either (a) tapped to a metronome that was synchronous to the participant's metronome, (b) tapped to a metronome that was asynchronous, or (c) did not tap. As hypothesized, in both studies, the degree of synchrony predicted subsequent affiliation ratings. Experiment 3 found that the affiliative effects were unique to interpersonal synchrony.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\A7L9YWZD\\Hove and Risen - 2009 - It's All in the Timing Interpersonal Synchrony In.pdf;C\:\\Users\\u668173\\Zotero\\storage\\J2UAAN5U\\soco.2009.27.6.html},
  number = {6}
}

@article{hoveItAllTiming2009a,
  title = {It's All in the Timing: {{Interpersonal}} Synchrony Increases Affiliation},
  shorttitle = {It's All in the Timing},
  author = {Hove, Michael J. and Risen, Jane L.},
  date = {2009},
  journaltitle = {Social Cognition},
  volume = {27},
  pages = {949--961},
  publisher = {{Guilford Publications}},
  location = {{US}},
  issn = {0278-016X(Print)},
  doi = {10.1521/soco.2009.27.6.949},
  abstract = {The tendency to mimic and synchronize with others is well established. Although mimicry has been shown to lead to affiliation between co-actors, the effect of interpersonal synchrony on affiliation remains an open question. The authors investigated the relationship by having participants match finger movements with a visual moving metronome. In Experiment 1, affiliation ratings were examined based on the extent to which participants tapped in synchrony with the experimenter. In Experiment 2, synchrony was manipulated. Affiliation ratings were compared for an experimenter who either (a) tapped to a metronome that was synchronous to the participant's metronome, (b) tapped to a metronome that was asynchronous, or (c) did not tap. As hypothesized, in both studies, the degree of synchrony predicted subsequent affiliation ratings. Experiment 3 found that the affiliative effects were unique to interpersonal synchrony. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\23QM4CWP\\Hove and Risen - 2009 - It's all in the timing Interpersonal synchrony in.pdf;C\:\\Users\\u668173\\Zotero\\storage\\U8E2BLWB\\2009-24381-011.html},
  keywords = {Affiliation Motivation,Social Adjustment,Social Perception,Synchrony},
  number = {6}
}

@article{hoveItAllTiming2009b,
  title = {It's All in the Timing: {{Interpersonal}} Synchrony Increases Affiliation},
  shorttitle = {It's All in the Timing},
  author = {Hove, Michael J. and Risen, Jane L.},
  date = {2009},
  journaltitle = {Social Cognition},
  volume = {27},
  pages = {949--961},
  publisher = {{Guilford Publications}},
  location = {{US}},
  issn = {0278-016X(Print)},
  doi = {10.1521/soco.2009.27.6.949},
  abstract = {The tendency to mimic and synchronize with others is well established. Although mimicry has been shown to lead to affiliation between co-actors, the effect of interpersonal synchrony on affiliation remains an open question. The authors investigated the relationship by having participants match finger movements with a visual moving metronome. In Experiment 1, affiliation ratings were examined based on the extent to which participants tapped in synchrony with the experimenter. In Experiment 2, synchrony was manipulated. Affiliation ratings were compared for an experimenter who either (a) tapped to a metronome that was synchronous to the participant's metronome, (b) tapped to a metronome that was asynchronous, or (c) did not tap. As hypothesized, in both studies, the degree of synchrony predicted subsequent affiliation ratings. Experiment 3 found that the affiliative effects were unique to interpersonal synchrony. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4HWPKRBH\\Hove and Risen - 2009 - It's all in the timing Interpersonal synchrony in.pdf;C\:\\Users\\u668173\\Zotero\\storage\\LSF2X8IT\\2009-24381-011.html},
  keywords = {Affiliation Motivation,Social Adjustment,Social Perception,Synchrony},
  number = {6}
}

@article{hoveSpatiotemporalRelationsMovement2010,
  title = {Spatiotemporal {{Relations}} and {{Movement Trajectories}} in {{Visuomotor Synchronization}}},
  author = {Hove, Michael J. and Keller, Peter E.},
  date = {2010-09-01},
  journaltitle = {Music Perception},
  volume = {28},
  pages = {15--26},
  issn = {0730-7829, 1533-8312},
  doi = {10.1525/mp.2010.28.1.15},
  url = {https://online.ucpress.edu/mp/article/28/1/15/62471/Spatiotemporal-Relations-and-Movement-Trajectories},
  urldate = {2020-09-21},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\X4MGWHWI\\Hove and Keller - 2010 - Spatiotemporal Relations and Movement Trajectories.pdf},
  langid = {english},
  number = {1}
}

@article{hoveSynchronizingAuditoryVisual2013a,
  title = {Synchronizing with Auditory and Visual Rhythms: {{An fMRI}} Assessment of Modality Differences and Modality Appropriateness},
  shorttitle = {Synchronizing with Auditory and Visual Rhythms},
  author = {Hove, Michael J. and Fairhurst, Merle T. and Kotz, Sonja A. and Keller, Peter E.},
  date = {2013-02-15},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {67},
  pages = {313--321},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2012.11.032},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811912011445},
  urldate = {2020-09-17},
  abstract = {Synchronizing movements with auditory beats, compared to visual flashes, yields divergent activation in timing-related brain areas as well as more stable tapping synchronization. The differences in timing-related brain activation could reflect differences in tapping synchronization stability, rather than differences between modality (i.e., audio-motor vs. visuo-motor integration). In the current fMRI study, participants synchronized their finger taps with four types of visual and auditory pacing sequences: flashes and a moving bar, as well as beeps and a frequency-modulated ‘siren’. Behavioral tapping results showed that visuo-motor synchronization improved with moving targets, whereas audio-motor synchronization degraded with frequency-modulated sirens. Consequently, a modality difference in synchronization occurred between the discrete beeps and flashes, but not between the novel continuous siren and moving bar. Imaging results showed that activation in the putamen, a key timing area, paralleled the behavioral results: putamen activation was highest for beeps, intermediate for the continuous siren and moving bar, and was lowest for the flashes. Putamen activation differed between modalities for beeps and flashes, but not for the novel moving bar and siren. By dissociating synchronization performance from modality, we show that activation in the basal ganglia is associated with sensorimotor synchronization stability rather than modality-specificity in this task. Synchronization stability is apparently contingent upon the modality's processing affinity: discrete auditory and moving visual signals are modality appropriate, and can be encoded reliably for integration with the motor system.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\J7CIRTT3\\S1053811912011445.html},
  keywords = {Basal ganglia,Modality differences,Rhythm,Sensorimotor synchronization,Timing},
  langid = {english}
}

@online{hruschkaYouCanCharacterize,
  title = {You Can't Characterize Human Nature If Studies Overlook 85 Percent of People on {{Earth}}},
  author = {Hruschka, Daniel},
  url = {http://theconversation.com/you-cant-characterize-human-nature-if-studies-overlook-85-percent-of-people-on-earth-106670},
  urldate = {2020-07-09},
  abstract = {Ninety percent of psychology studies come from countries representing less than 15 percent of the world's population. Researchers are realizing that universalizing those findings might not make sense.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\J4ZLN8JZ\\you-cant-characterize-human-nature-if-studies-overlook-85-percent-of-people-on-earth-106670.html},
  langid = {english},
  organization = {{The Conversation}}
}

@article{hsuErnstKurthHis1966,
  title = {Ernst {{Kurth}} and {{His Concept}} of {{Music}} as {{Motion}}},
  author = {Hsu, Dolores Menstell},
  date = {1966},
  journaltitle = {Journal of Music Theory},
  volume = {10},
  pages = {2--17},
  publisher = {{[Duke University Press, Yale University Department of Music]}},
  issn = {0022-2909},
  doi = {10.2307/843297},
  eprint = {843297},
  eprinttype = {jstor},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5FFU43F2\\Hsu - 1966 - Ernst Kurth and His Concept of Music as Motion.pdf},
  number = {1}
}

@online{huaMoreRealisticHumanRobot2019,
  title = {Towards {{More Realistic Human}}-{{Robot Conversation}}: {{A Seq2Seq}}-Based {{Body Gesture Interaction System}}},
  shorttitle = {Towards {{More Realistic Human}}-{{Robot Conversation}}},
  author = {Hua, Minjie and Shi, Fuyuan and Nan, Yibing and Wang, Kai and Chen, Hao and Lian, Shiguo},
  date = {2019-11-15},
  url = {http://arxiv.org/abs/1905.01641},
  urldate = {2021-01-27},
  abstract = {This paper presents a novel system that enables intelligent robots to exhibit realistic body gestures while communicating with humans. The proposed system consists of a listening model and a speaking model used in corresponding conversational phases. Both models are adapted from the sequence-to-sequence (seq2seq) architecture to synthesize body gestures represented by the movements of twelve upper-body keypoints. All the extracted 2D keypoints are firstly 3D-transformed, then rotated and normalized to discard irrelevant information. Substantial videos of human conversations from Youtube are collected and preprocessed to train the listening and speaking models separately, after which the two models are evaluated using metrics of mean squared error (MSE) and cosine similarity on the test dataset. The tuned system is implemented to drive a virtual avatar as well as Pepper, a physical humanoid robot, to demonstrate the improvement on conversational interaction abilities of our method in practice.},
  archiveprefix = {arXiv},
  eprint = {1905.01641},
  eprinttype = {arxiv},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HFZZGB94\\Hua et al. - 2019 - Towards More Realistic Human-Robot Conversation A.pdf;C\:\\Users\\u668173\\Zotero\\storage\\SANHCGWW\\1905.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{hubbardGivingSpeechHand2009,
  title = {Giving Speech a Hand: {{Gesture}} Modulates Activity in Auditory Cortex during Speech Perception},
  shorttitle = {Giving Speech a Hand},
  author = {Hubbard, Amy L. and Wilson, Stephen M. and Callan, Daniel E. and Dapretto, Mirella},
  date = {2009},
  journaltitle = {Human Brain Mapping},
  volume = {30},
  pages = {1028--1037},
  publisher = {{John Wiley \& Sons}},
  location = {{US}},
  issn = {1097-0193(Electronic),1065-9471(Print)},
  doi = {10.1002/hbm.20565},
  abstract = {Viewing hand gestures during face-to-face communication affects speech perception and comprehension. Despite the visible role played by gesture in social interactions, relatively little is known about how the brain integrates hand gestures with co-occurring speech. Here we used functional magnetic resonance imaging (fMRI) and an ecologically valid paradigm to investigate how beat gesture—a fundamental type of hand gesture that marks speech prosody—might impact speech perception at the neural level. Subjects underwent fMRI while listening to spontaneously-produced speech accompanied by beat gesture, nonsense hand movement, or a still body; as additional control conditions, subjects also viewed beat gesture, nonsense hand movement, or a still body all presented without speech. Validating behavioral evidence that gesture affects speech perception, bilateral non primary auditory cortex showed greater activity when speech was accompanied by beat gesture than when speech was presented alone. Further, the left superior temporal gyrus/sulcus showed stronger activity when speech was accompanied by beat gesture than when speech was accompanied by nonsense hand movement. Finally, the right planum temporale was identified as a putative multi sensory integration site for beat gesture and speech (i.e., here activity in response to speech accompanied by beat gesture was greater than the summed responses to speech alone and beat gesture alone), indicating that this area may be pivotally involved in synthesizing the rhythmic aspects of both speech and gesture. Taken together, these findings suggest a common neural substrate for processing speech and gesture, likely reflecting their joint communicative role in social interactions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\B4YND2M6\\Hubbard et al. - 2009 - Giving speech a hand Gesture modulates activity i.pdf;C\:\\Users\\u668173\\Zotero\\storage\\VMIDBBRR\\2009-04205-028.html;C\:\\Users\\u668173\\Zotero\\storage\\ZMJYGGSI\\2009-04205-028.html},
  keywords = {Auditory Cortex,Comprehension,Functional Magnetic Resonance Imaging,Gestures,Interpersonal Communication,Speech Perception},
  number = {3}
}

@article{hubscherGesturalProsodicDevelopment2019,
  title = {Gestural and {{Prosodic Development Act}} as {{Sister Systems}} and {{Jointly Pave}} the {{Way}} for {{Children}}’s {{Sociopragmatic Development}}},
  author = {Hübscher, Iris and Prieto, Pilar},
  date = {2019-06-12},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.01259},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6581748/},
  urldate = {2019-09-06},
  abstract = {Children might combine gesture and prosody to express a pragmatic meaning such as a request, information focus, uncertainty or politeness, before they can convey these meanings in speech. However, little is known about the developmental trajectories of gestural and prosodic patterns and how they relate to a child’s growing understanding and propositional use of these sociopragmatic meanings. Do gesture and prosody act as sister systems in pragmatic development? Do children acquire these components of language before they are able to express themselves through spoken language, thus acting as forerunners in children’s pragmatic development? This review article assesses empirical evidence that demonstrates that gesture and prosody act as intimately related systems and, importantly, pave the way for pragmatic acquisition at different developmental stages. The review goes on to explore how the integration of gesture and prosody with semantics and syntax can impact language acquisition and how multimodal interventions can be used effectively in educational settings. Our review findings support the importance of simultaneously assessing both the prosodic and the gestural components of language in the fields of language development, language learning, and language intervention.},
  eprint = {31244716},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\T7A5B36S\\Hübscher and Prieto - 2019 - Gestural and Prosodic Development Act as Sister Sy.pdf},
  pmcid = {PMC6581748}
}

@article{huntMechanicalImplicationsChimpanzee1991,
  title = {Mechanical Implications of Chimpanzee Positional Behavior},
  author = {Hunt, Kevin D.},
  date = {1991-12},
  journaltitle = {American Journal of Physical Anthropology},
  shortjournal = {Am. J. Phys. Anthropol.},
  volume = {86},
  pages = {521--536},
  issn = {0002-9483, 1096-8644},
  doi = {10.1002/ajpa.1330860408},
  url = {http://doi.wiley.com/10.1002/ajpa.1330860408},
  urldate = {2020-11-20},
  abstract = {Mechanical hypotheses concerning the function of chimpanzee anatomical specializations are examined in light of recent positional behavior data. Arm-hanging was the only common chimpanzee positional behavior that required full abduction of the humerus, and vertical climbing was the only distinctive chimpanzee positional behavior that required forceful retraction of the humerus and flexion of the elbow. Some elements of the chimpanzee anatomy, including a n abductible humerus, a broad thorax, a cone-shaped torso, and a long, narrow scapula, are hypothesized to be a coadapted functional complex that reduces muscle action and structural fatigue during arm-hanging.Large muscles that retract the humerus (latissimus dorsi and probably sternocostal pectoralis major and posterior deltoid) and flex the elbow (biceps brachii, probably brachialis and brachioradialis) are argued to be adaptationsto vertical climbing alone. A large ulnar excursion of the manus and long, curved metacarpals and phalanges are interpreted a s adaptations to gripping vertical weight-bearing structures during vertical climbing and arm-hanging.A short torso, a n iliac origin of the latissimus dorsi, and large muscles for arm-raising (caudal serratus, teres minor, cranial trapezius, and probably anterior deltoid and clavicular pectoralis major) are interpreted as adaptations to both climbing and unimanual suspension.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7Y5QDCYV\\Hunt - 1991 - Mechanical implications of chimpanzee positional b.pdf},
  langid = {english},
  number = {4}
}

@online{IEEEXploreFullText,
  title = {{{IEEE Xplore Full}}-{{Text PDF}}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8373809},
  urldate = {2020-10-18},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\H28GFY34\\stamp.html}
}

@article{iharaNeuroimagingStudyBrain2010,
  title = {Neuroimaging Study on Brain Asymmetries in Situs Inversus Totalis},
  author = {Ihara, Aya and Hirata, Masayuki and Fujimaki, Norio and Goto, Tetsu and Umekawa, Yuka and Fujita, Norihiko and Terazono, Yasushi and Matani, Ayumu and Wei, Qiang and Yoshimine, Toshiki and Yorifuji, Shiro and Murata, Tsutomu},
  date = {2010-01-15},
  journaltitle = {Journal of the Neurological Sciences},
  shortjournal = {Journal of the Neurological Sciences},
  volume = {288},
  pages = {72--78},
  publisher = {{Elsevier}},
  issn = {0022-510X, 1878-5883},
  doi = {10.1016/j.jns.2009.10.002},
  url = {https://www.jns-journal.com/article/S0022-510X(09)00898-3/abstract},
  urldate = {2020-06-09},
  abstract = {{$<$}h2{$>$}Abstract{$<$}/h2{$><$}p{$>$}Situs inversus totalis (SI) is a rare condition in which all visceral organs are arranged as mirror images of the usual pattern. The objective of this study was to determine whether SI individuals have reversed brain asymmetries. We performed a neuroimaging study on 3 SI subjects and 11 control individuals with normally arranged visceral organs. The language-dominant hemisphere was determined by magnetoencephalography. Left-hemispheric dominance was observed in 1 SI subject and all controls, whereas right-hemispheric dominance was observed in the remaining 2 SI subjects. Statistical analysis revealed that language dominance patterns in SI subjects were different from those in the controls, suggesting that the developmental mechanisms underlying visceral organ asymmetries are related to those underlying functional brain asymmetry. Anatomical brain asymmetries were determined by magnetic resonance imaging. SI subjects had the same planum temporale (PT) asymmetry pattern as the controls, but a reversed petalia asymmetry pattern. The inferior frontal gyrus (IFG) asymmetry pattern varied within both groups, indicating a relationship between the rightward IFG and right-hemispheric language dominance. These results suggest that the developmental mechanisms underlying visceral organ asymmetries are related to those underlying petalia asymmetry but not to those underlying PT and IFG asymmetries, and that brain asymmetries might develop via multiple region-dependent mechanisms.{$<$}/p{$>$}},
  eprint = {19897211},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\T674TD4D\\abstract.html},
  langid = {english},
  number = {1}
}

@article{ImpairmentsSpeechProduction1994,
  title = {Impairments of Speech Production and Speech Perception in Aphasia},
  date = {1994-10-29},
  journaltitle = {Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences},
  volume = {346},
  pages = {29--36},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.1994.0125},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.1994.0125},
  urldate = {2020-07-21},
  abstract = {The basis of speech production and speech perception deficits in aphasia relates to im plem entation and access rather than to the underlying representation or knowledge base of the sound structure of language. Speech production deficits occur on the phonological level in which the incorrect phonological form of the word is selected but is implem ented correctly, and the phonetic level in which the correct sound segments are selected but articulatory im plem entation is impaired. Phonological deficits emerge regardless of lesion site, whereas phonetic deficits have a specific localized neuroanatom ical substrate. Phonetic deficits are not linguistic but affect particular articulatory movements. Speech perception impairm ents emerge in nearly all aphasic patients, suggesting that the neural basis for speech perception is broadly distributed in the language hemisphere. The im pairm ent reflects the misperception of phonetic features rather than a deficit in the auditory processing of speech and emerges particularly as the sound properties of speech contact the lexicon.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NSE6Q9P6\\1994 - Impairments of speech production and speech percep.pdf},
  langid = {english},
  number = {1315}
}

@article{imProbabilisticRelationCospeech2020,
  title = {Probabilistic Relation between Co-Speech Gestures, Pitch Accents and Information Status},
  author = {Im, Suyeon and Baumann, Stefan},
  date = {2020-03-23},
  journaltitle = {Proceedings of the Linguistic Society of America},
  volume = {5},
  pages = {685--697},
  issn = {2473-8689},
  doi = {10.3765/plsa.v5i1.4755},
  url = {http://journals.linguisticsociety.org/proceedings/index.php/PLSA/article/view/4755},
  urldate = {2020-03-30},
  abstract = {This study investigates the occurrence of co-speech gestures as a function of prosodic prominence (pitch accents) and discourse meaning (information status) in a clear and engaging speech style. Among several types of co-speech gestures, we examine non-referential gestures, which are claimed to be prosodic in nature (Shattuck-Hufnagel \& Ren 2018). In particular, we want to find out to what extent these gestures co-occur with specific accent types and whether they are used to encode referential, lexical, or contrastive information. Our results show that the occurrence of gestures was highest for L+H*, followed by H*, !H*, and unaccented words. Gestures were accompanied by L* only in continuations. Also, co-speech gestures were more likely to occur with new or accessible, and especially contrastive, information than with given information. The patterns differed between the referential and lexical level of information status, though. In general, this study suggests that co-speech gestures contribute to the probabilistic encoding of a word’s information status in conjunction with pitch accents.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8ZKMNLIH\\Im and Baumann - 2020 - Probabilistic relation between co-speech gestures,.pdf;C\:\\Users\\u668173\\Zotero\\storage\\K69ZSHPN\\4755.html},
  issue = {1},
  keywords = {clear speech,co-speech gestures,information status,pitch accents,prosody,RefLex Scheme},
  langid = {english},
  number = {1}
}

@article{inbarSequencesIntonationUnits2020,
  title = {Sequences of {{Intonation Units}} Form a \textasciitilde{} 1 {{Hz}} Rhythm},
  author = {Inbar, Maya and Grossman, Eitan and Landau, Ayelet N.},
  date = {2020-09-28},
  journaltitle = {Scientific Reports},
  volume = {10},
  pages = {15846},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-72739-4},
  url = {https://www.nature.com/articles/s41598-020-72739-4},
  urldate = {2020-09-30},
  abstract = {Studies of speech processing investigate the relationship between temporal structure in speech stimuli and neural activity. Despite clear evidence that the brain tracks speech at low frequencies (\textasciitilde{} 1 Hz), it is not well understood what linguistic information gives rise to this rhythm. In this study, we harness linguistic theory to draw attention to Intonation Units (IUs), a fundamental prosodic unit of human language, and characterize their temporal structure as captured in the speech envelope, an acoustic representation relevant to the neural processing of speech. IUs are defined by a specific pattern of syllable delivery, together with resets in pitch and articulatory force. Linguistic studies of spontaneous speech indicate that this prosodic segmentation paces new information in language use across diverse languages. Therefore, IUs provide a universal structural cue for the cognitive dynamics of speech production and comprehension. We study the relation between IUs and periodicities in the speech envelope, applying methods from investigations of neural synchronization. Our sample includes recordings from every-day speech contexts of over 100 speakers and six languages. We find that sequences of IUs form a consistent low-frequency rhythm and constitute a significant periodic cue within the speech envelope. Our findings allow to predict that IUs are utilized by the neural system when tracking speech. The methods we introduce here facilitate testing this prediction in the future (i.e., with physiological data).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\E77U2H4A\\Inbar et al. - 2020 - Sequences of Intonation Units form a ~ 1 Hz rhythm.pdf;C\:\\Users\\u668173\\Zotero\\storage\\SU269DYD\\s41598-020-72739-4.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{indefreySpatialTemporalSignatures2004,
  title = {The Spatial and Temporal Signatures of Word Production Components},
  author = {Indefrey, P. and Levelt, W. J. M.},
  year = {2004 May-Jun},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {92},
  pages = {101--144},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2002.06.001},
  abstract = {This paper presents the results of a comprehensive meta-analysis of the relevant imaging literature on word production (82 experiments). In addition to the spatial overlap of activated regions, we also analyzed the available data on the time course of activations. The analysis specified regions and time windows of activation for the core processes of word production: lexical selection, phonological code retrieval, syllabification, and phonetic/articulatory preparation. A comparison of the word production results with studies on auditory word/non-word perception and reading showed that the time course of activations in word production is, on the whole, compatible with the temporal constraints that perception processes impose on the production processes they affect in picture/word interference paradigms.},
  eprint = {15037128},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PGZQF9BK\\Indefrey and Levelt - 2004 - The spatial and temporal signatures of word produc.pdf},
  keywords = {Humans,Reading,Space Perception,Speech Production Measurement,Time Factors,Vocabulary},
  langid = {english},
  number = {1-2}
}

@article{indenRapidEntrainmentSpontaneous,
  title = {Rapid Entrainment to Spontaneous Speech: {{A}} Comparison of Oscillator Models},
  author = {Inden, Benjamin},
  pages = {7},
  abstract = {Oscillator models may be used for modeling synchrony between gestures and speech, or timing of backchanneling and turn-taking in dialogues. We find support for the hypothesis that oscillator networks can better predict rhythmic events on the syllable and foot level than single oscillators, but we do not find support for the hypothesis that phase resetting oscillators perform better that phase adapting oscillators. Overall, oscillators can be used to predict rhythmic events in speech, but higher level information needs to be integrated into such models to reach a satisfactory performance.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZDPHZYTH\\Inden - Rapid entrainment to spontaneous speech A compari.pdf},
  langid = {english}
}

@article{ingberTensegrityMechanotransduction2008,
  title = {Tensegrity and Mechanotransduction},
  author = {Ingber, D. W.},
  date = {2008-07},
  journaltitle = {Journal of Bodywork and Movement Therapies},
  volume = {12},
  pages = {198--200},
  issn = {13608592},
  doi = {10.1016/j.jbmt.2008.04.038},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1360859208000788},
  urldate = {2019-08-09},
  abstract = {Anyone who is skilled in the art of physical therapy knows that the mechanical properties, behavior and movement of our bodies are as important for human health as chemicals and genes. However, only recently have scientists and physicians begun to appreciate the key role that mechanical forces play in biological control at the molecular and cellular levels. This article provides a brief overview of a lecture presented at the 1st International Fascia Research Congress that convened at Harvard Medical School in Boston, MA on October 4, 2007. (see figure 1) In this lecture, I described what we have learned over the past thirty years as a result of our research focused on the molecular mechanisms by which cells sense mechanical forces and convert them into changes in intracellular biochemistry and gene expression – a process called “mechanotransduction”. This work has revealed that molecules, cells, tissues, organs, and our entire bodies use “tensegrity” architecture to mechanically stabilize their shape, and to seamlessly integrate structure and function at all size scales. Through use of this tension-dependent building system, mechanical forces applied at the macroscale produce changes in biochemistry and gene expression within individual living cells. This structurebased system provides a mechanistic basis to explain how application of physical therapies might influence cell and tissue physiology.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DK6AVJRE\\Ingber - 2008 - Tensegrity and mechanotransduction.pdf},
  langid = {english},
  number = {3}
}

@article{intoyFinelyTunedEye2020,
  title = {Finely Tuned Eye Movements Enhance Visual Acuity},
  author = {Intoy, Janis and Rucci, Michele},
  date = {2020-12},
  journaltitle = {Nature Communications},
  volume = {11},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-14616-2},
  url = {http://www.nature.com/articles/s41467-020-14616-2},
  urldate = {2020-11-02},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8Q3IPKQ9\\Intoy and Rucci - 2020 - Finely tuned eye movements enhance visual acuity.pdf},
  langid = {english},
  number = {1}
}

@unpublished{itoMediumFacilitatesPerception2015,
  title = {Medium {{Facilitates}} the Perception of Affordances for Touch},
  author = {Ito, K. and Sawada, M. and Mishima, Mamoru and Mishima, H. and Takiyama, M. and Kikuchi, Y.;},
  date = {2015-07-18},
  eventtitle = {{{XVIII International Conference}} on {{Perception}}-{{Action}}},
  venue = {{Minneapolis}}
}

@article{itoMotorCortexInvolved2005,
  title = {The Motor Cortex Is Involved in Reflexive Compensatory Adjustment of Speech Articulation},
  author = {Ito, Takayuki and Kimura, Toshitaka and Gomi, Hiroaki},
  date = {2005-11-07},
  journaltitle = {Neuroreport},
  shortjournal = {Neuroreport},
  volume = {16},
  pages = {1791--1794},
  issn = {0959-4965},
  doi = {10.1097/01.wnr.0000185956.58099.f4},
  abstract = {Although speech articulation relies heavily on the sensorimotor processing, little is known about its brain control mechanisms. Here, we investigate, using transcranial magnetic stimulation, whether the motor cortex contributes to the generation of quick sensorimotor responses involved in speech motor coordination. By applying a jaw-lowering perturbation, we induced a reflexive compensatory upper-lip response, which assists in maintaining the intact labial aperture in the production of bilabial fricative consonants. This reflex response was significantly facilitated by subthreshold transcranial magnetic stimulation over the motor cortex, whereas a simple perioral reflex that is mediated only within the brainstem was not. This suggests that the motor cortex is involved in generating this functional reflexive articulatory compensation.},
  eprint = {16237328},
  eprinttype = {pmid},
  keywords = {Electric Stimulation,Electromyography,Evoked Potentials; Motor,Humans,Jaw,Lip,Masticatory Muscles,Motor Cortex,Psychomotor Performance,Reaction Time,Reflex,Speech Acoustics,Transcranial Magnetic Stimulation},
  langid = {english},
  number = {16}
}

@article{itoSomatosensoryFunctionSpeech2009,
  title = {Somatosensory Function in Speech Perception},
  author = {Ito, Takayuki and Tiede, Mark and Ostry, David J.},
  date = {2009-01-27},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {Proc Natl Acad Sci U S A},
  volume = {106},
  pages = {1245--1248},
  issn = {1091-6490},
  doi = {10.1073/pnas.0810063106},
  abstract = {Somatosensory signals from the facial skin and muscles of the vocal tract provide a rich source of sensory input in speech production. We show here that the somatosensory system is also involved in the perception of speech. We use a robotic device to create patterns of facial skin deformation that would normally accompany speech production. We find that when we stretch the facial skin while people listen to words, it alters the sounds they hear. The systematic perceptual variation we observe in conjunction with speech-like patterns of skin stretch indicates that somatosensory inputs affect the neural processing of speech sounds and shows the involvement of the somatosensory system in the perceptual processing in speech.},
  eprint = {19164569},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2KTRFIRY\\Ito et al. - 2009 - Somatosensory function in speech perception.pdf},
  keywords = {Adult,Humans,Phonetics,Sensation,Skin Physiological Phenomena,Speech Perception,Time Factors},
  langid = {english},
  number = {4},
  pmcid = {PMC2633542}
}

@article{iversenSynchronizationAuditoryVisual2015,
  title = {Synchronization to Auditory and Visual Rhythms in Hearing and Deaf Individuals},
  author = {Iversen, John R. and Patel, Aniruddh D. and Nicodemus, Brenda and Emmorey, Karen},
  date = {2015-01},
  journaltitle = {Cognition},
  volume = {134},
  pages = {232--244},
  issn = {00100277},
  doi = {10.1016/j.cognition.2014.10.018},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027714002194},
  urldate = {2020-02-10},
  abstract = {A striking asymmetry in human sensorimotor processing is that humans synchronize movements to rhythmic sound with far greater precision than to temporally equivalent visual stimuli (e.g., to an auditory vs. a flashing visual metronome). Traditionally, this finding is thought to reflect a fundamental difference in auditory vs. visual processing, i.e., superior temporal processing by the auditory system and/or privileged coupling between the auditory and motor systems. It is unclear whether this asymmetry is an inevitable consequence of brain organization or whether it can be modified (or even eliminated) by stimulus characteristics or by experience. With respect to stimulus characteristics, we found that a moving, colliding visual stimulus (a silent image of a bouncing ball with a distinct collision point on the floor) was able to drive synchronization nearly as accurately as sound in hearing participants. To study the role of experience, we compared synchronization to flashing metronomes in hearing and profoundly deaf individuals. Deaf individuals performed better than hearing individuals when synchronizing with visual flashes, suggesting that cross-modal plasticity enhances the ability to synchronize with temporally discrete visual stimuli. Furthermore, when deaf (but not hearing) individuals synchronized with the bouncing ball, their tapping patterns suggest that visual timing may access higherorder beat perception mechanisms for deaf individuals. These results indicate that the auditory advantage in rhythmic synchronization is more experience- and stimulusdependent than has been previously reported.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\X85INBQ3\\Iversen et al. - 2015 - Synchronization to auditory and visual rhythms in .pdf},
  langid = {english}
}

@article{iversonHandLeadsMouth2003,
  title = {The Hand Leads the Mouth in Ontogenesis Too},
  author = {Iverson, Jana M. and Thelen, Esther},
  date = {2003-04},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {26},
  pages = {225--226},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X03410066},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/hand-leads-the-mouth-in-ontogenesis-too/F7FE257EA194B518ABB0CBD4241FB0F2},
  urldate = {2020-12-03},
  abstract = {The evolutionary scenario described in this target article parallels developmental patterns observed in human infants. Early vocalizations are largely expressive, manual control develops more rapidly than intentional vocal articulation, and vocal and manual activity are linked. In ontogenetic development, language is strongly rooted in bodily action and gesture.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KKK2DIMG\\F7FE257EA194B518ABB0CBD4241FB0F2.html},
  langid = {english},
  number = {2}
}

@article{iversonHandMouthBrain1999,
  title = {Hand, Mouth and Brain: {{The}} Dynamic Emergence of Speech and Gesture},
  author = {Iverson, Jana M and Thelen, Esther},
  date = {1999},
  journaltitle = {Journal of Consciousness Studies},
  volume = {6},
  pages = {19--40},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NIX9C9GZ\\Iverson and Thelen - 2005 - Journal of Consciousness Studies.pdf},
  langid = {english},
  number = {11-12}
}

@article{iversonInfantVocalmotorCoordination2004,
  title = {Infant Vocal-Motor Coordination: Precursor to the Gesture-Speech System?},
  shorttitle = {Infant Vocal-Motor Coordination},
  author = {Iverson, Jana M. and Fagan, Mary K.},
  year = {2004 Jul-Aug},
  journaltitle = {Child Development},
  shortjournal = {Child Dev},
  volume = {75},
  pages = {1053--1066},
  issn = {0009-3920},
  doi = {10.1111/j.1467-8624.2004.00725.x},
  abstract = {This study was designed to provide a general picture of infant vocal-motor coordination and test predictions generated by Iverson and Thelen's (1999) model of the development of the gesture-speech system. Forty-seven 6- to 9-month-old infants were videotaped with a primary caregiver during rattle and toy play. Results indicated an age-related increase in frequency of vocal-motor coordination, greater coordination with arm (specifically right arm) than leg or torso movements, and a temporal pattern similar to that in adult gesture-speech coproductions. Rhythmic vocalizations (consonant-vowel repetitions) were more likely to occur with than without rhythmic movement, and with rhythmic manual than with nonmanual activity, and the rate of vocal-manual coordination was higher in babblers than in prebabblers.},
  eprint = {15260864},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NLH9WKRR\\Iverson and Fagan - 2004 - Infant vocal-motor coordination precursor to the .pdf},
  keywords = {Child Development,Communication,Extremities,Female,Gestures,Humans,Infant,Male,Motor Skills,Movement,Periodicity,Speech,Videotape Recording},
  langid = {english},
  number = {4}
}

@article{iversonMultimodalityInfancyVocalmotor2010,
  title = {Multimodality in Infancy: Vocal-Motor and Speech-Gesture Coordinations in Typical and Atypical Development},
  shorttitle = {Multimodality in Infancy},
  author = {Iverson, Jana M.},
  date = {2010},
  journaltitle = {Enfance},
  volume = {N° 3},
  pages = {257--274},
  publisher = {{NecPlus}},
  issn = {0013-7545},
  url = {https://www.cairn.info/revue-enfance2-2010-3-page-257.htm},
  urldate = {2020-12-01},
  abstract = {{$<$}titre{$>$}R\&\#233;sum\&\#233;{$<$}/titre{$>$}D\&\#232;s le d\&\#233;but de la vie, le comportement expressif est multimodal\&\#160;; il se compose de coordinations comportementales pr\&\#233;coces qui vont s\&\#8217;affiner et se renforcer avec le temps de fa\&\#231;on \&\#224; \&\#234;tre utilis\&\#233;es pour la communication de sens. Parmi ces coordinations communicatives, celles qui impliquent les gestes et le langage sont certainement celles qui ont re\&\#231;u la plus grande attention empirique, mais on sait peu de chose sur les origines d\&\#233;veloppementales des liens entre gestes et langage. Une des possibilit\&\#233;s r\&\#233;side dans le fait que les origines des coordinations gestes-langage reposent sur les relations entre la main et la bouche observ\&\#233;es dans les activit\&\#233;s sensorimotrices de tous les jours des tr\&\#232;s jeunes enfants qui n\&\#8217;utilisent pas encore la main ou la bouche pour communiquer du sens. Dans cet article, je passe en revue des preuves qui sugg\&\#232;rent que l\&\#8217;\&\#233;tude des liens entre gestes et langage et les couplages d\&\#233;veloppementalement pr\&\#233;coces entre des syst\&\#232;mes vocaux et moteurs dans la petite enfance peuvent fournir un aper\&\#231;u pr\&\#233;cieux sur un certain nombre de d\&\#233;veloppements ult\&\#233;rieurs qui refl\&\#232;tent l\&\#8217;interd\&\#233;pendance cognitive entre gestes et langage. Ceci comprend des aspects li\&\#233;s au d\&\#233;veloppement du langage et \&\#224; des retards dans cette acquisition, les origines infantiles du syst\&\#232;me gestes/langage que l\&\#8217;on retrouve chez l\&\#8217;adulte et les signes pr\&\#233;coces des troubles autistiques. L\&\#8217;implication de ces r\&\#233;sultats quant \&\#224; l\&\#8217;\&\#233;tude du d\&\#233;veloppement de la communication multimodale est envisag\&\#233;e.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WCU9CL34\\Iverson - 2010 - Multimodality in infancy vocal-motor and speech-g.pdf},
  langid = {english},
  number = {3}
}

@article{iversonRelationshipReduplicatedBabble2007,
  title = {The Relationship between Reduplicated Babble Onset and Laterality Biases in Infant Rhythmic Arm Movements},
  author = {Iverson, Jana M. and Hall, Amanda J. and Nickel, Lindsay and Wozniak, Robert H.},
  date = {2007-06},
  journaltitle = {Brain and Language},
  shortjournal = {Brain Lang},
  volume = {101},
  pages = {198--207},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2006.11.004},
  abstract = {This study examined changes in rhythmic arm shaking and laterality biases in infants observed longitudinally at three points: just prior to, at, and just following reduplicated babble onset. Infants (ranging in age from 4 to 9 months at babble onset) were videotaped at home as they played with two visually identical audible and silent rattles presented at midline for 1.5 min each. Rate of rattle shaking increased sharply from the pre-babble to the babble onset session; but there was no indication that this increase was specific to the right arm. This finding suggests that the link between babble onset and increased rhythmic arm activity may not be the product of language-specific mechanisms, but is rather part of a broader developmental process that is also perceptual and motor.},
  eprint = {17196644},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NNVHXFC8\\Iverson et al. - 2007 - The relationship between reduplicated babble onset.pdf},
  keywords = {Analysis of Variance,Child Development,Child Language,Female,Functional Laterality,Gestures,Humans,Infant,Language Development,Longitudinal Studies,Male,Motor Skills,Psycholinguistics},
  langid = {english},
  number = {3},
  pmcid = {PMC2034511}
}

@article{iversonResilienceGestureTalk2001,
  title = {The Resilience of Gesture in Talk: Gesture in Blind Speakers and Listeners},
  shorttitle = {The Resilience of Gesture in Talk},
  author = {Iverson, Jana M. and Goldin‐Meadow, Susan},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {416--422},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00183},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00183},
  urldate = {2019-05-05},
  abstract = {Spontaneous gesture frequently accompanies speech. The question is why. In these studies, we tested two non-mutually exclusive possibilities. First, speakers may gesture simply because they see others gesture and learn from this model to move their hands as they talk. We tested this hypothesis by examining spontaneous communication in congenitally blind children and adolescents. Second, speakers may gesture because they recognize that gestures can be useful to the listener. We tested this hypothesis by examining whether speakers gesture even when communicating with a blind listener who is unable to profit from the information that the hands convey. We found that congenitally blind speakers, who had never seen gestures, nevertheless gestured as they spoke, conveying the same information and producing the same range of gesture forms as sighted speakers. Moreover, blind speakers gestured even when interacting with another blind individual who could not have benefited from the information contained in those gestures. These findings underscore the robustness of gesture in talk and suggest that the gestures that co-occur with speech may serve a function for the speaker as well as for the listener.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BBKPR8QQ\\1467-7687.html},
  langid = {english},
  number = {4}
}

@article{iversonResilienceGestureTalk2001a,
  title = {The Resilience of Gesture in Talk: Gesture in Blind Speakers and Listeners},
  shorttitle = {The Resilience of Gesture in Talk},
  author = {Iverson, Jana M. and Goldin‐Meadow, Susan},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {416--422},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00183},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00183},
  urldate = {2019-10-14},
  abstract = {Spontaneous gesture frequently accompanies speech. The question is why. In these studies, we tested two non-mutually exclusive possibilities. First, speakers may gesture simply because they see others gesture and learn from this model to move their hands as they talk. We tested this hypothesis by examining spontaneous communication in congenitally blind children and adolescents. Second, speakers may gesture because they recognize that gestures can be useful to the listener. We tested this hypothesis by examining whether speakers gesture even when communicating with a blind listener who is unable to profit from the information that the hands convey. We found that congenitally blind speakers, who had never seen gestures, nevertheless gestured as they spoke, conveying the same information and producing the same range of gesture forms as sighted speakers. Moreover, blind speakers gestured even when interacting with another blind individual who could not have benefited from the information contained in those gestures. These findings underscore the robustness of gesture in talk and suggest that the gestures that co-occur with speech may serve a function for the speaker as well as for the listener.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\G6DTJTEE\\1467-7687.html},
  langid = {english},
  number = {4}
}

@incollection{iversonTransitionsIntentionalSymbolic2016,
  title = {Transitions to Intentional and Symbolic Communication in Typical Development and in Autism Spectrum Disorder},
  booktitle = {Prelinguistic and Minimally Verbal Communicators on the Autism Spectrum},
  author = {Iverson, Jana M. and Wozniak, Robert H.},
  date = {2016},
  pages = {51--72},
  publisher = {{Springer Science + Business Media}},
  location = {{New York, NY, US}},
  doi = {10.1007/978-981-10-0713-2_4},
  abstract = {We begin by reviewing research focused on the way in which the emergence of new forms of intentional and symbolic communication alters the typically developing child's communicative environment. Our central thesis is that these alterations not only change the nature of the input that the child receives but also influence the availability of opportunities for learning that support future development. We then review what is known about delays and atypicalities in the development of intentional and symbolic communication in individuals with autism spectrum disorder. Based on these data, we suggest that these communicative delays and atypicalities have far-reaching, cascading effects that extend beyond the individuals themselves to impact the behavior of social partners, the communicative environment more broadly, and the course of subsequent development. We then present a conceptual framework that identifies ways in which delays in the emergence of basic, early emerging communicative behaviors—eye contact, gesture, and vocalization—may lead to delays in tae emergence of the individual's ability to initiate instances of joint attention and impact tae caregiver's sense of the child's developmental level. These changes in turn may lead to a reduction in shared topics for communication and, therefore, to a reduction in instances in which linguistic input adapted to moments of shared attention is most effective in facilitating tae early development of language. Finally, we conclude with some recommendations for research and clinical practice suggested by this framework. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XTYNWLAB\\2016-30158-004.html},
  isbn = {978-981-10-0711-8 978-981-10-0713-2},
  keywords = {Autism Spectrum Disorders,Communication Skills,Infant Development}
}

@article{iversonVariationVocalmotorDevelopment2007,
  title = {Variation in Vocal-Motor Development in Infant Siblings of Children with Autism},
  author = {Iverson, Jana M. and Wozniak, Robert H.},
  date = {2007-01},
  journaltitle = {Journal of Autism and Developmental Disorders},
  shortjournal = {J Autism Dev Disord},
  volume = {37},
  pages = {158--170},
  issn = {0162-3257},
  doi = {10.1007/s10803-006-0339-z},
  abstract = {In this study we examined early motor, vocal, and communicative development in a group of younger siblings of children diagnosed with autism (Infant Siblings). Infant Siblings and no-risk comparison later-born infants were videotaped at home with a primary caregiver each month from 5 to 14 months, with follow-up at 18 months. As a group, Infant Siblings were delayed in the onset of early developmental milestones and spent significantly less time in a greater number of postures, suggestive of relative postural instability. In addition, they demonstrated attenuated patterns of change in rhythmic arm activity around the time of reduplicated babble onset; and they were highly likely to exhibit delayed language development at 18 months.},
  eprint = {17191097},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YQNHQ2ZX\\Iverson and Wozniak - 2007 - Variation in vocal-motor development in infant sib.pdf},
  keywords = {Autistic Disorder,Communication Disorders,Female,Follow-Up Studies,Humans,Infant,Language Development Disorders,Male,Mass Screening,Motor Skills Disorders,Periodicity,Posture,Prevalence,Speech Disorders,Stereotypic Movement Disorder,Verbal Behavior},
  langid = {english},
  number = {1},
  pmcid = {PMC3521582}
}

@article{iversonWhyPeopleGesture1998,
  title = {Why People Gesture When They Speak},
  author = {Iverson, Jana M. and Goldin-Meadow, Susan},
  date = {1998-11},
  journaltitle = {Nature},
  volume = {396},
  pages = {228--228},
  issn = {1476-4687},
  doi = {10.1038/24300},
  url = {https://www.nature.com/articles/24300},
  urldate = {2019-11-30},
  abstract = {People use gestures when they talk, but is this behaviour learned from watching others move their hands when talking? Individuals who are blind from birth never see such gestures and so have no model for gesturing. But here we show that congenitally blind speakers gesture despite their lack of a visual model, even when they speak to a blind listener. Gestures therefore require neither a model nor an observant partner.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\69RUKT7D\\24300.html},
  langid = {english},
  number = {6708}
}

@article{jarvisEvolutionVocalLearning2019,
  title = {Evolution of Vocal Learning and Spoken Language},
  author = {Jarvis, Erich D.},
  date = {2019-10-04},
  journaltitle = {Science},
  volume = {366},
  pages = {50--54},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax0287},
  url = {https://science.sciencemag.org/content/366/6461/50},
  urldate = {2019-10-18},
  abstract = {Although language, and therefore spoken language or speech, is often considered unique to humans, the past several decades have seen a surge in nonhuman animal studies that inform us about human spoken language. Here, I present a modern, evolution-based synthesis of these studies, from behavioral to molecular levels of analyses. Among the key concepts drawn are that components of spoken language are continuous between species, and that the vocal learning component is the most specialized and rarest and evolved by brain pathway duplication from an ancient motor learning pathway. These concepts have important implications for understanding brain mechanisms and disorders of spoken language.},
  eprint = {31604300},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GGTIKRCC\\50.html},
  langid = {english},
  number = {6461}
}

@article{jasminCohesionJointSpeech2016,
  title = {Cohesion and {{Joint Speech}}: {{Right Hemisphere Contributions}} to {{Synchronized Vocal Production}}},
  shorttitle = {Cohesion and {{Joint Speech}}},
  author = {Jasmin, K. and McGettigan, C. and Agnew, Z. K. and Lavan, N. and Josephs, O. and Cummins, F. and Scott, S. K.},
  date = {2016-04-27},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {36},
  pages = {4669--4680},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4075-15.2016},
  url = {https://www.jneurosci.org/content/36/17/4669},
  urldate = {2020-10-21},
  abstract = {Synchronized behavior (chanting, singing, praying, dancing) is found in all human cultures and is central to religious, military, and political activities, which require people to act collaboratively and cohesively; however, we know little about the neural underpinnings of many kinds of synchronous behavior (e.g., vocal behavior) or its role in establishing and maintaining group cohesion. In the present study, we measured neural activity using fMRI while participants spoke simultaneously with another person. We manipulated whether the couple spoke the same sentence (allowing synchrony) or different sentences (preventing synchrony), and also whether the voice the participant heard was “live” (allowing rich reciprocal interaction) or prerecorded (with no such mutual influence). Synchronous speech was associated with increased activity in posterior and anterior auditory fields. When, and only when, participants spoke with a partner who was both synchronous and “live,” we observed a lack of the suppression of auditory cortex, which is commonly seen as a neural correlate of speech production. Instead, auditory cortex responded as though it were processing another talker's speech. Our results suggest that detecting synchrony leads to a change in the perceptual consequences of one's own actions: they are processed as though they were other-, rather than self-produced. This may contribute to our understanding of synchronized behavior as a group-bonding tool. SIGNIFICANCE STATEMENT Synchronized human behavior, such as chanting, dancing, and singing, are cultural universals with functional significance: these activities increase group cohesion and cause participants to like each other and behave more prosocially toward each other. Here we use fMRI brain imaging to investigate the neural basis of one common form of cohesive synchronized behavior: joint speaking (e.g., the synchronous speech seen in chants, prayers, pledges). Results showed that joint speech recruits additional right hemisphere regions outside the classic speech production network. Additionally, we found that a neural marker of self-produced speech, suppression of sensory cortices, did not occur during joint synchronized speech, suggesting that joint synchronized behavior may alter self-other distinctions in sensory processing.},
  eprint = {27122026},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MQTTISK6\\Jasmin et al. - 2016 - Cohesion and Joint Speech Right Hemisphere Contri.pdf;C\:\\Users\\u668173\\Zotero\\storage\\9P4JF6CH\\4669.html},
  keywords = {coordinated action,fMRI,joint speech,right hemisphere,social cohesion,speech control},
  langid = {english},
  number = {17}
}

@article{jasminTailoredPerceptionIndividuals2020,
  title = {Tailored {{Perception}}: {{Individuals}}’ {{Speech}} and {{Music Perception Strategies Fit Their Perceptual Abilities}}},
  shorttitle = {Tailored {{Perception}}},
  author = {Jasmin, K. and Dick, F. and Holt, L. L. and Tierney, A.},
  date = {2020-05},
  journaltitle = {Journal of Experimental Psychology. General},
  shortjournal = {J Exp Psychol Gen},
  volume = {149},
  pages = {914--934},
  issn = {0096-3445},
  doi = {10.1037/xge0000688},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7133494/},
  urldate = {2020-10-29},
  abstract = {Perception involves integration of multiple dimensions that often serve overlapping, redundant functions, for example, pitch, duration, and amplitude in speech. Individuals tend to prioritize these dimensions differently (stable, individualized perceptual strategies), but the reason for this has remained unclear. Here we show that perceptual strategies relate to perceptual abilities. In a speech cue weighting experiment (trial N = 990), we first demonstrate that individuals with a severe deficit for pitch perception (congenital amusics; N = 11) categorize linguistic stimuli similarly to controls (N = 11) when the main distinguishing cue is duration, which they perceive normally. In contrast, in a prosodic task where pitch cues are the main distinguishing factor, we show that amusics place less importance on pitch and instead rely more on duration cues—even when pitch differences in the stimuli are large enough for amusics to discern. In a second experiment testing musical and prosodic phrase interpretation (N = 16 amusics; 15 controls), we found that relying on duration allowed amusics to overcome their pitch deficits to perceive speech and music successfully. We conclude that auditory signals, because of their redundant nature, are robust to impairments for specific dimensions, and that optimal speech and music perception strategies depend not only on invariant acoustic dimensions (the physical signal), but on perceptual dimensions whose precision varies across individuals. Computational models of speech perception (indeed, all types of perception involving redundant cues e.g., vision and touch) should therefore aim to account for the precision of perceptual dimensions and characterize individuals as well as groups.},
  eprint = {31589067},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LFU684RJ\\Jasmin et al. - 2020 - Tailored Perception Individuals’ Speech and Music.pdf},
  number = {5},
  pmcid = {PMC7133494}
}

@article{johansenStaticLungVolumes1993,
  title = {Static Lung Volumes in Healthy Subjects Assessed by Helium Dilution during Occlusion of One Mainstem Bronchus.},
  author = {Johansen, B and Bjortuft, O and Boe, J},
  date = {1993-04-01},
  journaltitle = {Thorax},
  volume = {48},
  pages = {381--384},
  issn = {0040-6376},
  doi = {10.1136/thx.48.4.381},
  url = {http://thorax.bmj.com/cgi/doi/10.1136/thx.48.4.381},
  urldate = {2020-06-10},
  abstract = {Background-Single lung function is usually assessed by radioisotopes or, more rarely, by bronchospirometry in which a double lumen catheter is used to separate ventilation of the two lungs. The latter is more precise but less comfortable. An alternative bronchoscopic method is described for determniiiing the volume of a single lung. Methods-One mainstem bronchus was temporarily occluded with an inflatable balloon during fibreoptic bronchoscopy in 12 healthy volunteers aged 18-29 years. The functional residual capacities (FRC) of the right, left, and both lungs were measured in duplicate by closed circuit helium dilution. Supplementary vital capacity (VC) manoeuvres permitted calculation of single lung capacities (TLC) and residual volumes (RV). Results-The standard deviation of a single determination of capacities of the right, left, and both lungs were: TLC, 80, 96, and 308 ml; VC, 56, 139, 171 ml; FRC, 131, 74, and 287 ml; RV, 112, 185, and 303 ml, respectively. The sum of the right and left unilateral TLC was not different from bilateral TLC (6-12 v 5 95 1) and the sum of the unilateral FRC was not different from the bilateral FRC (2-60 v 2*78 1). The sum of the unilateral VC was lower than bilateral VC (4.52 v 4-80 1), that of the unilateral RV was higher than bilateral RV (1.60 v 1-16 1). For all subdivisions of lung volume, the right lung was larger than the left. The most common complaint was substernal discomfort during complete exhalation. Oxygen saturation rarely fell below 90\%. Conclusions-Temporary occlusion of a mainstem bronchus in normal subjects is safe, relatively simple, and allows fairly precise and accurate measurements of unilateral static lung volumes. Occlusion at TLC, however, probably prevents proper emptying of the non-occluded lung.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GNTFU6TS\\Johansen et al. - 1993 - Static lung volumes in healthy subjects assessed b.pdf},
  langid = {english},
  number = {4}
}

@article{johnsonDemandsProfessionalOpera2009,
  title = {The Demands of Professional Opera Singing on Cranio-Cervical Posture},
  author = {Johnson, Gillian and Skinner, Margot},
  date = {2009-04},
  journaltitle = {European Spine Journal},
  shortjournal = {Eur Spine J},
  volume = {18},
  pages = {562--569},
  issn = {0940-6719},
  doi = {10.1007/s00586-009-0884-1},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2899464/},
  urldate = {2020-10-06},
  abstract = {Difficulty with singing is a rare but important complication following cervical spine surgery but there is little objective information regarding the cervical and head postural changes taking place during singing. The aim of this study was to identify postural changes in the cranio-cervical region associated with the demands of voice production in professional opera singing. The two Roentgen-cephalograms, one of which are taken whilst performing a specified singing task were taken from 18 professional opera students, 12 females (mean age 20.86~±~3.07~years) and six males (18.66~±~1.36~years). A paired t test compared mean cranio-cervical postural and pharyngeal/hyoid variables between the two registrations (P~=~0.05). The association between the cranio-cervical postural variables and the pharyngeal/hyoid region in each registration position was examined using Spearman’s rank correlation coefficient. In singing, the position of the atlas with respect to the true vertical (P~{$<~$}0.001), the axis (P~{$<~$}0.001) and the C4 vertebra both with respect to the horizontal (P~{$<~$}0.001), and the axis with respect to the cranium (P~{$<~$}0.001), were all significantly different to those at rest. Of the cranio-cervical postural variables in the singing registration, the angles measuring positional change of the atlas and C4 relative to the true horizontal were shown be significantly related to an increased pharyngeal airway space at the C3 level (P~{$<~$}0.01). An appreciation of the requirement for the cervical spine to undergo postural change during professional opera singing has relevance to the potential impact on voice quality in professional opera singers should they undergo cervical spine surgery.},
  eprint = {19165506},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BAP46K4A\\Johnson and Skinner - 2009 - The demands of professional opera singing on crani.pdf},
  number = {4},
  pmcid = {PMC2899464}
}

@online{JointActionInteractive,
  title = {Joint {{Action}}, {{Interactive Alignment}}, and {{Dialog}} - {{Garrod}} - 2009 - {{Topics}} in {{Cognitive Science}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1756-8765.2009.01020.x},
  urldate = {2019-12-18},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HR5LMI5K\\j.1756-8765.2009.01020.html}
}

@article{jokinenGazeTurntakingBehavior2013,
  title = {Gaze and Turn-Taking Behavior in Casual Conversational Interactions},
  author = {Jokinen, Kristiina and Furukawa, Hirohisa and Nishida, Masafumi and Yamamoto, Seiichi},
  date = {2013-08-05},
  journaltitle = {ACM Transactions on Interactive Intelligent Systems},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  volume = {3},
  pages = {12:1--12:30},
  issn = {2160-6455},
  doi = {10.1145/2499474.2499481},
  url = {https://doi.org/10.1145/2499474.2499481},
  urldate = {2021-03-11},
  abstract = {Eye gaze is an important means for controlling interaction and coordinating the participants' turns smoothly. We have studied how eye gaze correlates with spoken interaction and especially focused on the combined effect of the speech signal and gazing to predict turn taking possibilities. It is well known that mutual gaze is important in the coordination of turn taking in two-party dialogs, and in this article, we investigate whether this fact also holds for three-party conversations. In group interactions, it may be that different features are used for managing turn taking than in two-party dialogs. We collected casual conversational data and used an eye tracker to systematically observe a participant's gaze in the interactions. By studying the combined effect of speech and gaze on turn taking, we aimed to answer our main questions: How well can eye gaze help in predicting turn taking? What is the role of eye gaze when the speaker holds the turn? Is the role of eye gaze as important in three-party dialogs as in two-party dialogue? We used Support Vector Machines (SVMs) to classify turn taking events with respect to speech and gaze features, so as to estimate how well the features signal a change of the speaker or a continuation of the same speaker. The results confirm the earlier hypothesis that eye gaze significantly helps in predicting the partner's turn taking activity, and we also get supporting evidence for our hypothesis that the speaker is a prominent coordinator of the interaction space. Such a turn taking model could be used in interactive applications to improve the system's conversational performance.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VCWHTZD9\\Jokinen et al. - 2013 - Gaze and turn-taking behavior in casual conversati.pdf},
  number = {2}
}

@article{jollyFlatlandFallacyMoving2019,
  title = {The {{Flatland Fallacy}}: {{Moving Beyond Low}}–{{Dimensional Thinking}}},
  shorttitle = {The {{Flatland Fallacy}}},
  author = {Jolly, Eshin and Chang, Luke J.},
  date = {2019},
  journaltitle = {Topics in Cognitive Science},
  volume = {11},
  pages = {433--454},
  issn = {1756-8765},
  doi = {10.1111/tops.12404},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12404},
  urldate = {2020-05-07},
  abstract = {Psychology is a complicated science. It has no general axioms or mathematical proofs, is rarely directly observable, and is the only discipline in which the subject matter (i.e., human psychological phenomena) is also the tool of investigation. Like the Flatlanders in Edwin Abbot's famous short story (), we may be led to believe that the parsimony offered by our low-dimensional theories reflects the reality of a much higher-dimensional problem. Here we contend that this “Flatland fallacy” leads us to seek out simplified explanations of complex phenomena, limiting our capacity as scientists to build and communicate useful models of human psychology. We suggest that this fallacy can be overcome through (a) the use of quantitative models, which force researchers to formalize their theories to overcome this fallacy, and (b) improved quantitative training, which can build new norms for conducting psychological research.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12404},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\W6AYR598\\Jolly and Chang - 2019 - The Flatland Fallacy Moving Beyond Low–Dimensiona.pdf;C\:\\Users\\u668173\\Zotero\\storage\\T882X5TX\\tops.html},
  keywords = {Computational,Decision-making,Dual-processing,Psychological education,Social},
  langid = {english},
  number = {2}
}

@article{jonesWEIRDViewHuman2010,
  title = {A {{WEIRD View}} of {{Human Nature Skews Psychologists}}' {{Studies}}},
  author = {Jones, Dan},
  date = {2010-06-25},
  journaltitle = {Science},
  volume = {328},
  pages = {1627--1627},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.328.5986.1627},
  url = {https://science.sciencemag.org/content/328/5986/1627},
  urldate = {2020-07-22},
  abstract = {Although undergraduates from wealthy nations are numerous and willing research subjects, psychologists are beginning to realize that they have a drawback: They are WEIRDos. That is, they are people from Western, educated, industrialized, rich, and democratic cultures. In a provocative review paper published last week, a pair of researchers argues that WEIRDos aren't representative of humans as a whole and that psychologists routinely use them to make broad, and quite likely false, claims about what drives human behavior. Relying on undergraduates from developed nations as research subjects creates a false picture of human behavior, some psychologists argue. Relying on undergraduates from developed nations as research subjects creates a false picture of human behavior, some psychologists argue.},
  eprint = {20576866},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\APTDYP2R\\1627.html},
  langid = {english},
  number = {5986}
}

@article{jonge-hoekstraEasierSaidDone2020,
  title = {Easier Said than Done? {{Task}} Difficulty’s Influence on Temporal Alignment, Semantic Similarity, and Complexity Matching between Gestures and Speech},
  shorttitle = {Easier Said than Done?},
  author = {de Jonge-Hoekstra, Lisette and Cox, R. F. A. and van der Steen, S. and Dixon, J. A.},
  date = {2020-03-04T21:29:54},
  journaltitle = {PsyArXiv},
  pages = {1--46},
  doi = {10.31234/osf.io/zsjhf},
  url = {https://psyarxiv.com/zsjhf/},
  urldate = {2020-10-15},
  abstract = {Gestures and speech are clearly synchronized in many ways. However, previous studies have shown the semantic similarity between gestures and speech breaks down as people approach transitions in understanding. Explanations for these gesture-speech mismatches which focus on gestures and speech expressing different cognitive strategies, have been criticized for disregarding gestures’ and speech’s integration and synchronization. In the current study, we applied three different perspectives to investigate gesture-speech synchronization in an easy and a difficult task: temporal alignment, semantic similarity, and complexity matching. Participants engaged in a simple cognitive task, and were assigned to either an easy or a difficult condition. We automatically measured pointing gestures, and we coded participant’s speech, to determine the temporal alignment and semantic similarity between gestures and speech. Multifractal Detrended Fluctuation Analysis (MFDFA) was used to determine the extent of complexity matching between gestures and speech. We found that task difficulty indeed influenced gesture-speech synchronization in all three domains. We thereby extended the phenomenon of gesture-speech mismatches to difficult tasks in general. Furthermore, we investigated how temporal alignment, semantic similarity, and complexity matching were related in each condition, and how they predicted participants’ task performance. Our study illustrates how combining multiple perspectives, originating from different research areas (i.e., coordination dynamics, complexity science, cognitive psychology) provides novel understanding about cognitive concepts in general, and about gesture-speech synchronization and task difficulty in specific.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ATR6HWTI\\Jonge-Hoekstra et al. - 2020 - Easier said than done Task difficulty’s influence.pdf},
  keywords = {complexity matching,gesture-speech mismatches,gestures,Multi Fractal Detrended Fluctuation Analysis,Social and Behavioral Sciences,speech,synchronization},
  options = {useprefix=true}
}

@book{jrAnatomyVoiceIllustrated2018,
  title = {Anatomy of the {{Voice}}: {{An Illustrated Guide}} for {{Singers}}, {{Vocal Coaches}}, and {{Speech Therapists}}},
  shorttitle = {Anatomy of the {{Voice}}},
  author = {Jr, Theodore Dimon},
  date = {2018-04-24},
  publisher = {{North Atlantic Books}},
  abstract = {The first comprehensive, fully-illustrated approach to the voice that explains the anatomy and mechanics in detailed yet down-to-earth terms, for voice users and professionals of all kindsThis book is the first to explain, in clear and concise language, the anatomy and mechanics of the mysterious and complex bodily system we call the voice. Beautifully illustrated with more than 100 detailed images, Anatomy of the Voice guides voice teachers and students, vocal coaches, professional singers and actors, and anyone interested in the voice through the complex landscape of breathing, larynx, throat, face, and jaw. Theodore Dimon, an internationally recognized authority on the subject, as well as an expert in the Alexander Technique, makes unfamiliar terrain accessible and digestible by describing each vocal system in short, manageable sections and explaining complex terminology. The topics he covers include ribs, diaphragm, and muscles of breathing; the intrinsic musculature of the larynx, its structure and action; the suspensory muscles of the throat; the face and jaw; the tongue and palate; and the evolution and function of the larynx.},
  eprint = {M_gsDwAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-1-62317-198-8},
  keywords = {Health & Fitness / Hearing & Speech,Medical / Anatomy,Music / Instruction & Study / Voice},
  langid = {english},
  pagetotal = {233}
}

@article{julienEnigmaMayerWaves2006,
  title = {The Enigma of {{Mayer}} Waves: {{Facts}} and Models},
  shorttitle = {The Enigma of {{Mayer}} Waves},
  author = {Julien, Claude},
  date = {2006-04-01},
  journaltitle = {Cardiovascular Research},
  shortjournal = {Cardiovasc Res},
  volume = {70},
  pages = {12--21},
  issn = {0008-6363},
  doi = {10.1016/j.cardiores.2005.11.008},
  abstract = {Mayer waves are oscillations of arterial pressure occurring spontaneously in conscious subjects at a frequency lower than respiration (approximately 0.1 Hz in humans). Mayer waves are tightly coupled with synchronous oscillations of efferent sympathetic nervous activity and are almost invariably enhanced during states of sympathetic activation. For this reason, the amplitude of these oscillations has been proposed as a surrogate measure of sympathetic activity, although in the absence of a clear knowledge of their underlying physiology. Some studies have suggested that Mayer waves result from the activity of an endogenous oscillator located either in the brainstem or in the spinal cord. Other studies, mainly based on the effects of sinoaortic baroreceptor denervation, have challenged this view. Several models of dynamic arterial pressure control have been developed to predict Mayer waves. In these models, it was anticipated that the numerous dynamic components and fixed time delays present in the baroreflex loop would result in the production of a resonant, self-sustained oscillation of arterial pressure. Recent analysis of the various transfer functions of the rat baroreceptor reflex suggests that Mayer waves are transient oscillatory responses to hemodynamic perturbations rather than true feedback oscillations. Within this frame, the amplitude of Mayer waves would be determined both by the strength of the triggering perturbations and the sensitivity of the sympathetic component of the baroreceptor reflex.},
  eprint = {16360130},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XS7DGLVN\\Julien - 2006 - The enigma of Mayer waves Facts and models.pdf},
  keywords = {Animals,Arteries,Baroreflex,Blood Pressure,Computer Simulation,Humans,Models; Cardiovascular,Oscillometry,Sympathetic Nervous System,Vascular Resistance},
  langid = {english},
  number = {1}
}

@article{jurgensNeuronalControlMammalian1998,
  title = {Neuronal Control of Mammalian Vocalization, with Special Reference to the Squirrel Monkey},
  author = {Jürgens, U.},
  date = {1998-08},
  journaltitle = {Die Naturwissenschaften},
  shortjournal = {Naturwissenschaften},
  volume = {85},
  pages = {376--388},
  issn = {0028-1042},
  doi = {10.1007/s001140050519},
  abstract = {Squirrel monkey vocalization can be considered as a suitable model for the study in humans of the neurobiological basis of nonverbal emotional vocal utterances, such as laughing, crying, and groaning. Evaluation of electrical and chemical brain stimulation data, lesioning studies, single-neurone recordings, and neuroanatomical tracing work leads to the following conclusions: The periaqueductal gray and laterally bordering tegmentum of the midbrain represent a crucial area for the production of vocalization. This area collects the various vocalization-triggering stimuli, such as auditory, visual, and somatosensory input from diverse sensory-processing structures, motivation-controlling input from some limbic structures, and volitional impulses from the anterior cingulate cortex. Destruction of this area causes mutism. It is still under dispute whether the periaqueductal region harbors the vocal pattern generator or merely couples vocalization-triggering information to motor-coordinating structures further downward in the brainstem. The periaqueductal region is connected with the phonatory motoneuron pools indirectly via one or several interneurons. The nucleus retroambiguus represents a crucial relay station for the laryngeal and expiratory component of vocalization. The articulatory component reaches the orofacial motoneuron pools via the parvocellular reticular formation. Essential proprioceptive feedback from the larynx and lungs enter the vocal-controlling network via the solitary tract nucleus.},
  eprint = {9762689},
  eprinttype = {pmid},
  keywords = {Animals,Brain,Brain Mapping,Emotions,Humans,Mammals,Models; Neurological,Motor Neurons,Neurons,Saimiri,Speech,Vocalization; Animal},
  langid = {english},
  number = {8}
}

@article{karuzaLocalPatternsGlobal2016,
  title = {Local {{Patterns}} to {{Global Architectures}}: {{Influences}} of {{Network Topology}} on {{Human Learning}}},
  shorttitle = {Local {{Patterns}} to {{Global Architectures}}},
  author = {Karuza, Elisabeth A. and Thompson-Schill, Sharon L. and Bassett, Danielle S.},
  date = {2016-08},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {20},
  pages = {629--640},
  issn = {13646613},
  doi = {10.1016/j.tics.2016.06.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661316300717},
  urldate = {2021-01-23},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LD7C6BD4\\Karuza et al. - 2016 - Local Patterns to Global Architectures Influences.pdf},
  langid = {english},
  number = {8}
}

@article{karuzaLocalPatternsGlobal2016a,
  title = {Local {{Patterns}} to {{Global Architectures}}: {{Influences}} of {{Network Topology}} on {{Human Learning}}},
  shorttitle = {Local {{Patterns}} to {{Global Architectures}}},
  author = {Karuza, Elisabeth A. and Thompson-Schill, Sharon L. and Bassett, Danielle S.},
  date = {2016-08-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {20},
  pages = {629--640},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2016.06.003},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661316300717},
  urldate = {2021-01-23},
  abstract = {A core question in cognitive science concerns how humans acquire and represent knowledge about their environments. To this end, quantitative theories of learning processes have been formalized in an attempt to explain and predict changes in brain and behavior. We connect here statistical learning approaches in cognitive science, which are rooted in the sensitivity of learners to local distributional regularities, and network science approaches to characterizing global patterns and their emergent properties. We focus on innovative work that describes how learning is influenced by the topological properties underlying sensory input. The confluence of these theoretical approaches and this recent empirical evidence motivate the importance of scaling-up quantitative approaches to learning at both the behavioral and neural levels.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\T9DFLSEQ\\Karuza et al. - 2016 - Local Patterns to Global Architectures Influences.pdf;C\:\\Users\\u668173\\Zotero\\storage\\WQZBXMAJ\\S1364661316300717.html},
  keywords = {complex systems,network science,statistical learning},
  langid = {english},
  number = {8}
}

@article{kashiwadateSyntacticStructureInfluences2020,
  title = {Syntactic {{Structure Influences Speech}}-{{Gesture Synchronization}}},
  author = {Kashiwadate, Kei and Yasuda, Tetsuya and Fujita, Koji and Kita, Sotaro and Kobayashi, Harumi},
  date = {2020-03-16},
  journaltitle = {Letters on Evolutionary Behavioral Science},
  volume = {11},
  pages = {10--14},
  issn = {1884-927X},
  doi = {10.5178/lebs.2020.73},
  url = {https://lebs.hbesj.org/index.php/lebs/article/view/lebs.2020.73},
  urldate = {2020-03-27},
  abstract = {It is known that a phrase may have multiple meanings. Phrases such as “green tea cup” may be interpreted with two different meanings—a “green-colored tea cup” or a “cup of green tea.” Then how people know the exact meanings of apparently syntactically ambiguous linguistic expressions? We propose that gesture that accompanies speech may help disambiguate syntactically ambiguous structures. The present study investigated whether the difference in phrase structures influences the production of gestures. Participants produced gestures as they produced a Japanese four-word phrases. We examined all possible synchronization patterns of speech and gestures. We found, for the first time, gestures tended to synchronize with the chunks of words that form a constituent in syntactic structures. Our study suggests that gestures may play an important role in disambiguating syntactically ambiguous phrases. This could be a reason why humans have continuously used gestures even after they acquired a powerful tool of language and why today, they still produce language-redundant gestures.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FY7H47HD\\Kashiwadate et al. - 2020 - Syntactic Structure Influences Speech-Gesture Sync.pdf;C\:\\Users\\u668173\\Zotero\\storage\\YJTVUZC4\\lebs.2020.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{kashiwadateSyntacticStructureInfluences2020a,
  title = {Syntactic {{Structure Influences Speech}}-{{Gesture Synchronization}}},
  author = {Kashiwadate, Kei and Yasuda, Tetsuya and Fujita, Koji and Kita, Sotaro and Kobayashi, Harumi},
  date = {2020-03-16},
  journaltitle = {Letters on Evolutionary Behavioral Science},
  volume = {11},
  pages = {10--14},
  issn = {1884-927X},
  doi = {10.5178/lebs.2020.73},
  url = {https://lebs.hbesj.org/index.php/lebs/article/view/lebs.2020.73},
  urldate = {2020-03-26},
  abstract = {It is known that a phrase may have multiple meanings. Phrases such as “green tea cup” may be interpreted with two different meanings—a “green-colored tea cup” or a “cup of green tea.” Then how people know the exact meanings of apparently syntactically ambiguous linguistic expressions? We propose that gesture that accompanies speech may help disambiguate syntactically ambiguous structures. The present study investigated whether the difference in phrase structures influences the production of gestures. Participants produced gestures as they produced a Japanese four-word phrases. We examined all possible synchronization patterns of speech and gestures. We found, for the first time, gestures tended to synchronize with the chunks of words that form a constituent in syntactic structures. Our study suggests that gestures may play an important role in disambiguating syntactically ambiguous phrases. This could be a reason why humans have continuously used gestures even after they acquired a powerful tool of language and why today, they still produce language-redundant gestures.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XZ4IWUUF\\Kashiwadate et al. - 2020 - Syntactic Structure Influences Speech-Gesture Sync.pdf;C\:\\Users\\u668173\\Zotero\\storage\\LD97IERN\\lebs.2020.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{kaufeldLinguisticStructureMeaning2020,
  title = {Linguistic Structure and Meaning Organize Neural Oscillations into a Content-Specific Hierarchy},
  author = {Kaufeld, Greta and Bosker, Hans Rutger and ten Oever, Sanne and Alday, Phillip M. and Meyer, Antje S. and Martin, Andrea E.},
  date = {2020-10-23},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0302-20.2020},
  url = {https://www.jneurosci.org/content/early/2020/10/23/JNEUROSCI.0302-20.2020},
  urldate = {2020-11-02},
  abstract = {Neural oscillations track linguistic information during speech comprehension (e.g., Ding et al., 2016; Keitel et al., 2018), and are known to be modulated by acoustic landmarks and speech intelligibility (e.g., Doelling et al., 2014; Zoefel \& VanRullen, 2015). However, studies investigating linguistic tracking have either relied on non-naturalistic isochronous stimuli or failed to fully control for prosody. Therefore, it is still unclear whether low frequency activity tracks linguistic structure during natural speech, where linguistic structure does not follow such a palpable temporal pattern. Here, we measured electroencephalography (EEG) and manipulated the presence of semantic and syntactic information apart from the timescale of their occurrence, while carefully controlling for the acoustic-prosodic and lexical-semantic information in the signal. EEG was recorded while 29 adult native speakers (22 women, 7 men) listened to naturally-spoken Dutch sentences, jabberwocky controls with morphemes and sentential prosody, word lists with lexical content but no phrase structure, and backwards acoustically-matched controls. Mutual information (MI) analysis revealed sensitivity to linguistic content: MI was highest for sentences at the phrasal (0.8-1.1 Hz) and lexical timescale (1.9-2.8 Hz), suggesting that the delta-band is modulated by lexically-driven combinatorial processing beyond prosody, and that linguistic content (i.e., structure and meaning) organizes neural oscillations beyond the timescale and rhythmicity of the stimulus. This pattern is consistent with neurophysiologically inspired models of language comprehension (Martin, 2016, 2020; Martin \& Doumas, 2017) where oscillations encode endogenously generated linguistic content over and above exogenous or stimulus-driven timing and rhythm information. SIGNIFICANCE STATEMENT Biological systems like the brain encode their environment not only by reacting in a series of stimulus-driven responses, but by combining stimulus-driven information with endogenous, internally-generated, inferential knowledge and meaning. Understanding language from speech is the human benchmark for this. Much research focusses on the purely stimulus-driven response, but here, we focus on the goal of language behavior: conveying structure and meaning. To that end, we use naturalistic stimuli that contrast acoustic-prosodic and lexical-semantic information to show that, during spoken language comprehension, oscillatory modulations reflect computations related to inferring structure and meaning from the acoustic signal. Our experiment provides the first evidence to date that compositional structure and meaning organize the oscillatory response, above and beyond prosodic and lexical controls.},
  eprint = {33097640},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NLMTJLJB\\JNEUROSCI.0302-20.html},
  langid = {english}
}

@article{kaukomaaForeshadowingProblemTurnopening2014,
  title = {Foreshadowing a Problem: {{Turn}}-Opening Frowns in Conversation},
  shorttitle = {Foreshadowing a Problem},
  author = {Kaukomaa, Timo and Peräkylä, Anssi and Ruusuvuori, Johanna},
  date = {2014-09},
  journaltitle = {Journal of Pragmatics},
  shortjournal = {Journal of Pragmatics},
  volume = {71},
  pages = {132--147},
  issn = {03782166},
  doi = {10.1016/j.pragma.2014.08.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378216614001519},
  urldate = {2020-12-08},
  abstract = {Occasionally in conversation, a participant starts to frown during a silence between utterances, before starting to talk. The purpose of our study was to determine how these frowns contribute both to the upcoming turn and to the larger conversational context. The results suggest that these frowns mark the following utterance as dealing with something problematic in relation to the expectations created in the preceding talk. As pre-beginning elements, these frowns anticipate utterances that involve difficulties associated with negative evaluation, disaffiliation, or epistemic challenge. All three types of problem involve some complication that arises in the expected course of events within the interaction. These frowns seem to foreshadow utterances that somehow deviate from the recipient’s routine expectation. As these frowns persist into the utterances they anticipate, they become intertwined with what is being said. Furthermore, the utterance or utterances that follow(s) the turn-opening frown expose(s) the grounds for that problem. Turn-opening frowns are typically produced by the frowning participant gazing downward and away from the recipient. The recipients of these frowns do not typically reciprocate them even though they notice the frown. However, these facial expressions work as an important interactional resource for the interlocutors, hinting beforehand at a problem in the conversation that will be addressed in the upcoming turn of talk.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ASNHIHQN\\Kaukomaa et al. - 2014 - Foreshadowing a problem Turn-opening frowns in co.pdf},
  langid = {english}
}

@article{kaukomaaTurnopeningSmilesFacial2013,
  title = {Turn-Opening Smiles: {{Facial}} Expression Constructing Emotional Transition in Conversation},
  shorttitle = {Turn-Opening Smiles},
  author = {Kaukomaa, Timo and Peräkylä, Anssi and Ruusuvuori, Johanna},
  date = {2013-09-01},
  journaltitle = {Journal of Pragmatics},
  shortjournal = {Journal of Pragmatics},
  volume = {55},
  pages = {21--42},
  issn = {0378-2166},
  doi = {10.1016/j.pragma.2013.05.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0378216613001288},
  urldate = {2020-12-08},
  abstract = {Sometimes in conversation, a participant begins to smile during a silence that occurs between utterances. The purpose of our study was to determine how these smiles contribute to the upcoming turn as well as to the larger conversational context. The results suggest that these smiles can work as a first step in the construction of an emotional transition in conversation. These turn-opening smiles initiate a shift from a neutral or serious emotional stance to a positive or humorous emotional stance. The utterance(s) that follow(s) the smile explicate the grounds for the displayed emotional stance. These utterances also exhibit other (prosodic, lexical or gestural) markers of the emotional stance that the smile initiated. In our data, all the recipients of these stance-introducing smiles reciprocated them. By reciprocating the smile (and other emotional markers), the recipients share the emotional transition that is initiated by the turn-opening smiles. However, the timing of reciprocation varies. For example, sometimes the recipients of the smile reciprocate the emotion display at a point where the smiling participant has explicated the grounds for the transition. In other cases, the reciprocation takes place before such the explication, soon after the first occurrence of the turn-opening smile.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\URWXYBIJ\\S0378216613001288.html;C\:\\Users\\u668173\\Zotero\\storage\\VHXEVNVX\\S0378216613001288.html},
  keywords = {Conversation analysis,Emotion,Facial expression,Smile,Stance,Turn construction},
  langid = {english}
}

@article{kaynarPrevalenceLeftHandednessPatients2003,
  title = {Prevalence of {{Left}}-{{Handedness Among Patients}} with {{Different Respiratory Diseases}}},
  author = {Kaynar, H. and Dane, S.},
  date = {2003-01-01},
  journaltitle = {International Journal of Neuroscience},
  volume = {113},
  pages = {1371--1377},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7454},
  doi = {10.1080/00207450390231419},
  url = {https://doi.org/10.1080/00207450390231419},
  urldate = {2020-06-10},
  abstract = {We investigated the prevalence of left-handedness among patients with different respiratory diseases. Hand preference was assessed using the modified Edinburg Handedness Inventory. There was a statistically significant decline in left-handedness with shortened life span in patients. Difference among the rates of left-handedness in different pulmonary diseases was significant. The rate of left-handedness was lowest in patients with chronic obstructive pulmonary disease (2.14\%) and pneumonia (2.04\%) and highest in patients with asthma (12.96\%). The results suggest that left-handed subjects may be protected from certain pulmonary diseases while being more susceptible to others.­­},
  annotation = {\_eprint: https://doi.org/10.1080/00207450390231419},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BVQHDQBC\\KAYNAR and DANE - 2003 - Prevalence of Left-Handedness Among Patients with .pdf;C\:\\Users\\u668173\\Zotero\\storage\\IHNG6KKL\\00207450390231419.html},
  keywords = {handedness,left-handedness,life span; pulmonary disease},
  number = {10}
}

@article{kayserEarlySensoryCortices2007,
  title = {Do Early Sensory Cortices Integrate Cross-Modal Information?},
  author = {Kayser, Christoph and Logothetis, Nikos K.},
  date = {2007-09-01},
  journaltitle = {Brain Structure and Function},
  shortjournal = {Brain Struct Funct},
  volume = {212},
  pages = {121--132},
  issn = {1863-2661},
  doi = {10.1007/s00429-007-0154-0},
  url = {https://doi.org/10.1007/s00429-007-0154-0},
  urldate = {2020-12-05},
  abstract = {Our different senses provide complementary evidence about the environment and their interaction often aids behavioral performance or alters the quality of the sensory percept. A traditional view defers the merging of sensory information to higher association cortices, and posits that a large part of the brain can be reduced into a collection of unisensory systems that can be studied in isolation. Recent studies, however, challenge this view and suggest that cross-modal interactions can already occur in areas hitherto regarded as unisensory. We review results from functional imaging and electrophysiology exemplifying cross-modal interactions that occur early during the evoked response, and at the earliest stages of sensory cortical processing. Although anatomical studies revealed several potential origins of these cross-modal influences, there is yet no clear relation between particular functional observations and specific anatomical connections. In addition, our view on sensory integration at the neuronal level is coined by many studies on subcortical model systems of sensory integration; yet, the patterns of cross-modal interaction in cortex deviate from these model systems in several ways. Consequently, future studies on cortical sensory integration need to leave the descriptive level and need to incorporate cross-modal influences into models of the organization of sensory processing. Only then will we be able to determine whether early cross-modal interactions truly merit the label sensory integration, and how they increase a sensory system’s ability to scrutinize its environment and finally aid behavior.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\REXLJF65\\Kayser and Logothetis - 2007 - Do early sensory cortices integrate cross-modal in.pdf},
  langid = {english},
  number = {2}
}

@article{keilNeuralOscillationsOrchestrate2018,
  title = {Neural {{Oscillations Orchestrate Multisensory Processing}}},
  author = {Keil, Julian and Senkowski, Daniel},
  date = {2018-12-01},
  journaltitle = {The Neuroscientist},
  shortjournal = {Neuroscientist},
  volume = {24},
  pages = {609--626},
  publisher = {{SAGE Publications Inc STM}},
  issn = {1073-8584},
  doi = {10.1177/1073858418755352},
  url = {https://doi.org/10.1177/1073858418755352},
  urldate = {2020-12-05},
  abstract = {At any given moment, we receive input through our different sensory systems, and this information needs to be processed and integrated. Multisensory processing requires the coordinated activity of distinct cortical areas. Key mechanisms implicated in these processes include local neural oscillations and functional connectivity between distant cortical areas. Evidence is now emerging that neural oscillations in distinct frequency bands reflect different mechanisms of multisensory processing. Moreover, studies suggest that aberrant neural oscillations contribute to multisensory processing deficits in clinical populations, such as schizophrenia. In this article, we review recent literature on the neural mechanisms underlying multisensory processing, focusing on neural oscillations. We derive a framework that summarizes findings on (1) stimulus-driven multisensory processing, (2) the influence of top-down information on multisensory processing, and (3) the role of predictions for the formation of multisensory perception. We propose that different frequency band oscillations subserve complementary mechanisms of multisensory processing. These processes can act in parallel and are essential for multisensory processing.},
  langid = {english},
  number = {6}
}

@article{kelkarEvaluatingCollectionSoundTracing2018,
  title = {Evaluating a Collection of {{Sound}}-{{Tracing Data}} of {{Melodic Phrases}}},
  author = {Kelkar, Tejaswinee and Roy, Udit and Jensenius, Alexander Refsum},
  date = {2018},
  journaltitle = {978-2-9540351-2-3},
  shortjournal = {ENEngelskEnglishEvaluating a collection of Sound-Tracing Data of Melodic Phrases},
  publisher = {{Institut de Recherche et Coordination Acoustique/Musique}},
  url = {https://www.duo.uio.no/handle/10852/65560},
  urldate = {2021-03-16},
  abstract = {Melodic contour, the ‘shape’ of a melody, is a common way to visualize and remember a musical piece. The purpose of this paper is to explore the building blocks of a future ‘gesture-based’ melody retrieval system. We present a dataset containing 16 melodic phrases from four musical styles and with a large range of contour variability. This is accompanied by full-body motion capture data of 26 participants performing sound-tracing to the melodies. The dataset is analyzed using canonical correlation analysis (CCA), and its neural network variant (Deep CCA), to understand how melodic contours and sound tracings relate to each other. The analyses reveal non-linear relationships between sound and motion. The link between pitch and verticality does not appear strong enough for complex melodies. We also find that descending melodic contours have the least correlation with tracing.},
  annotation = {Accepted: 2018-11-15T16:47:03Z},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\L389UD7T\\Kelkar et al. - 2018 - Evaluating a collection of Sound-Tracing Data of M.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZF6PITE4\\65560.html},
  langid = {english}
}

@article{kellerRhythmJointAction2014,
  title = {Rhythm in Joint Action: Psychological and Neurophysiological Mechanisms for Real-Time Interpersonal Coordination},
  shorttitle = {Rhythm in Joint Action},
  author = {Keller, Peter E. and Novembre, Giacomo and Hove, Michael J.},
  date = {2014-12-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Phil. Trans. R. Soc. B},
  volume = {369},
  pages = {20130394},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2013.0394},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2013.0394},
  urldate = {2020-12-17},
  abstract = {Human interaction often requires simultaneous precision and flexibility in the coordination of rhythmic behaviour between individuals engaged in joint activity, for example, playing a musical duet or dancing with a partner. This review article addresses the psychological processes and brain mechanisms that enable such rhythmic interpersonal coordination. First, an overview is given of research on the cognitive-motor processes that enable individuals to represent joint action goals and to anticipate, attend and adapt to other's actions in real time. Second, the neurophysiological mechanisms that underpin rhythmic interpersonal coordination are sought in studies of sensorimotor and cognitive processes that play a role in the representation and integration of self- and other-related actions within and between individuals' brains. Finally, relationships between social–psychological factors and rhythmic interpersonal coordination are considered from two perspectives, one concerning how social-cognitive tendencies (e.g. empathy) affect coordination, and the other concerning how coordination affects interpersonal affiliation, trust and prosocial behaviour. Our review highlights musical ensemble performance as an ecologically valid yet readily controlled domain for investigating rhythm in joint action.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VMAGBVJE\\Keller et al. - 2014 - Rhythm in joint action psychological and neurophy.pdf},
  langid = {english},
  number = {1658}
}

@article{kelloHierarchicalTemporalStructure2017,
  title = {Hierarchical Temporal Structure in Music, Speech and Animal Vocalizations: Jazz Is like a Conversation, Humpbacks Sing like Hermit Thrushes},
  shorttitle = {Hierarchical Temporal Structure in Music, Speech and Animal Vocalizations},
  author = {Kello, Christopher T. and Bella, Simone Dalla and Médé, Butovens and Balasubramaniam, Ramesh},
  date = {2017-10-31},
  journaltitle = {Journal of The Royal Society Interface},
  shortjournal = {Journal of The Royal Society Interface},
  volume = {14},
  pages = {20170231},
  publisher = {{Royal Society}},
  doi = {10.1098/rsif.2017.0231},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2017.0231},
  urldate = {2020-12-13},
  abstract = {Humans talk, sing and play music. Some species of birds and whales sing long and complex songs. All these behaviours and sounds exhibit hierarchical structure—syllables and notes are positioned within words and musical phrases, words and motives in sentences and musical phrases, and so on. We developed a new method to measure and compare hierarchical temporal structures in speech, song and music. The method identifies temporal events as peaks in the sound amplitude envelope, and quantifies event clustering across a range of timescales using Allan factor (AF) variance. AF variances were analysed and compared for over 200 different recordings from more than 16 different categories of signals, including recordings of speech in different contexts and languages, musical compositions and performances from different genres. Non-human vocalizations from two bird species and two types of marine mammals were also analysed for comparison. The resulting patterns of AF variance across timescales were distinct to each of four natural categories of complex sound: speech, popular music, classical music and complex animal vocalizations. Comparisons within and across categories indicated that nested clustering in longer timescales was more prominent when prosodic variation was greater, and when sounds came from interactions among individuals, including interactions between speakers, musicians, and even killer whales. Nested clustering also was more prominent for music compared with speech, and reflected beat structure for popular music and self-similarity across timescales for classical music. In summary, hierarchical temporal structures reflect the behavioural and social processes underlying complex vocalizations and musical performances.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VH8DTMBW\\Kello et al. - 2017 - Hierarchical temporal structure in music, speech a.pdf;C\:\\Users\\u668173\\Zotero\\storage\\2A6DI8XU\\rsif.2017.html},
  number = {135}
}

@incollection{kelsoActionperceptionPatternFormation1990,
  title = {Action-Perception as a Pattern Formation Process},
  booktitle = {Attention and Performance 13:  {{Motor}} Representation and Control},
  author = {Kelso, J. A. S. and Del Colle, J. D. and Schöner, G.},
  date = {1990},
  pages = {139--169},
  publisher = {{Lawrence Erlbaum Associates, Inc}},
  location = {{Hillsdale, NJ, US}},
  abstract = {aim to identify collective variables (or order parameters) and their dynamics (stability, loss of stability, hysteresis . . . ) for perception-action patterns / to do so we extend earlier results on phase transitions in human bimanual coordination to a perception-action task, synchronizing or syncopating finger flexion with an auditory metronome whose frequency is varied in steps (.25 Hz) over a wide range (1.0 Hz to 3.5 Hz) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Y6BQ6G7X\\1990-97330-005.html},
  isbn = {978-0-8058-0565-9},
  keywords = {Auditory Perception,Perceptual Motor Coordination,Response Parameters}
}

@article{kelsoConvergingEvidenceSupport1984,
  title = {Converging Evidence in Support of Common Dynamical Principles for Speech and Movement Coordination},
  author = {Kelso, J. A. S. and Tuller, B.},
  date = {1984-06},
  journaltitle = {The American Journal of Physiology},
  shortjournal = {Am. J. Physiol.},
  volume = {246},
  pages = {R928-935},
  issn = {0002-9513},
  doi = {10.1152/ajpregu.1984.246.6.R928},
  abstract = {We suggest that a principled analysis of language and action should begin with an understanding of the rate-dependent, dynamical processes that underlie their implementation. Here we present a summary of our ongoing speech production research, which reveals some striking similarities with other work on limb movements. Four design themes emerge for articulatory systems: 1) they are functionally rather than anatomically specific in the way they work; 2) they exhibit equifinality and in doing so fall under the generic category of a dynamical system called point attractor; 3) across transformations they preserve a relationally invariant topology; and 4) this, combined with their stable cyclic nature, suggests that they can function as nonlinear, limit cycle oscillators (periodic attractors). This brief inventory of regularities, though not mean to be inclusive, hints strongly that speech and other movements share a common, dynamical mode of operation.},
  eprint = {6742170},
  eprinttype = {pmid},
  issue = {6 Pt 2},
  keywords = {Animals,Brain,Humans,Locomotion,Models; Neurological,Models; Psychological,Motor Activity,Movement,Speech},
  langid = {english}
}

@incollection{kelsoDynamicPatternPerspective1983,
  title = {A “{{Dynamic Pattern}}” Perspective on the Control and Coordination of Movement},
  booktitle = {The Production of Speech},
  author = {Kelso, J. A. S. and Tuller, B. and Harris, K.},
  date = {1983},
  publisher = {{Springer-Verlag}},
  location = {{Berlin}},
  url = {https://link.springer.com/chapter/10.1007/978-1-4613-8202-7_7},
  urldate = {2019-08-08},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NZWWH973\\978-1-4613-8202-7_7.html}
}

@article{kelsoFunctionallySpecificArticulatory1984,
  title = {Functionally Specific Articulatory Cooperation Following Jaw Perturbations during Speech: Evidence for Coordinative Structures},
  shorttitle = {Functionally Specific Articulatory Cooperation Following Jaw Perturbations during Speech},
  author = {Kelso, J. A. S. and Tuller, B. and Vatikiotis-Bateson, E. and Fowler, C. A.},
  date = {1984-12},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {10},
  pages = {812--832},
  issn = {0096-1523},
  abstract = {In three experiments we show that articulatory patterns in response to jaw perturbations are specific to the utterance produced. In Experiments 1 and 2, an unexpected constant force load (5.88 N) applied during upward jaw motion for final /b/ closure in the utterance /baeb/ revealed nearly immediate compensation in upper and lower lips, but not the tongue, on the first perturbation trial. The same perturbation applied during the utterance /baez/ evoked rapid and increased tongue-muscle activity for /z/ frication, but no active lip compensation. Although jaw perturbation represented a threat to both utterances, no perceptible distortion of speech occurred. In Experiment 3, the phase of the jaw perturbation was varied during the production of bilabial consonants. Remote reactions in the upper lip were observed only when the jaw was perturbed during the closing phase of motion. These findings provide evidence for flexibly assembled coordinative structures in speech production.},
  eprint = {6239907},
  eprinttype = {pmid},
  keywords = {Adult,Electromyography,Humans,Lip,Male,Mandible,Masticatory Muscles,Phonation,Phonetics,Speech,Speech Production Measurement,Voice},
  langid = {english},
  number = {6}
}

@article{kelsoTheoryApracticSyndromes1981,
  title = {Toward a Theory of Apractic Syndromes},
  author = {Kelso, J. A. Scott and Tuller, Betty},
  date = {1981-03-01},
  journaltitle = {Brain and Language},
  shortjournal = {Brain and Language},
  volume = {12},
  pages = {224--245},
  issn = {0093-934X},
  doi = {10.1016/0093-934X(81)90016-X},
  url = {http://www.sciencedirect.com/science/article/pii/0093934X8190016X},
  urldate = {2020-07-21},
  abstract = {Theoretical development on human motor behavior has occurred largely independently of data on pathological movement disorders. This paper represents an initial attempt to interface findings from studies of apraxia and those of normal motor behavior with a view to formulating a common theoretical framework. Such an integration may ultimately aid in understanding the nature of skill acquisition and provide insights into the organization of motor systems. Three putative theoretical models of movement control are discussed with reference to apractic syndromes. The most commonly accepted view—the hierarchy—possesses properties such as linear transitivity and unidirectionality of information flow that render it inadequate in explaining functional plasticity in the central nervous system. The heterarchy, which incorporates reciprocity of function and circular transitivity, is a more likely candidate but suffers from an inability to regulate the degrees of freedom of the system. Our favored candidate is the coalition model which embodies heterarchical principles, but in addition, offers a solution to the problems of degrees of freedom and context for motor systems. Evidence is reviewed from apraxia of speech and limbs in terms of a coalitional style of control and an experimental approach, consonant with coalitional organization, is developed. We promote the claim that an understanding of apractic behavior—and perhaps motor systems in general—will benefit when clinicians and experimenters embrace a theory of context and constraints rather than a theory of commands as is currently in vogue.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KJ4NSUSD\\Kelso and Tuller - 1981 - Toward a theory of apractic syndromes.pdf;C\:\\Users\\u668173\\Zotero\\storage\\UUH5SRPC\\0093934X8190016X.html},
  langid = {english},
  number = {2}
}

@article{kendonGestureCommunicationStrategy2001,
  title = {Gesture as Communication Strategy},
  author = {Kendon, Adam},
  date = {2001-08-22},
  journaltitle = {Semiotica},
  volume = {2001},
  pages = {191--209},
  publisher = {{De Gruyter Mouton}},
  issn = {1613-3692, 0037-1998},
  doi = {10.1515/semi.2001.060},
  url = {https://www.degruyter.com/view/journals/semi/2001/135/article-p191.xml},
  urldate = {2020-07-14},
  abstract = {Article Gesture as communication strategy was published on 22 Aug 2001 in the journal Semiotica (Volume 2001, Issue 135).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YQ7BR2NY\\Kendon - 2001 - Gesture as communication strategy.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XTKCU2GW\\article-p191.html},
  langid = {english},
  number = {135}
}

@book{kendonGestureVisibleAction2004,
  title = {Gesture: {{Visible Action}} as {{Utterance}}},
  shorttitle = {Gesture},
  author = {Kendon, Adam},
  date = {2004-09-23},
  publisher = {{Cambridge University Press}},
  abstract = {Gesture, or visible bodily action that is seen as intimately involved in the activity of speaking, has long fascinated scholars and laymen alike. Written by a leading authority on the subject, this 2004 study provides a comprehensive treatment of gesture and its use in interaction, drawing on the analysis of everyday conversations to demonstrate its varied role in the construction of utterances. Adam Kendon accompanies his analyses with an extended discussion of the history of the study of gesture - a topic not dealt with in any previous publication - as well as exploring the relationship between gesture and sign language, and how the use of gesture varies according to cultural and language differences. Set to become the definitive account of the topic, Gesture will be invaluable to all those interested in human communication. Its publication marks a major development, both in semiotics and in the emerging field of gesture studies.},
  eprint = {hDXnnzmDkOkC},
  eprinttype = {googlebooks},
  isbn = {978-0-521-54293-7},
  keywords = {Language Arts & Disciplines / Linguistics / General,Language Arts & Disciplines / Linguistics / Sociolinguistics,Language Arts & Disciplines / Sign Language,Psychology / General},
  langid = {english},
  pagetotal = {418}
}

@article{kendonReflectionsGesturefirstHypothesis2017,
  title = {Reflections on the “Gesture-First” Hypothesis of Language Origins},
  author = {Kendon, Adam},
  date = {2017-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {24},
  pages = {163--170},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1117-3},
  url = {https://doi.org/10.3758/s13423-016-1117-3},
  urldate = {2019-10-14},
  abstract = {The main lines of evidence taken as support for the “gesture-first” hypothesis of language origins are briefly evaluated, and the problem that speech poses for this hypothesis is discussed. I conclude that language must have evolved in the oral–aural and kinesic modalities together, with neither modality taking precedence over the other.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\T3SYIPKP\\Kendon - 2017 - Reflections on the “gesture-first” hypothesis of l.pdf},
  keywords = {Gesture,Language origins,Primate communication,Sign language,Speech},
  langid = {english},
  number = {1}
}

@article{kendrickGazeDirectionSignals2017,
  title = {Gaze Direction Signals Response Preference in Conversation},
  author = {Kendrick, Kobin H. and Holler, Judith},
  date = {2017},
  journaltitle = {Research on Language and Social Interaction},
  volume = {50},
  pages = {12--32},
  publisher = {{Taylor \& Francis}},
  location = {{United Kingdom}},
  issn = {1532-7973(Electronic),0835-1813(Print)},
  doi = {10.1080/08351813.2017.1262120},
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 50(3) of Research on Language and Social Interaction (see record 2017-38244-007).On page 19, a citation was included that incorrectly referred to M. H. Goodwin (1980). The correct citation is C. Goodwin (1980) and the correct sentence should be noted as follows: Extract 3 shows how speakers can use self-repair to solicit the recipient’s gaze if it is not already present (C. Goodwin, 1980; Kidwell, 2006).] In this article, we examine gaze direction in responses to polar questions using both quantitative and conversation analytic (CA) methods. The data come from a novel corpus of conversations in which participants wore eye-tracking glasses to obtain direct measures of their eye movements. The results show that while most preferred responses are produced with gaze toward the questioner, most dispreferred responses are produced with gaze aversion. We further demonstrate that gaze aversion by respondents can occasion self-repair by questioners in the transition space between turns, indicating that the relationship between gaze direction and preference is more than a mere statistical association. We conclude that gaze direction in responses to polar questions functions as a signal of response preference. Data are in American, British, and Canadian English. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AAQYRCYW\\Kendrick and Holler - 2017 - Gaze direction signals response preference in conv.pdf;C\:\\Users\\u668173\\Zotero\\storage\\E5PEBHXG\\2017-07814-002.html},
  keywords = {Conversation,Eye Fixation,Language,Preferences},
  number = {1}
}

@article{kendrickTimingConstructionPreference2015,
  title = {The {{Timing}} and {{Construction}} of {{Preference}}: {{A Quantitative Study}}},
  shorttitle = {The {{Timing}} and {{Construction}} of {{Preference}}},
  author = {Kendrick, Kobin H. and Torreira, Francisco},
  date = {2015-05-19},
  journaltitle = {Discourse Processes},
  volume = {52},
  pages = {255--289},
  publisher = {{Routledge}},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2014.955997},
  url = {https://doi.org/10.1080/0163853X.2014.955997},
  urldate = {2020-12-08},
  abstract = {Conversation-analytic research has argued that the timing and construction of preferred responding actions (e.g., acceptances) differ from that of dispreferred responding actions (e.g., rejections), potentially enabling early response prediction by recipients. We examined 195 preferred and dispreferred responding actions in telephone corpora and found that the timing of the most frequent cases of each type did not differ systematically. Only for turn transitions of 700 ms or more was the proportion of dispreferred responding actions clearly greater than that of preferreds. In contrast, an analysis of the timing that included turn formats (i.e., those with or without qualification) revealed clearer differences. Small departures from a normal gap duration decrease the likelihood of a preferred action in a preferred turn format (e.g., a simple “yes”). We propose that the timing of a response is best understood as a turn-constructional feature, the first virtual component of a preferred or dispreferred turn format.},
  annotation = {\_eprint: https://doi.org/10.1080/0163853X.2014.955997},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TGUFRG2X\\Kendrick and Torreira - 2015 - The Timing and Construction of Preference A Quant.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XUJ7SJID\\0163853X.2014.html},
  number = {4}
}

@article{kenettSemanticDistanceTask2017,
  title = {The Semantic Distance Task: {{Quantifying}} Semantic Distance with Semantic Network Path Length},
  shorttitle = {The Semantic Distance Task},
  author = {Kenett, Yoed N. and Levi, Effi and Anaki, David and Faust, Miriam},
  date = {2017-09},
  journaltitle = {Journal of Experimental Psychology. Learning, Memory, and Cognition},
  shortjournal = {J Exp Psychol Learn Mem Cogn},
  volume = {43},
  pages = {1470--1489},
  issn = {1939-1285},
  doi = {10.1037/xlm0000391},
  abstract = {Semantic distance is a determining factor in cognitive processes, such as semantic priming, operating upon semantic memory. The main computational approach to compute semantic distance is through latent semantic analysis (LSA). However, objections have been raised against this approach, mainly in its failure at predicting semantic priming. We propose a novel approach to computing semantic distance, based on network science methodology. Path length in a semantic network represents the amount of steps needed to traverse from 1 word in the network to the other. We examine whether path length can be used as a measure of semantic distance, by investigating how path length affect performance in a semantic relatedness judgment task and recall from memory. Our results show a differential effect on performance: Up to 4 steps separating between word-pairs, participants exhibit an increase in reaction time (RT) and decrease in the percentage of word-pairs judged as related. From 4 steps onward, participants exhibit a significant decrease in RT and the word-pairs are dominantly judged as unrelated. Furthermore, we show that as path length between word-pairs increases, success in free- and cued-recall decreases. Finally, we demonstrate how our measure outperforms computational methods measuring semantic distance (LSA and positive pointwise mutual information) in predicting participants RT and subjective judgments of semantic strength. Thus, we provide a computational alternative to computing semantic distance. Furthermore, this approach addresses key issues in cognitive theory, namely the breadth of the spreading activation process and the effect of semantic distance on memory retrieval. (PsycINFO Database Record},
  eprint = {28240936},
  eprinttype = {pmid},
  keywords = {Analysis of Variance,Association,Cues,Female,Humans,Judgment,Language Tests,Linear Models,Male,Mental Recall,Models; Psychological,Psycholinguistics,Reaction Time,Semantic Web,Semantics,Young Adult},
  langid = {english},
  number = {9}
}

@article{kenettWhatCanQuantitative2019,
  title = {What Can Quantitative Measures of Semantic Distance Tell Us about Creativity?},
  author = {Kenett, Yoed N},
  date = {2019-06-01},
  journaltitle = {Current Opinion in Behavioral Sciences},
  shortjournal = {Current Opinion in Behavioral Sciences},
  volume = {27},
  pages = {11--16},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2018.08.010},
  url = {http://www.sciencedirect.com/science/article/pii/S2352154618301098},
  urldate = {2021-01-29},
  abstract = {Semantic distance plays an important role in the creative process: The farther one `moves away’ from a conventional idea, the more creative the new idea will likely be. Although intuitive, the role of semantic distance in creativity has been only indirectly examined due to the challenge of its measurement. Recent studies have started applying quantitative measures of semantic distance in creativity research. Such studies complement standard subjective measures of creativity; provide objective measures of the creative output; and also allow to more directly examine the role of semantic memory, and distance, in creativity. An overview of the main approaches that are being used will be described and the advantages of using such quantitative measures in creativity will be discussed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RL2RFLA8\\S2352154618301098.html},
  langid = {english},
  series = {Creativity}
}

@article{kershenbaumAcousticSequencesNonhuman2016,
  title = {Acoustic Sequences in Non-Human Animals: A Tutorial Review and Prospectus},
  shorttitle = {Acoustic Sequences in Non-Human Animals},
  author = {Kershenbaum, Arik and Blumstein, Daniel T. and Roch, Marie A. and Akçay, Çağlar and Backus, Gregory and Bee, Mark A. and Bohn, Kirsten and Cao, Yan and Carter, Gerald and Cäsar, Cristiane and Coen, Michael and DeRuiter, Stacy L. and Doyle, Laurance and Edelman, Shimon and Ferrer‐i‐Cancho, Ramon and Freeberg, Todd M. and Garland, Ellen C. and Gustison, Morgan and Harley, Heidi E. and Huetz, Chloé and Hughes, Melissa and Bruno, Julia Hyland and Ilany, Amiyaal and Jin, Dezhe Z. and Johnson, Michael and Ju, Chenghui and Karnowski, Jeremy and Lohr, Bernard and Manser, Marta B. and McCowan, Brenda and Mercado, Eduardo and Narins, Peter M. and Piel, Alex and Rice, Megan and Salmi, Roberta and Sasahara, Kazutoshi and Sayigh, Laela and Shiu, Yu and Taylor, Charles and Vallejo, Edgar E. and Waller, Sara and Zamora‐Gutierrez, Veronica},
  date = {2016},
  journaltitle = {Biological Reviews},
  volume = {91},
  pages = {13--52},
  issn = {1469-185X},
  doi = {10.1111/brv.12160},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12160},
  urldate = {2020-09-02},
  abstract = {Animal acoustic communication often takes the form of complex sequences, made up of multiple distinct acoustic units. Apart from the well-known example of birdsong, other animals such as insects, amphibians, and mammals (including bats, rodents, primates, and cetaceans) also generate complex acoustic sequences. Occasionally, such as with birdsong, the adaptive role of these sequences seems clear (e.g. mate attraction and territorial defence). More often however, researchers have only begun to characterise – let alone understand – the significance and meaning of acoustic sequences. Hypotheses abound, but there is little agreement as to how sequences should be defined and analysed. Our review aims to outline suitable methods for testing these hypotheses, and to describe the major limitations to our current and near-future knowledge on questions of acoustic sequences. This review and prospectus is the result of a collaborative effort between 43 scientists from the fields of animal behaviour, ecology and evolution, signal processing, machine learning, quantitative linguistics, and information theory, who gathered for a 2013 workshop entitled, ‘Analysing vocal sequences in animals’. Our goal is to present not just a review of the state of the art, but to propose a methodological framework that summarises what we suggest are the best practices for research in this field, across taxa and across disciplines. We also provide a tutorial-style introduction to some of the most promising algorithmic approaches for analysing sequences. We divide our review into three sections: identifying the distinct units of an acoustic sequence, describing the different ways that information can be contained within a sequence, and analysing the structure of that sequence. Each of these sections is further subdivided to address the key questions and approaches in that area. We propose a uniform, systematic, and comprehensive approach to studying sequences, with the goal of clarifying research terms used in different fields, and facilitating collaboration and comparative studies. Allowing greater interdisciplinary collaboration will facilitate the investigation of many important questions in the evolution of communication and sociality.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/brv.12160},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RZBLTDAG\\Kershenbaum et al. - 2016 - Acoustic sequences in non-human animals a tutoria.pdf;C\:\\Users\\u668173\\Zotero\\storage\\M8YUFPYG\\brv.html},
  keywords = {acoustic communication,information,information theory,machine learning,Markov model,meaning,network analysis,sequence analysis,vocalisation},
  langid = {english},
  number = {1}
}

@article{keyserTaskdependentResponsesMuscle2019,
  title = {Task-Dependent Responses to Muscle Vibration during Reaching},
  author = {Keyser, Johannes and Ramakers, Rob E. F. S. and Medendorp, W. Pieter and Selen, Luc P. J.},
  date = {2019},
  journaltitle = {European Journal of Neuroscience},
  volume = {49},
  pages = {1477--1490},
  issn = {1460-9568},
  doi = {10.1111/ejn.14292},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.14292},
  urldate = {2021-03-03},
  abstract = {Feedback corrections in reaching have been shown to be task-dependent for proprioceptive, visual and vestibular perturbations, in line with predictions from optimal feedback control theory. Mechanical perturbations have been used to elicit proprioceptive errors, but have the drawback to actively alter the limb's trajectory, making it nontrivial to dissociate the subject's compensatory response from the perturbation itself. In contrast, muscle vibration provides an alternative tool to perturb the muscle afferents without changing the hands trajectory, inducing only changes in the estimated, but not the actual, limb position and velocity. Here, we investigate whether upper-arm muscle vibration is sufficient to evoke task-dependent feedback corrections during goal-directed reaching to a narrow versus a wide target. Our main result is that for vibration of biceps and triceps, compensatory responses were down-regulated for the wide compared to the narrow target. The earliest detectable difference between these target-specific corrections is at about 100 ms, likely reflecting a task-dependent feedback control policy rather than a voluntary response.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.14292},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BRDIEJPT\\Keyser et al. - 2019 - Task-dependent responses to muscle vibration durin.pdf;C\:\\Users\\u668173\\Zotero\\storage\\4B6M6NJZ\\ejn.html},
  keywords = {minimal intervention principle,online feedback control,proprioception,sensorimotor control},
  langid = {english},
  number = {11}
}

@article{kienastACOUSTICALANALYSISSPECTRAL,
  title = {{{ACOUSTICAL ANALYSIS OF SPECTRAL AND TEMPORAL CHANGES IN EMOTIONAL SPEECH}}},
  author = {Kienast, M and Sendlmeier, W F},
  journaltitle = {Analysis ISCA Tutor},
  pages = {92--97},
  abstract = {In the present study, the vocal expressions of the emotions anger, happiness, fear, boredom and sadness are acoustically analyzed in relation to neutral speech. The emotional speech material produced by actors is investigated especially with regard to spectral and segmental changes which are caused by different articulatory behavior accompanying emotional arousal. The findings are interpreted in relation to temporal variations.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BIJ55YAL\\Kienast and Sendlmeier - ACOUSTICAL ANALYSIS OF SPECTRAL AND TEMPORAL CHANG.pdf},
  langid = {english}
}

@article{killinWhereDidLanguage2017,
  title = {Where Did Language Come from? {{Connecting}} Sign, Song, and Speech in Hominin Evolution},
  shorttitle = {Where Did Language Come From?},
  author = {Killin, Anton},
  date = {2017-12-01},
  journaltitle = {Biology \& Philosophy},
  shortjournal = {Biol Philos},
  volume = {32},
  pages = {759--778},
  issn = {1572-8404},
  doi = {10.1007/s10539-017-9607-x},
  url = {https://doi.org/10.1007/s10539-017-9607-x},
  urldate = {2021-02-23},
  abstract = {Recently theorists have developed competing accounts of the origins and nature of protolanguage and the subsequent evolution of language. Debate over these accounts is lively. Participants ask: Is music a direct precursor of language? Were the first languages gestural? Or is language continuous with primate vocalizations, such as the alarm calls of vervets? In this article I survey the leading hypotheses and lines of evidence, favouring a largely gestural conception of protolanguage. However, the “sticking point” of gestural accounts, to use Robbins Burling’s phrase, is the need to explain how language shifted to a largely vocal medium. So with a critical eye I consider Michael Corballis’s most recent expression of his ideas about this transition (2017’s The Truth About Language: What It Is And Where It Came From). Corballis’s view is an excellent foil to mine and I present it as such. Contrary to Corballis’s account, and developing Burling’s conjecture that musicality played some role, I argue that the foundations of an evolving musicality (i.e., evolving largely independently of language) provided the means and medium for the shift from gestural to vocal dominance in language. In other words, I suggest that an independently evolving musicality prepared ancient hominins, morphologically and cognitively, for intentional articulate vocal production, enabling the evolution of speech.},
  langid = {english},
  number = {6}
}

@article{kirbyCompressionCommunicationCultural2015,
  title = {Compression and Communication in the Cultural Evolution of Linguistic Structure},
  author = {Kirby, Simon and Tamariz, Monica and Cornish, Hannah and Smith, Kenny},
  date = {2015-08-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {141},
  pages = {87--102},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.03.016},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027715000815},
  urldate = {2020-03-18},
  abstract = {Language exhibits striking systematic structure. Words are composed of combinations of reusable sounds, and those words in turn are combined to form complex sentences. These properties make language unique among natural communication systems and enable our species to convey an open-ended set of messages. We provide a cultural evolutionary account of the origins of this structure. We show, using simulations of rational learners and laboratory experiments, that structure arises from a trade-off between pressures for compressibility (imposed during learning) and expressivity (imposed during communication). We further demonstrate that the relative strength of these two pressures can be varied in different social contexts, leading to novel predictions about the emergence of structured behaviour in the wild.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CV896X9L\\Kirby et al. - 2015 - Compression and communication in the cultural evol.pdf;C\:\\Users\\u668173\\Zotero\\storage\\DHT5PRIK\\S0010027715000815.html},
  keywords = {Cultural transmission,Iterated learning,Language evolution},
  langid = {english}
}

@article{kirbyCompressionCommunicationCultural2015a,
  title = {Compression and Communication in the Cultural Evolution of Linguistic Structure},
  author = {Kirby, S and Tamariz, M. and Cornish, H. and Smith, K.},
  date = {2015},
  journaltitle = {Cognition},
  volume = {141},
  pages = {87--102},
  doi = {10.1016/j.cognition.2015.03.016},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027715000815},
  urldate = {2020-01-21},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YR8VRAVS\\S0010027715000815.html}
}

@article{kirbyCumulativeCulturalEvolution2008,
  title = {Cumulative Cultural Evolution in the Laboratory: {{An}} Experimental Approach to the Origins of Structure in Human Language},
  shorttitle = {Cumulative Cultural Evolution in the Laboratory},
  author = {Kirby, Simon and Cornish, Hannah and Smith, Kenny},
  date = {2008-08-05},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {105},
  pages = {10681--10686},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0707835105},
  url = {https://www.pnas.org/content/105/31/10681},
  urldate = {2020-03-16},
  abstract = {We introduce an experimental paradigm for studying the cumulative cultural evolution of language. In doing so we provide the first experimental validation for the idea that cultural transmission can lead to the appearance of design without a designer. Our experiments involve the iterated learning of artificial languages by human participants. We show that languages transmitted culturally evolve in such a way as to maximize their own transmissibility: over time, the languages in our experiments become easier to learn and increasingly structured. Furthermore, this structure emerges purely as a consequence of the transmission of language over generations, without any intentional design on the part of individual language learners. Previous computational and mathematical models suggest that iterated learning provides an explanation for the structure of human language and link particular aspects of linguistic structure with particular constraints acting on language during its transmission. The experimental work presented here shows that the predictions of these models, and models of cultural evolution more generally, can be tested in the laboratory.},
  eprint = {18667697},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LMLYGGZK\\Kirby et al. - 2008 - Cumulative cultural evolution in the laboratory A.pdf;C\:\\Users\\u668173\\Zotero\\storage\\Z2D3EWBR\\10681.html},
  keywords = {cultural transmission,iterated learning,language evolution},
  langid = {english},
  number = {31}
}

@article{kirbyIteratedLearningEvolution2014,
  title = {Iterated Learning and the Evolution of Language},
  author = {Kirby, Simon and Griffiths, Tom and Smith, Kenny},
  date = {2014-10},
  journaltitle = {Current Opinion in Neurobiology},
  volume = {28},
  pages = {108--114},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.07.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438814001421},
  urldate = {2020-03-18},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WSUJ84ZR\\Kirby et al. - 2014 - Iterated learning and the evolution of language.pdf},
  langid = {english}
}

@incollection{kirbyLanguageLearningLanguage2003,
  title = {From {{Language Learning}} to {{Language Evolution}}},
  booktitle = {Language {{Evolution}}},
  author = {Kirby, Simon and Christiansen, M. H.},
  editor = {Christiansen, M. H. and Kirby, Simon},
  date = {2003-07-24},
  pages = {272--294},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199244843.003.0015},
  url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199244843.001.0001/acprof-9780199244843-chapter-15},
  urldate = {2020-03-16},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QKWSV3BA\\Kirby and Christiansen - 2003 - From Language Learning to Language Evolution.pdf},
  isbn = {978-0-19-924484-3},
  langid = {english}
}

@article{kirtonConstituentOrderSilent2021,
  title = {Constituent Order in Silent Gesture Reflects the Perspective of the Producer},
  author = {Kirton, Fiona and Kirby, Simon and Smith, Kenny and Culbertson, Jennifer and Schouwstra, Marieke},
  date = {2021-03-20},
  journaltitle = {Journal of Language Evolution},
  shortjournal = {Journal of Language Evolution},
  issn = {2058-458X},
  doi = {10.1093/jole/lzaa010},
  url = {https://doi.org/10.1093/jole/lzaa010},
  urldate = {2021-03-23},
  abstract = {Understanding the relationship between human cognition and linguistic structure is a central theme in language evolution research. Numerous studies have investigated this question using the silent gesture paradigm in which participants describe events using only gesture and no speech. Research using this paradigm has found that Agent–Patient–Action (APV) is the most commonly produced gesture order, regardless of the producer’s native language. However, studies have uncovered a range of factors that influence ordering preferences. One such factor is salience, which has been suggested as a key determiner of word order. Specifically, humans, who are typically agents, are more salient than inanimate objects, so tend to be mentioned first. In this study, we investigated the role of salience in more detail and asked whether manipulating the salience of a human agent would modulate the tendency to express humans before objects. We found, first, that APV was less common than expected based on previous literature. Secondly, salience influenced the relative ordering of the patient and action, but not the agent and patient. For events involving a non-salient agent, participants typically expressed the patient before the action and vice versa for salient agents. Thirdly, participants typically omitted non-salient agents from their descriptions. We present details of a novel computational solution that infers the orders participants would have produced had they expressed all three constituents on every trial. Our analysis showed that events involving salient agents tended to elicit AVP; those involving a non-salient agent were typically described with APV, modulated by a strong tendency to omit the agent. We argue that these findings provide evidence that the effect of salience is realized through its effect on the perspective from which a producer frames an event.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\K6T5DB9N\\6179035.html},
  issue = {lzaa010}
}

@article{kirzingerVocalizationcorrelatedSingleunitActivity1991,
  title = {Vocalization-Correlated Single-Unit Activity in the Brain Stem of the Squirrel Monkey},
  author = {Kirzinger, A. and Jürgens, U.},
  date = {1991-04-01},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {84},
  pages = {545--560},
  issn = {1432-1106},
  doi = {10.1007/BF00230967},
  url = {https://doi.org/10.1007/BF00230967},
  urldate = {2020-10-07},
  abstract = {The brain stems of 17 squirrel monkeys (Saimiri sciureus) were systematically explored for vocalization-related single-unit activity during calls electrically elicited from the periaqueductal grey. Of 12,280 cells tested, 1151 fired in relation to vocalization. Of these, 587 reacted to external acoustic stimuli and started firing after vocalization onset. As most of these cells were located in classical auditory relay structures, they probably represent auditory neurones reacting indirectly to self-produced vocalization due to auditory feedback. Seven cells reacted to acoustic stimuli but fired in advance of self-produced vocalization. These cells were locoated in the pericentral inferior colliculus, dorsal nucleus of the lateral lemniscus, dorsomedial to the ventral nucleus of the lateral lemniscus and immediately lateral to the central grey. They are probably engaged in tuning the auditory system to process self-generated sounds differently from external sounds. 261 neurones reacted to nonphonatory oral movements (chewing, swallowing) and started firing after vocalization onset. These neurones were widely distributed within the brain stem, with the highest density in the spinal trigeminal nucleus and medially adjacent reticular formation. The majority of these cells seem to react to proprioceptive and tactile stimuli generated by phonatory and nonphonatory oral activities. Some of them may exert motor control on muscles that come into play at later stages of phonation. 57 neurones reacted to nonphonatory oral movements but fired in advance of vocalization onset. These neurones were located mainly in the trigeminal motor nucleus, nucl. ambiguus, reticular formation around these nuclei, parabrachial region and lateral vestibular nucleus. Their role in motor control seems to be related to specific muscles rather than specific functions. 100 of the vocalization-related cells showed a correlation with respiration. Expiration-related cells were found in and around the rostral nucl. ambiguus and in the reticular formation dorsal to the facial nucleus. Inspiration-related cells were located in the rostral and caudal nucl. ambiguus regions, ventrolateral solitary tract nucleus and the lateral reticular formation below the trigeminal motor nucleus. Most of these cells probably represent premotor neurones of respiratory muscles and laryngeal motoneurones of the cricothyroid and posterior cricoarytenoid muscles. Finally, a last group of cells was found that was unresponsive to chewing and swallowing movements, quiet breathing and acoustic stimuli, but changed activity during vocalization. 38 of them became active before vocalization and cricothyroid activity, and 101 afterward. Both types were completely intermingled and scattered widely in the brain stem, including the nucl. ambiguus region, solitary tract nucleus, nucl. reticularis parvocellularis and gigantocellularis, parabrachial region, pericentral colliculus inferior, vestibular complex, periventricular grey and laterally adjacent tegmentum. Some of these cells may be related to vocalization in a more specific way.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WJRN5FSK\\Kirzinger and Jürgens - 1991 - Vocalization-correlated single-unit activity in th.pdf},
  langid = {english},
  number = {3}
}

@article{kitaHowGesturesInfluence2017,
  title = {How Do Gestures Influence Thinking and Speaking? {{The}} Gesture-for-Conceptualization Hypothesis},
  shorttitle = {How Do Gestures Influence Thinking and Speaking?},
  author = {Kita, Sotaro and Alibali, Martha W. and Chu, Mingyuan},
  date = {2017},
  journaltitle = {Psychological Review},
  volume = {124},
  pages = {245--266},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/rev0000059},
  abstract = {People spontaneously produce gestures during speaking and thinking. The authors focus here on gestures that depict or indicate information related to the contents of concurrent speech or thought (i.e., representational gestures). Previous research indicates that such gestures have not only communicative functions, but also self-oriented cognitive functions. In this article, the authors propose a new theoretical framework, the gesture-for-conceptualization hypothesis, which explains the self-oriented functions of representational gestures. According to this framework, representational gestures affect cognitive processes in 4 main ways: gestures activate, manipulate, package, and explore spatio-motoric information for speaking and thinking. These four functions are shaped by gesture’s ability to schematize information, that is, to focus on a small subset of available information that is potentially relevant to the task at hand. The framework is based on the assumption that gestures are generated from the same system that generates practical actions, such as object manipulation; however, gestures are distinct from practical actions in that they represent information. The framework provides a novel, parsimonious, and comprehensive account of the self-oriented functions of gestures. The authors discuss how the framework accounts for gestures that depict abstract or metaphoric content, and they consider implications for the relations between self-oriented and communicative functions of gestures. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Gestures,Nonverbal Communication,Oral Communication,Problem Solving,Social Influences,Spatial Perception,Thinking},
  number = {3}
}

@inproceedings{kitaMovementPhasesSigns1998,
  title = {Movement Phases in Signs and Co-Speech Gestures, and Their Transcription by Human Coders},
  booktitle = {Gesture and {{Sign Language}} in {{Human}}-{{Computer Interaction}}},
  author = {Kita, Sotaro and van Gijn, Ingeborg and van der Hulst, Harry},
  editor = {Wachsmuth, Ipke and Fröhlich, Martin},
  date = {1998},
  pages = {23--35},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The previous literature has suggested that the hand movement in co-speech gestures and signs consists of a series of phases with qualitatively different dynamic characteristics. In this paper, we propose a syntagmatic rule system for movement phases that applies to both co-speech gestures and signs. Descriptive criteria for the rule system were developed for the analysis video-recorded continuous production of signs and gesture. It involves segmenting a stream of body movement into phases and identifying different phase types. Two human coders used the criteria to analyze signs and cospeech gestures that are produced in natural discourse. It was found that the criteria yielded good inter-coder reliability. These criteria can be used for the technology of automatic recognition of signs and co-speech gestures in order to segment continuous production and identify the potentially meaningbearing phase.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\D6R8R2SP\\Kita et al. - 1998 - Movement phases in signs and co-speech gestures, a.pdf},
  isbn = {978-3-540-69782-4},
  keywords = {American Sign,Expressive Phase,Movement Phase,Phase Type,Sign Language},
  langid = {english},
  options = {useprefix=true},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{kitaPointingWhereLanguage2003,
  title = {Pointing:  {{Where}} Language, Culture, and Cognition Meet},
  shorttitle = {Pointing},
  author = {Kita, S.},
  date = {2003},
  publisher = {{Lawrence Erlbaum Associates Publishers}},
  location = {{Mahwah, NJ, US}},
  abstract = {This volume examines pointing gestures from a multidisciplinary viewpoint. Pointing has captured the interest of scholars from different disciplines who study communication, however, ideas and findings have been scattered across diverse journals and researchers are often not aware of results in other disciplines. The aim of this volume is to provide an arena for the interdisciplinary exchange of information on pointing. This volume will be of interest to researchers in linguistics, semiotics, psychology, anthropology, and primatology. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5URMXNHN\\2003-00985-000.html},
  isbn = {978-0-8058-4014-8},
  keywords = {Experimentation,Gestures,Interdisciplinary Research},
  pagetotal = {vii, 339},
  series = {Pointing:  {{Where}} Language, Culture, and Cognition Meet}
}

@article{kitaRelationsSyntacticEncoding2007,
  title = {Relations between Syntactic Encoding and Co-Speech Gestures: {{Implications}} for a Model of Speech and Gesture Production},
  shorttitle = {Relations between Syntactic Encoding and Co-Speech Gestures},
  author = {Kita, Sotaro and Özyürek, Asli and Allen, Shanley and Brown, Amanda and Furman, Reyhan and Ishizuka, Tomoko},
  date = {2007-12-01},
  journaltitle = {Language and Cognitive Processes},
  volume = {22},
  pages = {1212--1236},
  publisher = {{Routledge}},
  issn = {0169-0965},
  doi = {10.1080/01690960701461426},
  url = {https://doi.org/10.1080/01690960701461426},
  urldate = {2020-05-25},
  abstract = {Gestures that accompany speech are known to be tightly coupled with speech production. However little is known about the cognitive processes that underlie this link. Previous cross-linguistic research has provided preliminary evidence for online interaction between the two systems based on the systematic co-variation found between how different languages syntactically package Manner and Path information of a motion event and how gestures represent Manner and Path. Here we elaborate on this finding by testing whether speakers within the same language gesturally express Manner and Path differently according to their online choice of syntactic packaging of Manner and Path, or whether gestural expression is pre-determined by a habitual conceptual schema congruent with the linguistic typology. Typologically congruent and incongruent syntactic structures for expressing Manner and Path (i.e., in a single clause or multiple clauses) were elicited from English speakers. We found that gestural expressions were determined by the online choice of syntactic packaging rather than by a habitual conceptual schema. It is therefore concluded that speech and gesture production processes interface online at the conceptual planning phase. Implications of the findings for models of speech and gesture production are discussed.},
  annotation = {\_eprint: https://doi.org/10.1080/01690960701461426},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TCYK5WBH\\Kita et al. - 2007 - Relations between syntactic encoding and co-speech.pdf;C\:\\Users\\u668173\\Zotero\\storage\\AKFU8BIX\\01690960701461426.html},
  number = {8}
}

@article{kitaWhatDoesCrosslinguistic2003,
  title = {What Does Cross-Linguistic Variation in Semantic Coordination of Speech and Gesture Reveal?: {{Evidence}} for an Interface Representation of Spatial Thinking and Speaking},
  shorttitle = {What Does Cross-Linguistic Variation in Semantic Coordination of Speech and Gesture Reveal?},
  author = {Kita, Sotaro and Özyürek, Asli},
  date = {2003-01-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {48},
  pages = {16--32},
  issn = {0749-596X},
  doi = {10.1016/S0749-596X(02)00505-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0749596X02005053},
  urldate = {2020-05-25},
  abstract = {Gestures that spontaneously accompany speech convey information coordinated with the concurrent speech. There has been considerable theoretical disagreement about the process by which this informational coordination is achieved. Some theories predict that the information encoded in gesture is not influenced by how information is verbally expressed. However, others predict that gestures encode only what is encoded in speech. This paper investigates this issue by comparing informational coordination between speech and gesture across different languages. Narratives in Turkish, Japanese, and English were elicited using an animated cartoon as the stimulus. It was found that gestures used to express the same motion events were influenced simultaneously by (1) how features of motion events were expressed in each language, and (2) spatial information in the stimulus that was never verbalized. From this, it is concluded that gestures are generated from spatio-motoric processes that interact on-line with the speech production process. Through the interaction, spatio-motoric information to be expressed is packaged into chunks that are verbalizable within a processing unit for speech formulation. In addition, we propose a model of speech and gesture production as one of a class of frameworks that are compatible with the data.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\R3BJFMFQ\\Kita and Özyürek - 2003 - What does cross-linguistic variation in semantic c.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8EDDEQ3A\\S0749596X02005053.html},
  keywords = {Cross-linguistic comparison,Gesture production,Motion event,Semantic coordination,Speech production},
  langid = {english},
  number = {1}
}

@software{kleimanEMAtoolsDataManagement2017,
  title = {{{EMAtools}}: {{Data Management Tools}} for {{Real}}-{{Time Monitoring}}/{{Ecological Momentary Assessment Data}}},
  shorttitle = {{{EMAtools}}},
  author = {Kleiman, Evan},
  date = {2017-08-03},
  url = {https://CRAN.R-project.org/package=EMAtools},
  urldate = {2020-05-27},
  abstract = {Do data management functions common in real-time monitoring (also called: ecological momentary assessment, experience sampling, micro-longitudinal) data, including creating power curves for multilevel data, centering on participant means and merging event-level data into momentary data sets where you need the events to correspond to the nearest data point in the momentary data. This is VERY early release software, and more features will be added over time.},
  version = {0.1.3}
}

@article{kleinBreathingLocomotionComparative2010,
  title = {Breathing and Locomotion: {{Comparative}} Anatomy, Morphology and Function},
  shorttitle = {Breathing and Locomotion},
  author = {Klein, Wilfried and Codd, Jonathan R.},
  date = {2010-08-31},
  journaltitle = {Respiratory Physiology \& Neurobiology},
  shortjournal = {Respiratory Physiology \& Neurobiology},
  volume = {173},
  pages = {S26-S32},
  issn = {1569-9048},
  doi = {10.1016/j.resp.2010.04.019},
  url = {http://www.sciencedirect.com/science/article/pii/S1569904810001552},
  urldate = {2020-09-19},
  abstract = {Using specialized respiratory structures such as gills, lungs and or a tracheal system, animals take up oxygen and release carbon dioxide. The efficiency of gas exchange, however, may be constrained by the morphology of the respiratory organ itself as well as by other aspects of an animal's physiology such as feeding, circulation or locomotion. Herein we discuss some aspects of the functional link between the respiratory and locomotor systems, such as gill morphology of sharks as a factor limiting maximum aerobic scope, respiratory constraints among legless lizards, lung morphology of testudines, trade-offs between locomotion and respiration among birds, reconstruction of the respiratory system of sauropods, respiration of mice during locomotion as well as some aspects of gas exchange among insects. Data covering such a broad spectrum of interactions between the locomotor and respiratory systems shall allow us to place breathing and locomotion into a wider context of evolution of oxygen.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2IDYWA54\\Klein and Codd - 2010 - Breathing and locomotion Comparative anatomy, mor.pdf;C\:\\Users\\u668173\\Zotero\\storage\\UHAQI5Y5\\S1569904810001552.html},
  keywords = {Evolutionary constraints,Locomotion,Respiration},
  langid = {english},
  series = {Supplement of the 2nd {{International Congress}} of {{Respiratory Science}}}
}

@article{klingeIncreasedAmygdalaActivation2010,
  title = {Increased Amygdala Activation to Emotional Auditory Stimuli in the Blind},
  author = {Klinge, Corinna and Röder, Brigitte and Büchel, Christian},
  date = {2010-06-01},
  journaltitle = {Brain},
  shortjournal = {Brain},
  volume = {133},
  pages = {1729--1736},
  issn = {0006-8950},
  doi = {10.1093/brain/awq102},
  url = {https://academic.oup.com/brain/article/133/6/1729/355156},
  urldate = {2019-11-22},
  abstract = {Abstract.  Emotional signals are of pivotal relevance in social interactions. Neuroimaging and lesion studies have established an important role of the amygdala},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PEZPIEYL\\Klinge et al. - 2010 - Increased amygdala activation to emotional auditor.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8VNW26X9\\355156.html},
  langid = {english},
  number = {6}
}

@article{klingeIncreasedAmygdalaActivation2010a,
  title = {Increased Amygdala Activation to Emotional Auditory Stimuli in the Blind},
  author = {Klinge, Corinna and Röder, Brigitte and Büchel, Christian},
  date = {2010-06},
  journaltitle = {Brain},
  volume = {133},
  pages = {1729--1736},
  issn = {1460-2156, 0006-8950},
  doi = {10.1093/brain/awq102},
  url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awq102},
  urldate = {2019-11-22},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SSDBMUSK\\Klinge et al. - 2010 - Increased amygdala activation to emotional auditor.pdf},
  langid = {english},
  number = {6}
}

@article{knightEffectHeadFlexion2019,
  title = {The {{Effect}} of {{Head Flexion}}/{{Extension}} on {{Acoustic Measures}} of {{Singing Voice Quality}}},
  author = {Knight, Elizabeth Johnson and Austin, Stephen F.},
  date = {2019-08-06},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2019.06.019},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199719301171},
  urldate = {2020-10-19},
  abstract = {A study was undertaken to identify the effect of head flexion/extension on singing voice quality. The amplitude of the fundamental frequency (F0) and the singing power ratio (SPR), an indirect measure of Singer's Formant activity, were measured. F0 and SPR scores at four experimental head positions were compared with the subjects’ scores at their habitual positions. Three vowels and three pitch levels were tested. F0 amplitudes and low-frequency partials in general were greater with neck extension, while SPR increased with neck flexion. No effect of pitch or vowel was found. Gains in SPR appear to be the result of damping low-frequency partials rather than amplifying those in the Singer's Formant region. Raising the amplitude of F0 is an important resonance tool for female voices in the high range, and may be of benefit to other voice types in resonance, loudness, and laryngeal function.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\245U5CXV\\S0892199719301171.html},
  keywords = {Cervical spine,Head position,Posture,Singing,Singing power ratio},
  langid = {english}
}

@article{kodaSopranoSingingGibbons2012,
  title = {Soprano Singing in Gibbons},
  author = {Koda, Hiroki and Nishimura, Takeshi and Tokuda, Isao T. and Oyakawa, Chisako and Nihonmatsu, Toshikuni and Masataka, Nobuo},
  date = {2012},
  journaltitle = {American Journal of Physical Anthropology},
  volume = {149},
  pages = {347--355},
  issn = {1096-8644},
  doi = {10.1002/ajpa.22124},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ajpa.22124},
  urldate = {2020-07-03},
  abstract = {Diversifications in primate vocalization, including human speech, are believed to reflect evolutionary modifications in vocal anatomy and physiology. Gibbon song is acoustically unique, comprising loud, melodious, penetrating pure tone-like calls. In a white-handed gibbon, Hylobates lar, the fundamental frequency (f0) of song sounds is amplified distinctively from the higher harmonics in normal air. In a helium-enriched atmosphere, f0 does not shift, but it is significantly suppressed and 2f0 is emphasized. This implies that the source is independent of the resonance filter of the supralaryngeal vocal tract (SVT) in gibbons, in contrast to musical wind instruments, in which the filter primarily determines f0. Acoustic simulation further supported that gibbons' singing is produced analogously to professional human soprano singing, in which a precise tuning of the first formant (F1) of the SVT to f0 amplifies exclusively the f0 component of the source. Thus, in gibbons, as in humans, dynamic control over the vocal tract configuration, rather than anatomical modifications, has been a dominant factor in determining call structure. The varied dynamic movements were adopted in response to unique social and ecological pressures in gibbons, allowing monogamous gibbons to produce pure-tonal melodious songs in the dense tropical forests with poor visibility. Am J Phys Anthropol, 2012. © 2012 Wiley Periodicals, Inc.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ajpa.22124},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6RLNVGS6\\Koda et al. - 2012 - Soprano singing in gibbons.pdf;C\:\\Users\\u668173\\Zotero\\storage\\UXC3KK6N\\ajpa.html},
  keywords = {acoustic simulation,gibbon song,heliox vocalization,human speech,source–filter theory},
  langid = {english},
  number = {3}
}

@article{koenigRespiratoryElectroglottographicMeasures2019,
  title = {Respiratory and Electroglottographic Measures of Normal and Loud Speech across Vowels},
  author = {Koenig, Laura L. and Fuchs, Susanne},
  date = {2019-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {145},
  pages = {1931--1931},
  issn = {0001-4966},
  doi = {10.1121/1.5102030},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.5102030},
  urldate = {2019-08-08},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VVDTB7XM\\1.html},
  number = {3}
}

@thesis{kolorovaLexikonBulgarischenAlltagsgesten2011,
  title = {Lexikon Der Bulgarischen {{Alltagsgesten}}},
  author = {Kolorova, Z.},
  date = {2011},
  institution = {{Technische Universitat Berlin}}
}

@article{kozakMaleCourtshipSignal2019,
  title = {Male Courtship Signal Modality and Female Mate Preference in the Wolf Spider {{Schizocosa}} Ocreata: Results of Digital Multimodal Playback Studies},
  shorttitle = {Male Courtship Signal Modality and Female Mate Preference in the Wolf Spider {{Schizocosa}} Ocreata},
  author = {Kozak, Elizabeth C. and Uetz, George W.},
  date = {2019-12-01},
  journaltitle = {Current Zoology},
  shortjournal = {Curr Zool},
  volume = {65},
  pages = {705--711},
  publisher = {{Oxford Academic}},
  doi = {10.1093/cz/zoz025},
  url = {https://academic.oup.com/cz/article/65/6/705/5492622},
  urldate = {2020-09-02},
  abstract = {Abstract.  Females must be able to perceive and assess male signals, especially when they occur simultaneously with those of other males. Previous studies show},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CAXTIASS\\Kozak and Uetz - 2019 - Male courtship signal modality and female mate pre.pdf;C\:\\Users\\u668173\\Zotero\\storage\\3C4BCSSK\\5492622.html},
  langid = {english},
  number = {6}
}

@article{krahmerEffectsVisualBeats2007,
  title = {The Effects of Visual Beats on Prosodic Prominence: {{Acoustic}} Analyses, Auditory Perception and Visual Perception},
  shorttitle = {The Effects of Visual Beats on Prosodic Prominence},
  author = {Krahmer, Emiel and Swerts, Marc},
  date = {2007-10},
  journaltitle = {Journal of Memory and Language},
  volume = {57},
  pages = {396--414},
  issn = {0749596X},
  doi = {10.1016/j.jml.2007.06.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X07000708},
  urldate = {2019-04-18},
  abstract = {Speakers employ acoustic cues (pitch accents) to indicate that a word is important, but may also use visual cues (beat gestures, head nods, eyebrow movements) for this purpose. Even though these acoustic and visual cues are related, the exact nature of this relationship is far from well understood. We investigate whether producing a visual beat leads to changes in how acoustic prominence is realized in speech, and whether it leads to changes in how prominence is perceived by observers. For Experiment I (‘‘making beats’’) we use an original experimental paradigm in which speakers are instructed to realize a target sentence with different distributions of acoustic and visual cues for prominence. Acoustic analyses reveal that the production of a visual beat indeed has an effect on the acoustic realization of the co-occurring speech, in particular on duration and the higher formants (F2 and F3), independent of the kind of visual beat and of the presence and position of pitch accents. In Experiment II (‘‘hearing beats’’), it is found that visual beats have a significant effect on the perceived prominence of the target words. When a speaker produces a beat gesture, an eyebrow movement or a head nod, the accompanying word is produced with relatively more spoken emphasis. In Experiment III (‘‘seeing beats’’), finally, it is found that when participants see a speaker realize a visual beat on a word, they perceive it as more prominent than when they do not see the beat gesture.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AU3AWR57\\Krahmer and Swerts - 2007 - The effects of visual beats on prosodic prominence.pdf},
  langid = {english},
  number = {3}
}

@incollection{kraussLexicalGesturesLexical2000,
  title = {Lexical Gestures and Lexical {{accessL}} a Process},
  booktitle = {Language and {{Gesture}}},
  author = {Krauss, R. M. and Chen, Y. and Gottesman, R. F.},
  date = {2000},
  url = {https://scholar.googleusercontent.com/scholar.bib?q=info:0fpr8C9_kDMJ:scholar.google.com/&output=citation&scisdr=CgUQcZkfEO3Yy3o0M_I:AAGBfm0AAAAAXZMxK_KEmWpHM-VpMySbsZU2wKqbdaIZ&scisig=AAGBfm0AAAAAXZMxK7vMU-afRHI6nivhKGEXFplyqMZq&scisf=4&ct=citation&cd=-1&hl=en},
  urldate = {2019-10-01},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ECPAAISQ\\scholar.html}
}

@article{krivokapicGesturalCoordinationProsodic2014,
  title = {Gestural Coordination at Prosodic Boundaries and Its Role for Prosodic Structure and Speech Planning Processes},
  author = {Krivokapić, Jelena},
  date = {2014-12-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  pages = {1--44},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0397},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4240964/},
  urldate = {2020-01-26},
  abstract = {Prosodic structure is a grammatical component that serves multiple functions in the production, comprehension and acquisition of language. Prosodic boundaries are critical for the understanding of the nature of the prosodic structure of language, and important progress has been made in the past decades in illuminating their properties. We first review recent prosodic boundary research from the point of view of gestural coordination. We then go on to tie in this work to questions of speech planning and manual and head movement. We conclude with an outline of a new direction of research which is needed for a full understanding of prosodic boundaries and their role in the speech production process.},
  eprint = {25385775},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3ZRITNK5\\Krivokapić - 2014 - Gestural coordination at prosodic boundaries and i.pdf},
  number = {1658},
  pmcid = {PMC4240964}
}

@article{krivokapicKinematicStudyProsodic2017,
  title = {A {{Kinematic Study}} of {{Prosodic Structure}} in {{Articulatory}} and {{Manual Gestures}}: {{Results}} from a {{Novel Method}} of {{Data Collection}}},
  shorttitle = {A {{Kinematic Study}} of {{Prosodic Structure}} in {{Articulatory}} and {{Manual Gestures}}},
  author = {Krivokapić, Jelena and Tiede, Mark K. and Tyrone, Martha E.},
  date = {2017},
  journaltitle = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {8},
  pages = {3},
  issn = {1868-6354},
  doi = {10.5334/labphon.75},
  url = {http://www.journal-labphon.org/articles/10.5334/labphon.75/},
  urldate = {2020-01-15},
  abstract = {The primary goal of this work is to examine prosodic structure as expressed concurrently through articulatory and manual gestures. Specifically, we investigated the effects of phrase-level prominence (Experiment 1) and of prosodic boundaries (Experiments 2 and 3) on the kinematic properties of oral constriction and manual gestures. The hypothesis guiding this work is that prosodic structure will be similarly expressed in both modalities. To test this, we have developed a novel method of data collection that simultaneously records speech audio, vocal tract gestures (using electromagnetic articulometry) and manual gestures (using motion capture). This method allows us, for the first time, to investigate kinematic properties of body movement and vocal tract gestures simultaneously, which in turn allows us to examine the relationship between speech and body gestures with great precision. A second goal of the paper is thus to establish the validity of this method. Results from two speakers show that manual and oral gestures lengthen under prominence and at prosodic boundaries, indicating that the effects of prosodic structure extend beyond the vocal tract to include body movement.1},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QZ4WY9ZY\\Krivokapić et al. - 2017 - A Kinematic Study of Prosodic Structure in Articul.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZCZIUWFM\\labphon.75.html},
  keywords = {electro- magnetic articulometry,EMA,gestures,motion capture,Prosodic boundaries,prosodic prominence,speech production,Vicon},
  langid = {english},
  number = {1}
}

@incollection{krivokapicProsodyArticulatoryPhonology2020,
  title = {Prosody in Articulatory Phonology},
  booktitle = {Prosodic {{Theory}} and {{Practice}}},
  author = {Krivokapic, Jelena},
  editor = {Shattuck-Hufnagel, S. and Barnes, J.},
  date = {2020},
  publisher = {{MIT Press.}}
}

@inproceedings{krivokapicSpeechManualGesture2016,
  title = {Speech and Manual Gesture Coordination in a Pointing Task},
  author = {Krivokapic, Jelena and Tiede, Mark K. and Tyrone, Martha E. and Goldenberg, Dolly},
  date = {2016},
  doi = {10.21437/SpeechProsody.2016-255},
  abstract = {This study explores the coordination between manual pointing gestures and gestures of the vocal tract. Using a novel methodology that allows for concurrent collection of audio, kinematic body and speech articulator trajectories, we ask 1) which particular gesture (vowel gesture, consonant gesture, or tone gesture) the pointing gesture is coordinated with, and 2) with which landmarks the two gestures are coordinated (for example, whether the pointing gesture is coordinated to the speech gesture by the onset or maximum displacement). Preliminary results indicate coordination of the intonation gesture and the pointing gesture.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JBUHQ7XB\\Krivokapic et al. - 2016 - Speech and manual gesture coordination in a pointi.pdf},
  keywords = {Audio Media,Displacement mapping,Gesture recognition,Onset (audio),Pointing device,Psychologic Displacement,recurrent childhood visual pathway glioma,Tract (literature),vowels}
}

@article{krommydasLefthandednessAsthmaticChildren2003,
  title = {Left-Handedness in Asthmatic Children},
  author = {Krommydas, Georgios and Gourgoulianis, Konstantinos I. and Andreou, Georgia and Molyvdas, Paschalis-Adam},
  date = {2003},
  journaltitle = {Pediatric Allergy and Immunology},
  volume = {14},
  pages = {234--237},
  issn = {1399-3038},
  doi = {10.1034/j.1399-3038.2003.00013.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1034/j.1399-3038.2003.00013.x},
  urldate = {2020-09-11},
  abstract = {Left-handedness has been associated with asthma and allergic disorders. The Geschwind–Behan–Galaburda (GBG) hypothesis could explain this association. In view of previous findings, we investigated the distribution of laterality scores among asthmatic children and controls aged 4–8 years old. Seventy families with asthmatic children were administered the International Study of Asthma and Allergy in Childhood (ISAAC) questionnaire and the Edinburgh Left-handedness Inventory. A sample of 70 families with non-asthmatic, healthy children was used as controls. The majority of children had mild asthma. Ambidexterity was the main feature in the asthmatic children. A statistically significant difference in the laterality quotient (LQ) distribution was found in the group of asthmatic children with allergic rhinitis (LQ mean value in the asthmatic children with allergic rhinitis: 42.85 vs. 79.50 in the rest of the asthmatic children). These results suggest that there is a tendency towards left-handedness in asthmatic children and lend support to the GBG hypothesis.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1034/j.1399-3038.2003.00013.x},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SBVWCTYA\\Krommydas et al. - 2003 - Left-handedness in asthmatic children.pdf;C\:\\Users\\u668173\\Zotero\\storage\\UJTKUYV6\\j.1399-3038.2003.00013.html},
  keywords = {allergic rhinitis,asthma,childhood,eczema,left-handedness},
  langid = {english},
  number = {3}
}

@article{krumhanslCanDanceReflect1997,
  title = {Can {{Dance Reflect}} the {{Structural}} and {{Expressive Qualities}} of {{Music}}? {{A Perceptual Experiment}} on {{Balanchine}}'s {{Choreography}} of {{Mozart}}'s {{Divertimento No}}. 15},
  shorttitle = {Can {{Dance Reflect}} the {{Structural}} and {{Expressive Qualities}} of {{Music}}?},
  author = {Krumhansl, Carol L. and Schenck, Diana Lynn},
  date = {1997-03-01},
  journaltitle = {Musicae Scientiae},
  shortjournal = {Musicae Scientiae},
  volume = {1},
  pages = {63--85},
  publisher = {{SAGE Publications Ltd}},
  issn = {1029-8649},
  doi = {10.1177/102986499700100105},
  url = {https://doi.org/10.1177/102986499700100105},
  urldate = {2020-09-18},
  abstract = {A perceptual experiment investigated the structural and expressive mappings between music and dance. The Stimulus materials were based on the Minuetto from W. A. Mozart's Divertimento No. 15 choreographed by George Balanchine. Participants were assigned to one of three conditions: Music Only, Dance Only, and Both Music and Dance. They performed four on-line tasks: indicating the occurrence of section ends and new ideas, and judging the amount of tension and emotion expressed. Each of the tasks showed strong similarity across the three conditions, including the Music Only and the Dance Only conditions which contained none of the same Stimulus materials. Analysis of the music and dance uncovered a large variety of elements that define mappings between music and dance. These operate on different hierarchical levels and suggest non-accidental relationships between music and bodily movement. The Both Music and Dance condition could be predicted as a combination of the Music Only and Dance Only conditions, with a stronger contribution of the former. The findings for this excerpt suggest an additive, non-interactive relationship between the music and dance. All three conditions exhibited the same temporal pattern among the tasks. New ideas were introduced at section beginnings when levels of tension and emotion expressed were low. These levels tended to increase within sections, reaching a peak just before section ends. These results suggest that a general Schema of temporal Organization operates in both music and dance. Finally, the three conditions produced very similar judgments of the type of emotional response, supporting the idea that both music and dance can engage similar representations of emotions.},
  langid = {english},
  number = {1}
}

@article{kuberskiFittsLawTongue2019,
  title = {Fitts’ {{Law}} in {{Tongue Movements}} of {{Repetitive Speech}}},
  author = {Kuberski, Stephan R. and Gafos, Adamantios I.},
  date = {2019-10-01},
  journaltitle = {Phonetica},
  shortjournal = {Phonetica},
  pages = {1--20},
  issn = {0031-8388, 1423-0321},
  doi = {10.1159/000501644},
  url = {https://www.karger.com/Article/FullText/501644},
  urldate = {2021-02-27},
  abstract = {Fitts’ law, perhaps the most celebrated law of human motor control, expresses a relation between the kinematic property of speed and the non-kinematic, task-specific property of accuracy. We aimed to assess whether speech movements obey this law using a metronome-driven speech elicitation paradigm with a systematic speech rate control. Specifically, using the paradigm of repetitive speech, we recorded via electromagnetic articulometry speech movement data in sequences of the form /CV…/ from 6 adult speakers. These sequences were spoken at 8 distinct rates ranging from extremely slow to extremely fast. Our results demonstrate, first, that the present paradigm of extensive metronome-driven manipulations satisfies the crucial prerequisites for evaluating Fitts’ law in a subset of our elicited rates. Second, we uncover for the first time in speech evidence for Fitts’ law at the faster rates and specifically beyond a participant-specific critical rate. We find no evidence for Fitts’ law at the slowest metronome rates. Finally, we discuss implications of these results for models of speech.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\R4HI2QA4\\Kuberski and Gafos - 2019 - Fitts’ Law in Tongue Movements of Repetitive Speec.pdf},
  langid = {english}
}

@inproceedings{kucherenkoAnalyzingInputOutput2019,
  title = {Analyzing {{Input}} and {{Output Representations}} for {{Speech}}-{{Driven Gesture Generation}}},
  booktitle = {Proceedings of the 19th {{ACM International Conference}} on {{Intelligent Virtual Agents}}  - {{IVA}} '19},
  author = {Kucherenko, Taras and Hasegawa, Dai and Henter, Gustav Eje and Kaneko, Naoshi and Kjellström, Hedvig},
  date = {2019},
  pages = {97--104},
  publisher = {{ACM Press}},
  location = {{Paris, France}},
  doi = {10.1145/3308532.3329472},
  url = {http://dl.acm.org/citation.cfm?doid=3308532.3329472},
  urldate = {2019-10-01},
  eventtitle = {The 19th {{ACM International Conference}}},
  isbn = {978-1-4503-6672-4},
  langid = {english}
}

@article{kucherenkoMovingFastSlow2021,
  title = {Moving {{Fast}} and {{Slow}}: {{Analysis}} of {{Representations}} and {{Post}}-{{Processing}} in {{Speech}}-{{Driven Automatic Gesture Generation}}},
  shorttitle = {Moving {{Fast}} and {{Slow}}},
  author = {Kucherenko, Taras and Hasegawa, Dai and Kaneko, Naoshi and Henter, Gustav Eje and Kjellström, Hedvig},
  date = {2021-02-17},
  journaltitle = {International Journal of Human–Computer Interaction},
  volume = {0},
  pages = {1--17},
  publisher = {{Taylor \& Francis}},
  issn = {1044-7318},
  doi = {10.1080/10447318.2021.1883883},
  url = {https://doi.org/10.1080/10447318.2021.1883883},
  urldate = {2021-02-26},
  abstract = {This paper presents a novel framework for speech-driven gesture production, applicable to virtual agents to enhance human-computer interaction. Specifically, we extend recent deep-learning-based, data-driven methods for speech-driven gesture generation by incorporating representation learning. Our model takes speech as input and produces gestures as output, in the form of a sequence of 3D coordinates. We provide an analysis of different representations for the input (speech) and the output (motion) of the network by both objective and subjective evaluations. We also analyze the importance of smoothing of the produced motion. Our results indicated that the proposed method improved on our baseline in terms of objective measures. For example, it better captured the motion dynamics and better matched the motion-speed distribution. Moreover, we performed user studies on two different datasets. The studies confirmed that our proposed method is perceived as more natural than the baseline, although the difference in the studies was eliminated by appropriate post-processing: hip-centering and smoothing. We conclude that it is important to take both motion representation and post-processing into account when designing an automatic gesture-production method.},
  annotation = {\_eprint: https://doi.org/10.1080/10447318.2021.1883883},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I78EJHA8\\Kucherenko et al. - 2021 - Moving Fast and Slow Analysis of Representations .pdf;C\:\\Users\\u668173\\Zotero\\storage\\2WJ4RIC7\\10447318.2021.html},
  number = {0}
}

@incollection{kuglerConceptCoordinativeStructures1980,
  title = {On the Concept of Coordinative Structures as Dissipative Structures: {{I}}. {{Theoretical}} Lines of Convergence},
  shorttitle = {1 {{On}} the {{Concept}} of {{Coordinative Structures}} as {{Dissipative Structures}}},
  booktitle = {Advances in {{Psychology}}},
  author = {Kugler, P. N. and Scott Kelso, J. A. and Turvey, M. T.},
  editor = {Stelmach, George E. and Requin, Jean},
  date = {1980-01-01},
  volume = {1},
  pages = {3--47},
  publisher = {{North-Holland}},
  doi = {10.1016/S0166-4115(08)61936-6},
  url = {http://www.sciencedirect.com/science/article/pii/S0166411508619366},
  urldate = {2019-08-08},
  abstract = {A model construct for coordination and control is pursued according to three related guidelines: (1) that it directly address Bernstein's problem of how to explain the regulation of the many biokinematic degrees of freedom with minimal recourse to an “intelligent regulator”; (2) that it be miserly on the number of explanatory principles, sui generis; and (3) that it be consistent with established strictures of non-equilibrium thermodynamics, that is, physical principles that inform biological design. Argument is given that a group of muscles constrained to act as a unit, a coordinative structure, is a member of the class of thermodynamic engines qua dissipative structures and that this membership gives a principled basis for understanding the characteristics of coordination and control.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LEMVCFRN\\S0166411508619366.html},
  series = {Tutorials in {{Motor Behavior}}}
}

@book{kuglerInformationNaturalLaw1987,
  title = {Information, Natural Law, and the Self-Assembly of Rhythmic Movement},
  author = {Kugler, P. N. and Turvey, M. T.},
  date = {1987},
  publisher = {{Lawrence Erlbaum Associates, Inc.}},
  series = {Resources for Ecological Psychology}
}

@article{kumarDistantConnectivityMultiplestep2020,
  title = {Distant Connectivity and Multiple-Step Priming in Large-Scale Semantic Networks},
  author = {Kumar, Abhilasha A. and Balota, David A. and Steyvers, Mark},
  date = {2020},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {46},
  pages = {2261--2276},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1285(Electronic),0278-7393(Print)},
  doi = {10.1037/xlm0000793},
  abstract = {We examined 3 different network models of representing semantic knowledge (5,018-word directed and undirected step distance networks, and an association-correlation network) to predict lexical priming effects. In Experiment 1, participants made semantic relatedness judgments for word pairs with varying path lengths. Response latencies for judgments followed a quadratic relationship with network path lengths, replicating and extending a recent pattern reported by Kenett, Levi, Anaki, and Faust (2017) for an 800-word association-correlation network in Hebrew. In Experiment 2, participants identified target words in a progressive demasking task, immediately following a briefly presented prime (120 ms). Response latencies to identify the target showed a linear trend for all network path lengths. Importantly, there were statistically significant differences between relatively distant words in the step distance networks, for example, path lengths 4 and beyond, suggesting that association networks can indeed capture distant functional semantic relationships. Additional comparisons with 2 distributional models (LSA and word2vec) suggested that distributional models also successfully predicted response latencies, although there appear to be fundamental differences in the types of semantic relationships captured by the different models. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CS92XTDS\\2019-72838-001.html},
  keywords = {Judgment,Models,Response Latency,Semantic Networks,Semantic Priming,Test Construction,Words (Phonetic Units)},
  number = {12}
}

@article{kussnerMusiciansAreMore2014,
  title = {Musicians Are More Consistent: {{Gestural}} Cross-Modal Mappings of Pitch, Loudness and Tempo in Real-Time},
  shorttitle = {Musicians Are More Consistent},
  author = {Küssner, Mats B. and Tidhar, Dan and Prior, Helen M. and Leech-Wilkinson, Daniel},
  date = {2014},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00789},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00789/full},
  urldate = {2020-12-07},
  abstract = {Cross-modal mappings of auditory stimuli reveal valuable insights into how humans make sense of sound and music. Whereas researchers have investigated cross-modal mappings of sound features varied in isolation within paradigms such as speeded classification and forced-choice matching tasks, investigations of representations of concurrently varied sound features (e.g., pitch, loudness and tempo) with overt gestures—accounting for the intrinsic link between movement and sound—are scant. To explore the role of bodily gestures in cross-modal mappings of auditory stimuli we asked sixty-four musically trained and untrained participants to represent pure tones—continually sounding and concurrently varied in pitch, loudness and tempo—with gestures while the sound stimuli were played. We hypothesised musical training to lead to more consistent mappings between pitch and height, loudness and distance/height, and tempo and speed of hand movement and muscular energy. Our results corroborate previously reported pitch vs. height (higher pitch leading to higher elevation in space) and tempo vs. speed (increasing tempo leading to increasing speed of hand movement) associations, but also reveal novel findings pertaining to musical training which influenced consistency of pitch mappings, annulling a commonly observed bias for convex (i.e. rising-falling) pitch contours. Moreover, we reveal effects of interactions between musical parameters on cross-modal mappings (e.g., pitch and loudness on speed of hand movement), highlighting the importance of studying auditory stimuli concurrently varied in different musical parameters. Results are discussed in light of cross-modal cognition, with particular emphasis on studies within (embodied) music cognition. Implications for theoretical refinements and potential clinical applications are provided.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Y8SQ76G2\\Küssner et al. - 2014 - Musicians are more consistent Gestural cross-moda.pdf},
  keywords = {cross-modal mappings,embodied music cognition,Gesture,musical training,real-time mappings},
  langid = {english}
}

@incollection{ladefogedLinguisticAspectsRespiratory1968,
  title = {Linguistic Aspects of Respiratory Phenomena},
  booktitle = {Sound {{Production}} in {{Man}} ({{Ed}}. {{A}}. Bouhuys)},
  author = {Ladefoged, P.},
  date = {1968},
  pages = {141--151},
  publisher = {{New York Academy of Sciences}},
  location = {{New York}}
}

@article{lagierCoordinationPosturePhonation2010,
  title = {Coordination between {{Posture}} and {{Phonation}} in {{Vocal Effort Behavior}}},
  author = {Lagier, Aude and Vaugoyeau, Marianne and Ghio, Alain and Legou, Thierry and Giovanni, Antoine and Assaiante, Christine},
  date = {2010},
  journaltitle = {Folia Phoniatrica et Logopaedica},
  shortjournal = {FPL},
  volume = {62},
  pages = {195--202},
  publisher = {{Karger Publishers}},
  issn = {1021-7762, 1421-9972},
  doi = {10.1159/000314264},
  url = {https://www.karger.com/Article/FullText/314264},
  urldate = {2020-11-06},
  abstract = {\emph{Background:} Postural correlates of vocal effort are rarely described in the literature, while they are extensively dealt with in speech therapy. \emph{Objectives:} This study aims at determining whether body movement is a side effect of vocal effort or an integral part of communication effort behavior. The answer to this question is mainly based on correlations between posture and phonation. \emph{Method:} Twenty healthy subjects participated in this study. They had to communicate with a listener under 3 conditions requiring different levels of vocal effort. \emph{Results:} The vocal parameters increased and confirmed that the subjects had made a vocal effort. The kinematic parameters (amplitude and duration of body movement) increased with vocal effort. Lastly, vocal and kinematic characteristics were significantly correlated. \emph{Conclusion:} The close correlation of posture with vocal production shows that movement is not a mere consequence of vocal effort. Posture and voice are coordinated in communication behavior, and each body segment plays its specific role in the vocal effort behavior.},
  eprint = {20460932},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6Q9WXZXG\\Lagier et al. - 2010 - Coordination between Posture and Phonation in Voca.pdf;C\:\\Users\\u668173\\Zotero\\storage\\28Y3AVPJ\\314264.html},
  langid = {english},
  number = {4}
}

@article{lancasterRespiratoryMuscleActivity1995,
  title = {Respiratory Muscle Activity in Relation to Vocalization in Flying Bats.},
  author = {Lancaster, W. C. and Henson, O. W. and Keating, A. W.},
  date = {1995-01-01},
  journaltitle = {Journal of Experimental Biology},
  volume = {198},
  pages = {175--191},
  issn = {0022-0949, 1477-9145},
  url = {https://jeb.biologists.org/content/198/1/175},
  urldate = {2020-01-27},
  abstract = {Skip to Next Section The structure of the thoracic and abdominal walls of Pteronotus parnellii (Microchiroptera: Mormoopidae) was described with respect to their function in respiration and vocalization. We monitored electromyographic activity of respiratory and flight muscles in relation to echolocative vocalization. In flight, signals were telemetered with a small FM transmitter modified to summate the low-frequency myopotentials with biosonar signals from a ceramic-crystal microphone. Recordings were also made from the same bats confined to a small cage. Vocalizations were used as the parameter by which all muscle activities were correlated. A discrete burst of activity in the lateral abdominal wall muscles accompanied each vocalization. Diaphragmatic myopotentials occurred between groups of calls and did not coincide with activity of the abdominal wall or with vocalizations. Flight muscles were not active in resting bats. During flight, vocalizations and the abdominal muscle activity that accompanied them coincided with myopotentials of the pectoralis and serratus ventralis muscles. We propose that contractions of the lateral abdominal wall provide the primary power for the production of intense biosonar vocalization in flying and in stationary bats. In flight, synchronization of vocalization with activity of the pectoralis and serratus ventralis jointly contribute to the pressurization of the thoraco-abdominal cavity. This utilization of pressure that is normally generated in flight facilitates respiration and allows for the production of intense vocalizations with little additional energetic expenditure.},
  eprint = {7891034},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\S6EMXYR3\\Lancaster et al. - 1995 - Respiratory muscle activity in relation to vocaliz.pdf;C\:\\Users\\u668173\\Zotero\\storage\\7Q8YBGKR\\175.html},
  langid = {english},
  number = {1}
}

@article{langerEffectsLimbImmobilization2012,
  title = {Effects of Limb Immobilization on Brain Plasticity},
  author = {Langer, N. and Hänggi, J. and Müller, N. A. and Simmen, H. P. and Jäncke, L.},
  date = {2012-01-17},
  journaltitle = {Neurology},
  shortjournal = {Neurology},
  volume = {78},
  pages = {182--188},
  issn = {1526-632X},
  doi = {10.1212/WNL.0b013e31823fcd9c},
  abstract = {OBJECTIVE: Little is known about the effects of reduced sensory input and motor output in the human brain. Therefore, we conducted a longitudinal study to investigate whether limb immobilization after unilateral arm injury is reflected in structural plastic changes in gray matter (cortical thickness) and white matter (fractional anisotropy [FA]). METHODS: We examined 10 right-handed subjects with injury of the right upper extremity that required at least 14 days of limb immobilization. Subjects underwent 2 MRI examinations, the first within 48 hours postinjury and the second after an average time interval of 16 days of immobilization. Based on the MRI scans, we measured cortical thickness of sensorimotor regions and FA of the corticospinal tracts. RESULTS: After immobilization, we revealed a decrease in cortical thickness in the left primary motor and somatosensory area as well as a decrease in FA in the left corticospinal tract. In addition, the motor skill of the left (noninjured) hand improved and is related to increased cortical thickness and FA in the right motor cortex. CONCLUSIONS: The present study illustrates that cortical thickness of the sensorimotor cortex and FA of the corticospinal tract changed during right arm immobilization and that these changes are associated with skill transfer from the right to the left hand. Thus, immobilization induces rapid reorganization of the sensorimotor system. Given that limb immobilization is a standard intervention technique in constraint-induced therapy, therapists should be aware of both the positive and negative effects of this intervention.},
  eprint = {22249495},
  eprinttype = {pmid},
  keywords = {Adult,Arm Injuries,Casts; Surgical,Female,Humans,Immobilization,Male,Motor Cortex,Neuronal Plasticity,Somatosensory Cortex},
  langid = {english},
  number = {3}
}

@article{larssonBipedalStepsDevelopment2019,
  title = {Bipedal Steps in the Development of Rhythmic Behavior in Humans},
  author = {Larsson, Matz and Richter, Joachim and Ravignani, Andrea},
  date = {2019-01-01},
  journaltitle = {Music \& Science},
  shortjournal = {Music \& Science},
  volume = {2},
  pages = {2059204319892617},
  issn = {2059-2043},
  doi = {10.1177/2059204319892617},
  url = {https://doi.org/10.1177/2059204319892617},
  urldate = {2020-01-03},
  abstract = {We contrast two related hypotheses of the evolution of dance: H1: Maternal bipedal walking influenced the fetal experience of sound and associated movement patterns; H2: The human transition to bipedal gait produced more isochronous/predictable locomotion sound resulting in early music-like behavior associated with the acoustic advantages conferred by moving bipedally in pace. The cadence of walking is around 120 beats per minute, similar to the tempo of dance and music. Human walking displays long-term constancies. Dyads often subconsciously synchronize steps. The major amplitude component of the step is a distinctly produced beat. Human locomotion influences, and interacts with, emotions, and passive listening to music activates brain motor areas. Across dance-genres the footwork is most often performed in time to the musical beat. Brain development is largely shaped by early sensory experience, with hearing developed from week 18 of gestation. Newborns reacts to sounds, melodies, and rhythmic poems to which they have been exposed in utero. If the sound and vibrations produced by footfalls of a walking mother are transmitted to the fetus in coordination with the cadence of the motion, a connection between isochronous sound and rhythmical movement may be developed. Rhythmical sounds of the human mother locomotion differ substantially from that of nonhuman primates, while the maternal heartbeat heard is likely to have a similar isochronous character across primates, suggesting a relatively more influential role of footfall in the development of rhythmic/musical abilities in humans. Associations of gait, music, and dance are numerous. The apparent absence of musical and rhythmic abilities in nonhuman primates, which display little bipedal locomotion, corroborates that bipedal gait may be linked to the development of rhythmic abilities in humans. Bipedal stimuli in utero may primarily boost the ontogenetic development. The acoustical advantage hypothesis proposes a mechanism in the phylogenetic development.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\K8Z2Y2G9\\Larsson et al. - 2019 - Bipedal Steps in the Development of Rhythmic Behav.pdf},
  keywords = {Bipedal gait,evolution of dance,hominids,intra-uterine development,music},
  langid = {english}
}

@article{larssonDidHeartAsymmetry2017,
  title = {Did Heart Asymmetry Play a Role in the Evolution of Human Handedness?},
  author = {Larsson, Matz},
  date = {2017-12},
  journaltitle = {Journal of Cultural Cognitive Science},
  volume = {1},
  pages = {65--76},
  issn = {2520-100X, 2520-1018},
  doi = {10.1007/s41809-017-0009-z},
  url = {http://link.springer.com/10.1007/s41809-017-0009-z},
  urldate = {2020-09-11},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\337BDPS7\\Larsson - 2017 - Did heart asymmetry play a role in the evolution o.pdf},
  langid = {english},
  number = {2}
}

@article{larssonSelfgeneratedSoundsLocomotion2014,
  title = {Self-Generated Sounds of Locomotion and Ventilation and the Evolution of Human Rhythmic Abilities},
  author = {Larsson, Matz},
  date = {2014-01-01},
  journaltitle = {Animal Cognition},
  shortjournal = {Anim Cogn},
  volume = {17},
  pages = {1--14},
  issn = {1435-9456},
  doi = {10.1007/s10071-013-0678-z},
  url = {https://doi.org/10.1007/s10071-013-0678-z},
  urldate = {2020-09-11},
  abstract = {It has been suggested that the basic building blocks of music mimic sounds of moving humans, and because the brain was primed to exploit such sounds, they eventually became incorporated in human culture. However, that raises further questions. Why do genetically close, culturally well-developed apes lack musical abilities? Did our switch to bipedalism influence the origins of music? Four hypotheses are raised: (1) Human locomotion and ventilation can mask critical sounds in the environment. (2) Synchronization of locomotion reduces that problem. (3) Predictable sounds of locomotion may stimulate the evolution of synchronized behavior. (4) Bipedal gait and the associated sounds of locomotion influenced the evolution of human rhythmic abilities. Theoretical models and research data suggest that noise of locomotion and ventilation may mask critical auditory information. People often synchronize steps subconsciously. Human locomotion is likely to produce more predictable sounds than those of non-human primates. Predictable locomotion sounds may have improved our capacity of entrainment to external rhythms and to feel the beat in music. A sense of rhythm could aid the brain in distinguishing among sounds arising from discrete sources and also help individuals to synchronize their movements with one another. Synchronization of group movement may improve perception by providing periods of relative silence and by facilitating auditory processing. The adaptive value of such skills to early ancestors may have been keener detection of prey or stalkers and enhanced communication. Bipedal walking may have influenced the development of entrainment in humans and thereby the evolution of rhythmic abilities.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\S23DHQU3\\Larsson - 2014 - Self-generated sounds of locomotion and ventilatio.pdf},
  langid = {english},
  number = {1}
}

@article{larssonSelfgeneratedSoundsLocomotion2014a,
  title = {Self-Generated Sounds of Locomotion and Ventilation and the Evolution of Human Rhythmic Abilities},
  author = {Larsson, Matz},
  date = {2014},
  journaltitle = {Animal Cognition},
  volume = {17},
  pages = {1--14},
  publisher = {{Springer}},
  location = {{Germany}},
  issn = {1435-9456(Electronic),1435-9448(Print)},
  doi = {10.1007/s10071-013-0678-z},
  abstract = {It has been suggested that the basic building blocks of music mimic sounds of moving humans, and because the brain was primed to exploit such sounds, they eventually became incorporated in human culture. However, that raises further questions. Why do genetically close, culturally well-developed apes lack musical abilities? Did our switch to bipedalism influence the origins of music? Four hypotheses are raised: (1) Human locomotion and ventilation can mask critical sounds in the environment. (2) Synchronization of locomotion reduces that problem. (3) Predictable sounds of locomotion may stimulate the evolution of synchronized behavior. (4) Bipedal gait and the associated sounds of locomotion influenced the evolution of human rhythmic abilities. Theoretical models and research data suggest that noise of locomotion and ventilation may mask critical auditory information. People often synchronize steps subconsciously. Human locomotion is likely to produce more predictable sounds than those of non-human primates. Predictable locomotion sounds may have improved our capacity of entrainment to external rhythms and to feel the beat in music. A sense of rhythm could aid the brain in distinguishing among sounds arising from discrete sources and also help individuals to synchronize their movements with one another. Synchronization of group movement may improve perception by providing periods of relative silence and by facilitating auditory processing. The adaptive value of such skills to early ancestors may have been keener detection of prey or stalkers and enhanced communication. Bipedal walking may have influenced the development of entrainment in humans and thereby the evolution of rhythmic abilities. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\S8ZBIVPN\\Larsson - 2014 - Self-generated sounds of locomotion and ventilatio.pdf;C\:\\Users\\u668173\\Zotero\\storage\\9R8TJV5X\\2013-31349-001.html},
  keywords = {Animal Biological Rhythms,Auditory Stimulation,Brain,Locomotion,Music},
  number = {1}
}

@article{larssonSelfgeneratedSoundsLocomotion2014b,
  title = {Self-Generated Sounds of Locomotion and Ventilation and the Evolution of Human Rhythmic Abilities},
  author = {Larsson, Matz},
  date = {2014-01-01},
  journaltitle = {Animal Cognition},
  shortjournal = {Anim Cogn},
  volume = {17},
  pages = {1--14},
  issn = {1435-9456},
  doi = {10.1007/s10071-013-0678-z},
  url = {https://doi.org/10.1007/s10071-013-0678-z},
  urldate = {2020-09-20},
  abstract = {It has been suggested that the basic building blocks of music mimic sounds of moving humans, and because the brain was primed to exploit such sounds, they eventually became incorporated in human culture. However, that raises further questions. Why do genetically close, culturally well-developed apes lack musical abilities? Did our switch to bipedalism influence the origins of music? Four hypotheses are raised: (1) Human locomotion and ventilation can mask critical sounds in the environment. (2) Synchronization of locomotion reduces that problem. (3) Predictable sounds of locomotion may stimulate the evolution of synchronized behavior. (4) Bipedal gait and the associated sounds of locomotion influenced the evolution of human rhythmic abilities. Theoretical models and research data suggest that noise of locomotion and ventilation may mask critical auditory information. People often synchronize steps subconsciously. Human locomotion is likely to produce more predictable sounds than those of non-human primates. Predictable locomotion sounds may have improved our capacity of entrainment to external rhythms and to feel the beat in music. A sense of rhythm could aid the brain in distinguishing among sounds arising from discrete sources and also help individuals to synchronize their movements with one another. Synchronization of group movement may improve perception by providing periods of relative silence and by facilitating auditory processing. The adaptive value of such skills to early ancestors may have been keener detection of prey or stalkers and enhanced communication. Bipedal walking may have influenced the development of entrainment in humans and thereby the evolution of rhythmic abilities.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HBGSWYSY\\Larsson - 2014 - Self-generated sounds of locomotion and ventilatio.pdf},
  langid = {english},
  number = {1}
}

@incollection{lashleyProblemSerialOrder1951,
  title = {The Problem of Serial Order in Behavior},
  booktitle = {Cerebral Mechanisms in Behavior; the {{Hixon Symposium}}},
  author = {Lashley, K. S.},
  date = {1951},
  pages = {112--146},
  publisher = {{Wiley}},
  location = {{Oxford, England}},
  abstract = {A discussion of "the logical and orderly arrangement of thought and action" from the point of view that "the input is never into a quiescent or static system, but always into a system which is already actively excited and organized" and that "behavior is the result of interaction of this background of excitation with input from any designated stimulus." Particular emphasis is placed on the time factor in behavior. A panel discussion is included between pages 136-146. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@book{latashSynergy2008,
  title = {Synergy},
  author = {Latash, Mark L.},
  date = {2008-03-18},
  publisher = {{Oxford University Press}},
  abstract = {Synergy discusses a general problem in biology: The lack of an adequate language for formulating biologically specific problems. Written for an inquisitive reader who is not necessarily a professional in the area of movement studies, this book describes the recent progress in the control and coordination of human movement. The book begins with a brief history of movement studies and reviews the current central controversies in the area of control of movements with an emphasis on the equilibrium-point hypothesis. An operational definition of synergy is introduced and a method of analysis of synergies is described based on the uncontrolled manifold hypothesis. Further this method is used to characterize synergies in a variety of tasks including such common motor tasks as standing, pointing, reaching, standing-up, and manipulation of hand-held objects. Applications of this method to movements by persons with neurological disorders, persons with atypical development and healthy elderly persons are illustrated, as well as changes in motor synergies with practice. Possible neurophysiological mechanisms of synergies are also discussed with the focus on such conspicuous structures as the spinal cord, the cerebellum, the basal ganglia, and the cortex of the large hemispheres. A variety of models are discussed based on different computational and neurophysiological principles. Possible applications of the introduced definition of synergies to other areas such as perception and language are discussed.},
  eprint = {Z45Oj8yCQMIC},
  eprinttype = {googlebooks},
  isbn = {978-0-19-971556-5},
  keywords = {Medical / Neurology,Medical / Neuroscience},
  langid = {english},
  pagetotal = {429}
}

@article{launaySynchronyAdaptiveMechanism2016,
  title = {Synchrony as an {{Adaptive Mechanism}} for {{Large}}-{{Scale Human Social Bonding}}},
  author = {Launay, Jacques and Tarr, Bronwyn and Dunbar, Robin I. M.},
  date = {2016},
  journaltitle = {Ethology},
  volume = {122},
  pages = {779--789},
  issn = {1439-0310},
  doi = {10.1111/eth.12528},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/eth.12528},
  urldate = {2020-09-27},
  abstract = {Humans have developed a number of specific mechanisms that allow us to maintain much larger social networks than would be expected given our brain size. For our primate cousins, social bonding is primarily supported using grooming, and the bonding effect this produces is primarily mechanistically underpinned by the release of endorphins (although other neurohormones are also likely to be involved). Given large group sizes and time budgeting constraints, grooming is not viable as the primary social bonding mechanism in humans. Instead, during our evolutionary history, we developed other behaviours that helped us to feel connected to our social communities. Here, we propose that synchrony might act as direct means to encourage group cohesion by causing the release of neurohormones that influence social bonding. By acting on ancient neurochemical bonding mechanisms, synchrony can act as a primal and direct social bonding agent, and this might explain its recurrence throughout diverse human cultures and contexts (e.g. dance, prayer, marching, music-making). Recent evidence supports the theory that endorphins are released during synchronised human activities, including sport, but particularly during musical interaction. Thus, synchrony-based activities are likely to have developed due to the fact that they allow the release of these hormones in large-scale human communities, providing an alternative to social bonding mechanisms such as grooming.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/eth.12528},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TKHXMZXN\\Launay et al. - 2016 - Synchrony as an Adaptive Mechanism for Large-Scale.pdf;C\:\\Users\\u668173\\Zotero\\storage\\95GVEQPS\\eth.html},
  keywords = {endorphins,humans,social bonding,synchronisation},
  langid = {english},
  number = {10}
}

@article{leavellFirefliesThwartBat2018,
  title = {Fireflies Thwart Bat Attack with Multisensory Warnings},
  author = {Leavell, Brian C. and Rubin, Juliette J. and McClure, Christopher J. W. and Miner, Krystie A. and Branham, Marc A. and Barber, Jesse R.},
  date = {2018-08-01},
  journaltitle = {Science Advances},
  volume = {4},
  pages = {eaat6601},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aat6601},
  url = {https://advances.sciencemag.org/content/4/8/eaat6601},
  urldate = {2020-02-28},
  abstract = {Many defended animals prevent attacks by displaying warning signals that are highly conspicuous to their predators. We hypothesized that bioluminescing fireflies, widely known for their vibrant courtship signals, also advertise their noxiousness to echolocating bats. To test this postulate, we pit naïve big brown bats (Eptesicus fuscus) against chemically defended fireflies (Photinus pyralis) to examine whether and how these beetles transmit salient warnings to bats. We demonstrate that these nocturnal predators learn to avoid noxious fireflies using either vision or echolocation and that bats learn faster when integrating information from both sensory streams—providing fundamental evidence that multisensory integration increases the efficacy of warning signals in a natural predator-prey system. Our findings add support for a warning signal origin of firefly bioluminescence and suggest that bat predation may have driven evolution of firefly bioluminescence. Naïve bats learn to avoid noxious fireflies fastest when integrating bioluminescent and echo-derived warnings. Naïve bats learn to avoid noxious fireflies fastest when integrating bioluminescent and echo-derived warnings.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\K285A8AT\\Leavell et al. - 2018 - Fireflies thwart bat attack with multisensory warn.pdf;C\:\\Users\\u668173\\Zotero\\storage\\EA65PT2Y\\eaat6601.html},
  langid = {english},
  number = {8}
}

@article{lebartonAssociationsGrossMotor2016,
  title = {Associations between Gross Motor and Communicative Development in At-Risk Infants},
  author = {LeBarton, Eve Sauer and Iverson, Jana M.},
  date = {2016-08},
  journaltitle = {Infant Behavior and Development},
  shortjournal = {Infant Behavior and Development},
  volume = {44},
  pages = {59--67},
  issn = {01636383},
  doi = {10.1016/j.infbeh.2016.05.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0163638315300783},
  urldate = {2020-12-04},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SQ67HPGG\\LeBarton and Iverson - 2016 - Associations between gross motor and communicative.pdf},
  langid = {english}
}

@article{lecanuetFetalSensoryCompetencies1996,
  title = {Fetal Sensory Competencies},
  author = {Lecanuet, Jean-Pierre and Schaal, Benoist},
  date = {1996-09-01},
  journaltitle = {European Journal of Obstetrics \& Gynecology and Reproductive Biology},
  shortjournal = {European Journal of Obstetrics \& Gynecology and Reproductive Biology},
  volume = {68},
  pages = {1--23},
  issn = {0301-2115},
  doi = {10.1016/0301-2115(96)02509-2},
  url = {http://www.sciencedirect.com/science/article/pii/0301211596025092},
  urldate = {2020-06-30},
  abstract = {A growing body of evidence is available about the functioning of fetal sensory systems during gestation. This article aims at reviewing data concerning (i) the presence of potential sensory stimulation in the fetal milieu, (ii) the sequential functional development of the sensory systems and (iii) physiological and behavioral responses of fetuses to various types of stimulation. Human data are compared with data collected in other mammalian species. Most studies have investigated auditory and chemosensory (olfactory and gustatory) responsiveness of the fetus in the second half of gestation. They demonstrate that (i) motor and heart rate responsiveness depends on gestational age and characteristics of stimulation; (ii) fetal sensory experience has short- and long-term effects at morphological, functional and behavioral levels (for example transnatal learning). The clinical consequences of the fetal sensory functioning are developed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ADZN999G\\Lecanuet and Schaal - 1996 - Fetal sensory competencies.pdf;C\:\\Users\\u668173\\Zotero\\storage\\QJT2PBGB\\0301211596025092.html},
  keywords = {Audition,Behavioral states,Fetal learning,Fetus,Gustation,Olfaction,Sensory functioning,Somesthetic system,Vestibular system,Vision},
  langid = {english}
}

@article{leeAnticipatoryPosturalAdjustments2009,
  title = {Anticipatory Postural Adjustments to Arm Movement Reveal Complex Control of Paraspinal Muscles in the Thorax},
  author = {Lee, Linda-Joy and Coppieters, Michel W. and Hodges, Paul W.},
  date = {2009-02-01},
  journaltitle = {Journal of Electromyography and Kinesiology},
  shortjournal = {Journal of Electromyography and Kinesiology},
  volume = {19},
  pages = {46--54},
  issn = {1050-6411},
  doi = {10.1016/j.jelekin.2007.06.015},
  url = {https://www.sciencedirect.com/science/article/pii/S1050641107001083},
  urldate = {2021-03-01},
  abstract = {Anatomical and empirical data suggest that deep and superficial muscles may have different functions for thoracic spine control. This study investigated thoracic paraspinal muscle activity during anticipatory postural adjustments associated with arm movement. Electromyographic (EMG) recordings were made from the right deep (multifidus/rotatores) and superficial (longissimus) muscles at T5, T8, and T11 levels using fine-wire electrodes. Ten healthy participants performed fast unilateral and bilateral flexion and extension arm movements in response to a light. EMG amplitude was measured during 25ms epochs for 150ms before and 400ms after deltoid EMG onset. During arm flexion movements, multifidus and longissimus had two bursts of activity, one burst prior to deltoid and a late burst. With arm extension both muscles were active in a single burst after deltoid onset. There was differential activity with respect to direction of trunk rotation induced by arm movement. Right longissimus was most active with left arm movements and right multifidus was most active with right arm movements. All levels of the thorax responded similarly. We suggest that although thoracic multifidus and longissimus function similarly to control sagittal plane perturbations, these muscles are differentially active with rotational forces on the trunk.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FXYCNB8K\\Lee et al. - 2009 - Anticipatory postural adjustments to arm movement .pdf;C\:\\Users\\u668173\\Zotero\\storage\\K8GC7WRH\\S1050641107001083.html},
  keywords = {Electromyography,Longissimus,Multifidus,Postural control,Thoracic spine},
  langid = {english},
  number = {1}
}

@article{lemanActivatingRelaxingMusic2013,
  title = {Activating and {{Relaxing Music Entrains}} the {{Speed}} of {{Beat Synchronized Walking}}},
  author = {Leman, Marc and Moelants, Dirk and Varewyck, Matthias and Styns, Frederik and van Noorden, Leon and Martens, Jean-Pierre},
  date = {2013-07-10},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  pages = {e67932},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0067932},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0067932},
  urldate = {2020-09-25},
  abstract = {Inspired by a theory of embodied music cognition, we investigate whether music can entrain the speed of beat synchronized walking. If human walking is in synchrony with the beat and all musical stimuli have the same duration and the same tempo, then differences in walking speed can only be the result of music-induced differences in stride length, thus reflecting the vigor or physical strength of the movement. Participants walked in an open field in synchrony with the beat of 52 different musical stimuli all having a tempo of 130 beats per minute and a meter of 4 beats. The walking speed was measured as the walked distance during a time interval of 30 seconds. The results reveal that some music is ‘activating’ in the sense that it increases the speed, and some music is ‘relaxing’ in the sense that it decreases the speed, compared to the spontaneous walked speed in response to metronome stimuli. Participants are consistent in their observation of qualitative differences between the relaxing and activating musical stimuli. Using regression analysis, it was possible to set up a predictive model using only four sonic features that explain 60\% of the variance. The sonic features capture variation in loudness and pitch patterns at periods of three, four and six beats, suggesting that expressive patterns in music are responsible for the effect. The mechanism may be attributed to an attentional shift, a subliminal audio-motor entrainment mechanism, or an arousal effect, but further study is needed to figure this out. Overall, the study supports the hypothesis that recurrent patterns of fluctuation affecting the binary meter strength of the music may entrain the vigor of the movement. The study opens up new perspectives for understanding the relationship between entrainment and expressiveness, with the possibility to develop applications that can be used in domains such as sports and physical rehabilitation.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DPEULGTY\\Leman et al. - 2013 - Activating and Relaxing Music Entrains the Speed o.pdf;C\:\\Users\\u668173\\Zotero\\storage\\IRZZ44DP\\article.html},
  keywords = {Acoustics,Evolutionary rate,Gait analysis,Music cognition,Music perception,Regression analysis,Relaxation (psychology),Walking},
  langid = {english},
  number = {7}
}

@article{lenthPackageLsmeans2017,
  title = {Package ‘lsmeans'},
  author = {Lenth, R. and Lenth, M. R.},
  date = {2017},
  journaltitle = {The American Statistician},
  volume = {34},
  pages = {216--221},
  number = {4}
}

@article{leonardTemporalRelationBeat2011,
  title = {The Temporal Relation between Beat Gestures and Speech},
  author = {Leonard, Thomas and Cummins, Fred},
  date = {2011-12-01},
  journaltitle = {Language and Cognitive Processes},
  volume = {26},
  pages = {1457--1471},
  issn = {0169-0965},
  doi = {10.1080/01690965.2010.500218},
  url = {https://doi.org/10.1080/01690965.2010.500218},
  urldate = {2019-04-23},
  abstract = {The temporal relation between beat gestures and accompanying speech is examined in two experiments. In the first, we find that subjects are very quick to spot altered timing between gesture and speech if the gesture is later than normal, but are considerably less sensitive to alterations that result in an earlier gesture. This suggests an asymmetry in the expectation on the part of listeners/watchers and raises immediate questions about which elements within the speech are being perceived as linked to which elements in the gestural series. We therefore examine the variability between several kinematic landmarks in a beat gesture, and three potential anchor points in the accompanying speech. We find the least variable relationship obtains between the point of maximum extension of the gesture and the accompanying pitch accent. Together, these findings contribute to our understanding of both the production and perception of beat gestures along with speech, and support an account of speech communication as a strongly embodied activity.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RKA5ALLQ\\Leonard and Cummins - 2011 - The temporal relation between beat gestures and sp.pdf;C\:\\Users\\u668173\\Zotero\\storage\\6TTZCMG9\\01690965.2010.html},
  keywords = {Coordination,Embodiment,Gesture},
  number = {10}
}

@article{lepicTakingMeaningHand2016,
  title = {Taking Meaning in Hand: {{Iconic}} Motivations in Two-Handed Signs},
  shorttitle = {Taking Meaning in Hand},
  author = {Lepic, Ryan and Börstell, Carl and Belsitzman, Gal and Sandler, Wendy},
  date = {2016-01-01},
  journaltitle = {Sign Language \& Linguistics},
  volume = {19},
  pages = {37--81},
  publisher = {{John Benjamins}},
  issn = {1387-9316, 1569-996X},
  doi = {10.1075/sll.19.1.02lep},
  url = {https://www.jbe-platform.com/content/journals/10.1075/sll.19.1.02lep},
  urldate = {2020-12-29},
  abstract = {Traditionally in sign language research, the issue of whether a lexical sign is articulated with one hand or two has been treated as a strictly phonological matter. We argue that accounting for two-handed signs also requires considering meaning as a motivating factor. We report results from a Swadesh list comparison, an analysis of semantic patterns among two-handed signs, and a picture-naming task. Comparing four unrelated languages, we demonstrate that the two hands are recruited to encode various relationship types in sign language lexicons. We develop the general principle that inherently “plural” concepts are straightforwardly mapped onto our paired human hands, resulting in systematic use of the two hands across sign languages. In our analysis, “plurality” subsumes four primary relationship types — interaction, location, dimension, and composition — and we predict that signs with meanings that encompass these relationships — such as ‘meet’, ‘empty’, ‘large’, or ‘machine’ — will preferentially be two-handed in any sign language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DAQEWL5G\\sll.19.1.html},
  langid = {english},
  number = {1}
}

@book{lerdahlGenerativeTheoryTonal1983,
  title = {A {{Generative Theory}} of {{Tonal Music}}},
  author = {Lerdahl, Fred and Jackendoff, Ray S. and Jackendoff, Ray},
  date = {1983},
  publisher = {{MIT Press}},
  abstract = {A search for a grammar of music with the aid of generative linguistics.This work, which has become a classic in music theory since its original publication in 1983, models music understanding from the perspective of cognitive science.The point of departure is a search for the grammar of music with the aid of generative linguistics.The theory, which is illustrated with numerous examples from Western classical music, relates the aural surface of a piece to the musical structure unconsciously inferred by the experienced listener. From the viewpoint of traditional music theory, it offers many innovations in notation as well as in the substance of rhythmic and reductional theory.},
  eprint = {oVoGEAAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-262-62107-6},
  keywords = {Language Arts & Disciplines / Linguistics / General},
  langid = {english},
  pagetotal = {385}
}

@article{lerdahlTonalPitchSpace1988,
  title = {Tonal {{Pitch Space}}},
  author = {Lerdahl, Fred},
  date = {1988},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  volume = {5},
  pages = {315--349},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.2307/40285402},
  abstract = {Models of pitch space have been developed in music psychology to account for perceived proximity among pitches, chords, or regions. This article introduces a different model that (1) treats pitches, chords, and regions within one framework, (2) correlates with the experimental data, and (3) connects in interesting ways with a variety of music theories.},
  eprint = {40285402},
  eprinttype = {jstor},
  number = {3}
}

@article{leroiRevolutions2020,
  title = {On Revolutions},
  author = {Leroi, Armand M. and Lambert, Ben and Mauch, Matthias and Papadopoulou, Marina and Ananiadou, Sophia and Lindberg, Staffan I. and Lindenfors, Patrik},
  date = {2020-12},
  journaltitle = {Palgrave Communications},
  volume = {6},
  issn = {2055-1045},
  doi = {10.1057/s41599-019-0371-1},
  url = {http://www.nature.com/articles/s41599-019-0371-1},
  urldate = {2020-02-21},
  abstract = {Sometimes the normal course of events is disrupted by a particularly swift and profound change. Historians have often referred to such changes as “revolutions”, and, though they have identified many of them, they have rarely supported their claims with statistical evidence. Here, we present a method to identify revolutions based on a measure of multivariate rate of change called Foote novelty. We define revolutions as those periods of time when the value of this measure is, by a non-parametric test, shown to significantly exceed the background rate. Our method also identifies conservative periods when the rate of change is unusually low. We apply it to several quantitative data sets that capture long-term political, social and cultural changes and, in some of them, identify revolutions — both well known and not. Our method is general and can be applied to any phenomenon captured by multivariate time series data of sufficient quality.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YASBXFRN\\Leroi et al. - 2020 - On revolutions.pdf},
  langid = {english},
  number = {1}
}

@incollection{leveltLexicalAccessSpeech1993,
  title = {Lexical {{Access}} in {{Speech Production}}},
  booktitle = {Knowledge and {{Language}}: {{Volume I From Orwell}}’s {{Problem}} to {{Plato}}’s {{Problem}}},
  author = {Levelt, Willem J. M.},
  editor = {Reuland, Eric and Abraham, Werner},
  date = {1993},
  pages = {241--251},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-011-1840-8_11},
  url = {https://doi.org/10.1007/978-94-011-1840-8_11},
  urldate = {2020-09-11},
  abstract = {Lexical access in speech production proceeds at a rate of, on the average, two to three words per second. At this rate words are selected from a production lexicon which contains thousands, and probably tens of thousands, of words. These words are not only selected, but also phonologically encoded. This happens at a rate of about 15 speech sounds per second. The problem to be addressed in this chapter is how these high-rate and fairly accurate processes of lexical selection and phonological encoding are organized.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Q5GCZJD5\\Levelt - 1993 - Lexical Access in Speech Production.pdf},
  isbn = {978-94-011-1840-8},
  keywords = {Lexical Access,Lexical Decision,Lexical Item,Speech Production,Stimulus Onset Asynchrony},
  langid = {english}
}

@article{levinPuttingShoulderWheel1997,
  title = {Putting the Shoulder to the Wheel: A New Biomechanical Model for the Shoulder Girdle},
  shorttitle = {Putting the Shoulder to the Wheel},
  author = {Levin, S. M.},
  date = {1997},
  journaltitle = {Biomedical Sciences Instrumentation},
  shortjournal = {Biomed Sci Instrum},
  volume = {33},
  pages = {412--417},
  issn = {0067-8856},
  abstract = {The least successfully modeled joint complex has been the shoulder. In multi-segmented mathematical shoulder models rigid beams (the bones) act as a series of columns or levers to transmit forces or loads to the axial skeleton. Forces passing through the almost frictionless joints must, somehow, always be directed perfectly perpendicular to the joints as only loads directed at right angles to the surfaces could transfer across frictionless joints. Loads transmitted to the axial skeleton would have to pass through the moving ribs or the weak jointed clavicle and then through the ribs. A new model of the shoulder girdle, based on the tension icosahedron described by Buckminster Fuller, is proposed that permits the compression loads passing through the arm and shoulder to be transferred to the axial skeleton through its soft tissues. In this model the scapula 'floats' in the tension network of shoulder girdle muscles just as the hub of the wire wheel is suspended in its tension network of spokes. With this construct inefficient beams and levers are eliminated. A more energy efficient, load distributing, integrated, hierarchical system is created.},
  eprint = {9731395},
  eprinttype = {pmid},
  keywords = {Biomechanical Phenomena,Humans,Models; Biological,Models; Structural,Shoulder,Shoulder Joint},
  langid = {english}
}

@article{levinSignificanceClosedKinematic2017,
  title = {The Significance of Closed Kinematic Chains to Biological Movement and Dynamic Stability},
  author = {Levin, Stephen and de Solórzano, Susan Lowell and Scarr, Graham},
  date = {2017-07},
  journaltitle = {Journal of Bodywork and Movement Therapies},
  shortjournal = {J Bodyw Mov Ther},
  volume = {21},
  pages = {664--672},
  issn = {1532-9283},
  doi = {10.1016/j.jbmt.2017.03.012},
  abstract = {Closed kinematic chains (CKCs) are widely used in mechanical engineering because they provide a simple and efficient mechanism with multiple applications, but they are much less appreciated in living tissues. Biomechanical research has been dominated by the use of lever models and their kinematic analysis, which has largely ignored the geometric organization of these ubiquitous and evolutionary-conserved systems, yet CKCs contribute substantially to our understanding of biological motion. Closed-chain kinematics couple multiple parts into continuous mechanical loops that allow the structure itself to regulate complex movements, and are described in a wide variety of different organisms, including humans. In a biological context, CKCs are modular units nested within others at multiple size scales as part of an integrated movement system that extends throughout the organism and can act in synergy with the nervous system, where present. They provide an energy-efficient mechanism that enables multiple mechanical functions to be optimized during embryological development and increases evolutionary diversity.},
  eprint = {28750982},
  eprinttype = {pmid},
  keywords = {Biomechanical Phenomena,Biomechanics,Biotensegrity,Closed-chain,Computer Simulation,Development,Evolution,Four-bar,Humans,Joint,Kinematics,Kinesis,Linkage,Models; Biological,Morphology,Movement},
  langid = {english},
  number = {3},
  options = {useprefix=true}
}

@inbook{levinsonInteractionalFoundationsLanguage2019,
  title = {Interactional Foundations of Language: {{The}} Interaction Engine Hypothesis},
  shorttitle = {Interactional Foundations of Language},
  booktitle = {Human Language: {{From}} Genes and Brain to Behavior},
  author = {Levinson, Stephen C.},
  date = {2019-10},
  pages = {189--200},
  publisher = {{MIT Press}},
  url = {https://pure.mpg.de/pubman/faces/ViewItemFullPage.jsp?itemId=item_3171367_2},
  urldate = {2020-12-17},
  abstract = {Author: Levinson, Stephen C.; Genre: Book Chapter; Published in Print: 2019-10; Title: Interactional foundations of language: The interaction engine hypothesis},
  bookauthor = {Hagoort, P.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UMA3IZ55\\ViewItemFullPage.html},
  langid = {english}
}

@article{levinsonOriginHumanMultimodal2014,
  title = {The Origin of Human Multi-Modal Communication},
  author = {Levinson, Stephen C. and Holler, Judith},
  date = {2014-09-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0302},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4123681/},
  urldate = {2020-01-26},
  abstract = {One reason for the apparent gulf between animal and human communication systems is that the focus has been on the presence or the absence of language as a complex expressive system built on speech. But language normally occurs embedded within an interactional exchange of multi-modal signals. If this larger perspective takes central focus, then it becomes apparent that human communication has a layered structure, where the layers may be plausibly assigned different phylogenetic and evolutionary origins—especially in the light of recent thoughts on the emergence of voluntary breathing and spoken language. This perspective helps us to appreciate the different roles that the different modalities play in human communication, as well as how they function as one integrated system despite their different roles and origins. It also offers possibilities for reconciling the ‘gesture-first hypothesis’ with that of gesture and speech having evolved together, hand in hand—or hand in mouth, rather—as one system.},
  eprint = {25092670},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2UD3JMPX\\Levinson and Holler - 2014 - The origin of human multi-modal communication.pdf},
  number = {1651},
  pmcid = {PMC4123681}
}

@article{levinsonTimingTurntakingIts2015,
  title = {Timing in Turn-Taking and Its Implications for Processing Models of Language},
  author = {Levinson, Stephen C. and Torreira, Francisco},
  date = {2015},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00731},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00731/full},
  urldate = {2020-12-09},
  abstract = {The core niche for language use is in verbal interaction, involving the rapid exchange of turns at talking. This paper reviews the extensive literature about this system, adding new statistical analyses of behavioural data where they have been missing, demonstrating that turn-taking has the systematic properties originally noted by Sacks, Schegloff and Jefferson (1974; hereafter SSJ). This system poses some significant puzzles for current theories of language processing: the gaps between turns are short (of the order of 200 ms), but the latencies involved in language production are much longer (over 600 ms). This seems to imply that participants in conversation must predict (or ‘project’ as SSJ have it) the end of the current speaker’s turn in order to prepare their response in advance. This in turn implies some overlap between production and comprehension despite their use of common processing resources. Collecting together what is known behaviourally and experimentally about the system, the space for systematic explanations of language processing for conversation can be significantly narrowed, and we sketch some first model of the mental processes involved for the participant preparing to speak next.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DFSBLAYP\\Levinson and Torreira - 2015 - Timing in turn-taking and its implications for pro.pdf},
  keywords = {conversation,language comprehension.,language processing,Language production,turn-taking},
  langid = {english}
}

@article{levinsonTurntakingHumanCommunication2016,
  title = {Turn-Taking in Human Communication--{{Origins}} and Implications for Language Processing},
  author = {Levinson, Stephen C.},
  date = {2016-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends Cogn Sci},
  volume = {20},
  pages = {6--14},
  issn = {1879-307X},
  doi = {10.1016/j.tics.2015.10.010},
  abstract = {Most language usage is interactive, involving rapid turn-taking. The turn-taking system has a number of striking properties: turns are short and responses are remarkably rapid, but turns are of varying length and often of very complex construction such that the underlying cognitive processing is highly compressed. Although neglected in cognitive science, the system has deep implications for language processing and acquisition that are only now becoming clear. Appearing earlier in ontogeny than linguistic competence, it is also found across all the major primate clades. This suggests a possible phylogenetic continuity, which may provide key insights into language evolution.},
  eprint = {26651245},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RTX4PVSI\\Levinson - 2016 - Turn-taking in Human Communication--Origins and Im.pdf},
  keywords = {child language,Communication,conversation,Cooperative Behavior,Humans,Language,language evolution,language processing,pragmatics,turn-taking},
  langid = {english},
  number = {1}
}

@incollection{levinTensegrityNewBiomechanics2006,
  title = {Tensegrity: {{The}} New Biomechanics},
  booktitle = {Textbook of Muscularskeletal Medicine},
  author = {Levin, S. M.},
  editor = {Hutson, M. and Ellis, R.},
  date = {2006},
  pages = {69--80},
  publisher = {{Oxford University Press}},
  location = {{Oxford, England}}
}

@online{limWhyManyClick,
  title = {Why Many Click Farm Jobs Should Be Understood as Digital Slavery},
  author = {Lim, Ming},
  url = {http://theconversation.com/why-many-click-farm-jobs-should-be-understood-as-digital-slavery-83530},
  urldate = {2020-07-09},
  abstract = {The digital economy has created millions of jobs that involve intense competition, unregulated working conditions and extremely low rates of pay.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VI4ZMWVF\\why-many-click-farm-jobs-should-be-understood-as-digital-slavery-83530.html},
  langid = {english},
  organization = {{The Conversation}}
}

@article{linEffectsPostureNewborn2007,
  title = {Effects of {{Posture}} on {{Newborn Crying}}},
  author = {Lin, Hung-Chu and Green, James A.},
  date = {2007},
  journaltitle = {Infancy},
  volume = {11},
  pages = {175--189},
  issn = {1532-7078},
  doi = {10.1111/j.1532-7078.2007.tb00221.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1532-7078.2007.tb00221.x},
  urldate = {2020-12-04},
  abstract = {Acoustic properties of the cries of 14 infants were evaluated at both 2 and 4 weeks of age when the infants were lying in a supine position and when they were sitting upright in a car seat. In the upright position, infants' breathing was more rapid and showed less individual variability. The fundamental frequency of their cries increased in the upright position, but this increase was likely attributable to increased arousal or distress, not to posture per se. There were no differences in acoustic measures related to vocal tract shape in the supine versus upright positions. Across age, there was a decline in fundamental frequency. Individual difference stability of most acoustic measures was moderate to high. The importance of postural effects on the acoustic features of cries was discussed.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1532-7078.2007.tb00221.x},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GAKX3SIF\\Lin and Green - 2007 - Effects of Posture on Newborn Crying.pdf;C\:\\Users\\u668173\\Zotero\\storage\\GICZRJUE\\j.1532-7078.2007.tb00221.html},
  langid = {english},
  number = {2}
}

@inproceedings{linHowHitThat2020,
  title = {How to Hit That Beat: {{Testing}} Acoustic Anchors of Rhythmic Movement with Speech},
  shorttitle = {How to Hit That Beat},
  booktitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  author = {Lin, Chia-Yuan and Rathcke, Tamara},
  date = {2020-05-25},
  pages = {1--5},
  publisher = {{ISCA}},
  doi = {10.21437/SpeechProsody.2020-1},
  url = {http://www.isca-speech.org/archive/SpeechProsody_2020/abstracts/31.html},
  urldate = {2020-05-26},
  abstract = {Sensorimotor synchronisation with metronome and music have been extensively studied, while synchronisation with speech is still relatively poorly understood. The present study looks into the question how to define the best anchor of synchronised movement (finger tapping) in speech, and compares manually identified vowel onsets with four acoustic landmarks that were derived by different signal processing algorithms. Participants listened to repetitions of natural English sentences and were instructed to tap in synchrony with what they perceived to be the sentence beat. The time course of the sentences was tagged for a number of rhythmically relevant events, including vowel onsets, fastest energy increase (maxD), a combination of high local pitch and periodic energy (PPP), and the largest amplitude of intersyllabic and interstress timescales (IMF1 and IMF2). Vowel onsets and maxD showed consistent tapping patterns, while other landmarks performed worse than vowel onsets. These findings suggest that local energy changes shape sensorimotor synchronisation with speech and that energy contours might serve as anchors of rhythmic attention in spoken language.},
  eventtitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9MFVVMQC\\Lin and Rathcke - 2020 - How to hit that beat Testing acoustic anchors of .pdf},
  langid = {english}
}

@inproceedings{linHowHitThat2020a,
  title = {How to Hit That Beat: {{Testing}} Acoustic Anchors of Rhythmic Movement with Speech},
  shorttitle = {How to Hit That Beat},
  booktitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  author = {Lin, Chia-Yuan and Rathcke, Tamara},
  date = {2020-05-25},
  pages = {1--5},
  publisher = {{ISCA}},
  doi = {10.21437/SpeechProsody.2020-1},
  url = {http://www.isca-speech.org/archive/SpeechProsody_2020/abstracts/31.html},
  urldate = {2020-08-26},
  abstract = {Sensorimotor synchronisation with metronome and music have been extensively studied, while synchronisation with speech is still relatively poorly understood. The present study looks into the question how to define the best anchor of synchronised movement (finger tapping) in speech, and compares manually identified vowel onsets with four acoustic landmarks that were derived by different signal processing algorithms. Participants listened to repetitions of natural English sentences and were instructed to tap in synchrony with what they perceived to be the sentence beat. The time course of the sentences was tagged for a number of rhythmically relevant events, including vowel onsets, fastest energy increase (maxD), a combination of high local pitch and periodic energy (PPP), and the largest amplitude of intersyllabic and interstress timescales (IMF1 and IMF2). Vowel onsets and maxD showed consistent tapping patterns, while other landmarks performed worse than vowel onsets. These findings suggest that local energy changes shape sensorimotor synchronisation with speech and that energy contours might serve as anchors of rhythmic attention in spoken language.},
  eventtitle = {10th {{International Conference}} on {{Speech Prosody}} 2020},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\E4AAQB55\\Lin and Rathcke - 2020 - How to hit that beat Testing acoustic anchors of .pdf},
  langid = {english}
}

@article{lipkindDevelopmentStructuredVocalizations2020,
  title = {The {{Development}} of {{Structured Vocalizations}} in {{Songbirds}} and {{Humans}}: {{A Comparative Analysis}}},
  shorttitle = {The {{Development}} of {{Structured Vocalizations}} in {{Songbirds}} and {{Humans}}},
  author = {Lipkind, Dina and Geambasu, Andreea and Levelt, Clara C.},
  date = {2020},
  journaltitle = {Topics in Cognitive Science},
  volume = {12},
  pages = {894--909},
  issn = {1756-8765},
  doi = {10.1111/tops.12414},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12414},
  urldate = {2020-12-01},
  abstract = {Humans and songbirds face a common challenge: acquiring the complex vocal repertoire of their social group. Although humans are thought to be unique in their ability to convey symbolic meaning through speech, speech and birdsong are comparable in their acoustic complexity and the mastery with which the vocalizations of adults are acquired by young individuals. In this review, we focus on recent advances in the study of vocal development in humans and songbirds that shed new light on the emergence of distinct structural levels of vocal behavior and point to new possible parallels between both groups.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12414},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8652XQWI\\Lipkind et al. - 2020 - The Development of Structured Vocalizations in Son.pdf;C\:\\Users\\u668173\\Zotero\\storage\\J9WG79DI\\tops.html},
  keywords = {Acoustic structure,Birdsong,Human speech,Vocal development,Vocal learning},
  langid = {english},
  number = {3}
}

@article{liuF0relatedHeadMovement2020,
  title = {F0-Related Head Movement in Blind versus Sighted Speakers},
  author = {Liu, Yadong and Shamei, Arian and Chow, Una Y. and Soo, Rachel and Pineda Mora, Gina and de Boer, Gillian and Gick, Bryan},
  date = {2020-08-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {148},
  pages = {EL190-EL194},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/10.0001353},
  url = {https://asa.scitation.org/doi/full/10.1121/10.0001353},
  urldate = {2020-08-25},
  abstract = {This study investigated the relationship between head movement and fundamental frequency (F0) during speech by comparing continuous speech of congenitally blind and sighted speakers from YouTube videos. Positive correlations were found between F0 (measured in semitones) and vertical head movement for both speaker groups, with a stronger correlation for blind speakers. In addition, larger head movements and larger head movement per semitone ratios were observed for sighted speakers. These results suggest that physiological processes may account for part of the F0-related head movement and that sighted speakers use the visual modality to communicate F0 information through augmented head movement.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FSBE9B7D\\Liu et al. - 2020 - F0-related head movement in blind versus sighted s.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XKJWJYKV\\10.html},
  number = {2},
  options = {useprefix=true}
}

@article{localHowPhoneticFeatures2012,
  title = {How Phonetic Features Project More Talk},
  author = {Local, John and Walker, Gareth},
  date = {2012-12},
  journaltitle = {Journal of the International Phonetic Association},
  volume = {42},
  pages = {255--280},
  issn = {0025-1003, 1475-3502},
  doi = {10.1017/S0025100312000187},
  url = {https://www.cambridge.org/core/product/identifier/S0025100312000187/type/journal_article},
  urldate = {2020-10-27},
  abstract = {Investigations into the management of turn-taking have typically focussed on pitch and other prosodic phenomena, particularly pitch-accents. Here, non-pitch phonetic features and their role in turn-taking are described. Through sustained phonetic and interactional analysis of a naturally occurring, 12 minute long, telephone call between two adult speakers of British English sets of talk-projecting and turn-projecting features are identified. Talk-projecting features include the avoidance of durational lengthening, articulatory anticipation, continuation of voicing, the production of talk in maximally close proximity to a preceding point of possible turn-completion, and the reduction of consonants and vowels. Turnprojecting features include the converse of each of the talk-projecting features, and two other distinct features: release of plosives at the point of possible turn-completion, and the production of audible outbreaths. We show that features of articulatory and phonatory quality and duration are relevant factors in the design and treatment of talk as talk- or turn-projective.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DNTJH7Q9\\Local and Walker - 2012 - How phonetic features project more talk.pdf},
  langid = {english},
  number = {3}
}

@article{lockeEmergentControlManual1995,
  title = {Emergent {{Control}} of {{Manual}} and {{Vocal}}-{{Motor Activity}} in {{Relation}} to the {{Development}} of {{Speech}}},
  author = {Locke, J. L. and Bekken, K. E. and Mcminnlarson, L. and Wein, D.},
  date = {1995-12-01},
  journaltitle = {Brain and Language},
  shortjournal = {Brain and Language},
  volume = {51},
  pages = {498--508},
  issn = {0093-934X},
  doi = {10.1006/brln.1995.1073},
  url = {http://www.sciencedirect.com/science/article/pii/S0093934X85710735},
  urldate = {2020-12-02},
  abstract = {Babbling typically precedes, resembles, and conceivably facilitates development of speech, and yet there is no accepted neurobiological characterization of babbling. Here we report a study of infants′ developing control of vocal behavior in relation to manual activity performed under differing conditions of audibility. We hypothesized that babbling is associated with the onset of left-lateralized motor control, as expressed in repetitive right-handed activity, and that audibility facilitates such activity, Sixty-one normally developing infants were seen before (N = 21) or at various intervals following (N = 40) the onset of babbling. In experimental trials, audible or inaudible rattles were placed in left or right hands equally often. Analysis of manual activity revealed little shaking movement in the youngest and vocally least differentiated infants, and a sharp increase in shaking in slightly older infants who had recently begun to babble. Surprisingly, audibility only marginally enhanced shaking activity. A dextral bias was evident in the shaking of infants who had recently begun to babble, but not in younger or older infants. These and other findings suggest that the left cerebral hemisphere may be disproportionately involved in the production of repetitive vocal-motor activity as occurs in babbling.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FC8MSNM4\\S0093934X85710735.html},
  langid = {english},
  number = {3}
}

@article{loehrTemporalStructuralPragmatic2012,
  title = {Temporal, Structural, and Pragmatic Synchrony between Intonation and Gesture},
  author = {Loehr, Daniel P.},
  date = {2012},
  journaltitle = {Laboratory Phonology},
  volume = {3},
  pages = {71--89},
  issn = {1868-6346},
  doi = {10.1515/lp-2012-0006},
  url = {https://www.degruyter.com/view/j/lp.2012.3.issue-1/lp-2012-0006/lp-2012-0006.xml},
  urldate = {2019-04-23},
  abstract = {This paper explores the interaction between intonation and gesture, noting temporal, structural, and pragmatic synchrony. Videotapes of subjects conversing freely were annotated for intonation according to ToBI (Beckman and Elam 1997), and for gesture according to Kendon (1980) and McNeill (1992). The time-stamped annotations were analyzed statistically, as well as visually in the Anvil tool (Kipp 2001), which allows time-aligned viewing of videos with their annotations. Alignments were investigated between three levels of intonational units and four levels of gestural units. The intonational units were, from smallest to largest, pitch accents, intermediate phrases, and intonational phrases. The gestural units, again from smallest to largest, were apices of strokes, gesture phases, gesture phrases, and gesture units. Of these possible combinations, two pairs aligned. Apices clearly aligned with pitch accents, and gesture phrases tended to align with intermediate phrases. The existence of intermediate phrases in English has been the subject of some debate (Ladd 2008), and this paper suggests that a probable gestural correlate to intermediate phrases provides independent evidence for their existence. In addition, intonation and gesture were found to perform simultaneous complementary pragmatic functions. This temporal, structural, and pragmatic synchrony between the two channels reinforces the claim that speech and gesture are two surface forms of the same underlying and emerging cognitive content.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FKELJLTZ\\Loehr - 2012 - Temporal, structural, and pragmatic synchrony betw.pdf},
  number = {1}
}

@book{londonHearingTimePsychological2012,
  title = {Hearing in Time: {{Psychological}} Aspects of Musical Metre},
  author = {London, J.},
  date = {2012},
  edition = {2},
  location = {{Oxford, UK}}
}

@article{longoVoiceParameterChanges2020,
  title = {Voice {{Parameter Changes}} in {{Professional Musician}}-{{Singers Singing}} with and without an {{Instrument}}: {{The Effect}} of {{Body Posture}}},
  shorttitle = {Voice {{Parameter Changes}} in {{Professional Musician}}-{{Singers Singing}} with and without an {{Instrument}}},
  author = {Longo, Lucia and Di Stadio, Arianna and Ralli, Massimo and Marinucci, Irene and Ruoppolo, Giovanni and Dipietro, Laura and de Vincentiis, Marco and Greco, Antonio},
  date = {2020},
  journaltitle = {Folia Phoniatrica et Logopaedica},
  shortjournal = {FPL},
  volume = {72},
  pages = {309--315},
  publisher = {{Karger Publishers}},
  issn = {1021-7762, 1421-9972},
  doi = {10.1159/000501202},
  url = {https://www.karger.com/Article/FullText/501202},
  urldate = {2020-10-29},
  abstract = {\textbf{\emph{Background and Aim:}} The impact of body posture on vocal emission is well known. Postural changes may increase muscular resistance in tracts of the phono-articulatory apparatus and lead to voice disorders. This work aimed to assess whether and to which extent body posture during singing and playing a musical instrument impacts voice performance in professional musicians. \textbf{\emph{Subjects and Methods:}} Voice signals were recorded from 17 professional musicians (pianists and guitarists) while they were singing and while they were singing and playing a musical instrument simultaneously. Metrics were extracted from their voice spectrogram using the Multi-Dimensional Voice Program (MDVP) and included jitter, shift in fundamental voice frequency (sF0), shimmer, change in peak amplitude, noise to harmonic ratio, Voice Turbulence Index, Soft Phonation Index (SPI), Frequency Tremor Intensity Index, Amplitude Tremor Intensity Index, and maximum phonatory time (MPT). Statistical analysis was performed using two-tailed \emph{t} tests, one-way ANOVA, and χ\textsuperscript{2} tests. Subjects’ body posture was visually assessed following the recommendations of the Italian Society of Audiology and Phoniatrics. Thirty-seven voice signals were collected, 17 during singing and 20 during singing and playing a musical instrument. \textbf{\emph{Results:}} Data showed that playing an instrument while singing led to an impairment of the “singer formant” and to a decrease in jitter, sF0, shimmer, SPI, and MPT. However, statistical analysis showed that none of the MDVP metrics changed significantly when subjects played an instrument compared to when they did not. Shoulder and back position affected voice features as measured by the MDVP metrics, while head and neck position did not. In particular, playing the guitar decreased the amplitude of the “singer formant” and increased noise, causing a typical “raucous rock voice.” \textbf{\emph{Conclusions:}} Voice features may be affected by the use of the instrument the musicians play while they sing. Body posture selected by the musician while playing the instrument may affect expiration and phonation.},
  eprint = {31307041},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3CDT6LMI\\501202.html},
  langid = {english},
  number = {4},
  options = {useprefix=true}
}

@article{lorasTimingContinuousDiscontinuous2012,
  title = {Timing Continuous or Discontinuous Movements across Effectors Specified by Different Pacing Modalities and Intervals},
  author = {Lorås, H. and Sigmundsson, H. and Talcott, J. B. and Öhberg, F. and Stensdotter, A. K.},
  date = {2012-08-01},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {220},
  pages = {335--347},
  issn = {1432-1106},
  doi = {10.1007/s00221-012-3142-4},
  url = {https://doi.org/10.1007/s00221-012-3142-4},
  urldate = {2020-05-19},
  abstract = {Sensorimotor synchronization is hypothesized to arise through two different processes, associated with continuous or discontinuous rhythmic movements. This study investigated synchronization of continuous and discontinuous movements to different pacing signals (auditory or visual), pacing interval (500, 650, 800, 950~ms) and across effectors (non-dominant vs. non-dominant hand). The results showed that mean and variability of asynchronization errors were consistently smaller for discontinuous movements compared to continuous movements. Furthermore, both movement types were timed more accurately with auditory pacing compared to visual pacing and were more accurate with the dominant hand. Shortening the pacing interval also improved sensorimotor synchronization accuracy in both continuous and discontinuous movements. These results show the dependency of temporal control of movements on the nature of the motor task, the type and rate of extrinsic sensory information as well as the efficiency of the motor actuators for sensory integration.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IZFZPSYK\\Lorås et al. - 2012 - Timing continuous or discontinuous movements acros.pdf},
  langid = {english},
  number = {3}
}

@article{lou-magnusonSocialNetworkLimits2018,
  title = {Social {{Network Limits Language Complexity}}},
  author = {Lou‐Magnuson, Matthew and Onnis, Luca},
  date = {2018},
  journaltitle = {Cognitive Science},
  volume = {42},
  pages = {2790--2817},
  issn = {1551-6709},
  doi = {10.1111/cogs.12683},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12683},
  urldate = {2020-03-06},
  abstract = {Natural languages vary widely in the degree to which they make use of nested compositional structure in their grammars. It has long been noted by linguists that the languages historically spoken in small communities develop much deeper levels of compositional embedding than those spoken by larger groups. Recently, this observation has been confirmed by a robust statistical analysis of the World Atlas of Language Structures. In order to examine this connection mechanistically, we propose an agent-based model that accounts for key cultural evolutionary features of language transfer and language change. We identify transitivity as a physical parameter of social networks critical for the evolution of compositional structure and the hierarchical patterning of scale-free distributions as inhibitory.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12683},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QKZ2DILP\\Lou‐Magnuson and Onnis - 2018 - Social Network Limits Language Complexity.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZDB2VL9A\\cogs.html},
  keywords = {Agent-based model,Grammaticalization,Language change,Language complexity,Language evolution,Social network},
  langid = {english},
  number = {8}
}

@article{luckingDatabasedAnalysisSpeech2013,
  title = {Data-Based Analysis of Speech and Gesture: The {{Bielefeld Speech}} and {{Gesture Alignment}} Corpus ({{SaGA}}) and Its Applications},
  shorttitle = {Data-Based Analysis of Speech and Gesture},
  author = {Lücking, Andy and Bergman, Kirsten and Hahn, Florian and Kopp, Stefan and Rieser, Hannes},
  date = {2013-03-01},
  journaltitle = {Journal on Multimodal User Interfaces},
  shortjournal = {J Multimodal User Interfaces},
  volume = {7},
  pages = {5--18},
  issn = {1783-8738},
  doi = {10.1007/s12193-012-0106-8},
  url = {https://doi.org/10.1007/s12193-012-0106-8},
  urldate = {2021-03-03},
  abstract = {Communicating face-to-face, interlocutors frequently produce multimodal meaning packages consisting of speech and accompanying gestures. We discuss a systematically annotated speech and gesture corpus consisting of 25 route-and-landmark-description dialogues, the Bielefeld Speech and Gesture Alignment corpus (SaGA), collected in experimental face-to-face settings. We first describe the primary and secondary data of the corpus and its reliability assessment. Then we go into some of the projects carried out using SaGA demonstrating the wide range of its usability: on the empirical side, there is work on gesture typology, individual and contextual parameters influencing gesture production and gestures’ functions for dialogue structure. Speech-gesture interfaces have been established extending unification-based grammars. In addition, the development of a computational model of speech-gesture alignment and its implementation constitutes a research line we focus on.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\C3L3E63T\\Lücking et al. - 2013 - Data-based analysis of speech and gesture the Bie.pdf},
  langid = {english},
  number = {1}
}

@article{luckPerceptionExpressionConductors2010,
  title = {Perception of {{Expression}} in {{Conductors}}' {{Gestures}}: {{A Continuous Response Study}}},
  shorttitle = {Perception of {{Expression}} in {{Conductors}}' {{Gestures}}},
  author = {Luck, Geoff and Toiviainen, Petri and Thompson, Marc R.},
  date = {2010},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  volume = {28},
  pages = {47--57},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.1525/mp.2010.28.1.47},
  abstract = {The visual channel has been shown to be more informative than the auditory channel in perceptual judgments of a performer's level of expression. Previous work has revealed a positive relationship between amplitude of music-related movement and ratings of expression, for example, and observers have been shown to be sensitive to kinematic features of music-related movement. In this study, we investigate relationships between the kinematics of a conductors' expressive gestures and ratings of perceived expression. Point-light representations (totalling 10 minutes) of two professional conductors were presented to participants who provided continuous ratings of perceived valence, activity, power, and overall expression using a virtual slider interface. Relationships between these ratings and 11 kinematic variables computationally extracted from the movement data were subsequently examined using linear regression. Higher levels of expressivity were found to be conveyed by gestures characterized by increased amplitude, greater variance, and higher speed of movement.},
  eprint = {10.1525/mp.2010.28.1.47},
  eprinttype = {jstor},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9AVTY6KL\\Luck et al. - 2010 - Perception of Expression in Conductors' Gestures .pdf},
  number = {1}
}

@article{luckSpatioTemporalCuesVisually2009,
  title = {Spatio-{{Temporal Cues}} for {{Visually Mediated Synchronization}}},
  author = {Luck, Geoff and Sloboda, John A.},
  date = {2009-06-01},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  volume = {26},
  pages = {465--473},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.1525/mp.2009.26.5.465},
  url = {/mp/article/26/5/465/62442/Spatio-Temporal-Cues-for-Visually-Mediated},
  urldate = {2020-12-07},
  abstract = {THE ACCURACY WITH WHICH INDIVIDUALS ARE ABLE to synchronize with each other using vision alone is well documented. Less attention, however, has been given to the spatio-temporal characteristics of human movement that offer cues for such synchronization. The present study investigated such cues in the context of conductor-musician synchronization. Twenty-four participants tapped in time with dynamic point-light representations of traditional conducting gestures, in which the clarity of the beat and overall tempo was manipulated. A series of nine linear regression analyses identified absolute acceleration along the trajectory as the main cue for synchronization, while beat clarity and tempo influenced the weights of the variables in the emergent models.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BM4GPHB4\\Spatio-Temporal-Cues-for-Visually-Mediated.html},
  langid = {english},
  number = {5}
}

@article{lugoShoulderBiomechanics2008,
  title = {Shoulder Biomechanics},
  author = {Lugo, Roberto and Kung, Peter and Ma, C. Benjamin},
  date = {2008-10-01},
  journaltitle = {European Journal of Radiology},
  shortjournal = {European Journal of Radiology},
  volume = {68},
  pages = {16--24},
  issn = {0720-048X},
  doi = {10.1016/j.ejrad.2008.02.051},
  url = {http://www.sciencedirect.com/science/article/pii/S0720048X08001277},
  urldate = {2020-04-21},
  abstract = {The biomechanics of the glenohumeral joint depend on the interaction of both static and dynamic-stabilizing structures. Static stabilizers include the bony anatomy, negative intra-articular pressure, the glenoid labrum, and the glenohumeral ligaments along with the joint capsule. The dynamic-stabilizing structures include the rotator cuff muscles and the other muscular structures surrounding the shoulder joint. The combined effect of these stabilizers is to support the multiple degrees of motion within the glenohumeral joint. The goal of this article is to review how these structures interact to provide optimal stability and how failure of some of these mechanisms can lead to shoulder joint pathology.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BD3KB837\\Lugo et al. - 2008 - Shoulder biomechanics.pdf;C\:\\Users\\u668173\\Zotero\\storage\\MJ3CNYRB\\S0720048X08001277.html},
  keywords = {Biomechanics,Glenohumeral joint,Shoulder},
  langid = {english},
  number = {1},
  series = {Shoulder {{Imaging}}}
}

@article{lumExtractingInsightsShape2013,
  title = {Extracting Insights from the Shape of Complex Data Using Topology},
  author = {Lum, P. Y. and Singh, G. and Lehman, A. and Ishkanov, T. and Vejdemo-Johansson, M. and Alagappan, M. and Carlsson, J. and Carlsson, G.},
  date = {2013-02-07},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {3},
  issn = {2045-2322},
  doi = {10.1038/srep01236},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3566620/},
  urldate = {2020-03-11},
  abstract = {This paper applies topological methods to study complex high dimensional data sets by extracting shapes (patterns) and obtaining insights about them. Our method combines the best features of existing standard methodologies such as principal component and cluster analyses to provide a geometric representation of complex data sets. Through this hybrid method, we often find subgroups in data sets that traditional methodologies fail to find. Our method also permits the analysis of individual data sets as well as the analysis of relationships between related data sets. We illustrate the use of our method by applying it to three very different kinds of data, namely gene expression from breast tumors, voting data from the United States House of Representatives and player performance data from the NBA, in each case finding stratifications of the data which are more refined than those produced by standard methods.},
  eprint = {23393618},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9FGPYUKT\\Lum et al. - 2013 - Extracting insights from the shape of complex data.pdf},
  pmcid = {PMC3566620}
}

@article{luptonMotorLearningSign1990,
  title = {Motor {{Learning}} in {{Sign Language Students}}},
  author = {Lupton, Linda K. and Zelaznik, Howard N.},
  date = {1990},
  journaltitle = {Sign Language Studies},
  volume = {1067},
  pages = {153--174},
  issn = {1533-6263},
  doi = {10.1353/sls.1990.0020},
  url = {http://muse.jhu.edu/content/crossref/journals/sign_language_studies/v1067/67.lupton.html},
  urldate = {2020-03-09},
  abstract = {We examined the changes in movement trajectories of two initially naive students from near the beginning until the end of an introductory course in American Sign Language. The movement patterns increased in speed, symmetry, replicability, and grew more constrained in movement amplitude as the semester progressed. Analysis of phase portraits (graphs of displacement versus velocity) revealed increasing limit-cycle behavior for complicated two-handed signs over the testing sessions. One-handed signs, however, exhibited limit-cycle behavior from the first session.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FFYANREN\\Lupton and Zelaznik - 1990 - Motor Learning in Sign Language Students.pdf},
  langid = {english},
  number = {1}
}

@article{lupyanLanguageStructurePartly2010,
  title = {Language {{Structure Is Partly Determined}} by {{Social Structure}}},
  author = {Lupyan, G. and Dale, R.},
  date = {2010-01-20},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {5},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0008559},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2798932/},
  urldate = {2020-03-18},
  abstract = {Background Languages differ greatly both in their syntactic and morphological systems and in the social environments in which they exist. We challenge the view that language grammars are unrelated to social environments in which they are learned and used. Methodology/Principal Findings We conducted a statistical analysis of {$>$}2,000 languages using a combination of demographic sources and the World Atlas of Language Structures— a database of structural language properties. We found strong relationships between linguistic factors related to morphological complexity, and demographic/socio-historical factors such as the number of language users, geographic spread, and degree of language contact. The analyses suggest that languages spoken by large groups have simpler inflectional morphology than languages spoken by smaller groups as measured on a variety of factors such as case systems and complexity of conjugations. Additionally, languages spoken by large groups are much more likely to use lexical strategies in place of inflectional morphology to encode evidentiality, negation, aspect, and possession. Our findings indicate that just as biological organisms are shaped by ecological niches, language structures appear to adapt to the environment (niche) in which they are being learned and used. As adults learn a language, features that are difficult for them to acquire, are less likely to be passed on to subsequent learners. Languages used for communication in large groups that include adult learners appear to have been subjected to such selection. Conversely, the morphological complexity common to languages used in small groups increases redundancy which may facilitate language learning by infants. Conclusions/Significance We hypothesize that language structures are subjected to different evolutionary pressures in different social environments. Just as biological organisms are shaped by ecological niches, language structures appear to adapt to the environment (niche) in which they are being learned and used. The proposed Linguistic Niche Hypothesis has implications for answering the broad question of why languages differ in the way they do and makes empirical predictions regarding language acquisition capacities of children versus adults.},
  eprint = {20098492},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I5NTQZLW\\Lupyan and Dale - 2010 - Language Structure Is Partly Determined by Social .pdf},
  number = {1},
  pmcid = {PMC2798932}
}

@article{maatenVisualizingDataUsing2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  date = {2008},
  journaltitle = {Journal of Machine Learning Research},
  volume = {9},
  pages = {2579--2605},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
  urldate = {2020-12-29},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7CH4L4N7\\Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf;C\:\\Users\\u668173\\Zotero\\storage\\AUSR26ZN\\vandermaaten08a.html;C\:\\Users\\u668173\\Zotero\\storage\\IECUYRGQ\\vandermaaten08a.html},
  number = {86}
}

@article{macdougallMarchingBeatSame2005,
  title = {Marching to the Beat of the Same Drummer: The Spontaneous Tempo of Human Locomotion},
  shorttitle = {Marching to the Beat of the Same Drummer},
  author = {MacDougall, Hamish G. and Moore, Steven T.},
  date = {2005-09},
  journaltitle = {Journal of Applied Physiology (Bethesda, Md.: 1985)},
  shortjournal = {J Appl Physiol (1985)},
  volume = {99},
  pages = {1164--1173},
  issn = {8750-7587},
  doi = {10.1152/japplphysiol.00138.2005},
  abstract = {Laboratory studies have suggested that the preferred cadence of walking is approximately 120 steps/min, and the vertical acceleration of the head exhibits a dominant peak at this step frequency (2 Hz). These studies have been limited to short periods of walking along a predetermined path or on a treadmill, and whether such a highly tuned frequency of movement can be generalized to all forms of locomotion in a natural setting is unknown. The aim of this study was to determine whether humans exhibit a preferred cadence during extended periods of uninhibited locomotor activity and whether this step frequency is consistent with that observed in laboratory studies. Head linear acceleration was measured over a 10-h period in 20 subjects during the course of a day, which encompassed a broad range of locomotor (walking, running, cycling) and nonlocomotor (working at a desk, driving a car, riding a bus or subway) activities. Here we show a highly tuned resonant frequency of human locomotion at 2 Hz (SD 0.13) with no evidence of correlation with gender, age, height, weight, or body mass index. This frequency did not differ significantly from the preferred step frequency observed in the seminal laboratory study of Murray et al. (Murray MP, Drought AB, and Kory RC. J Bone Joint Surg 46A: 335-360, 1964). [1.95 Hz (SD 0.19)]. On the basis of the frequency characteristics of otolith-spinal reflexes, which drive lower body movement via the lateral vestibulospinal tract, and otolith-mediated collic and ocular reflexes that maintain gaze when walking, we speculate that this spontaneous tempo of locomotion represents some form of central "resonant frequency" of human movement.},
  eprint = {15890757},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KH7W4Y8J\\MacDougall and Moore - 2005 - Marching to the beat of the same drummer the spon.pdf},
  keywords = {Activities of Daily Living,Adult,Biological Clocks,Female,Head Movements,Humans,Locomotion,Male,Middle Aged,Monitoring; Ambulatory,Motor Activity,NASA Discipline Neuroscience,Non-NASA Center,Periodicity,Time Factors},
  langid = {english},
  number = {3}
}

@article{maclarnonEvolutionHumanSpeech1999,
  title = {The Evolution of Human Speech: The Role of Enhanced Breathing Control},
  shorttitle = {The Evolution of Human Speech},
  author = {MacLarnon, A. M. and Hewitt, G. P.},
  date = {1999-07},
  journaltitle = {American Journal of Physical Anthropology},
  shortjournal = {Am. J. Phys. Anthropol.},
  volume = {109},
  pages = {341--363},
  issn = {0002-9483},
  doi = {10.1002/(SICI)1096-8644(199907)109:3<341::AID-AJPA5>3.0.CO;2-2},
  abstract = {Many cognitive and physical features must have undergone change for the evolution of fully modern human language. One neglected aspect is the evolution of increased breathing control. Evidence presented herein shows that modern humans and Neanderthals have an expanded thoracic vertebral canal compared with australopithecines and Homo ergaster, who had canals of the same relative size as extant nonhuman primates. Based on previously published analyses, these results demonstrate that there was an increase in thoracic innervation during human evolution. Possible explanations for this increase include postural control for bipedalism, increased difficulty of parturition, respiration for endurance running, an aquatic phase, and choking avoidance. These can all be ruled out, either because of their evolutionary timing, or because they are insufficiently demanding neurologically. The remaining possible functional cause is increased control of breathing for speech. The main muscles involved in the fine control of human speech breathing are the intercostals and a set of abdominal muscles which are all thoracically innervated. Modifications to quiet breathing are essential for modern human speech, enabling the production of long phrases on single expirations punctuated with quick inspirations at meaningful linguistic breaks. Other linguistically important features affected by variation in subglottal air pressure include emphasis of particular sound units, and control of pitch and intonation. Subtle, complex muscle movements, integrated with cognitive factors, are involved. The vocalizations of nonhuman primates involve markedly less respiratory control. Without sophisticated breath control, early hominids would only have been capable of short, unmodulated utterances, like those of extant nonhuman primates. Fine respiratory control, a necessary component for fully modern language, evolved sometime between 1.6 Mya and 100,000 ya.},
  eprint = {10407464},
  eprinttype = {pmid},
  keywords = {Animals,Biological Evolution,Fossils,Hominidae,Humans,Language,Respiration,Speech,Thorax},
  langid = {english},
  number = {3}
}

@article{maclarnonIncreasedBreathingControl2004,
  title = {Increased Breathing Control: {{Another}} Factor in the Evolution of Human Language},
  shorttitle = {Increased Breathing Control},
  author = {Maclarnon, Ann and Hewitt, Gwen},
  date = {2004},
  journaltitle = {Evolutionary Anthropology: Issues, News, and Reviews},
  volume = {13},
  pages = {181--197},
  issn = {1520-6505},
  doi = {10.1002/evan.20032},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/evan.20032},
  urldate = {2020-07-07},
  abstract = {Investigation into the evolution of human language has involved evidence of many different kinds and approaches from many different disciplines. For full modern language, humans must have evolved a range of physical abilities for the production of our complex speech sounds, as well as sophisticated cognitive abilities. Human speech involves free-flowing, intricately varied, rapid sound sequences suitable for the fast transfer of complex, highly flexible communication. Some aspects of human speech, such as our ability to manipulate the vocal tract to produce a wide range of different types of sounds that form vowels and consonants, have attracted considerable attention from those interested in the evolution of language.1, 2 However, one very important contributory skill, the human ability to attain very fine control of breathing during speech, has been neglected. Here we present evidence of the importance of breathing control to human speech, as well as evidence that our capabilities greatly exceed those of nonhuman primates. Human speech breathing demands fine neurological control of the respiratory muscles, integrated with cognitive processes and other factors. Evidence from comparison of the vertebral canals of fossil hominids and those of extant primates suggests that a major increase in thoracic innervation evolved in later hominid evolution, providing enhanced breathing control. If that is so, then earlier hominids would have had quite restricted speech patterns, whereas more recent hominids, with human-like breath control abilities, would have been capable of faster, more varied speech sequences.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/evan.20032},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Z4BC8SCV\\Maclarnon and Hewitt - 2004 - Increased breathing control Another factor in the.pdf;C\:\\Users\\u668173\\Zotero\\storage\\Q3RH6JFT\\evan.html},
  keywords = {KNM-WT 15000,respiratory muscles,speech,vertebral canal,vocalization},
  langid = {english},
  number = {5}
}

@article{macneilageFrameContentTheory1998,
  title = {The Frame/Content Theory of Evolution of Speech Production},
  author = {MacNeilage, Peter F.},
  date = {1998-08},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {21},
  pages = {499--511},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X98001265},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/framecontent-theory-of-evolution-of-speech-production/4863C373F8A52560D49DF516AD20FA17},
  urldate = {2020-04-22},
  abstract = {The species-specific organizational property of speech is a continual mouth open-close alternation, the two phases of which are subject to continual articulatory modulation. The cycle constitutes the syllable, and the open and closed phases are segments – vowels and consonants, respectively. The fact that segmental serial ordering errors in normal adults obey syllable structure constraints suggests that syllabic “frames” and segmental “content” elements are separately controlled in the speech production process. The frames may derive from cycles of mandibular oscillation present in humans from babbling onset, which are responsible for the open-close alternation. These communication- related frames perhaps first evolved when the ingestion-related cyclicities of mandibular oscillation (associated with mastication [chewing] sucking and licking) took on communicative significance as lipsmacks, tonguesmacks, and teeth chatters – displays that are prominent in many nonhuman primates. The new role of Broca's area and its surround in human vocal communication may have derived from its evolutionary history as the main cortical center for the control of ingestive processes. The frame and content components of speech may have subsequently evolved separate realizations within two general purpose primate motor control systems: (1) a motivation-related medial “intrinsic” system, including anterior cingulate cortex and the supplementary motor area, for self-generated behavior, formerly responsible for ancestral vocalization control and now also responsible for frames, and (2) a lateral “extrinsic” system, including Broca's area and surround, and Wernicke's area, specialized for response to external input (and therefore the emergent vocal learning capacity) and more responsible for content.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\B3G9HCHA\\MacNeilage - 1998 - The framecontent theory of evolution of speech pr.pdf;C\:\\Users\\u668173\\Zotero\\storage\\KNPGPLFE\\4863C373F8A52560D49DF516AD20FA17.html},
  keywords = {Broca's aphasia,chewing,consonants,lipsmacks,speech evolution syllables,supplementary motor area,vowels,Wernicke's aphasia},
  langid = {english},
  number = {4}
}

@article{macneilageOriginInternalStructure2000,
  title = {On the Origin of Internal Structure of Word Forms},
  author = {MacNeilage, Peter F. and Davis, Barbara L.},
  date = {2000},
  journaltitle = {Science},
  volume = {288},
  pages = {527--531},
  publisher = {{American Assn for the Advancement of Science}},
  location = {{US}},
  issn = {1095-9203(Electronic),0036-8075(Print)},
  doi = {10.1126/science.288.5465.527},
  abstract = {The authors conducted statistical studies of the babbling of 6 infants and the first words of 10 infants. This study shows that a corpus of proto-word forms shares 4 sequential sound patterns with words of modern languages and the first words of infants. Three of the patterns involve intrasyllabic consonant-vowel (CV) co-occurrence: labial (lip) consonants with central vowels, coronal (tongue front) consonants with front vowels, and dorsal (tongue back) consonants with back vowels. The fourth pattern is an intersyllabic preference for initiating words with a labial consonant-vowel-coronal consonant sequence (LC). The CV effects may be primarily biomechanically motivated. The LC effect may be self-organizational, with multivariate causality. The findings support the hypothesis that these 4 patterns were basic to the origin of words. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8HE2CAND\\MacNeilage and Davis - 2000 - On the origin of internal structure of word forms.pdf;C\:\\Users\\u668173\\Zotero\\storage\\SHEGXPPB\\2000-15388-001.html},
  keywords = {Auditory Localization,Infant Vocalization,Language,Words (Phonetic Units)},
  number = {5465}
}

@book{macneilageOriginSpeech2010,
  title = {The {{Origin}} of {{Speech}}},
  author = {MacNeilage, Peter F.},
  date = {2010},
  publisher = {{Oxford University Press}},
  abstract = {This book explores the origin and evolution of speech. The human speech system is in a league of its own in the animal kingdom and its possession dwarfs most other evolutionary achievements. During every second of speech we unconsciously use about 225 distinct muscle actions. To investigate the evolutionary origins of this prodigious ability, Peter MacNeilage draws on work in linguistics, cognitive science, evolutionary biology, and animal behavior. He puts forward a neo-Darwinian account of speech as a process of descent in which ancestral vocal capabilities became modified in response to natural selection pressures for more efficient communication. His proposals include the crucial observation that present-day infants learning to produce speech reveal constraints that were acting on our ancestors as they invented new words long ago.This important and original investigation integrates the latest research on modern speech capabilities, their acquisition, and their neurobiology, including the issues surrounding the cerebral hemispheric specialization for speech. Written in a clear style with minimal recourse to jargon the book will interest a wide range of readers in cognitive, neuro-, and evolutionary science, as well as all those seeking to understand the nature and evolution of speech and human communication.},
  eprint = {BNQUDAAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-19-958158-0},
  keywords = {Language Arts & Disciplines / Linguistics / General,Social Science / Anthropology / Cultural & Social,Social Science / Sociology / Marriage & Family},
  langid = {english},
  pagetotal = {402}
}

@incollection{macneilagePresentStatusPostural2007,
  title = {Present {{Status}} of the {{Postural Origins Theory}}},
  booktitle = {Special {{Topics}} in {{Primatology}}},
  author = {MacNeilage, Peter F.},
  editor = {Hopkins, William D.},
  date = {2007-01-01},
  volume = {5},
  pages = {58--91},
  publisher = {{Elsevier}},
  doi = {10.1016/S1936-8526(07)05003-8},
  url = {http://www.sciencedirect.com/science/article/pii/S1936852607050038},
  urldate = {2020-06-09},
  abstract = {In 1987, MacNeilage et al. argued that evidence of population-level handedness was evident in nonhuman primates, a claim that stood in strong contrast to the prevailing scientific view. Based on this paper, MacNeilage et al. went on to propose the postural origin of handedness theory. Since 1987, a plethora of studies have been conducted in nonhuman animals, notably primates. This chapter updates the extant data on handedness in nonhuman primates as it relates to the Postural Origins theory of handedness and argues that many aspects of the original theory are still supported by the new findings.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RTMBUDSM\\S1936852607050038.html},
  keywords = {frame content theory,handedness,language,posture},
  langid = {english},
  series = {The {{Evolution}} of {{Hemispheric Specialization}} in {{Primates}}}
}

@article{macuchsilvaMultimodalityOriginNovel,
  title = {Multimodality and the Origin of a Novel Communication System in Face-to-Face Interaction},
  author = {Macuch Silva, Vinicius and Holler, Judith and Ozyurek, Asli and Roberts, Seán G.},
  journaltitle = {Royal Society Open Science},
  shortjournal = {Royal Society Open Science},
  volume = {7},
  pages = {182056},
  doi = {10.1098/rsos.182056},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.182056},
  urldate = {2020-01-28},
  abstract = {Face-to-face communication is multimodal at its core: it consists of a combination of vocal and visual signalling. However, current evidence suggests that, in the absence of an established communication system, visual signalling, especially in the form of visible gesture, is a more powerful form of communication than vocalization and therefore likely to have played a primary role in the emergence of human language. This argument is based on experimental evidence of how vocal and visual modalities (i.e. gesture) are employed to communicate about familiar concepts when participants cannot use their existing languages. To investigate this further, we introduce an experiment where pairs of participants performed a referential communication task in which they described unfamiliar stimuli in order to reduce reliance on conventional signals. Visual and auditory stimuli were described in three conditions: using visible gestures only, using non-linguistic vocalizations only and given the option to use both (multimodal communication). The results suggest that even in the absence of conventional signals, gesture is a more powerful mode of communication compared with vocalization, but that there are also advantages to multimodality compared to using gesture alone. Participants with an option to produce multimodal signals had comparable accuracy to those using only gesture, but gained an efficiency advantage. The analysis of the interactions between participants showed that interactants developed novel communication systems for unfamiliar stimuli by deploying different modalities flexibly to suit their needs and by taking advantage of multimodality when required.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QQPP464Y\\Macuch Silva et al. - Multimodality and the origin of a novel communicat.pdf;C\:\\Users\\u668173\\Zotero\\storage\\AUYXMJ53\\rsos.html},
  number = {1}
}

@article{madisonCorrelationsIntelligenceComponents2009,
  title = {Correlations between Intelligence and Components of Serial Timing Variability},
  author = {Madison, Guy and Forsman, Lea and Blom, Örjan and Karabanov, Anke and Ullén, Fredrik},
  date = {2009-01-01},
  journaltitle = {Intelligence},
  shortjournal = {Intelligence},
  volume = {37},
  pages = {68--75},
  issn = {0160-2896},
  doi = {10.1016/j.intell.2008.07.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0160289608000937},
  urldate = {2020-05-19},
  abstract = {Psychometric intelligence correlates with reaction time in elementary cognitive tasks, as well as with performance in time discrimination and judgment tasks. It has remained unclear, however, to what extent these correlations are due to top–down mechanisms, such as attention, and bottom–up mechanisms, i.e. basic neural properties that influence both temporal accuracy and cognitive processes. Here, we assessed correlations between intelligence (Raven SPM Plus) and performance in isochronous serial interval production, a simple, automatic timing task where participants first make movements in synchrony with an isochronous sequence of sounds and then continue with self-paced production to produce a sequence of intervals with the same inter-onset interval (IOI). The target IOI varied across trials. A number of different measures of timing variability were considered, all negatively correlated with intelligence. Across all stimulus IOIs, local interval-to-interval variability correlated more strongly with intelligence than drift, i.e. gradual changes in response IOI. The strongest correlations with intelligence were found for IOIs between 400 and 900~ms, rather than above 1~s, which is typically considered a lower limit for cognitive timing. Furthermore, poor trials, i.e. trials arguably most affected by lapses in attention, did not predict intelligence better than the most accurate trials. We discuss these results in relation to the human timing literature, and argue that they support a bottom–up model of the relation between temporal variability of neural activity and intelligence.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GA35RNT3\\Madison et al. - 2009 - Correlations between intelligence and components o.pdf;C\:\\Users\\u668173\\Zotero\\storage\\2GXFAP3Q\\S0160289608000937.html},
  keywords = {Duration-specificity,Intelligence,Interval production,Isochronous serial interval production,Neural mechanisms,Neural noise,Noise,Ravens progressive matrices,Tapping,Timing},
  langid = {english},
  number = {1}
}

@article{magneSpeechRhythmSensitivity2016,
  title = {Speech Rhythm Sensitivity and Musical Aptitude: {{ERPs}} and Individual Differences},
  shorttitle = {Speech Rhythm Sensitivity and Musical Aptitude},
  author = {Magne, Cyrille and Jordan, Deanna K. and Gordon, Reyna L.},
  date = {2016-02},
  journaltitle = {Brain and Language},
  shortjournal = {Brain Lang},
  volume = {153-154},
  pages = {13--19},
  issn = {1090-2155},
  doi = {10.1016/j.bandl.2016.01.001},
  abstract = {This study investigated the electrophysiological markers of rhythmic expectancy during speech perception. In addition, given the large literature showing overlaps between cognitive and neural resources recruited for language and music, we considered a relation between musical aptitude and individual differences in speech rhythm sensitivity. Twenty adults were administered a standardized assessment of musical aptitude, and EEG was recorded as participants listened to sequences of four bisyllabic words for which the stress pattern of the final word either matched or mismatched the stress pattern of the preceding words. Words with unexpected stress patterns elicited an increased fronto-central mid-latency negativity. In addition, rhythm aptitude significantly correlated with the size of the negative effect elicited by unexpected iambic words, the least common type of stress pattern in English. The present results suggest shared neurocognitive resources for speech rhythm and musical rhythm.},
  eprint = {26828758},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Adult,Aptitude,Brain,Electroencephalography,ERPs,Evoked Potentials; Auditory,Expectancy,Female,Humans,Individuality,Male,Music,Musical aptitude,Periodicity,Speech,Speech meter,Speech Perception,Young Adult},
  langid = {english}
}

@article{magnussonBodyLanguageAdults2008,
  title = {The {{Body Language}} of {{Adults Who Are Blind}}},
  author = {Magnusson, Anna-Karin and Karlsson, Gunnar},
  date = {2008-06-01},
  journaltitle = {Scandinavian Journal of Disability Research},
  volume = {10},
  pages = {71--89},
  issn = {1501-7419},
  doi = {10.1080/15017410701685927},
  url = {https://doi.org/10.1080/15017410701685927},
  urldate = {2019-11-30},
  abstract = {The body expressions of adults who are blind have been relatively unexplored. The aim of this study was therefore to deepen the understanding of different forms of body expression, or “body language”, in adults who are blind. More specifically the study aimed at answering the following questions: What forms of body expression do adults who are blind display? What can the conditions for some different forms of body expression be? What importance can individual, social and cultural factors have for different forms of body expression? Data consisted of video-taped interviews with five congenitally blind, two adventitiously blind and two sighted individuals. The data were analysed in a hermeneutical and phenomenological sense. The results consisted of a typology of 19 different forms of body expression. All in all, we found that the congenitally blind participants expressed themselves mainly in a functional and concrete manner. They also seemed to have limited experiences with abstract, symbolic body expressions. The conditions and the importance of different factors for different body expressions are discussed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\S93QF2EA\\Magnusson and Karlsson - 2008 - The Body Language of Adults Who Are Blind.pdf;C\:\\Users\\u668173\\Zotero\\storage\\JR8FZJIT\\15017410701685927.html},
  number = {2}
}

@article{magnussonNonverbalConversationRegulatingSignals2006,
  title = {Nonverbal {{Conversation}}-{{Regulating Signals}} of the {{Blind Adult}}},
  author = {Magnusson, Anna-Karin},
  date = {2006-12-01},
  journaltitle = {Communication Studies},
  volume = {57},
  pages = {421--433},
  issn = {1051-0974},
  doi = {10.1080/10510970600946004},
  url = {https://doi.org/10.1080/10510970600946004},
  urldate = {2019-11-30},
  abstract = {The purpose of this study is to describe nonverbal conversation-regulating signals among the blind adult and to describe how these signals are manifested through body movements/positions and paralinguistic sounds/silences. Data consist of videotaped conversations between blind-blind pairs and blind-sighted pairs. The data are analyzed in a hermeneutical/phenomenological sense. The analysis is also inspired by Conversational Analysis. The analysis resulted in descriptions of four major signal types, i.e. the listener's back-channeling signals, the speaker's turn-holding signals, the listener's starting signals, and the speaker's turn-yielding signals. One central conclusion is that earlier experience of vision does not seem to be a prerequisite for showing a variety of different types of nonverbal conversation-regulating signals in a systematic and distinct manner.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UF965FMB\\Magnusson - 2006 - Nonverbal Conversation-Regulating Signals of the B.pdf;C\:\\Users\\u668173\\Zotero\\storage\\43GRBUV9\\10510970600946004.html},
  keywords = {Blindness,Conversation analysis,Conversation-regulating signals,Hermeneutics,Nonverbal,Phenomenology},
  number = {4}
}

@article{magnussonNonverbalConversationRegulatingSignals2006a,
  title = {Nonverbal {{Conversation}}-{{Regulating Signals}} of the {{Blind Adult}}},
  author = {Magnusson, Anna-Karin},
  date = {2006-12-01},
  journaltitle = {Communication Studies},
  volume = {57},
  pages = {421--433},
  issn = {1051-0974},
  doi = {10.1080/10510970600946004},
  url = {https://doi.org/10.1080/10510970600946004},
  urldate = {2019-11-30},
  abstract = {The purpose of this study is to describe nonverbal conversation-regulating signals among the blind adult and to describe how these signals are manifested through body movements/positions and paralinguistic sounds/silences. Data consist of videotaped conversations between blind-blind pairs and blind-sighted pairs. The data are analyzed in a hermeneutical/phenomenological sense. The analysis is also inspired by Conversational Analysis. The analysis resulted in descriptions of four major signal types, i.e. the listener's back-channeling signals, the speaker's turn-holding signals, the listener's starting signals, and the speaker's turn-yielding signals. One central conclusion is that earlier experience of vision does not seem to be a prerequisite for showing a variety of different types of nonverbal conversation-regulating signals in a systematic and distinct manner.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SDPMBNKS\\Magnusson - 2006 - Nonverbal Conversation-Regulating Signals of the B.pdf;C\:\\Users\\u668173\\Zotero\\storage\\UGW4LCVA\\10510970600946004.html},
  keywords = {Blindness,Conversation analysis,Conversation-regulating signals,Hermeneutics,Nonverbal,Phenomenology},
  number = {4}
}

@article{maidhofPredictiveErrorDetection2013,
  title = {Predictive Error Detection in Pianists: {{A}} Combined {{ERP}} and Motion Capture Study},
  shorttitle = {Predictive Error Detection in Pianists},
  author = {Maidhof, Clemens and Pitkäniemi, Anni and Tervaniemi, Mari},
  date = {2013},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {7},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00587},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2013.00587/full},
  urldate = {2020-09-25},
  abstract = {Performing a piece of music involves the interplay of several cognitive and motor processes and requires extensive training to achieve a high skill level. However, even professional musicians commit errors occasionally. Previous event-related potential (ERP) studies have investigated the neurophysiological correlates of pitch errors during piano performance, and reported pre-error negativity already occurring approximately 70-100 ms before the error had been committed and audible. It was assumed that this pre-error negativity reflects predictive control processes that compare predicted consequences with actual consequences of one’s own actions. However, in previous investigations, correct and incorrect pitch events were confounded by their different tempi. In addition, no data about the underlying movements were available. In the present study, we exploratively recorded the ERPs and 3D movement data of pianists’ fingers simultaneously while they performed fingering exercises from memory. Results showed a pre-error negativity for incorrect keystrokes when both correct and incorrect keystrokes were performed with comparable tempi. Interestingly, even correct notes immediately preceding erroneous keystrokes elicited a very similar negativity. In addition, we explored the possibility of computing ERPs time-locked to a kinematic landmark in the finger motion trajectories defined by when a finger makes initial contact with the key surface, that is, at the onset of tactile feedback. Results suggest that incorrect notes elicited a small difference after the onset of tactile feedback, whereas correct notes preceding incorrect ones elicited negativity before the onset of tactile feedback. The results tentatively suggest that tactile feedback plays an important role in error-monitoring during piano performance, because the comparison between predicted and actual sensory (tactile) feedback may provide the information necessary for the detection of an upcoming error.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9D58PDHG\\Maidhof et al. - 2013 - Predictive error detection in pianists A combined.pdf},
  keywords = {EEG,motor control,music performance,Musical expertise,Performance monitoring},
  langid = {english}
}

@article{manderaExplainingHumanPerformance2017,
  title = {Explaining Human Performance in Psycholinguistic Tasks with Models of Semantic Similarity Based on Prediction and Counting: {{A}} Review and Empirical Validation},
  shorttitle = {Explaining Human Performance in Psycholinguistic Tasks with Models of Semantic Similarity Based on Prediction and Counting},
  author = {Mandera, Paweł and Keuleers, Emmanuel and Brysbaert, Marc},
  date = {2017-02-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {92},
  pages = {57--78},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2016.04.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0749596X16300079},
  urldate = {2020-11-19},
  abstract = {Recent developments in distributional semantics (Mikolov, Chen, Corrado, \& Dean, 2013; Mikolov, Sutskever, Chen, Corrado, \& Dean, 2013) include a new class of prediction-based models that are trained on a text corpus and that measure semantic similarity between words. We discuss the relevance of these models for psycholinguistic theories and compare them to more traditional distributional semantic models. We compare the models’ performances on a large dataset of semantic priming (Hutchison et al., 2013) and on a number of other tasks involving semantic processing and conclude that the prediction-based models usually offer a better fit to behavioral data. Theoretically, we argue that these models bridge the gap between traditional approaches to distributional semantics and psychologically plausible learning principles. As an aid to researchers, we release semantic vectors for English and Dutch for a range of models together with a convenient interface that can be used to extract a great number of semantic similarity measures.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TB6RQFEZ\\S0749596X16300079.html},
  keywords = {Distributional semantics,Psycholinguistic resource,Semantic model,Semantic priming},
  langid = {english}
}

@article{marienConsensusPaperLanguage2014,
  title = {Consensus {{Paper}}: {{Language}} and the {{Cerebellum}}: An {{Ongoing Enigma}}},
  shorttitle = {Consensus {{Paper}}},
  author = {Mariën, Peter and Ackermann, Herman and Adamaszek, Michael and Barwood, Caroline H. S. and Beaton, Alan and Desmond, John and De Witte, Elke and Fawcett, Angela J. and Hertrich, Ingo and Küper, Michael and Leggio, Maria and Marvel, Cherie and Molinari, Marco and Murdoch, Bruce E. and Nicolson, Roderick I. and Schmahmann, Jeremy D. and Stoodley, Catherine J. and Thürling, Markus and Timmann, Dagmar and Wouters, Ellen and Ziegler, Wolfram},
  date = {2014-06},
  journaltitle = {Cerebellum (London, England)},
  shortjournal = {Cerebellum},
  volume = {13},
  pages = {386--410},
  issn = {1473-4222},
  doi = {10.1007/s12311-013-0540-5},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4090012/},
  urldate = {2020-07-10},
  abstract = {In less than three decades, the concept “cerebellar neurocognition” has evolved from a mere afterthought to an entirely new and multifaceted area of neuroscientific research. A close interplay between three main strands of contemporary neuroscience induced a substantial modification of the traditional view of the cerebellum as a mere coordinator of autonomic and somatic motor functions. Indeed, the wealth of current evidence derived from detailed neuroanatomical investigations, functional neuroimaging studies with healthy subjects and patients and in-depth neuropsychological assessment of patients with cerebellar disorders shows that the cerebellum has a cardinal role to play in affective regulation, cognitive processing, and linguistic function. Although considerable progress has been made in models of cerebellar function, controversy remains regarding the exact role of the “linguistic cerebellum” in a broad variety of nonmotor language processes. This consensus paper brings together a range of different viewpoints and opinions regarding the contribution of the cerebellum to language function. Recent developments and insights in the nonmotor modulatory role of the cerebellum in language and some related disorders will be discussed. The role of the cerebellum in speech and language perception, in motor speech planning including apraxia of speech, in verbal working memory, in phonological and semantic verbal fluency, in syntax processing, in the dynamics of language production, in reading and in writing will be addressed. In addition, the functional topography of the linguistic cerebellum and the contribution of the deep nuclei to linguistic function will be briefly discussed. As such, a framework for debate and discussion will be offered in this consensus paper.},
  eprint = {24318484},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2ZIXPTB2\\Mariën et al. - 2014 - Consensus Paper Language and the Cerebellum an O.pdf},
  number = {3},
  pmcid = {PMC4090012}
}

@article{marinovicEffectsPreparationAcoustic2015,
  title = {The Effects of Preparation and Acoustic Stimulation on Contralateral and Ipsilateral Corticospinal Excitability},
  author = {Marinovic, Welber and Flannery, Victoria and Riek, Stephan},
  date = {2015-08-01},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {42},
  pages = {81--88},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2015.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S016794571500086X},
  urldate = {2021-03-03},
  abstract = {A loud auditory stimulus (LAS) presented together with an imperative stimulus during preparation for motor actions can speed their initiation. The effects of LAS on corticospinal excitability (CSE), however, depend on the state of preparation of the motor system for action. CSE also depends on the brain hemisphere controlling the responding limb. Usually, CSE is increased just before movement onset in the hemisphere controlling the movement and inhibited on the other side. This study investigated the impact of LAS on CSE of the contralateral and ipsilateral hemispheres, while participants prepared for a voluntary abduction of the index finger. In Experiment 1, we attempted to identify the pattern of modulation of the ipsilateral cortex (resting side) by determining the time course of corticospinal changes in anticipatory timing actions using transcranial magnetic stimulation. In Experiment 2, we investigated the impact of LAS on the ipsilateral and contralateral CSE during anticipatory preparation. Results found no modulation of ipsilateral CSE during preparation, but indicate an increase in CSE after EMG onset on the acting limb. Moreover, we found that LAS presentation increased CSE on the contralateral side (active side).},
  keywords = {Auditory stimulus,Motor control,Motor cortex,Movement preparation},
  langid = {english}
}

@article{marstallerCommonFunctionalNeural2015,
  title = {A Common Functional Neural Network for Overt Production of Speech and Gesture},
  author = {Marstaller, L. and Burianová, H.},
  date = {2015-01-22},
  journaltitle = {Neuroscience},
  shortjournal = {Neuroscience},
  volume = {284},
  pages = {29--41},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2014.09.067},
  url = {http://www.sciencedirect.com/science/article/pii/S0306452214008306},
  urldate = {2020-12-05},
  abstract = {The perception of co-speech gestures, i.e., hand movements that co-occur with speech, has been investigated by several studies. The results show that the perception of co-speech gestures engages a core set of frontal, temporal, and parietal areas. However, no study has yet investigated the neural processes underlying the production of co-speech gestures. Specifically, it remains an open question whether Broca’s area is central to the coordination of speech and gestures as has been suggested previously. The objective of this study was to use functional magnetic resonance imaging to (i) investigate the regional activations underlying overt production of speech, gestures, and co-speech gestures, and (ii) examine functional connectivity with Broca’s area. We hypothesized that co-speech gesture production would activate frontal, temporal, and parietal regions that are similar to areas previously found during co-speech gesture perception and that both speech and gesture as well as co-speech gesture production would engage a neural network connected to Broca’s area. Whole-brain analysis confirmed our hypothesis and showed that co-speech gesturing did engage brain areas that form part of networks known to subserve language and gesture. Functional connectivity analysis further revealed a functional network connected to Broca’s area that is common to speech, gesture, and co-speech gesture production. This network consists of brain areas that play essential roles in motor control, suggesting that the coordination of speech and gesture is mediated by a shared motor control network. Our findings thus lend support to the idea that speech can influence co-speech gesture production on a motoric level.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\H4KHYUG7\\S0306452214008306.html},
  keywords = {fMRI,functional connectivity,gesture,speech production},
  langid = {english}
}

@article{martinezHumanHyoidBones2008,
  title = {Human Hyoid Bones from the Middle {{Pleistocene}} Site of the {{Sima}} de Los {{Huesos}} ({{Sierra}} de {{Atapuerca}}, {{Spain}})},
  author = {Martínez, I. and Arsuaga, J. L. and Quam, R. and Carretero, J. M. and Gracia, A. and Rodríguez, L.},
  date = {2008-01-01},
  journaltitle = {Journal of Human Evolution},
  shortjournal = {Journal of Human Evolution},
  volume = {54},
  pages = {118--124},
  issn = {0047-2484},
  doi = {10.1016/j.jhevol.2007.07.006},
  url = {http://www.sciencedirect.com/science/article/pii/S004724840700139X},
  urldate = {2020-09-19},
  abstract = {This study describes and compares two hyoid bones from the middle Pleistocene site of the Sima de los Huesos in the Sierra de Atapuerca (Spain). The Atapuerca SH hyoids are humanlike in both their morphology and dimensions, and they clearly differ from the hyoid bones of chimpanzees and Australopithecus afarensis. Their comparison with the Neandertal specimens Kebara 2 and SDR-034 makes it possible to begin to approach the question of temporal variation and sexual dimorphism in this bone in fossil humans. The results presented here show that the degree of metric and anatomical variation in the fossil sample was similar in magnitude and kind to living humans. Modern hyoid morphology was present by at least 530kya and appears to represent a shared derived feature of the modern human and Neandertal evolutionary lineages inherited from their last common ancestor.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NW2T2YVN\\Martínez et al. - 2008 - Human hyoid bones from the middle Pleistocene site.pdf;C\:\\Users\\u668173\\Zotero\\storage\\R943VIK9\\S004724840700139X.html},
  keywords = {Atapuerca,Hyoid bone,Middle Pleistocene,Sima de los Huesos},
  langid = {english},
  number = {1}
}

@incollection{masatakaEmpiricalEvidenceClaim2020,
  title = {Empirical {{Evidence}} for the {{Claim That}} the {{Vocal Theory}} of {{Language Origins}} and the {{Gestural Theory}} of {{Language Origins Are Not Incompatible}} with {{One Another}}},
  booktitle = {The {{Origins}} of {{Language Revisited}}: {{Differentiation}} from {{Music}} and the {{Emergence}} of {{Neurodiversity}} and {{Autism}}},
  author = {Masataka, Nobuo},
  editor = {Masataka, Nobuo},
  date = {2020},
  pages = {1--24},
  publisher = {{Springer}},
  location = {{Singapore}},
  doi = {10.1007/978-981-15-4250-3_1},
  url = {https://doi.org/10.1007/978-981-15-4250-3_1},
  urldate = {2020-09-20},
  abstract = {The author argues about the implications of the evolution of motherese for the emergence of language in the human history, and it occurred in both the vocal mode and the manual mode, the fact indicating that the gestural theory of and the vocal theory of language origins are not incompatible with one another. It is a commonplace observation that hearing adults tend to modify their speech in an unusual and characteristic fashion when they address infants and young children. The available data indicate that motherese or infant-directed speech is a prevalent form of language input to hearing infants and that its salience for preverbal infants results both from the infant’s attentional responsiveness to certain sounds more readily than others and from the infant’s affective responsiveness to certain attributes of the auditory signal. In the signing behavior of deaf mothers when communicating with their deaf infants, a phenomenon quite analogous to motherese in maternal speech is observed. Concerning the aspect of linguistic input, moreover, there is evidence for the presence of predispositional preparedness in human infants to detect motherese characteristics equally in the manual mode and in the vocal mode. Such cognitive preparedness, in fact, serves as a basis on which sign language learning proceeds in deaf infants. One can seek its evolutionary origins in the rudimentary form of teaching behavior that occurs in the adult–infant interaction in nonhuman primates as well as in humans, by which the cross-generational transmission of parenting is made possible, including that in the deaf community.},
  isbn = {9789811542503},
  keywords = {Infant-directed signing,Infant-directed speech,Language learning,Motherese,Signed language},
  langid = {english}
}

@article{masseryMultisystemConsequencesImpaired,
  title = {Multisystem {{Consequences}} of {{Impaired Breathing Mechanics}} and/or {{Postural Control}}},
  author = {Massery, Mary},
  pages = {24},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LHHKL79F\\Massery - Multisystem Consequences of Impaired Breathing Mec.pdf},
  langid = {english}
}

@article{mcclavePitchManualGestures1998,
  title = {Pitch and {{Manual Gestures}}},
  author = {McClave, E.},
  date = {1998},
  journaltitle = {Journal of Psycholinguistic Research},
  volume = {27},
  pages = {69--89},
  doi = {10.1023/A:1023274823974},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DM2EK6C7\\10.html},
  number = {2}
}

@book{mccoyYourVoiceView2012,
  title = {Your {{Voice}}: {{An Inside View}}},
  shorttitle = {Your {{Voice}}},
  author = {McCoy, Scott Jeffrey},
  date = {2012},
  publisher = {{Inside View Press}},
  abstract = {A multimedia exploration of voice science and pedagogy that focuses on the needs of the singer. Disc includes extensive use of audio, video, and high-resolution photographic examples to support the text.},
  eprint = {GHbLLwEACAAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-9755307-8-8},
  langid = {english},
  pagetotal = {181}
}

@article{mcdermottRunningTrainingAdaptive2003,
  title = {Running Training and Adaptive Strategies of Locomotor-Respiratory Coordination},
  author = {McDermott, William J. and Van Emmerik, Richard E. A. and Hamill, Joseph},
  date = {2003-06},
  journaltitle = {European Journal of Applied Physiology},
  shortjournal = {Eur. J. Appl. Physiol.},
  volume = {89},
  pages = {435--444},
  issn = {1439-6319},
  doi = {10.1007/s00421-003-0831-5},
  abstract = {It has been suggested that stronger coupling between locomotory and breathing rhythms may occur as a result of training in the particular movement pattern and also may reduce the perceived workload or metabolic cost of the movement. Research findings on human locomotor-respiratory coordination are equivocal, due in part to the fact that assessment techniques range in sensitivity to important aspects of coordination (e.g. temporal ordering of patterns, half-integer couplings and changes in frequency and phase coupling). An additional aspect that has not received much attention is the adaptability of this coordination to changes in task constraints. The current study investigated the effect of running training on the locomotor-respiratory coordination and the adaptive strategies observed across a wide range of walking and running speeds. Locomotor-respiratory coordination was evaluated by the strength and variability of both frequency and phase coupling patterns that subjects displayed within and across the speed conditions. Male subjects (five runners, five non-runners) locomoted at seven different treadmill speeds. Group results indicated no differences between runners and non-runners with respect to breathing parameters, stride parameters, as well as the strength and variability of the coupling at each speed. Individual results, however, showed that grouping subjects masks large individual differences and strategies across speeds. Coupling strategies indicated that runners show more stable dominant couplings across locomotory speeds than non-runners do. These findings suggest that running training does not change the strength of locomotor-respiratory coupling but rather how these systems adapt to changing speeds.},
  eprint = {12712351},
  eprinttype = {pmid},
  keywords = {Adaptation; Physiological,Adult,Biological Clocks,Exercise,Gait,Homeostasis,Humans,Locomotion,Male,Physical Education and Training,Physical Fitness,Respiratory Mechanics,Running,Statistics as Topic},
  langid = {english},
  number = {5}
}

@article{mcellinSynchronicitiesThatShape2020,
  title = {Synchronicities That Shape the Perception of Joint Action},
  author = {McEllin, Luke and Knoblich, Günther and Sebanz, Natalie},
  date = {2020-09-23},
  journaltitle = {Scientific Reports},
  volume = {10},
  pages = {15554},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-72729-6},
  url = {https://www.nature.com/articles/s41598-020-72729-6},
  urldate = {2020-12-05},
  abstract = {In joint performances spanning from jazz improvisation to soccer, expert performers synchronize their movements in ways that novices cannot. Particularly, experts can align the velocity profiles of their movements in order to achieve synchrony on a fine-grained time scale, compared to novices who can only synchronize the duration of their movement intervals. This study investigated how experts’ ability to engage in velocity-based synchrony affects observers’ perception of coordination and their aesthetic experience of joint performances. Participants observed two moving dots on a screen and were told that these reflect the hand movements of two performers engaging in joint improvisation. The dots were animated to reflect the velocity-based synchrony characteristic of expert performance (in terms of jitter of the velocity profile: Experiment 1, or through aligning sharpness of the velocity profile: Experiment 2) or contained only interval-based synchrony. Performances containing velocity-based synchrony were judged as more coordinated with performers rated as liking each other more, and were rated as more beautiful, providing observers with a stronger aesthetic experience. These findings demonstrate that subtle timing cues fundamentally shape the experience of watching joint actions, directly influencing how beautiful and enjoyable we find these interactions, as well as our perception of the relationship between co-actors.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UQ2RFJPI\\McEllin et al. - 2020 - Synchronicities that shape the perception of joint.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8CMHJBGF\\s41598-020-72729-6.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{mcfarlandPhysiologicalMeasuresMother2020,
  title = {Physiological Measures of Mother–Infant Interactional Synchrony},
  author = {McFarland, David H. and Fortin, Annie Joëlle and Polka, Linda},
  date = {2020},
  journaltitle = {Developmental Psychobiology},
  volume = {62},
  pages = {50--61},
  issn = {1098-2302},
  doi = {10.1002/dev.21913},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/dev.21913},
  urldate = {2020-10-27},
  abstract = {Mother–infant interactional synchrony has been hypothesized to be crucial for the development of many key neurodevelopmental behaviors in infants, including speech and language. Assessing synchrony is challenging because many interactive behaviors may be subtlety, if at all, observable in overt behaviors. Physiological measures, therefore, may provide valuable physiological/biological markers of mother–infant synchrony. We have developed a multilevel measurement platform to assess physiological synchrony, attention, and vocal congruency during dynamic face-to-face mother–infant interactions. The present investigation was designed to provide preliminary data on its application in a group of 10 mother–infant dyads (20 subjects) ranging in age from 7 to 8.5 months at the time of the experimentation. Respiratory kinematics, heart rate, and vocalization were recorded simultaneously from mothers and infants during nonstructured, face-to-face interactions. Novel statistical methods were used to identify reliable moments of synchrony from cross-correlated, mother–infant respiration and to tag infant attention from heart rate deceleration. Results revealed that attention, vocal contingency, and respiratory synchrony are temporally clustered within the dyad interaction. This temporal alignment is consistent with the notion that biological synchrony provides a supportive platform for infant attention and mother–infant contingent vocalization.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/dev.21913},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XUZ3ZXIC\\McFarland et al. - 2020 - Physiological measures of mother–infant interactio.pdf;C\:\\Users\\u668173\\Zotero\\storage\\HLSYIJ4H\\dev.html;C\:\\Users\\u668173\\Zotero\\storage\\LKXVHWSZ\\dev.html},
  keywords = {attention,heart rate,mother–infant,respiration,synchrony,vocal contingency},
  langid = {english},
  number = {1}
}

@article{mcgurkHearingLipsSeeing1976,
  title = {Hearing Lips and Seeing Voices},
  author = {McGurk, H. and MacDonald, J.},
  date = {1976-12-23/0030},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {264},
  pages = {746--748},
  issn = {0028-0836},
  doi = {10.1038/264746a0},
  eprint = {1012311},
  eprinttype = {pmid},
  keywords = {Adolescent,Adult,Age Factors,Auditory Perception,Child,Child; Preschool,Female,Humans,Illusions,Male,Speech,Visual Perception},
  langid = {english},
  number = {5588}
}

@article{mcilroyEarlyActivationArm1995,
  title = {Early Activation of Arm Muscles Follows External Perturbation of Upright Stance},
  author = {McIlroy, William E. and Maki, Brian E.},
  date = {1995-01-30},
  journaltitle = {Neuroscience Letters},
  shortjournal = {Neuroscience Letters},
  volume = {184},
  pages = {177--180},
  issn = {0304-3940},
  doi = {10.1016/0304-3940(94)11200-3},
  url = {http://www.sciencedirect.com/science/article/pii/0304394094112003},
  urldate = {2020-06-30},
  abstract = {Grasping, counterbalancing and protective arm movements are an important defence against external postural perturbation, but are commonly constrained in studies of postural control. We treasured muscle activity at the shoulder, and the lower leg, during unconstrained responses to platform translation. Results revealed very early activation in shoulder muscles, similar in timing to the ‘automatic’ ankle responses. The arm activation occurred even when the reaction provided no immediate defence against destabilization but would appear to be more than a ‘startle’ response, since the activation was scaled to the perturbation magnitude and persisted even when perturbations were expected. The arm activation would appear to be driven from a remote sensory source, since there was negligible loading or stretch of the arm muscles.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4SWS2UST\\McIlroy and Maki - 1995 - Early activation of arm muscles follows external p.pdf;C\:\\Users\\u668173\\Zotero\\storage\\TCL54MZM\\0304394094112003.html},
  keywords = {Arms,Automatic postural response,Balance,Electromyography,Perturbation},
  langid = {english},
  number = {3}
}

@article{mcmanusHandednessSitusInversus2004,
  title = {Handedness and Situs Inversus in Primary Ciliary Dyskinesia},
  author = {McManus, I. C. and Martin, N. and Stubbings, G. F. and Chung, E. M. K. and Mitchison, H. M.},
  date = {2004-12-22},
  journaltitle = {Proceedings. Biological Sciences},
  shortjournal = {Proc. Biol. Sci.},
  volume = {271},
  pages = {2579--2582},
  issn = {0962-8452},
  doi = {10.1098/rspb.2004.2881},
  abstract = {... The limbs on the right side are stronger. [The] cause may be ... [that] ... motion, and abilities of moving, are somewhat holpen from the liver, which lieth on the right side. (Sir Francis Bacon, Sylva sylvarum (1627).)Fifty per cent of people with primary ciliary dyskinesia (PCD) (also known as immotile cilia syndrome or Siewert-Kartagener syndrome) have situs inversus, which is thought to result from absent nodal ciliary rotation and failure of normal symmetry breaking. In a study of 88 people with PCD, only 15.2\% of 46 individuals with situs inversus, and 14.3\% of 42 individuals with situs solitus, were left handed. Because cerebral lateralization is therefore still present, the nodal cilia cannot be the primary mechanism responsible for symmetry breaking in the vertebrate body. Intriguingly, one behavioural lateralization, wearing a wrist-watch on the right wrist, did correlate with situs inversus.},
  eprint = {15615683},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8E5T9FUV\\McManus et al. - 2004 - Handedness and situs inversus in primary ciliary d.pdf},
  keywords = {Functional Laterality,Humans,Kartagener Syndrome,Logistic Models,Models; Biological,Situs Inversus,Surveys and Questionnaires},
  langid = {english},
  number = {1557},
  pmcid = {PMC1691902}
}

@book{mcneilageOriginSpeech2008,
  title = {The Origin of Speech},
  author = {McNeilage, P.},
  date = {2008},
  publisher = {{Oxford University Press}},
  location = {{New York}}
}

@online{mcneillCatchmentsProsodyDiscourse2001,
  title = {Catchments, Prosody and Discourse},
  author = {McNeill, D. and Quek, F. and McCullough, K.-E. and Duncan, S. and Furuyama, N. and Bryll, R. and Ma, X.-F. and Ansari, R.},
  date = {2001},
  publisher = {{John Benjamins Publishing Company}},
  doi = {info:doi/10.1075/gest.1.1.03mcn},
  url = {https://www.ingentaconnect.com/content/jbp/gest/2001/00000001/00000001/art00002},
  urldate = {2020-03-17},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6MN3EUT9\\art00002.html},
  langid = {english},
  type = {Text}
}

@book{mcneillGestureThought2005,
  title = {Gesture and {{Thought}}},
  author = {McNeill, David},
  date = {2005},
  publisher = {{The University of Chicago Press}},
  location = {{Chicago}},
  url = {https://www.press.uchicago.edu/ucp/books/book/chicago/G/bo3633713.html},
  urldate = {2019-04-16},
  abstract = {Gesturing is such an integral yet unconscious part of communication that we are mostly oblivious to it. But if you observe anyone in conversation, you are likely to see his or her fingers, hands, and arms in some form of spontaneous motion. Why? David McNeill, a pioneer in the ongoing study of the relationship between gesture and language, set about answering this question over twenty-five years ago. In Gesture and Thought he brings together years of this research, arguing that gesturing, an act which has been popularly understood as an accessory to speech, is actually a dialectical component of language. Gesture and Thought expands on McNeill’s acclaimed classic Hand and Mind. While that earlier work demonstrated what gestures reveal about thought, here gestures are shown to be active participants in both speaking and thinking. Expanding on an approach introduced by Lev Vygotsky in the 1930s, McNeill posits that gestures are key ingredients in an “imagery-language dialectic” that fuels both speech and thought. Gestures are both the “imagery” and components of “language.” The smallest element of this dialectic is the “growth point,” a snapshot~of an utterance at its beginning psychological stage. Utilizing several innovative experiments he created and administered with subjects spanning several different age, gender, and language groups, McNeill shows how growth points organize themselves into utterances and extend to discourse at the moment of speaking.An ambitious project in the ongoing study of the relationship of human communication and thought, Gesture and Thought is a work of such consequence that it will influence all subsequent theory on the subject.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\S7LGLV93\\bo3633713.html},
  pagetotal = {328}
}

@book{mcneillHandMindWhat1992,
  title = {Hand and {{Mind}}: {{What Gestures Reveal}} about {{Thought}}},
  shorttitle = {Hand and {{Mind}}},
  author = {McNeill, David},
  date = {1992},
  publisher = {{University Of Chicago Press}},
  location = {{Chicago}},
  abstract = {Will be shipped from US. Used books may not include companion materials, may have some shelf wear, may contain highlighting/notes, may not include CDs or access codes. 100\% money back guarantee.}
}

@incollection{mcneillIWManWho2010,
  title = {{{IW}} - “{{The Man Who Lost His Body}}”},
  booktitle = {Handbook of {{Phenomenology}} and {{Cognitive Science}}},
  author = {McNeill, David and Quaeghebeur, Liesbet and Duncan, Susan},
  editor = {Schmicking, Daniel and Gallagher, Shaun},
  date = {2010},
  pages = {519--543},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-90-481-2646-0_27},
  url = {https://doi.org/10.1007/978-90-481-2646-0_27},
  urldate = {2019-04-16},
  abstract = {Mr. Ian Waterman, sometimes referred to as ‘IW’, suffered at age 19 a sudden, total deafferentation of his body from the neck down - the near total loss of all the touch, proprioception, and limb spatial position senses that tell you, without looking, where your body is and what it is doing. The loss followed a never-diagnosed fever that is believed to have set off an auto-immune reaction. The immediate behavioral effect was immobility, even though IW’s motor system was unaffected and there was no paralysis. The problem was not lack of movement per se but lack of control. Upon awakening after 3 days, IW nightmarishly found that he had no control over what his body did - he was unable to sit up, walk, feed himself or manipulate objects; none of the ordinary actions of everyday life, let alone the complex actions required for his vocation. To imagine what deafferentation is like, try this experiment suggested by Shaun Gallagher: sit down at a table (something IW could not have done at first) and place your hands below the surface; open and close one hand, close the other and extend a finger; put the open hand over the closed hand, and so forth. You know at all times what your hands are doing and where they are but IW would not know any of this - he would know that he had willed his hands to move but, without vision, would have no idea of what they are doing or where they are located.},
  isbn = {978-90-481-2646-0},
  keywords = {Bodily Expression,Growth Point,Idea Unit,Instrumental Action,Linguistic Meaning},
  langid = {english}
}

@book{mcneillLanguageGesture2000,
  title = {Language and {{Gesture}}},
  author = {McNeill, David},
  date = {2000-08-03},
  publisher = {{Cambridge University Press}},
  abstract = {This landmark study examines the role of gestures in relation to speech and thought. Leading scholars, including psychologists, linguists and anthropologists, offer state-of-the-art analyses to demonstrate that gestures are not merely an embellishment of speech but are integral parts of language itself. Language and Gesture offers a wide range of theoretical approaches, with emphasis not simply on behavioural descriptions but also on the underlying processes. The book has strong cross-linguistic and cross-cultural components, examining gestures by speakers of Mayan, Australian, East Asian as well as English and other European languages. The content is diverse including chapters on gestures during aphasia and severe stuttering, the first emergence of speech-gesture combinations of children, and a section on sign language. In a rapidly growing field of study this volume opens up the agenda for research into a new approach to understanding language, thought and society.},
  eprint = {DRBcMQuSrf8C},
  eprinttype = {googlebooks},
  isbn = {978-0-521-77761-2},
  keywords = {Language Arts & Disciplines / Linguistics / General,Language Arts & Disciplines / Linguistics / Sociolinguistics,Language Arts & Disciplines / Speech},
  langid = {english},
  pagetotal = {424}
}

@incollection{mcneillRightBrainGesture1995,
  title = {Right Brain and Gesture},
  booktitle = {Language, Gesture, and Space},
  author = {McNeill, David and Pedelty, Laura L.},
  date = {1995},
  pages = {63--85},
  publisher = {{Lawrence Erlbaum Associates, Inc}},
  location = {{Hillsdale, NJ, US}},
  abstract = {concerned with the gestures that occur when people speak, and the possible role of cerebral laterality in coordinating gestures with speech / interested [in delineating] the cerebral substrate of language by studying language and gesture in patients with unilateral right or left brain damage / argue that in the production of narrative discourse, there are contributions from both cerebral hemispheres, and therefore that an important aspect of the cerebral control of language is the combining of the inputs from the 2 sides of the brain / gestures provide a unique source of information about cerebral functioning and the effects of brain damage  argue that the right hemisphere plays an important role in narrative discourse, and this role is more apparent when gesture is taken into consideration / collected samples of gesticulations and speech in a quasi-experimental paradigm that constrains the content of discourse adequately to allow us to compare language and gestures across speakers / divide the gestures into 4 categories . . . : iconic, metaphoric, beat, and abstract pointing or deictic gestures  the importance of space / gestures and narrative in right hemisphere damaged patients / left-side-neglect and dissociations of space / explaining the right hemisphere syndrome / disconnection syndromes [the alien hand, commissurotomy] (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  isbn = {978-0-8058-1378-4},
  keywords = {Brain Damage,Cerebral Cortex,Cognitive Processes,Gestures,Lateral Dominance,Oral Communication,Right Hemisphere}
}

@article{mcnultyNeedleElectromyographicEvaluation1994,
  title = {Needle Electromyographic Evaluation of Trigger Point Response to a Psychological Stressor},
  author = {McNulty, W. H. and Gevirtz, R. N. and Hubbard, D. R. and Berkoff, G. M.},
  date = {1994-05},
  journaltitle = {Psychophysiology},
  shortjournal = {Psychophysiology},
  volume = {31},
  pages = {313--316},
  issn = {0048-5772},
  doi = {10.1111/j.1469-8986.1994.tb02220.x},
  abstract = {Fourteen subjects were evaluated by needle electromyography in a trapezius myofascial trigger point and simultaneously in adjacent nontender trapezius muscle fibers during a control condition (forward counting), a stressful condition (mental arithmetic), and resting baselines. Based on recent data implicating autonomic innervation in muscle function, we hypothesized that the trigger point would be more responsive than the adjacent muscle to psychological stress. The results showed increased trigger point electromyographic activity during stress, whereas the adjacent muscle remained electrically silent. These results suggest a mechanism by which emotional factors influence muscle pain. This may have significant implications for the psychophysiology of pain associated with trigger points.},
  eprint = {8008795},
  eprinttype = {pmid},
  keywords = {Adult,Arousal,Biofeedback; Psychology,Electrodes,Electromyography,Female,Galvanic Skin Response,Humans,Male,Middle Aged,Muscles,Myofascial Pain Syndromes,Stress; Psychological,Sympathetic Nervous System},
  langid = {english},
  number = {3}
}

@article{medinSystemsNonDiversity2017,
  title = {Systems of (Non-)Diversity},
  author = {Medin, Douglas and Ojalehto, Bethany and Marin, Ananda and Bang, Megan},
  date = {2017-04-18},
  journaltitle = {Nature Human Behaviour},
  volume = {1},
  pages = {1--5},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0088},
  url = {https://www.nature.com/articles/s41562-017-0088},
  urldate = {2020-07-22},
  abstract = {Intrinsic to the social, educational and behavioural sciences is the aim of addressing patterned variation in human thought and action across settings. Surprisingly, however, empirical work in these sciences continues to be limited by a lack of diversity in study populations, research methodology and the researchers themselves. This Perspective analyses these dimensions of diversity as they are situated in and affected by the larger organizational systems for publication, grants and academic advancement. This complex system appears to operate in a mutually reinforcing manner to discourage diversity. Our analysis suggests that diversity goals central to our sciences will require systems-level action rather than a focus on any one component in isolation.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TTPNSD6F\\s41562-017-0088.html},
  issue = {5},
  langid = {english},
  number = {5}
}

@article{meierSilentMandibularOscillations1997,
  title = {Silent {{Mandibular Oscillations}} in {{Vocal Babbling}}},
  author = {Meier, Richard P. and McGarvin, Lynn and Zakia, Renée A. E. and Willerman, Raquel},
  date = {1997},
  journaltitle = {Phonetica},
  shortjournal = {PHO},
  volume = {54},
  pages = {153--171},
  publisher = {{Karger Publishers}},
  issn = {0031-8388, 1423-0321},
  doi = {10.1159/000262219},
  url = {https://www.karger.com/Article/FullText/262219},
  urldate = {2020-12-01},
  abstract = {Early babbling has been characterized as being fundamentally a mandibular oscillation: the infant’s repeated lowering and raising of its mandible yields a perceived contrast between consonants produced in a closed vocal tract configuration and vowels produced with an open tract. We wondered whether babblers produce rhythmic mandibular oscillations without phonation and, if so, whether there might be a relationship between such ‘jaw wags’ and early speech. We report two studies: the first is a longitudinal, observational study of 14 infants, some of whom were hearing and some Deaf. Seven infants (3 hearing, 3 Deaf, and 1 hearing-impaired) produced numerous speech-like, rhythmic jaw wags without phonation; sometimes jaw wags formed a single utterance with phonated babbling. Most jaw wags reported here were produced when these infants were ages 8–13 months. The second study, a survey of 90 parents of 4- to 10-month-old hearing infants, suggests that silent babbles may be a widespread phenomenon of early speech development.},
  eprint = {9396166},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LU9EFCWP\\262219.html},
  langid = {english},
  number = {3-4}
}

@article{meltzoffNewbornInfantsImitate1983,
  title = {Newborn Infants Imitate Adult Facial Gestures},
  author = {Meltzoff, A. N. and Moore, M. K.},
  date = {1983},
  journaltitle = {Child Development},
  volume = {54},
  pages = {702--709},
  eprint = {1130058},
  eprinttype = {jstor},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4KFMMS5P\\1130058.html},
  number = {3}
}

@article{mendoza-dentonSemioticLayeringGesture2011,
  title = {Semiotic {{Layering}} through {{Gesture}} and {{Intonation}}: {{A Case Study}} of {{Complementary}} and {{Supplementary Multimodality}} in {{Political Speech}}},
  shorttitle = {Semiotic {{Layering}} through {{Gesture}} and {{Intonation}}},
  author = {Mendoza-Denton, Norma and Jannedy, Stefanie},
  date = {2011-09-01},
  journaltitle = {Journal of English Linguistics},
  shortjournal = {Journal of English Linguistics},
  volume = {39},
  pages = {265--299},
  issn = {0075-4242},
  doi = {10.1177/0075424211405941},
  url = {https://doi.org/10.1177/0075424211405941},
  urldate = {2020-01-26},
  abstract = {Face-to-face communication is multimodal. In face-to-face interaction scholars can observe the interplay of several “semiotic layers,” modalities of information such as syntax, discourse structure, gesture, and intonation. The authors explore the role of gesture in structuring and aligning information in spoken discourse through a study of (1) the complementary co-occurrence of gestural apices and intonational pitch accents and (2) the supplementary co-occurrence of metaphorical gestures and elements in discourse. In the naturally occurring political speech situation the authors examine, metaphorical spatialization through gesture is key in indexing contextual relationships among the speaker, the politicians or government, and other external forces. The use of gestures simultaneously aligns with intonation and metaphorically manipulates political entities in space. Discourse context and social meaning are thus constructed together through the spoken and gestural channels and are supported through fine-grained structural alignment between intonation and gesture.},
  keywords = {embodiment,gesture,intonation,multimodality,political speech,public sphere,semiotics,spoken discourse},
  langid = {english},
  number = {3}
}

@article{menezesMethodLexicalTone2020,
  title = {A Method for Lexical Tone Classification in Audio-Visual Speech},
  author = {Menezes, J. V. P. and Cantoni, M. M. and Burnham, D. and Barbosa, A. V.},
  date = {2020-09-09},
  journaltitle = {Journal of Speech Sciences},
  volume = {9},
  pages = {93--104},
  issn = {2236-9740},
  url = {http://revistas.iel.unicamp.br/ojs_joss/index.php/journalofspeechsciences/article/view/189},
  urldate = {2020-09-17},
  abstract = {This work presents a method for lexical tone classification in audio-visual speech. The method is applied to a speech data set consisting of syllables and words produced by a female native speaker of Cantonese. The data were recorded in an audio-visual speech production experiment. The visual component of speech was measured by tracking the positions of active markers placed on the speaker's face, whereas the acoustic component was measured with an ordinary microphone. A pitch tracking algorithm is used to estimate F0 from the acoustic signal. A procedure for head motion compensation is applied to the tracked marker positions in order to separate the head and face motion components. The data are then organized into four signal groups: F0, Face, Head, Face+Head. The signals in each of these groups are parameterized by means of a polynomial approximation and then used to train an LDA (Linear Discriminant Analysis) classifier that maps the input signals into one of the output classes (the lexical tones of the language). One classifier is trained for each signal group. The ability of each signal group to predict the correct lexical tones was assessed by the accuracy of the corresponding LDA classifier. The accuracy of the classifiers was obtained by means of a k-fold cross validation method. The classifiers for all signal groups performed above chance, with F0 achieving the highest accuracy, followed by Face+Head, Face, and Head, respectively. The differences in performance between all signal groups were statistically significant. Our results show that the proposed method is able to assess how well lexical tone can be predicted from different speech modalities. These results are in agreement with previous findings in the literature suggesting that lexical tone can be predicted not only from F0 signals but also, to a lesser degree, from face and head motion signals.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NRDFTJJV\\Menezes et al. - 2020 - A method for lexical tone classification in audio-.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8D4DQ9DS\\189.html},
  issue = {0},
  langid = {english},
  number = {0}
}

@article{merchantAreNonhumanPrimates2014,
  title = {Are Non-Human Primates Capable of Rhythmic Entrainment? {{Evidence}} for the Gradual Audiomotor Evolution Hypothesis},
  shorttitle = {Are Non-Human Primates Capable of Rhythmic Entrainment?},
  author = {Merchant, Hugo and Honing, Henkjan},
  date = {2014},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {7},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2013.00274},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2013.00274/full},
  urldate = {2020-12-16},
  abstract = {We propose a decomposition of the neurocognitive mechanisms that might underlie interval-based timing and rhythmic entrainment. Next to reviewing the concepts central to the definition of rhythmic entrainment, we discuss recent studies that suggest rhythmic entrainment to be specific to humans and a selected group of bird species, but, surprisingly, is not obvious in nonhuman primates. On the basis of these studies we propose the gradual audiomotor evolution hypothesis that suggests that humans fully share interval-based timing with other primates, but only partially share the ability of rhythmic entrainment (or beat-based timing). This hypothesis accommodates the fact that nonhuman primates (i.e. macaques) performance is comparable to humans in single interval tasks (such as interval reproduction, categorization, and interception), but show differences in multiple interval tasks (such as rhythmic entrainment, synchronization and continuation). Furthermore, it is in line with the observation that macaques can, apparently, synchronize in the visual domain, but show less sensitivity in the auditory domain. And finally, while macaques are sensitive to interval-based timing and rhythmic grouping, the absence of a strong coupling between the auditory and motor system of nonhuman primates might be the reason why macaques cannot rhythmically entrain in the way humans do.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Y9M78MBI\\Merchant and Honing - 2014 - Are non-human primates capable of rhythmic entrain.pdf},
  keywords = {Auditory Pathways,Basal Ganglia,Humans,macaque monkey,premotor cortex,rhythmic entrainment},
  langid = {english}
}

@article{meredithVisualAuditorySomatosensory1986,
  title = {Visual, Auditory, and Somatosensory Convergence on Cells in Superior Colliculus Results in Multisensory Integration},
  author = {Meredith, M. A. and Stein, B. E.},
  date = {1986-09-01},
  journaltitle = {Journal of Neurophysiology},
  volume = {56},
  pages = {640--662},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.1986.56.3.640},
  url = {https://journals.physiology.org/doi/abs/10.1152/jn.1986.56.3.640},
  urldate = {2020-12-05},
  abstract = {Convergence of inputs from different sensory modalities onto individual neurons is a phenomenon that occurs widely throughout the brain at many phyletic levels and appears to represent a basic neural mechanism by which an organism integrates complex environmental stimuli. In the present study, neurons in the superior colliculus (SC) were used as a model to examine how single neurons deal with simultaneous cues from different sensory modalities (e.g., visual, auditory, somatosensory). The functional result of multisensory convergence on an individual cell was determined by comparing the responses evoked from it by a combined-modality (multimodal) stimulus with those elicited by each (unimodal) component of that stimulus presented alone. Superior colliculus cells exhibited profound changes in their activity when individual sensory stimuli were combined. These "multisensory interactions" were found to be widespread among deep laminae cells and fell into one of two functional categories: response enhancement, characterized by a significant increase in the number of discharges evoked; and response depression, characterized by a significant decrease in the discharges elicited. Multisensory response interactions most often reflected a multiplicative, rather than summative, change in activity. Their absolute magnitude varied from cell to cell and, when stimulus conditions were altered, within the same cell. However, the percentage change of enhanced interactions was generally inversely related to the vigor of the responses that could be evoked by presenting each unimodal stimulus alone and suggest that the potential for response amplification was greatest when responses evoked by individual stimuli were weakest. The majority of cells exhibiting multi-sensory characteristics were demonstrated to have descending efferent projections and thus had access to premotor and motor areas of the brain stem and spinal cord involved in SC-mediated attentive and orientation behaviors. These data show that multisensory convergence provides the descending efferent cells of the SC with a dynamic response character. The responses of these cells and the SC-mediated behaviors that they underlie need not be immutably tied to the presence of any single stimulus, but can vary in response to the particular complex of stimuli present in the environment at any given moment.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TUCYE59R\\Meredith and Stein - 1986 - Visual, auditory, and somatosensory convergence on.pdf;C\:\\Users\\u668173\\Zotero\\storage\\S22N3ZT2\\jn.1986.56.3.html},
  number = {3}
}

@article{mergellModelingRoleNonhuman1999,
  title = {Modeling the Role of Nonhuman Vocal Membranes in Phonation},
  author = {Mergell, Patrick and Fitch, W. Tecumseh and Herzel, Hanspeter},
  date = {1999-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {105},
  pages = {2020--2028},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.426735},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.426735},
  urldate = {2020-09-18},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IV8FCW2H\\Mergell et al. - 1999 - Modeling the role of nonhuman vocal membranes in p.pdf;C\:\\Users\\u668173\\Zotero\\storage\\Q9BWQLQZ\\1.html},
  number = {3}
}

@article{mesquitamontesAbdominalMuscleActivity2017,
  title = {Abdominal Muscle Activity during Breathing in Different Postural Sets in Healthy Subjects},
  author = {Mesquita Montes, António and Gouveia, Sara and Crasto, Carlos and de Melo, Cristina Argel and Carvalho, Paulo and Santos, Rita and Vilas-Boas, João Paulo},
  date = {2017-04-01},
  journaltitle = {Journal of Bodywork and Movement Therapies},
  shortjournal = {Journal of Bodywork and Movement Therapies},
  volume = {21},
  pages = {354--361},
  issn = {1360-8592},
  doi = {10.1016/j.jbmt.2016.09.004},
  url = {http://www.sciencedirect.com/science/article/pii/S1360859216301930},
  urldate = {2020-06-09},
  abstract = {Objective This study aims to evaluate the effect of different postural sets on abdominal muscle activity during breathing in healthy subjects. Methods Twenty-nine higher education students (20.86~±~1.48 years; 9 males) breathed at the same rhythm (inspiration: 2~s; expiration: 4~s) in supine, standing, tripod and 4-point-kneeling positions. Surface electromyography was performed to assess the activation intensity of rectus abdominis, external oblique and transversus abdominis/internal oblique muscles during inspiration and expiration. Results During both breathing phases, the activation intensity of external oblique and transversus abdominis/internal oblique was significantly higher in standing when compared to supine (p~≤~0.001). No significant differences were found between tripod position and 4-point-kneeling positions. Transversus abdominis/internal oblique activation intensity in these positions was higher than in supine and lower than in standing. Conclusions Postural load and gravitational stretch are factors that should be considered in relation to the specific recruitment of abdominal muscles for breathing mechanics.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NUZL6WQI\\Mesquita Montes et al. - 2017 - Abdominal muscle activity during breathing in diff.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5SAP7YEB\\S1360859216301930.html},
  keywords = {Body position,Core abdominal,Postural control,Respiration,Surface electromyographic activity},
  langid = {english},
  number = {2},
  options = {useprefix=true}
}

@article{miallProprioceptiveLossPerception2018,
  title = {Proprioceptive Loss and the Perception, Control and Learning of Arm Movements in Humans: Evidence from Sensory Neuronopathy},
  shorttitle = {Proprioceptive Loss and the Perception, Control and Learning of Arm Movements in Humans},
  author = {Miall, R. Chris and Kitchen, Nick M. and Nam, Se-Ho and Lefumat, Hannah and Renault, Alix G. and Ørstavik, Kristin and Cole, Jonathan D. and Sarlegna, Fabrice R.},
  date = {2018-08},
  journaltitle = {Experimental Brain Research},
  volume = {236},
  pages = {2137--2155},
  issn = {1432-1106},
  doi = {10.1007/s00221-018-5289-0},
  abstract = {It is uncertain how vision and proprioception contribute to adaptation of voluntary arm movements. In normal participants, adaptation to imposed forces is possible with or without vision, suggesting that proprioception is sufficient; in participants with proprioceptive loss (PL), adaptation is possible with visual feedback, suggesting that proprioception is unnecessary. In experiment 1 adaptation to, and retention of, perturbing forces were evaluated in three chronically deafferented participants. They made rapid reaching movements to move a cursor toward a visual target, and a planar robot arm applied orthogonal velocity-dependent forces. Trial-by-trial error correction was observed in all participants. Such adaptation has been characterized with a dual-rate model: a fast process that learns quickly, but retains poorly and a slow process that learns slowly and retains well. Experiment 2 showed that the PL participants had large individual differences in learning and retention rates compared to normal controls. Experiment 3 tested participants' perception of applied forces. With visual feedback, the PL participants could report the perturbation's direction as well as controls; without visual feedback, thresholds were elevated. Experiment 4 showed, in healthy participants, that force direction could be estimated from head motion, at levels close to the no-vision threshold for the PL participants. Our results show that proprioceptive loss influences perception, motor control and adaptation but that~proprioception from the moving limb is not essential for adaptation to, or detection of, force fields. The differences in learning and retention seen between the three deafferented participants suggest that they achieve these tasks in idiosyncratic ways after proprioceptive loss, possibly integrating visual and vestibular information with individual cognitive strategies.},
  eprint = {29779050},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QRKIZIF9\\Miall et al. - 2018 - Proprioceptive loss and the perception, control an.pdf},
  keywords = {Adaptation,Adaptation; Physiological,Aged,Arm,Deafferentation,Female,Force-field adaptation,Human movement,Humans,Learning,Limb dynamics,Male,Middle Aged,Movement,Neuronopathy,Peripheral Nervous System Diseases,Photic Stimulation,Proprioception,Psychomotor Performance,Sensation Disorders,Sensorimotor,Vision,Vision; Ocular,Visual Perception},
  langid = {english},
  number = {8},
  pmcid = {PMC6061502}
}

@article{michelettaMulticomponentMultimodalLipsmacking2013,
  title = {Multicomponent and {{Multimodal Lipsmacking}} in {{Crested Macaques}} ( {{{\emph{Macaca}}}}{\emph{ Nigra}} ): {{Lipsmacking Behavior}} in {{Crested Macaques}}},
  shorttitle = {Multicomponent and {{Multimodal Lipsmacking}} in {{Crested Macaques}} ( {{{\emph{Macaca}}}}{\emph{ Nigra}} )},
  author = {Micheletta, Jérôme and Engelhardt, Antje and Matthews, Lee and Agil, Muhammad and Waller, Bridget M.},
  date = {2013-07},
  journaltitle = {American Journal of Primatology},
  volume = {75},
  pages = {763--773},
  issn = {02752565},
  doi = {10.1002/ajp.22105},
  url = {http://doi.wiley.com/10.1002/ajp.22105},
  urldate = {2019-09-17},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DFIKF4E2\\Micheletta et al. - 2013 - Multicomponent and Multimodal Lipsmacking in Crest.pdf},
  langid = {english},
  number = {7}
}

@article{millerEffectsHummingPitch2012,
  title = {The {{Effects}} of {{Humming}} and {{Pitch}} on {{Craniofacial}} and {{Craniocervical Morphology Measured Using MRI}}},
  author = {Miller, Nicola A. and Gregory, Jennifer S. and Semple, Scott I. K. and Aspden, Richard M. and Stollery, Peter J. and Gilbert, Fiona J.},
  date = {2012-01-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {26},
  pages = {90--101},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2010.10.017},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199710002109},
  urldate = {2020-10-06},
  abstract = {Objectives/Hypothesis Traditional voice research occurs within a phonetic context. Accordingly, pitch-related contributions are inseparable from those due to articulator input. In humming, articulator input is negligible. Using magnetic resonance imaging, we test the hypothesis that voice production is accompanied by pitch-related adjustments unrelated to articulatory or postural input. Study Design/Method In this cross-sectional study, 10 healthy volunteers (five men, five women, aged 20–47 years, median 25 years), including singers (6 months to 10 years tuition, median 2 years) and non-singers, were assessed to establish the lowest and highest notes they could comfortably sustain while humming over 20 seconds. With head position stable, midsagittal images were acquired while volunteers hummed these predetermined low and high notes. Twenty-two craniocervical, angular, and linear dimensions defined on these images were compared using one-way repeated-measures analysis of variance. Correlations between variables were sought using Pearson correlation coefficient. Results We found significant differences between low- and high-note conditions in six of 22 measures and widespread pitch-related correlations between variables (r≥0.63, P{$<$}0.05). Compared with low-note humming, high-note humming was accompanied by increased craniocervical angles opt/nsl and cvt/nsl (P=0.008 and 0.002, respectively); widening of the C3-menton distance (P=0.003), a rise of the larynx and hyoid in relation to the cranial base (P=0.012 and {$<$}0.001, respectively), and an increased sternum-hyoid distance (P{$<$}0.001). Conclusion Voice production is accompanied by pitch-related adjustments that are currently being masked by, or~mistakenly attributed to, articulatory or postural input, identification of which could improve understanding of~mechanisms underlying speech and song.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\S56UHDWS\\Miller et al. - 2012 - The Effects of Humming and Pitch on Craniofacial a.pdf;C\:\\Users\\u668173\\Zotero\\storage\\Y2Z4DGLA\\S0892199710002109.html},
  keywords = {Craniocervical,Craniofacial,Humming,Morphology,MRI,Pitch,Vocal tract},
  langid = {english},
  number = {1}
}

@article{millerMusicalListeningKinesthesis2016,
  title = {Musical Listening and Kinesthesis: {{Is}} There an Audio-Vocal Tuning System?},
  shorttitle = {Musical Listening and Kinesthesis},
  author = {Miller, Nicola A.},
  date = {2016},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  volume = {26},
  pages = {117--131},
  publisher = {{Educational Publishing Foundation}},
  location = {{US}},
  issn = {2162-1535(Electronic),0275-3987(Print)},
  doi = {10.1037/pmu0000138},
  abstract = {Kinesthesis, the sense of muscular effort that accompanies bodily movement, is known to be important in musical performance. Less well understood is the role of the kinesthetic sense in musical listening. Recent observations that listening to music is associated with fast, subtle, pitch-related patterns of kinesthetic sensations that involve the ears, eustachian tubes, nasopharynx, vocal tract, and even muscles of facial expression challenges traditional accounts of auditory processing divorced from peripheral vocal input and suggests, instead, the hypothesis that auditory and vocal processing mechanisms rely on shared peripheral substrates in addition to shared central (brain-based) substrates. Furthermore, the presence of kinesthetic sensations that arise in response to novelty, following voluntary switches of attention and even in anticipation of familiar sounds suggests that the kinesthetic sense plays an important part in the listening process. Here, the significance of kinesthetic sensations associated with listening behavior is discussed within the context of recent MRI investigations (where pitch-related changes associated with vocal production are investigated under conditions that reduce articulatory and postural input to a minimum) together with evidence from a diverse range of historical and contemporary sources. Overall, evidence from a wide range of disciplines supports the hypothesis that auditory-vocal processing relies on shared peripheral substrates in addition to shared central substrates and suggests a framework within which kinesthetic vocal sensations may be further investigated. Wide-ranging implications arising from improved awareness of the part played by the kinesthetic sense in musical listening are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WF4R6AUJ\\2016-10829-001.html},
  keywords = {Auditory Perception,Kinesthetic Perception,Pitch (Frequency),Voice},
  number = {2}
}

@article{millerRelationshipsVocalStructures2012,
  title = {Relationships {{Between Vocal Structures}}, the {{Airway}}, and {{Craniocervical Posture Investigated Using Magnetic Resonance Imaging}}},
  author = {Miller, Nicola A. and Gregory, Jennifer S. and Semple, Scott I. K. and Aspden, Richard M. and Stollery, Peter J. and Gilbert, Fiona J.},
  date = {2012-01-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {26},
  pages = {102--109},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2010.10.016},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199710002092},
  urldate = {2020-10-06},
  abstract = {Objectives Traditional voice research focuses on the vocal tract, articulators, and larynx. By ignoring their direct/indirect attachments (skull, cervical spine, and sternum) important information may be missed. We aim to investigate vocal structures within this wider context and assess the validity of this approach for subsequent voice production studies. Study Design/Method Using a cross-sectional study design, we obtained midsagittal MR images from 10 healthy adults (five males and five females) while at rest and breathing quietly. With reference points based on cephalometry, 17~craniocervical, craniocaudal, and anteroposterior variables were chosen to describe craniofacial morphology, craniocervical posture, and airway dimensions. Relationships between variables were sought using Pearson’s correlation coefficient. Results We found widespread correlations relating vocal structures to the craniofacial skeleton and cervical spine (r{$>$}0.6). Increasing airway size (hyocervical distance) was associated with greater distances from the cranial base of the hyoid, larynx, epiglottis tip and uvula tip, and of C3 from the menton. A wider velopharyngeal opening was associated with a shorter and higher soft palate, and a greater (lower) craniocervical angle was associated with a wider laryngeal~tube opening, narrower airway at the uvula tip and shorter distances of the hyoid and uvula tip from the cranial base. Conclusion Finding widespread correlations relating vocal structures to the craniofacial skeleton and cervical spine confirms the potential of this approach to uncover functional activity during voice production and demonstrates the importance~of considering vocal structures and the airway within this wider context if important information is not to be missed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\F2DMFUMV\\Miller et al. - 2012 - Relationships Between Vocal Structures, the Airway.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZM65M7ZS\\S0892199710002092.html},
  keywords = {Cephalometry,MRI,Posture,Speech,Vocal tract},
  langid = {english},
  number = {1}
}

@article{millerUsingActiveShape2014,
  title = {Using {{Active Shape Modeling Based}} on {{MRI}} to {{Study Morphologic}} and {{Pitch}}-{{Related Functional Changes Affecting Vocal Structures}} and the {{Airway}}},
  author = {Miller, Nicola A. and Gregory, Jennifer S. and Aspden, Richard M. and Stollery, Peter J. and Gilbert, Fiona J.},
  date = {2014-09-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {28},
  pages = {554--564},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2013.12.002},
  url = {http://www.sciencedirect.com/science/article/pii/S089219971300249X},
  urldate = {2020-10-06},
  abstract = {Objective The shape of the vocal tract and associated structures (eg, tongue and velum) is complicated and varies according to development and function. This variability challenges interpretation of voice experiments. Quantifying differences between shapes and understanding how vocal structures move in relation to each other is difficult using traditional linear and angle measurements. With statistical shape models, shape can be characterized in terms of independent modes of variation. Here, we build an active shape model (ASM) to assess morphologic and pitch-related functional changes affecting vocal structures and the airway. Method Using a cross-sectional study design, we obtained six midsagittal magnetic resonance images from 10 healthy adults (five men and five women) at rest, while breathing out, and while listening to, and humming low and high notes. Eighty landmark points were chosen to define the shape of interest and an ASM was built using these (60) images. Principal component analysis was used to identify independent modes of variation, and statistical analysis was performed using one-way repeated-measures analysis of variance. Results Twenty modes of variation were identified with modes 1 and 2 accounting for half the total variance. Modes 1~and 9 were significantly associated with humming low and high notes (P~{$<~$}0.001) and showed coordinated changes affecting the cervical spine, vocal structures, and airway. Mode 2 highlighted wide structural variations between subjects. Conclusion This study highlights the potential of active shape modeling to advance understanding of factors underlying morphologic and pitch-related functional variations affecting vocal structures and the airway in health and disease.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AEY6JING\\Miller et al. - 2014 - Using Active Shape Modeling Based on MRI to Study .pdf;C\:\\Users\\u668173\\Zotero\\storage\\DYZNFR74\\S089219971300249X.html},
  keywords = {Active appearance model,Active shape model,Cervical spine,Humming,MRI,Pitch,Posture,Vocal tract},
  langid = {english},
  number = {5}
}

@article{millikanDefenseProperFunctions1989,
  title = {In {{Defense}} of {{Proper Functions}}},
  author = {Millikan, Ruth Garrett},
  date = {1989},
  journaltitle = {Philosophy of Science},
  volume = {56},
  pages = {288--302},
  issn = {0031-8248},
  abstract = {I defend the historical definition of "function" originally given in my Language, Thought and Other Biological Categories (1984a). The definition was not offered in the spirit of conceptual analysis but is more akin to a theoretical definition of "function". A major theme is that nonhistorical analyses of "function" fail to deal adequately with items that are not capable of performing their functions.},
  eprint = {187875},
  eprinttype = {jstor},
  number = {2}
}

@article{milneAuditoryVisualSequence2018,
  title = {Auditory and {{Visual Sequence Learning}} in {{Humans}} and {{Monkeys}} Using an {{Artificial Grammar Learning Paradigm}}},
  author = {Milne, Alice E. and Petkov, Christopher I. and Wilson, Benjamin},
  date = {2018-10-01},
  journaltitle = {Neuroscience},
  shortjournal = {Neuroscience},
  volume = {389},
  pages = {104--117},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2017.06.059},
  url = {http://www.sciencedirect.com/science/article/pii/S0306452217304645},
  urldate = {2020-03-18},
  abstract = {Language flexibly supports the human ability to communicate using different sensory modalities, such as writing and reading in the visual modality and speaking and listening in the auditory domain. Although it has been argued that nonhuman primate communication abilities are inherently multisensory, direct behavioural comparisons between human and nonhuman primates are scant. Artificial grammar learning (AGL) tasks and statistical learning experiments can be used to emulate ordering relationships between words in a sentence. However, previous comparative work using such paradigms has primarily investigated sequence learning within a single sensory modality. We used an AGL paradigm to evaluate how humans and macaque monkeys learn and respond to identically structured sequences of either auditory or visual stimuli. In the auditory and visual experiments, we found that both species were sensitive to the ordering relationships between elements in the sequences. Moreover, the humans and monkeys produced largely similar response patterns to the visual and auditory sequences, indicating that the sequences are processed in comparable ways across the sensory modalities. These results provide evidence that human sequence processing abilities stem from an evolutionarily conserved capacity that appears to operate comparably across the sensory modalities in both human and nonhuman primates. The findings set the stage for future neurobiological studies to investigate the multisensory nature of these sequencing operations in nonhuman primates and how they compare to related processes in humans.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NPC4T9E9\\Milne et al. - 2018 - Auditory and Visual Sequence Learning in Humans an.pdf;C\:\\Users\\u668173\\Zotero\\storage\\GLI6UCE7\\S0306452217304645.html},
  keywords = {auditory,comparative,human,macaque,structured sequence learning,visual},
  langid = {english},
  series = {Sensory {{Sequence Processing}} in the {{Brain}}}
}

@article{mirandaComplexUpperLimbMovements2018,
  title = {Complex {{Upper}}-{{Limb Movements Are Generated}} by {{Combining Motor Primitives}} That {{Scale}} with the {{Movement Size}}},
  author = {Miranda, Jose Garcia Vivas and Daneault, Jean-François and Vergara-Diaz, Gloria and e Torres, Ângelo Frederico Souza de Oliveira and Quixadá, Ana Paula and Fonseca, Marcus de Lemos and Vieira, João Paulo Bomfim Cruz and dos Santos, Vitor Sotero and da Figueiredo, Thiago Cruz and Pinto, Elen Beatriz and Peña, Norberto and Bonato, Paolo},
  date = {2018-08-27},
  journaltitle = {Scientific Reports},
  volume = {8},
  pages = {12918},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-29470-y},
  url = {https://www.nature.com/articles/s41598-018-29470-y},
  urldate = {2021-02-17},
  abstract = {The hand trajectory of motion during the performance of one-dimensional point-to-point movements has been shown to be marked by motor primitives with a bell-shaped velocity profile. Researchers have investigated if motor primitives with the same shape mark also complex upper-limb movements. They have done so by analyzing the magnitude of the hand trajectory velocity vector. This approach has failed to identify motor primitives with a bell-shaped velocity profile as the basic elements underlying the generation of complex upper-limb movements. In this study, we examined upper-limb movements by analyzing instead the movement components defined according to a Cartesian coordinate system with axes oriented in the medio-lateral, antero-posterior, and vertical directions. To our surprise, we found out that a broad set of complex upper-limb movements can be modeled as a combination of motor primitives with a bell-shaped velocity profile defined according to the axes of the above-defined coordinate system. Most notably, we discovered that these motor primitives scale with the size of movement according to a power law. These results provide a novel key to the interpretation of brain and muscle synergy studies suggesting that human subjects use a scale-invariant encoding of movement patterns when performing upper-limb movements.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EGE66938\\Miranda et al. - 2018 - Complex Upper-Limb Movements Are Generated by Comb.pdf;C\:\\Users\\u668173\\Zotero\\storage\\NS8DISAL\\s41598-018-29470-y.html},
  issue = {1},
  langid = {english},
  number = {1},
  options = {useprefix=true}
}

@article{mocciaFascialBundlesInfraspinatus2016,
  title = {Fascial Bundles of the Infraspinatus Fascia: Anatomy, Function, and Clinical Considerations},
  shorttitle = {Fascial Bundles of the Infraspinatus Fascia},
  author = {Moccia, David and Nackashi, Andrew A. and Schilling, Rebecca and Ward, Peter J.},
  date = {2016-01},
  journaltitle = {Journal of Anatomy},
  shortjournal = {J. Anat.},
  volume = {228},
  pages = {176--183},
  issn = {1469-7580},
  doi = {10.1111/joa.12386},
  abstract = {The infraspinatus fascia is a tough sheet of connective tissue that covers the infraspinatus fossa of the scapula and the muscle within. Muscle fibers originate from the fossa and fascia and then travel laterally to insert on the greater tubercle of the humerus. Frequently the infraspinatus fascia is quickly removed to appreciate the underlying muscle, but the fascia is an interesting and complex structure in its own right. Despite having a characteristic set of fascial bundles, no contemporary anatomy texts or atlases describe the fascia in detail. The infraspinatus fascia was dissected in detail in 11 shoulders, to characterize the fascial bundles and connections that contribute to it. Thereafter, 70 shoulders were dissected to tabulate the variability of the fascial bundles and connections. Six characteristic features of the infraspinatus fascia were noted: a medial band, an inferior-lateral band, and superior-lateral band of fascia, insertion of the posterior deltoid into the infraspinatus fascia, a transverse connection from the posterior deltoid muscle to the infraspinatus fascia, and a retinacular sheet deep to the deltoid and superficial to the infraspinatus and teres minor muscles. Although other structures of the shoulder are more frequently injured, the infraspinatus fascia is involved in compartment syndromes and the fascial bundles of this structure are certain to impact the biomechanical function of the muscles of the posterior shoulder.},
  eprint = {26403802},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KM4AHRGU\\Moccia et al. - 2016 - Fascial bundles of the infraspinatus fascia anato.pdf},
  keywords = {Cadaver,compartment syndrome,connective tissue,Dissection,fascia,Fascia,Female,Humans,infraspinatus,Male,Muscle; Skeletal,scapula,Scapula,shoulder,Shoulder Joint},
  langid = {english},
  number = {1},
  pmcid = {PMC4694164}
}

@article{mooreDiffusionTensorMRI2017,
  title = {Diffusion Tensor {{MRI}} Tractography Reveals Increased Fractional Anisotropy ({{FA}}) in Arcuate Fasciculus Following Music-Cued Motor Training},
  author = {Moore, Emma and Schaefer, Rebecca S. and Bastin, Mark E. and Roberts, Neil and Overy, Katie},
  date = {2017-08-01},
  journaltitle = {Brain and Cognition},
  shortjournal = {Brain and Cognition},
  volume = {116},
  pages = {40--46},
  issn = {0278-2626},
  doi = {10.1016/j.bandc.2017.05.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0278262617300210},
  urldate = {2020-12-07},
  abstract = {Auditory cues are frequently used to support movement learning and rehabilitation, but the neural basis of this behavioural effect is not yet clear. We investigated the microstructural neuroplasticity effects of adding musical cues to a motor learning task. We hypothesised that music-cued, left-handed motor training would increase fractional anisotropy (FA) in the contralateral arcuate fasciculus, a fibre tract connecting auditory, pre-motor and motor regions. Thirty right-handed participants were assigned to a motor learning condition either with (Music Group) or without (Control Group) musical cues. Participants completed 20minutes of training three times per week over four weeks. Diffusion tensor MRI and probabilistic neighbourhood tractography identified FA, axial (AD) and radial (RD) diffusivity before and after training. Results revealed that FA increased significantly in the right arcuate fasciculus of the Music group only, as hypothesised, with trends for AD to increase and RD to decrease, a pattern of results consistent with activity-dependent increases in myelination. No significant changes were found in the left ipsilateral arcuate fasciculus of either group. This is the first evidence that adding musical cues to movement learning can induce rapid microstructural change in white matter pathways in adults, with potential implications for therapeutic clinical practice.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\78ZHMKG3\\Moore et al. - 2017 - Diffusion tensor MRI tractography reveals increase.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5SCUKXU9\\S0278262617300210.html},
  keywords = {Arcuate fasciculus,Auditory-motor training,Diffusion tensor MRI,Music-cued motor training,Neuroplasticity,White matter},
  langid = {english}
}

@article{morettContrastEyeBeholder2020,
  title = {Contrast {{Is}} in the {{Eye}} of the {{Beholder}}: {{Infelicitous Beat Gesture Increases Cognitive Load During Online Spoken Discourse Comprehension}}},
  shorttitle = {Contrast {{Is}} in the {{Eye}} of the {{Beholder}}},
  author = {Morett, Laura M. and Roche, Jennifer M. and Fraundorf, Scott H. and McPartland, James C.},
  date = {2020},
  journaltitle = {Cognitive Science},
  volume = {44},
  pages = {e12912},
  issn = {1551-6709},
  doi = {10.1111/cogs.12912},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12912},
  urldate = {2020-10-26},
  abstract = {We investigated how two cues to contrast—beat gesture and contrastive pitch accenting—affect comprehenders' cognitive load during processing of spoken referring expressions. In two visual-world experiments, we orthogonally manipulated the presence of these cues and their felicity, or fit, with the local (sentence-level) referential context in critical referring expressions while comprehenders' task-evoked pupillary responses (TEPRs) were examined. In Experiment 1, beat gesture and contrastive accenting always matched the referential context of filler referring expressions and were therefore relatively felicitous on the global (experiment) level, whereas in Experiment 2, beat gesture and contrastive accenting never fit the referential context of filler referring expressions and were therefore infelicitous on the global level. The results revealed that both beat gesture and contrastive accenting increased comprehenders' cognitive load. For beat gesture, this increase in cognitive load was driven by both local and global infelicity. For contrastive accenting, this increase in cognitive load was unaffected when cues were globally felicitous but exacerbated when cues were globally infelicitous. Together, these results suggest that comprehenders' cognitive resources are taxed by processing infelicitous use of beat gesture and contrastive accenting to convey contrast on both the local and global levels.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12912},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TJFNLDPE\\cogs.html},
  keywords = {Beat gesture,Eye-tracking,Linguistic contrast,Pitch accent,Pupillometry,Visual world},
  langid = {english},
  number = {10}
}

@article{morettN400AmplitudeLatency2020,
  title = {N400 Amplitude, Latency, and Variability Reflect Temporal Integration of Beat Gesture and Pitch Accent during Language Processing},
  author = {Morett, Laura M. and Landi, Nicole and Irwin, Julia and McPartland, James C.},
  date = {2020-11-15},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  volume = {1747},
  pages = {147059},
  issn = {0006-8993},
  doi = {10.1016/j.brainres.2020.147059},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899320304170},
  urldate = {2020-09-19},
  abstract = {This study examines how across-trial (average) and trial-by-trial (variability in) amplitude and latency of the N400 event-related potential (ERP) reflect temporal integration of pitch accent and beat gesture. Thirty native English speakers viewed videos of a talker producing sentences with beat gesture co-occurring with a pitch accented focus word (synchronous), beat gesture co-occurring with the onset of a subsequent non-focused word (asynchronous), or the absence of beat gesture (no beat). Across trials, increased amplitude and earlier latency were observed when beat gesture was temporally asynchronous with pitch accenting than when it was temporally synchronous with pitch accenting or absent. Moreover, temporal asynchrony of beat gesture relative to pitch accent increased trial-by-trial variability of N400 amplitude and latency and influenced the relationship between across-trial and trial-by-trial N400 latency. These results indicate that across-trial and trial-by-trial amplitude and latency of the N400 ERP reflect temporal integration of beat gesture and pitch accent during language comprehension, supporting extension of the integrated systems hypothesis of gesture-speech processing and neural noise theories to focus processing in typical adult populations.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IRH97JKD\\S0006899320304170.html},
  keywords = {Beat gesture,Evoked response variability,N400,Pitch accent,Temporal integration},
  langid = {english}
}

@article{morillonMotorOriginTemporal2017,
  title = {Motor Origin of Temporal Predictions in Auditory Attention},
  author = {Morillon, Benjamin and Baillet, Sylvain},
  date = {2017-10-17},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {114},
  pages = {E8913-E8921},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1705373114},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1705373114},
  urldate = {2019-09-17},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FV8UVQLQ\\Morillon and Baillet - 2017 - Motor origin of temporal predictions in auditory a.pdf},
  langid = {english},
  number = {42}
}

@article{morillonProminenceDeltaOscillatory2019,
  title = {Prominence of Delta Oscillatory Rhythms in the Motor Cortex and Their Relevance for Auditory and Speech Perception},
  author = {Morillon, Benjamin and Arnal, Luc H. and Schroeder, Charles E. and Keitel, Anne},
  date = {2019-12-01},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  shortjournal = {Neuroscience \& Biobehavioral Reviews},
  volume = {107},
  pages = {136--142},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2019.09.012},
  url = {http://www.sciencedirect.com/science/article/pii/S0149763419300922},
  urldate = {2020-10-30},
  abstract = {In the motor cortex, beta oscillations (∼12–30 Hz) are generally considered a principal rhythm contributing to movement planning and execution. Beta oscillations cohabit and dynamically interact with slow delta oscillations (0.5–4 Hz), but the role of delta oscillations and the subordinate relationship between these rhythms in the perception-action loop remains unclear. Here, we review evidence that motor delta oscillations shape the dynamics of motor behaviors and sensorimotor processes, in particular during auditory perception. We describe the functional coupling between delta and beta oscillations in the motor cortex during spontaneous and planned motor acts. In an active sensing framework, perception is strongly shaped by motor activity, in particular in the delta band, which imposes temporal constraints on the sampling of sensory information. By encoding temporal contextual information, delta oscillations modulate auditory processing and impact behavioral outcomes. Finally, we consider the contribution of motor delta oscillations in the perceptual analysis of speech signals, providing a contextual temporal frame to optimize the parsing and processing of slow linguistic information.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZZYMQKTV\\Morillon et al. - 2019 - Prominence of delta oscillatory rhythms in the mot.pdf},
  keywords = {Active inference,Active sensing,Audio-motor coupling,Delta,Neural oscillations,Rhythm,Speech perception},
  langid = {english}
}

@article{morrel-samuelsWordFamiliarityPredicts1992,
  title = {Word Familiarity Predicts Temporal Asynchrony of Hand Gestures and Speech},
  author = {Morrel-Samuels, Palmer and Krauss, Robert M.},
  date = {1992},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {18},
  pages = {615--622},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1285(Electronic),0278-7393(Print)},
  doi = {10.1037/0278-7393.18.3.615},
  abstract = {17 Ss were videotaped as they provided narrative descriptions of 13 photographs. Judgments from 129 naive untrained Ss were used to isolate 60 speech-related gestures and their lexical affiliates (i.e., the accompanying word or phrase judged as related in meaning) from these 221 narratives. A computer-video interface measured each gesture, and a 3rd group of Ss rated word familiarity of each lexical affiliate. Multiple regression revealed that gesture onset preceded voice onset by an interval whose magnitude was inversely related to the lexical affiliate's rated familiarity. The lexical affiliate's familiarity was also inversely related to gesture duration. Results suggest that difficulty encountered during lexical access affects both gesture and speech. Familiarity's systematic relations with gesture-speech asynchrony and gesture duration make it unlikely that speech and gesture are produced independently by autonomous modules. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RKUSUQDK\\1992-30036-001.html},
  keywords = {Familiarity,Gestures,Lexical Access,Words (Phonetic Units)},
  number = {3}
}

@article{motamediEvolvingArtificialSign2019,
  title = {Evolving Artificial Sign Languages in the Lab: {{From}} Improvised Gesture to Systematic Sign},
  shorttitle = {Evolving Artificial Sign Languages in the Lab},
  author = {Motamedi, Yasamin and Schouwstra, Marieke and Smith, Kenny and Culbertson, Jennifer and Kirby, Simon},
  date = {2019-11-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {192},
  pages = {103964},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.05.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027719301234},
  urldate = {2019-12-03},
  abstract = {Recent work on emerging sign languages provides evidence for how key properties of linguistic systems are created. Here we use laboratory experiments to investigate the contribution of two specific mechanisms—interaction and transmission—to the emergence of a manual communication system in silent gesturers. We show that the combined effects of these mechanisms, rather than either alone, maintain communicative efficiency, and lead to a gradual increase of regularity and systematic structure. The gestures initially produced by participants are unsystematic and resemble pantomime, but come to develop key language-like properties similar to those documented in newly emerging sign systems.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9QYZU2GS\\Motamedi et al. - 2019 - Evolving artificial sign languages in the lab Fro.pdf},
  keywords = {Interaction,Iterated learning,Language evolution,Sign language,Silent gesture,Transmission},
  langid = {english}
}

@inproceedings{mueenExtractingOptimalPerformance2016,
  title = {Extracting {{Optimal Performance}} from {{Dynamic Time Warping}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Mueen, Abdullah and Keogh, Eamonn},
  date = {2016},
  pages = {2129--2130},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2939672.2945383},
  url = {http://doi.acm.org/10.1145/2939672.2945383},
  urldate = {2019-06-24},
  abstract = {Dynamic Time Warping (DTW) is a distance measure that compares two time series after optimally aligning them. DTW is being used for decades in thousands of academic and industrial projects despite the very expensive computational complexity, O(n2). These applications include data mining, image processing, signal processing, robotics and computer graphics among many others. In spite of all this research effort, there are many myths and misunderstanding about DTW in the literature, for example "it is too slow to be useful" or "the warping window size does not matter much." In this tutorial, we correct these misunderstandings and we summarize the research efforts in optimizing both the efficiency and effectiveness of both the basic DTW algorithm, and of the higher-level algorithms that exploit DTW such as similarity search, clustering and classification. We will discuss variants of DTW such as constrained DTW, multidimensional DTW and asynchronous DTW, and optimization techniques such as lower bounding, early abandoning, run-length encoding, bounded approximation and hardware optimization. We will discuss a multitude of application areas including physiological monitoring, social media mining, activity recognition and animal sound processing. The optimization techniques are generalizable to other domains on various data types and problems.},
  isbn = {978-1-4503-4232-2},
  keywords = {approximation,dynamic time warping,lower bounds,pruning,time series},
  series = {{{KDD}} '16},
  venue = {San Francisco, California, USA}
}

@inproceedings{mueenExtractingOptimalPerformance2016a,
  title = {Extracting Optimal Performance from Dynamic Time Warping},
  booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Mueen, A. K. and Keogh, E.},
  date = {2016},
  pages = {2129--2130},
  doi = {10.1145/ 2939672.2945383}
}

@article{mullerCardiacRespiratoryPatterns2011,
  title = {Cardiac and {{Respiratory Patterns Synchronize}} between {{Persons}} during {{Choir Singing}}},
  author = {Müller, Viktor and Lindenberger, Ulman},
  date = {2011-09-21},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {6},
  pages = {e24893},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0024893},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0024893},
  urldate = {2020-10-27},
  abstract = {Dyadic and collective activities requiring temporally coordinated action are likely to be associated with cardiac and respiratory patterns that synchronize within and between people. However, the extent and functional significance of cardiac and respiratory between-person couplings have not been investigated thus far. Here, we report interpersonal oscillatory couplings among eleven singers and one conductor engaged in choir singing. We find that: (a) phase synchronization both in respiration and heart rate variability increase significantly during singing relative to a rest condition; (b) phase synchronization is higher when singing in unison than when singing pieces with multiple voice parts; (c) directed coupling measures are consistent with the presence of causal effects of the conductor on the singers at high modulation frequencies; (d) the different voices of the choir are reflected in network analyses of cardiac and respiratory activity based on graph theory. Our results suggest that oscillatory coupling of cardiac and respiratory patterns provide a physiological basis for interpersonal action coordination.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QADDBTIW\\Müller and Lindenberger - 2011 - Cardiac and Respiratory Patterns Synchronize betwe.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5BVR6YSG\\article.html;C\:\\Users\\u668173\\Zotero\\storage\\R8FWST5Z\\article.html},
  keywords = {Acoustic signals,Breathing,Conductors,Electrocardiography,Eyes,Heart rate,Phase determination,Phase diagrams},
  langid = {english},
  number = {9}
}

@incollection{mullerGesturalModesRepresentation2013,
  title = {Gestural Modes of Representation as Techniques of Depiction},
  booktitle = {Body–Language–Communication: {{An}} International Handbook on Multimodality in Human Interaction},
  author = {Müller, C},
  editor = {Müller, C},
  date = {2013},
  pages = {1687--1701},
  publisher = {{De Gruyter Mouton}},
  location = {{Berlin, Germany}}
}

@article{mullerGestureSignCataclysmic2018,
  title = {Gesture and {{Sign}}: {{Cataclysmic Break}} or {{Dynamic Relations}}?},
  shorttitle = {Gesture and {{Sign}}},
  author = {Müller, Cornelia},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {9},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.01651},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.01651/full},
  urldate = {2019-08-26},
  abstract = {Abstract The goal of the article is to offer a framework against which relations between gesture and sign can be systematically explored beyond the current literature. It does so by a) reconstructing the history of the discussion in the field of gesture studies, focusing on three leading positions (Kendon, McNeill, Goldin-Meadow); and b) by formulating a position to illustrate how this can be achieved. The paper concludes by emphasizing the need for systematic cross-linguistic research on multimodal use of language in its signed and spoken forms.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KM4GGRE6\\Müller - 2018 - Gesture and Sign Cataclysmic Break or Dynamic Rel.pdf},
  keywords = {Conventionalization processes,Emblems,Gesture and sign,McNeill's gesture-sign continua,Multimodality of language use,Recurrent gestures,Silent gestures,Singular gestures},
  langid = {english}
}

@article{mullerGestureSignCataclysmic2018a,
  title = {Gesture and {{Sign}}: {{Cataclysmic Break}} or {{Dynamic Relations}}?},
  shorttitle = {Gesture and {{Sign}}},
  author = {Müller, Cornelia},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {9},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.01651},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.01651/full},
  urldate = {2021-01-23},
  abstract = {Abstract The goal of the article is to offer a framework against which relations between gesture and sign can be systematically explored beyond the current literature. It does so by a) reconstructing the history of the discussion in the field of gesture studies, focusing on three leading positions (Kendon, McNeill, Goldin-Meadow); and b) by formulating a position to illustrate how this can be achieved. The paper concludes by emphasizing the need for systematic cross-linguistic research on multimodal use of language in its signed and spoken forms.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\APZR3N2G\\Müller - 2018 - Gesture and Sign Cataclysmic Break or Dynamic Rel.pdf},
  keywords = {Conventionalization processes,Emblems,Gesture and sign,McNeill's gesture-sign continua,Multimodality of language use,Recurrent gestures,Silent gestures,Singular gestures},
  langid = {english}
}

@book{mullerInformationRetrievalMusic2007,
  title = {Information Retrieval for Music and Motion},
  author = {Muller, M.},
  date = {2007},
  publisher = {{Springer}},
  location = {{Heidelberg, Germany}}
}

@article{munhallVisualProsodySpeech2004,
  title = {Visual Prosody and Speech Intelligibility: Head Movement Improves Auditory Speech Perception},
  shorttitle = {Visual Prosody and Speech Intelligibility},
  author = {Munhall, K. G. and Jones, Jeffery A. and Callan, Daniel E. and Kuratate, Takaaki and Vatikiotis-Bateson, Eric},
  date = {2004-02},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {15},
  pages = {133--137},
  issn = {0956-7976},
  doi = {10.1111/j.0963-7214.2004.01502010.x},
  abstract = {People naturally move their heads when they speak, and our study shows that this rhythmic head motion conveys linguistic information. Three-dimensional head and face motion and the acoustics of a talker producing Japanese sentences were recorded and analyzed. The head movement correlated strongly with the pitch (fundamental frequency) and amplitude of the talker's voice. In a perception study, Japanese subjects viewed realistic talking-head animations based on these movement recordings in a speech-in-noise task. The animations allowed the head motion to be manipulated without changing other characteristics of the visual or acoustic speech. Subjects correctly identified more syllables when natural head motion was present in the animation than when it was eliminated or distorted. These results suggest that nonverbal gestures such as head movements play a more direct role in the perception of speech than previously known.},
  eprint = {14738521},
  eprinttype = {pmid},
  keywords = {Adult,Biomechanical Phenomena,Facial Expression,Female,Gestures,Head Movements,Humans,Imaging; Three-Dimensional,Male,Perceptual Distortion,Phonetics,Semantics,Sound Localization,Sound Spectrography,Speech Acoustics,Speech Intelligibility,Speech Perception,User-Computer Interface},
  langid = {english},
  number = {2}
}

@article{munozOptimalMultisensoryIntegration2019,
  title = {Optimal Multisensory Integration},
  author = {Munoz, Nicole E and Blumstein, Daniel T},
  editor = {Simmons, Leigh},
  date = {2019-10-21},
  journaltitle = {Behavioral Ecology},
  issn = {1045-2249, 1465-7279},
  doi = {10.1093/beheco/arz175},
  url = {https://academic.oup.com/beheco/advance-article/doi/10.1093/beheco/arz175/5601418},
  urldate = {2020-10-13},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CUUDSVHB\\Munoz and Blumstein - 2019 - Optimal multisensory integration.pdf},
  langid = {english}
}

@article{murillooosterwijkCommunicativeKnowledgePervasively2017,
  title = {Communicative Knowledge Pervasively Influences Sensorimotor Computations},
  author = {Murillo Oosterwijk, Anke and de Boer, Miriam and Stolk, Arjen and Hartmann, Frank and Toni, Ivan and Verhagen, Lennart},
  date = {2017-06-27},
  journaltitle = {Scientific Reports},
  volume = {7},
  pages = {4268},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-04442-w},
  url = {https://www.nature.com/articles/s41598-017-04442-w},
  urldate = {2021-03-03},
  abstract = {Referential pointing is a characteristically human behavior, which involves moving a finger through space to direct an addressee towards a desired mental state. Planning this type of action requires an interface between sensorimotor and conceptual abilities. A simple interface could supplement spatially-guided motor routines with communicative-ostensive cues. For instance, a pointing finger held still for an extended period of time could aid the addressee’s understanding, without altering the movement’s trajectory. A more complex interface would entail communicative knowledge penetrating the sensorimotor system and directly affecting pointing trajectories. We compare these two possibilities using motion analyses of referential pointing during multi-agent interactions. We observed that communicators produced ostensive cues that were sensitive to the communicative context. Crucially, we also observed pervasive adaptations to the pointing trajectories: they were tailored to the communicative context and to partner-specific information. These findings indicate that human referential pointing is planned and controlled on the basis of partner-specific knowledge, over and above the tagging of motor routines with ostensive cues.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\K3PQ8MT5\\Murillo Oosterwijk et al. - 2017 - Communicative knowledge pervasively influences sen.pdf;C\:\\Users\\u668173\\Zotero\\storage\\TUHNPVTN\\s41598-017-04442-w.html},
  issue = {1},
  langid = {english},
  number = {1},
  options = {useprefix=true}
}

@article{murraySoundsModifiedFlight2017,
  title = {Sounds of {{Modified Flight Feathers Reliably Signal Danger}} in a {{Pigeon}}},
  author = {Murray, Trevor G. and Zeil, Jochen and Magrath, Robert D.},
  date = {2017-11-20},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {27},
  pages = {3520-3525.e4},
  publisher = {{Elsevier}},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2017.09.068},
  url = {https://www.cell.com/current-biology/abstract/S0960-9822(17)31268-X},
  urldate = {2021-02-23},
  eprint = {29129533},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\R4A5WIL2\\Murray et al. - 2017 - Sounds of Modified Flight Feathers Reliably Signal.pdf;C\:\\Users\\u668173\\Zotero\\storage\\IK4XKFUT\\S0960-9822(17)31268-X.html},
  keywords = {alarm signal,crested pigeon,fleeing,non-vocal acoustic alarm,sonation,wing sound},
  langid = {english},
  number = {22}
}

@article{nagelWhatItBe1974,
  title = {What {{Is It Like}} to {{Be}} a {{Bat}}?},
  author = {Nagel, Thomas},
  date = {1974},
  journaltitle = {The Philosophical Review},
  volume = {83},
  pages = {435--450},
  publisher = {{[Duke University Press, Philosophical Review]}},
  issn = {0031-8108},
  doi = {10.2307/2183914},
  eprint = {2183914},
  eprinttype = {jstor},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MI62B7RZ\\Nagel - 1974 - What Is It Like to Be a Bat.pdf},
  number = {4}
}

@article{namboodiripadMeasuringConventionalizationManual2016,
  title = {Measuring Conventionalization in the Manual Modality},
  author = {Namboodiripad, S. and Lenzen, D. and Lepic, R. and Verhoef, T.},
  date = {2016},
  journaltitle = {Journal of Language Evolution},
  volume = {1},
  pages = {109--118},
  doi = {10.1093/jole/lzw005},
  number = {2}
}

@article{namyWordsGesturesInfants1998,
  title = {Words and Gestures: Infants' Interpretations of Different Forms of Symbolic Reference},
  shorttitle = {Words and Gestures},
  author = {Namy, L. L. and Waxman, S. R.},
  date = {1998-04},
  journaltitle = {Child Development},
  shortjournal = {Child Dev},
  volume = {69},
  pages = {295--308},
  issn = {0009-3920},
  abstract = {In 3 experiments, we examine the relation between language acquisition and other symbolic abilities in the early stages of language acquisition. We introduce 18- and 26-month-olds to object categories (e.g., fruit, vehicles) using a novel word or a novel symbolic gesture to name the objects. We compare the influence of these two symbolic forms on infants' object categorization. Children at both ages interpreted novel words as names for object categories. However, infants' interpretations of gestures changed over development. At 18 months, infants spontaneously interpreted gestures, like words, as names for object categories; at 26 months, infants spontaneously interpreted words but not gestures as names. The older infants succeeded in interpreting novel gestures as names only when given additional practice with the gestural medium. This clear developmental pattern supports the prediction that an initial general ability to learn symbols (both words and gestures) develops into a more focused tendency to use words as the predominant symbolic form.},
  eprint = {9586206},
  eprinttype = {pmid},
  keywords = {Female,Gestures,Humans,Infant,Language Development,Male,Mental Recall,Psychology; Child,Sign Language,Symbolism,Verbal Learning,Vocabulary},
  langid = {english},
  number = {2}
}

@book{napierNaturalHistoryPrimates1985,
  title = {The {{Natural History}} of the {{Primates}}},
  author = {Napier, John Russell and Napier, Prue H. and History), British Museum (Natural},
  date = {1985},
  publisher = {{British Museum (Natural History)}},
  abstract = {Social behaviour - General characteristics - Human evolution\_},
  eprint = {10ggAQAAIAAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-565-00870-3},
  langid = {english},
  pagetotal = {212}
}

@article{narinsCrossmodalIntegrationDartpoison2005,
  title = {Cross-Modal Integration in a Dart-Poison Frog},
  author = {Narins, Peter M. and Grabul, Daniela S. and Soma, Kiran K. and Gaucher, Philippe and Hödl, Walter},
  date = {2005-02-15},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {102},
  pages = {2425--2429},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0406407102},
  url = {https://www.pnas.org/content/102/7/2425},
  urldate = {2020-12-16},
  abstract = {The mechanisms by which the brain binds together inputs from separate sensory modalities to effect a unified percept of events are poorly understood. This phenomenon was studied in males of the dart-poison frog Epipedobates femoralis. These animals physically and vigorously defend their territories against conspecific calling intruders. In prior field studies with an electromechanical model frog, we were able to experimentally evoke this aggressive behavior only when an auditory cue (advertisement call) was presented simultaneously with a visual cue (vocal-sac pulsations). In the present field experiments, we used a modified version of the electromechanical model frog to present territorial males with visual and auditory cues separated by experimentally introduced temporal delays or spatial disparities to probe temporal and spatial integration in this animal. In temporal integration experiments, bimodal stimuli with temporal overlap during calling bouts consistently evoked aggressive behavior; stimuli lacking bimodal temporal overlap were relatively ineffective at the same task. In spatial integration studies, despite presenting the components of the bimodal stimulus with an initial spatial disparity of up to 12 cm, fighting behavior persisted. These results demonstrate that temporal and spatial integration may be reliably estimated in a freely behaving animal in its natural habitat and that we can use aggressive behavior in this species as an index of cross-modal integration in the field.},
  eprint = {15677318},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LFAIZ3UB\\Narins et al. - 2005 - Cross-modal integration in a dart-poison frog.pdf;C\:\\Users\\u668173\\Zotero\\storage\\MAMT6HCI\\2425.html},
  keywords = {amphibian,animal communication,anuran,Dendrobatidae,territorial defense},
  langid = {english},
  number = {7}
}

@article{nassarEntrainingNaturalFrequencies2001,
  title = {Entraining the Natural Frequencies of Running and Breathing in Guinea Fowl ({{Numida}} Meleagris)},
  author = {Nassar, P. N. and Jackson, A. C. and Carrier, D. R.},
  date = {2001-05-01},
  journaltitle = {Journal of Experimental Biology},
  volume = {204},
  pages = {1641--1651},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0022-0949, 1477-9145},
  url = {https://jeb.biologists.org/content/204/9/1641},
  urldate = {2020-09-19},
  abstract = {Skip to Next Section Lung ventilation of tetrapods that synchronize their locomotory and ventilatory cycles during exercise could be economized if the resonant frequency of the respiratory system matched the animal's preferred step frequency. To test whether animals utilize this strategy, the input impedance of the respiratory system of five anesthetized, supine guinea fowl (Numida meleagris) was measured using a forced oscillation technique. The resonant frequency of the respiratory system was 7.12+/−0.27 Hz (N=5, mean +/− S.E.M.). No statistically significant difference was found between the resonant frequency of the respiratory system and the panting frequency used by guinea fowl at rest (6.67+/−0.16 Hz, N=11) or during treadmill locomotion (6.71+/−0.12 Hz, N=8) or to their preferred step frequency (6.73+/−0.09 Hz, N=7) (means +/− S.E.M.). These observations suggest (i) that, at rest and during exercise, panting guinea fowl maximize flow while expending minimal mechanical effort, and (ii) that natural selection has tuned the natural frequencies of the respiratory and locomotor systems to similar frequencies.},
  eprint = {11398753},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LSNTGZNM\\Nassar et al. - 2001 - Entraining the natural frequencies of running and .pdf;C\:\\Users\\u668173\\Zotero\\storage\\75HNIBST\\1641.html},
  langid = {english},
  number = {9}
}

@incollection{navarrettaBigDataMultimodal2019,
  title = {Big {{Data}} and {{Multimodal Communication}}: {{A Perspective View}}},
  booktitle = {Innovations in {{Big Data Mining}} and {{Embedded Knowledge}}, {{A}}. {{Esposito}} et al. (Eds.),},
  author = {Navarretta, Costanza and Oemig, Lucretia},
  date = {2019},
  pages = {167--184},
  publisher = {{Springer Nature}},
  location = {{Switzerland}},
  url = {https://doi.org/10.1007/978-3-030-15939-9_9}
}

@online{ngBody2HandsLearningInfer2020,
  title = {{{Body2Hands}}: {{Learning}} to {{Infer 3D Hands}} from {{Conversational Gesture Body Dynamics}}},
  shorttitle = {{{Body2Hands}}},
  author = {Ng, Evonne and Joo, Hanbyul and Ginosar, Shiry and Darrell, Trevor},
  date = {2020-07-23},
  url = {http://arxiv.org/abs/2007.12287},
  urldate = {2020-09-11},
  abstract = {We propose a novel learned deep prior of body motion for 3D hand shape synthesis and estimation in the domain of conversational gestures. Our model builds upon the insight that body motion and hand gestures are strongly correlated in non-verbal communication settings. We formulate the learning of this prior as a prediction task of 3D hand shape over time given body motion input alone. Trained with 3D pose estimations obtained from a large-scale dataset of internet videos, our hand prediction model produces convincing 3D hand gestures given only the 3D motion of the speaker's arms as input. We demonstrate the efficacy of our method on hand gesture synthesis from body motion input, and as a strong body prior for single-view image-based 3D hand pose estimation. We demonstrate that our method outperforms previous state-of-the-art approaches and can generalize beyond the monologue-based training data to multi-person conversations. Video results are available at http://people.eecs.berkeley.edu/\textasciitilde evonne\_ng/projects/body2hands/.},
  archiveprefix = {arXiv},
  eprint = {2007.12287},
  eprinttype = {arxiv},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\J93QULU9\\Ng et al. - 2020 - Body2Hands Learning to Infer 3D Hands from Conver.pdf;C\:\\Users\\u668173\\Zotero\\storage\\VWINQ5HF\\2007.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{ngLossGlypican3Function2009,
  title = {Loss of Glypican-3 Function Causes Growth Factor-Dependent Defects in Cardiac and Coronary Vascular Development},
  author = {Ng, Ann and Wong, Michelle and Viviano, Beth and Erlich, Jonathan M. and Alba, George and Pflederer, Camila and Jay, Patrick Y. and Saunders, Scott},
  date = {2009-11-01},
  journaltitle = {Developmental Biology},
  shortjournal = {Dev. Biol.},
  volume = {335},
  pages = {208--215},
  issn = {1095-564X},
  doi = {10.1016/j.ydbio.2009.08.029},
  abstract = {Glypican-3 (Gpc3) is a heparan sulfate proteoglycan (HSPG) expressed widely during vertebrate development. Loss-of-function mutations cause Simpson-Golabi-Behmel syndrome (SGBS), a rare and complex congenital overgrowth syndrome with a number of associated developmental abnormalities including congenital heart disease. We found that Gpc3-deficient mice display a high incidence of congenital cardiac malformations like ventricular septal defects, common atrioventricular canal and double outlet right ventricle. In addition we observed coronary artery fistulas, which have not been previously reported in SGBS. Coronary artery fistulas are noteworthy because little is known about the molecular basis of this abnormality. Formation of the coronary vascular plexus in Gpc3-deficient embryos was delayed compared to wild-type, and consistent with GPC3 functioning as a co-receptor for fibroblast growth factor-9 (FGF9), we found a reduction in Sonic Hedgehog (Shh) mRNA expression and signaling in embryonic mutant hearts. Interestingly, we found an asymmetric reduction in SHH signaling in cardiac myocytes, as compared with perivascular cells, resulting in excessive coronary artery formation in the Gpc3-deficient animals. We hypothesize that the excessive development of coronary arteries over veins enables the formation of coronary artery fistulas. This work has broad significance to understanding the genetic basis of coronary development and potentially to molecular mechanisms relevant to revascularization following ischemic injury to the heart.},
  eprint = {19733558},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GQ3S5TGS\\Ng et al. - 2009 - Loss of glypican-3 function causes growth factor-d.pdf},
  keywords = {Animals,Coronary Vessel Anomalies,Coronary Vessels,Fistula,Glypicans,Heart,Heart Defects; Congenital,Hedgehog Proteins,Humans,Male,Mice,Mice; Inbred C57BL,Mice; Knockout,Myocytes; Cardiac,Patched Receptors,Receptors; Cell Surface,RNA; Messenger,Signal Transduction},
  langid = {english},
  number = {1},
  pmcid = {PMC2763964}
}

@report{niarchouUnravellingGeneticArchitecture2019,
  title = {Unravelling the Genetic Architecture of Musical Rhythm: A Large-Scale Genome-Wide Association Study of Beat Synchronization},
  shorttitle = {Unravelling the Genetic Architecture of Musical Rhythm},
  author = {Niarchou, Maria and Gustavson, Daniel E. and Sathirapongsasuti, J. Fah and Anglada-Tort, Manuel and Eising, Else and Bell, Eamonn and McArthur, Evonne and Straub, Peter and {The 23andMe Research Team} and McAuley, J. Devin and Capra, John A. and Ullén, Fredrik and Creanza, Nicole and Mosing, Miriam A. and Hinds, David and Davis, Lea K. and Jacoby, Nori and Gordon, Reyna L.},
  date = {2019-11-09},
  institution = {{Genetics}},
  doi = {10.1101/836197},
  url = {http://biorxiv.org/lookup/doi/10.1101/836197},
  urldate = {2021-01-26},
  abstract = {Abstract                        Moving in synchrony to a musical beat is a fundamental component of musicality. Here, we conducted a genome-wide association study (GWAS) to identify common genetic variants associated with beat synchronization in 606,825 individuals. Beat synchronization exhibited a highly polygenic architecture, with sixty-seven loci reaching genome-wide significance (p{$<$}5×10             −8             ) and SNP-based heritability (on the liability scale) of 13\%-16\%. Heritability was enriched for genes expressed in brain tissues, and for fetal and adult brain-specific gene regulatory elements, underscoring the role of central nervous system biomarkers linked to the genetic basis of the trait. We performed validations of the self-report phenotype (through internet-based experiments) and of the GWAS (polygenic scores for beat synchronization were associated with patients algorithmically classified as musicians in medical records of a separate biobank). Genetic correlations with breathing function, motor function, processing speed, and chronotype suggest shared genetic architecture with beat synchronization and provide avenues for new phenotypic and genetic explorations.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4JEWG3MM\\Niarchou et al. - 2019 - Unravelling the genetic architecture of musical rh.pdf},
  langid = {english},
  type = {preprint}
}

@article{nicoladisGestureEarlyBilingual1999,
  title = {Gesture and Early Bilingual Development},
  author = {Nicoladis, E. and Mayberry, R. I. and Genesee, F.},
  date = {1999-03},
  journaltitle = {Developmental Psychology},
  shortjournal = {Dev Psychol},
  volume = {35},
  pages = {514--526},
  issn = {0012-1649},
  doi = {10.1037//0012-1649.35.2.514},
  abstract = {The relationship between speech and gestural proficiency was investigated longitudinally (from 2 years to 3 years 6 months, at 6-month intervals) in 5 French-English bilingual boys with varying proficiency in their 2 languages. Because of their different levels of proficiency in the 2 languages at the same age, these children's data were used to examine the relative contribution of language and cognitive development to gestural development. In terms of rate of gesture production, rate of gesture production with speech, and meaning of gesture and speech, the children used gestures much like adults from 2 years on. In contrast, the use of iconic and beat gestures showed differential development in the children's 2 languages as a function of mean length of utterance. These data suggest that the development of these kinds of gestures may be more closely linked to language development than other kinds (such as points). Reasons why this might be so are discussed.},
  eprint = {10082022},
  eprinttype = {pmid},
  keywords = {Age Factors,Child Behavior,Child; Preschool,Female,Gestures,Humans,Longitudinal Studies,Male,Multilingualism,Psychology; Child,Verbal Behavior,Videotape Recording},
  langid = {english},
  number = {2}
}

@article{nielsenIconicityWordLearning2020,
  title = {Iconicity in {{Word Learning}} and {{Beyond}}: {{A Critical Review}}},
  shorttitle = {Iconicity in {{Word Learning}} and {{Beyond}}},
  author = {Nielsen, Alan KS and Dingemanse, Mark},
  date = {2020-04-20},
  journaltitle = {Language and Speech},
  shortjournal = {Lang Speech},
  pages = {0023830920914339},
  publisher = {{SAGE Publications Ltd}},
  issn = {0023-8309},
  doi = {10.1177/0023830920914339},
  url = {https://doi.org/10.1177/0023830920914339},
  urldate = {2021-01-23},
  abstract = {Interest in iconicity (the resemblance-based mapping between aspects of form and meaning) is in the midst of a resurgence, and a prominent focus in the field has been the possible role of iconicity in language learning. Here we critically review theory and empirical findings in this domain. We distinguish local learning enhancement (where the iconicity of certain lexical items influences the learning of those items) and general learning enhancement (where the iconicity of certain lexical items influences the later learning of non-iconic items or systems). We find that evidence for local learning enhancement is quite strong, though not as clear cut as it is often described and based on a limited sample of languages. Despite common claims about broader facilitatory effects of iconicity on learning, we find that current evidence for general learning enhancement is lacking. We suggest a number of productive avenues for future research and specify what types of evidence would be required to show a role for iconicity in general learning enhancement. We also review evidence for functions of iconicity beyond word learning: iconicity enhances comprehension by providing complementary representations, supports communication about sensory imagery, and expresses affective meanings. Even if learning benefits may be modest or cross-linguistically varied, on balance, iconicity emerges as a vital aspect of language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\APIRYFNX\\Nielsen and Dingemanse - 2020 - Iconicity in Word Learning and Beyond A Critical .pdf}
}

@article{nieto-castanonModelingInvestigationArticulatory2005,
  title = {A Modeling Investigation of Articulatory Variability and Acoustic Stability during {{American English}} /r/ Production},
  author = {Nieto-Castanon, Alfonso and Guenther, Frank H. and Perkell, Joseph S. and Curtin, Hugh D.},
  date = {2005-05},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J Acoust Soc Am},
  volume = {117},
  pages = {3196--3212},
  issn = {0001-4966},
  doi = {10.1121/1.1893271},
  abstract = {This paper investigates the functional relationship between articulatory variability and stability of acoustic cues during American English /r/ production. The analysis of articulatory movement data on seven subjects shows that the extent of intrasubject articulatory variability along any given articulatory direction is strongly and inversely related to a measure of acoustic stability (the extent of acoustic variation that displacing the articulators in this direction would produce). The presence and direction of this relationship is consistent with a speech motor control mechanism that uses a third formant frequency (F3) target; i.e., the final articulatory variability is lower for those articulatory directions most relevant to determining the F3 value. In contrast, no consistent relationship across speakers and phonetic contexts was found between hypothesized vocal-tract target variables and articulatory variability. Furthermore, simulations of two speakers' productions using the DIVA model of speech production, in conjunction with a novel speaker-specific vocal-tract model derived from magnetic resonance imaging data, mimic the observed range of articulatory gestures for each subject, while exhibiting the same articulatory/acoustic relations as those observed experimentally. Overall these results provide evidence for a common control scheme that utilizes an acoustic, rather than articulatory, target specification for American English /r/.},
  eprint = {15957787},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UEH2VRFA\\Nieto-Castanon et al. - 2005 - A modeling investigation of articulatory variabili.pdf},
  keywords = {Cues,Humans,Language,Models; Biological,Phonetics,Speech,Speech Acoustics,Speech Production Measurement,Vocal Cords},
  langid = {english},
  number = {5}
}

@article{nishimuraDevelopmentLaryngealAir2007,
  title = {Development of the {{Laryngeal Air Sac}} in {{Chimpanzees}}},
  author = {Nishimura, Takeshi and Mikami, Akichika and Suzuki, Juri and Matsuzawa, Tetsuro},
  date = {2007-06-14},
  journaltitle = {International Journal of Primatology},
  volume = {28},
  pages = {483--492},
  issn = {0164-0291, 1573-8604},
  doi = {10.1007/s10764-007-9127-7},
  url = {http://link.springer.com/10.1007/s10764-007-9127-7},
  urldate = {2020-07-07},
  abstract = {Though many nonhuman primates possess a laryngeal sac, the great apes are unique in their great size. Though an enlarged sac probably arose in their common ancestor, its functional adaptations remain a matter of debate. Its development in extant great apes is likely to provide valuable information to clarify the issue. We used magnetic resonance imaging to examine the development of the laryngeal sac in 3 living chimpanzees, age 4 mo–5 yr, and identified 2 distinct growth phases of the sac. A gradual growth of the sac in early infancy results in a configuration so that it occupies the ventral region of the neck; many adult nonhominoid primates having a sac show the configuration. The subsequent rapid expansion of the sac in late infancy causes the final configuration in chimpanzees, wherein the sac expands into the pectoral, clavicular, and axillary regions. The latter phase possibly arose at latest in the last common ancestor of extant great apes and contributed to the evolution of the enlarged sac, despite the later evolutionary diversification in adult sac anatomy and growth. As many studies have advocated, the enlarged sac probably plays a role in vocalization in adults. However, physiological modifications in the laryngeal region during infancy are likely to provide valuable information to evaluate the functional adaptations of the enlarged sac in the great apes.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\V9WTQCN9\\Nishimura et al. - 2007 - Development of the Laryngeal Air Sac in Chimpanzee.pdf},
  langid = {english},
  number = {2}
}

@article{nonakaLocatingInexhaustibleMaterial2020,
  title = {Locating the {{Inexhaustible}}: {{Material}}, {{Medium}}, and {{Ambient Information}}},
  shorttitle = {Locating the {{Inexhaustible}}},
  author = {Nonaka, Tetsushi},
  date = {2020},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {11},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.00447},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00447/full},
  urldate = {2020-11-18},
  abstract = {The fundamental difference between the enactive approach and Gibson’s ecological approach lies in the view toward our shared environment. For Varela, Thompson, and Rosch (1991), a pregiven environment that exists “out there” is incompatible with the worlds enacted by various histories of life. For Gibson (1979/2015), the environment with its unlimited possibilities that exists out there offers many ways of life. Drawing on the recent empirical studies on the mechanical basis of information and pattern formation in a wide range of fields, this paper illustrates a principle regarding how pattern and change that are formed in an environmental medium, under certain conditions, could serve as the reservoir of information that makes available a variety of opportunities for perception. The second part of this paper offers a discussion about how the consideration of the materials that make up the terrestrial environment—the particles in the atmosphere and the textured surfaces—led Gibson to replace the concept of “space” with the notion of “medium” that allows for the open-ended activities of perception. Finally, I argue that given due consideration of the ambient information available in the medium, the apparent incompatibility between the world independent of the perceiver that exist out there and the worlds enacted by various histories of life could be resolved.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YA9IPYU8\\Nonaka - 2020 - Locating the Inexhaustible Material, Medium, and .pdf},
  keywords = {ambient information,exploratory activity,Medium,Reservoir computing (RC),tensegrity},
  langid = {english}
}

@incollection{noppeneyMultisensoryPerception,
  title = {Multisensory Perception},
  booktitle = {The {{Cognitive Neurosciences}}},
  author = {Noppeney, U.},
  editor = {Poeppel, D. and Mangun, G. R. and Gazzaniga, M.S.},
  pages = {141--151},
  publisher = {{MIT Press}},
  location = {{Massachusetts}}
}

@article{northcuttEgoComMultipersonMultimodal2020,
  title = {{{EgoCom}}: {{A Multi}}-Person {{Multi}}-Modal {{Egocentric Communications Dataset}}},
  shorttitle = {{{EgoCom}}},
  author = {Northcutt, Curtis and Zha, Shengxin and Lovegrove, Steven and Newcombe, Richard},
  date = {2020},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3025105},
  abstract = {Multi-modal datasets in artificial intelligence (AI) often capture a third-person perspective, but our embodied human intelligence evolved with sensory input from the egocentric, first-person perspective. Towards embodied AI, we introduce the Egocentric Communications (EgoCom) dataset to advance the state-of-the-art in conversational AI, natural language, audio speech analysis, computer vision, and machine learning. EgoCom is a first-of-its-kind natural conversations dataset containing multi-modal human communication data captured simultaneously from the participants' egocentric perspectives. EgoCom includes 38.5 hours of synchronized embodied stereo audio, egocentric video with 240,000 ground-truth, time-stamped word-level transcriptions and speaker labels from 34 diverse speakers. We study baseline performance on two novel applications that benefit from embodied data: (1) predicting turn-taking in conversations and (2) multi-speaker transcription. For (1), we investigate Bayesian baselines to predict turn-taking within 5\% of human performance. For (2), we use simultaneous egocentric capture to combine Google speech-to-text outputs, improving global transcription by 79\% relative to a single perspective. Both applications exploit EgoCom's synchronous multi-perspective data to augment performance of embodied AI tasks},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SUYZENMS\\Northcutt et al. - 2020 - EgoCom A Multi-person Multi-modal Egocentric Comm.pdf;C\:\\Users\\u668173\\Zotero\\storage\\KDIM9Z7C\\9200754.html},
  keywords = {communication,egocentric,EgoCom,embodied intelligence,human-centric,multi-modal data,turn-taking}
}

@article{novackGestureRepresentationalAction2017,
  title = {Gesture as Representational Action: {{A}} Paper about Function},
  shorttitle = {Gesture as Representational Action},
  author = {Novack, Miriam A. and Goldin-Meadow, Susan},
  date = {2017-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {24},
  pages = {652--665},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1145-z},
  url = {https://doi.org/10.3758/s13423-016-1145-z},
  urldate = {2020-09-10},
  abstract = {A great deal of attention has recently been paid to gesture and its effects on thinking and learning. It is well established that the hand movements that accompany speech are an integral part of communication, ubiquitous across cultures, and a unique feature of human behavior. In an attempt to understand this intriguing phenomenon, researchers have focused on pinpointing the mechanisms that underlie gesture production. One proposal––that gesture arises from simulated action (Hostetter \& Alibali Psychonomic Bulletin \& Review, 15, 495–514, 2008)––has opened up discussions about action, gesture, and the relation between the two. However, there is another side to understanding a phenomenon and that is to understand its function. A phenomenon’s function is its purpose rather than its precipitating cause––the why rather than the how. This paper sets forth a theoretical framework for exploring why gesture serves the functions that it does, and reviews where the current literature fits, and fails to fit, this proposal. Our framework proposes that whether or not gesture is simulated action in terms of its mechanism––it is clearly not reducible to action in terms of its function. Most notably, because gestures are abstracted representations and are not actions tied to particular events and objects, they can play a powerful role in thinking and learning beyond the particular, specifically, in supporting generalization and transfer of knowledge.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9VTX2NHA\\Novack and Goldin-Meadow - 2017 - Gesture as representational action A paper about .pdf},
  langid = {english},
  number = {3}
}

@article{nowickiMutualAdaptiveTiming2013,
  title = {Mutual Adaptive Timing in Interpersonal Action Coordination.},
  author = {Nowicki, Lena and Prinz, Wolfgang and Grosjean, Marc and Repp, Bruno H. and Keller, Peter E.},
  date = {2013},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  volume = {23},
  pages = {6--20},
  issn = {2162-1535, 0275-3987},
  doi = {10.1037/a0032039},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0032039},
  urldate = {2020-06-15},
  abstract = {Coperformers in musical ensembles continuously adapt the timing of their actions to maintain interpersonal coordination. The current study used a dyadic finger-tapping task to investigate whether such mutual adaptive timing is predominated by assimilation (i.e., copying relative timing, akin to mimicry) or compensation (local error correction). Our task was intended to approximate the demands that arise when coperformers coordinate complementary parts with a rhythm section in an ensemble. In two experiments, paired musicians (the coperformers) were required to tap in alternation, in synchrony with an auditory pacing signal (the rhythm section). Serial dependencies between successive asynchronies produced by alternating individuals’ taps relative to the pacing tones revealed greater evidence for temporal assimilation than compensation. By manipulating the availability of visual and auditory feedback across experiments, it was shown that this assimilation was strongest when coactors’ taps triggered sounds, while the effects of visual information were negligible. These results suggest that interpersonal temporal assimilation was mediated by perception–action coupling in the auditory modality. Mutual temporal assimilation may facilitate coordination in musical ensembles by automatically increasing stylistic compatibility between coperformers, thereby assisting them to sound cohesive.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8BXZMFLP\\Nowicki et al. - 2013 - Mutual adaptive timing in interpersonal action coo.pdf},
  langid = {english},
  number = {1}
}

@article{nymoenAnalyzingCorrespondenceSound2013,
  title = {Analyzing Correspondence between Sound Objects and Body Motion},
  author = {Nymoen, Kristian and Godøy, Rolf Inge and Jensenius, Alexander Refsum and Torresen, Jim},
  date = {2013-06-04},
  journaltitle = {ACM Transactions on Applied Perception},
  shortjournal = {ACM Trans. Appl. Percept.},
  volume = {10},
  pages = {9:1--9:22},
  issn = {1544-3558},
  doi = {10.1145/2465780.2465783},
  url = {https://doi.org/10.1145/2465780.2465783},
  urldate = {2021-03-16},
  abstract = {Links between music and body motion can be studied through experiments called sound-tracing. One of the main challenges in such research is to develop robust analysis techniques that are able to deal with the multidimensional data that musical sound and body motion present. The article evaluates four different analysis methods applied to an experiment in which participants moved their hands following perceptual features of short sound objects. Motion capture data has been analyzed and correlated with a set of quantitative sound features using four different methods: (a) a pattern recognition classifier, (b) t-tests, (c) Spearman's ρ correlation, and (d) canonical correlation. This article shows how the analysis methods complement each other, and that applying several analysis techniques to the same data set can broaden the knowledge gained from the experiment.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Z5G4ZH5G\\Nymoen et al. - 2013 - Analyzing correspondence between sound objects and.pdf},
  keywords = {music-related motion,Sound-tracing},
  number = {2}
}

@article{odellModelingTurntakingRhythms2012,
  title = {Modeling Turn-Taking Rhythms with Oscillators},
  author = {O'Dell, M. L. and Nieminen, M. and Mietta, L.},
  date = {2012-09-01},
  journaltitle = {Linguistica Uralica},
  volume = {48},
  pages = {218--228},
  publisher = {{Estonian Academy Publishers}},
  issn = {08684731},
  url = {https://go.gale.com/ps/i.do?p=AONE&sw=w&issn=08684731&v=2.1&it=r&id=GALE%7CA317588557&sid=googleScholar&linkaccess=abs},
  urldate = {2020-05-28},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VG88Q3S5\\anonymous.html},
  langid = {english},
  number = {3}
}

@incollection{ohalaRespiratoryActivitySpeech1990,
  title = {Respiratory {{Activity}} in Speech},
  booktitle = {Speech Production and Speech Modeling ({{Eds}}. {{W}}. {{J}}. {{Hardcastle}} and {{A}}. {{Marchal}})},
  author = {Ohala, J. J.},
  date = {1990},
  pages = {22--53},
  publisher = {{Kluwer}},
  location = {{Dordrecht}}
}

@article{ohalloranLocomotorrespiratoryCouplingPatterns2012,
  title = {Locomotor-Respiratory Coupling Patterns and Oxygen Consumption during Walking above and below Preferred Stride Frequency},
  author = {O'Halloran, Joseph and Hamill, Joseph and McDermott, William J. and Remelius, Jebb G. and Van Emmerik, Richard E. A.},
  date = {2012-03},
  journaltitle = {European Journal of Applied Physiology},
  shortjournal = {Eur. J. Appl. Physiol.},
  volume = {112},
  pages = {929--940},
  issn = {1439-6327},
  doi = {10.1007/s00421-011-2040-y},
  abstract = {Locomotor respiratory coupling patterns in humans have been assessed on the basis of the interaction between different physiological and motor subsystems; these interactions have implications for movement economy. A complex and dynamical systems framework may provide more insight than entrainment into the variability and adaptability of these rhythms and their coupling. The purpose of this study was to investigate the relationship between steady state locomotor-respiratory coordination dynamics and oxygen consumption [Formula: see text] of the movement by varying walking stride frequency from preferred. Twelve male participants walked on a treadmill at a self-selected speed. Stride frequency was varied from -20 to +20\% of preferred stride frequency (PSF) while respiratory airflow, gas exchange variables, and stride kinematics were recorded. Discrete relative phase and return map techniques were used to evaluate the strength, stability, and variability of both frequency and phase couplings. Analysis of [Formula: see text] during steady-state walking showed a U-shaped response (P = 0.002) with a minimum at PSF and PSF - 10\%. Locomotor-respiratory frequency coupling strength was not greater (P = 0.375) at PSF than any other stride frequency condition. The dominant coupling across all conditions was 2:1 with greater occurrences at the lower stride frequencies. Variability in coupling was the greatest during PSF, indicating an exploration of coupling strategies to search for the coupling frequency strategy with the least oxygen consumption. Contrary to the belief that increased strength of frequency coupling would decrease oxygen consumption; these results conclude that it is the increased variability of frequency coupling that results in lower oxygen consumption.},
  eprint = {21701846},
  eprinttype = {pmid},
  keywords = {Adaptation; Physiological,Adult,Biological Clocks,Biomechanical Phenomena,Exercise Test,Gait,Humans,Locomotion,Male,Oxygen Consumption,Respiration,Walking,Young Adult},
  langid = {english},
  number = {3}
}

@article{oleszkiewiczVoicebasedAssessmentsTrustworthiness2017,
  title = {Voice-Based Assessments of Trustworthiness, Competence, and Warmth in Blind and Sighted Adults},
  author = {Oleszkiewicz, Anna and Pisanski, Katarzyna and Lachowicz-Tabaczek, Kinga and Sorokowska, Agnieszka},
  date = {2017-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {24},
  pages = {856--862},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1146-y},
  url = {https://doi.org/10.3758/s13423-016-1146-y},
  urldate = {2019-11-30},
  abstract = {The study of voice perception in congenitally blind individuals allows researchers rare insight into how a lifetime of visual deprivation affects the development of voice perception. Previous studies have suggested that blind adults outperform their sighted counterparts in low-level auditory tasks testing spatial localization and pitch discrimination, as well as in verbal speech processing; however, blind persons generally show no advantage in nonverbal voice recognition or discrimination tasks. The present study is the first to examine whether visual experience influences the development of social stereotypes that are formed on the basis of nonverbal vocal characteristics (i.e., voice pitch). Groups of 27 congenitally or early-blind adults and 23 sighted controls assessed the trustworthiness, competence, and warmth of men and women speaking a series of vowels, whose voice pitches had been experimentally raised or lowered. Blind and sighted listeners judged both men’s and women’s voices with lowered pitch as being more competent and trustworthy than voices with raised pitch. In contrast, raised-pitch voices were judged as being warmer than were lowered-pitch voices, but only for women’s voices. Crucially, blind and sighted persons did not differ in their voice-based assessments of competence or warmth, or in their certainty of these assessments, whereas the association between low pitch and trustworthiness in women’s voices was weaker among blind than sighted participants. This latter result suggests that blind persons may rely less heavily on nonverbal cues to trustworthiness compared to sighted persons. Ultimately, our findings suggest that robust perceptual associations that systematically link voice pitch to the social and personal dimensions of a speaker can develop without visual input.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8ERSAHDE\\Oleszkiewicz et al. - 2017 - Voice-based assessments of trustworthiness, compet.pdf},
  keywords = {Blind,Nonverbal communication,Sightedness,Social perception,Voice perception},
  langid = {english},
  number = {3}
}

@article{ollerFunctionalFlexibilityInfant2013,
  title = {Functional Flexibility of Infant Vocalization and the Emergence of Language},
  author = {Oller, D. Kimbrough and Buder, Eugene H. and Ramsdell, Heather L. and Warlaumont, Anne S. and Chorna, Lesya and Bakeman, Roger},
  date = {2013-04-16},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {110},
  pages = {6318--6323},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1300337110},
  url = {https://www.pnas.org/content/110/16/6318},
  urldate = {2020-05-29},
  abstract = {We report on the emergence of functional flexibility in vocalizations of human infants. This vastly underappreciated capability becomes apparent when prelinguistic vocalizations express a full range of emotional content—positive, neutral, and negative. The data show that at least three types of infant vocalizations (squeals, vowel-like sounds, and growls) occur with this full range of expression by 3–4 mo of age. In contrast, infant cry and laughter, which are species-specific signals apparently homologous to vocal calls in other primates, show functional stability, with cry overwhelmingly expressing negative and laughter positive emotional states. Functional flexibility is a sine qua non in spoken language, because all words or sentences can be produced as expressions of varying emotional states and because learning conventional “meanings” requires the ability to produce sounds that are free of any predetermined function. Functional flexibility is a defining characteristic of language, and empirically it appears before syntax, word learning, and even earlier-developing features presumed to be critical to language (e.g., joint attention, syllable imitation, and canonical babbling). The appearance of functional flexibility early in the first year of human life is a critical step in the development of vocal language and may have been a critical step in the evolution of human language, preceding protosyntax and even primitive single words. Such flexible affect expression of vocalizations has not yet been reported for any nonhuman primate but if found to occur would suggest deep roots for functional flexibility of vocalization in our primate heritage.},
  eprint = {23550164},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\U3WNRPUP\\Oller et al. - 2013 - Functional flexibility of infant vocalization and .pdf;C\:\\Users\\u668173\\Zotero\\storage\\396QDUMR\\6318.html},
  keywords = {evolution of language,flexibility in communication,infant communication,language development,primate communication},
  langid = {english},
  number = {16}
}

@article{ollerLanguageOriginsViewed2019,
  title = {Language {{Origins Viewed}} in {{Spontaneous}} and {{Interactive Vocal Rates}} of {{Human}} and {{Bonobo Infants}}},
  author = {Oller, D. Kimbrough and Griebel, Ulrike and Iyer, Suneeti Nathani and Jhang, Yuna and Warlaumont, Anne S. and Dale, Rick and Call, Josep},
  date = {2019},
  journaltitle = {Frontiers in Psychology},
  volume = {10},
  doi = {10.3389/fpsyg.2019.00729},
  url = {https://www.readcube.com/articles/10.3389%2Ffpsyg.2019.00729},
  urldate = {2020-05-07},
  abstract = {From the first months of life, human infants produce “protophones,” speech-like, non-cry sounds, presumed absent, or only minimally present in other apes. But there have been no direct quantitative comparisons to support this presumption. In addition, by 2 months, human infants show sustained face-to-face interaction using protophones, a pattern thought also absent or very limited in other apes, but again, without quantitative comparison. Such comparison should provide evidence relevant to determining foundations of language, since substantially flexible vocalization, the inclination to explore vocalization, and the ability to interact socially by means of vocalization are foundations for language. Here we quantitatively compare data on vocalization rates in three captive bonobo (Pan paniscus) mother–infant pairs with various sources of data from our laboratories on human infant vocalization. Both humans and bonobos produced distress sounds (cries/screams) and laughter. The bonobo infants also produced sounds that were neither screams nor laughs and that showed acoustic similarities to the human protophones. These protophone-like sounds confirm that bonobo infants share with humans the capacity to produce vocalizations that appear foundational for language. Still, there were dramatic differences between the species in both quantity and function of the protophone and protophone-like sounds. The bonobo protophone-like sounds were far less frequent than the human protophones, and the human protophones were far less likely to be interpreted as complaints and more likely as vocal play. Moreover, we found extensive vocal interaction between human infants and mothers, but no vocal interaction in the bonobo mother–infant pairs—while bonobo mothers were physically responsive to their infants, we observed no case of a bonobo mother vocalization directed to her infant. Our cross-species comparison focuses on low- and moderate-arousal circumstances because we reason the roots of language entail vocalization not triggered by excitement, for example, during fighting or intense play. Language appears to be founded in flexible vocalization, used to regulate comfortable social interaction, to share variable affective states at various levels of arousal, and to explore vocalization itself.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DK39DD4W\\Oller et al. - 2019 - Language Origins Viewed in Spontaneous and Interac.pdf;C\:\\Users\\u668173\\Zotero\\storage\\MIIBZNLB\\fpsyg.2019.html},
  langid = {english}
}

@article{ollerPretermFullTerm2019,
  title = {Preterm and Full Term Infant Vocalization and the Origin of Language},
  author = {Oller, D. Kimbrough and Caskey, Melinda and Yoo, Hyunjoo and Bene, Edina R. and Jhang, Yuna and Lee, Chia-Cheng and Bowman, Dale D. and Long, Helen L. and Buder, Eugene H. and Vohr, Betty},
  date = {2019-10-14},
  journaltitle = {Scientific Reports},
  volume = {9},
  pages = {14734},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-51352-0},
  url = {https://www.nature.com/articles/s41598-019-51352-0},
  urldate = {2020-05-29},
  abstract = {How did vocal language originate? Before trying to determine how referential vocabulary or syntax may have arisen, it is critical to explain how ancient hominins began to produce vocalization flexibly, without binding to emotions or functions. A crucial factor in the vocal communicative split of hominins from the ape background may thus have been copious, functionally flexible vocalization, starting in infancy and continuing throughout life, long before there were more advanced linguistic features such as referential vocabulary. 2–3 month-old modern human infants produce “protophones”, including at least three types of functionally flexible non-cry precursors to speech rarely reported in other ape infants. But how early in life do protophones actually appear? We report that the most common protophone types emerge abundantly as early as vocalization can be observed in infancy, in preterm infants still in neonatal intensive care. Contrary to the expectation that cries are the predominant vocalizations of infancy, our all-day recordings showed that protophones occurred far more frequently than cries in both preterm and full-term infants. Protophones were not limited to interactive circumstances, but also occurred at high rates when infants were alone, indicating an endogenous inclination to vocalize exploratorily, perhaps the most fundamental capacity underlying vocal language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\J5UC4JLF\\Oller et al. - 2019 - Preterm and full term infant vocalization and the .pdf;C\:\\Users\\u668173\\Zotero\\storage\\LQNQSWB4\\s41598-019-51352-0.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{ollerVocalDevelopmentGuide2016,
  title = {Vocal {{Development}} as a {{Guide}} to {{Modeling}} the {{Evolution}} of {{Language}}},
  author = {Oller, D. Kimbrough and Griebel, Ulrike and Warlaumont, Anne S.},
  date = {2016},
  journaltitle = {Topics in Cognitive Science},
  volume = {8},
  pages = {382--392},
  issn = {1756-8765},
  doi = {10.1111/tops.12198},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12198},
  urldate = {2020-05-07},
  abstract = {Modeling of evolution and development of language has principally utilized mature units of spoken language, phonemes and words, as both targets and inputs. This approach cannot address the earliest phases of development because young infants are unable to produce such language features. We argue that units of early vocal development—protophones and their primitive illocutionary/perlocutionary forces—should be targeted in evolutionary modeling because they suggest likely units of hominin vocalization/communication shortly after the split from the chimpanzee/bonobo lineage, and because early development of spontaneous vocal capability is a logically necessary step toward vocal language, a root capability without which other crucial steps toward vocal language capability are impossible. Modeling of language evolution/development must account for dynamic change in early communicative units of form/function across time. We argue for interactive contributions of sender/infants and receiver/caregivers in a feedback loop involving both development and evolution and propose to begin computational modeling at the hominin break from the primate communicative background.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12198},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I8QEHP8Z\\Oller et al. - 2016 - Vocal Development as a Guide to Modeling the Evolu.pdf;C\:\\Users\\u668173\\Zotero\\storage\\CSFDMG7N\\tops.html},
  keywords = {Computational modeling,Illocution,Language evolution,Parent-infant interaction,Perlocution,Spontaneous vocalization,Vocal development},
  langid = {english},
  number = {2}
}

@article{oluladeNeuralBasisLanguage2020,
  title = {The Neural Basis of Language Development: {{Changes}} in Lateralization over Age},
  shorttitle = {The Neural Basis of Language Development},
  author = {Olulade, Olumide A. and Seydell-Greenwald, Anna and Chambers, Catherine E. and Turkeltaub, Peter E. and Dromerick, Alexander W. and Berl, Madison M. and Gaillard, William D. and Newport, Elissa L.},
  date = {2020-09-02},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1905590117},
  url = {https://www.pnas.org/content/early/2020/09/01/1905590117},
  urldate = {2020-09-11},
  abstract = {We have long known that language is lateralized to the left hemisphere (LH) in most neurologically healthy adults. In contrast, findings on lateralization of function during development are more complex. As in adults, anatomical, electrophysiological, and neuroimaging studies in infants and children indicate LH lateralization for language. However, in very young children, lesions to either hemisphere are equally likely to result in language deficits, suggesting that language is distributed symmetrically early in life. We address this apparent contradiction by examining patterns of functional MRI (fMRI) language activation in children (ages 4 through 13) and adults (ages 18 through 29). In contrast to previous studies, we focus not on lateralization per se but rather on patterns of left-hemisphere (LH) and right-hemisphere (RH) activation across individual participants over age. Our analyses show significant activation not only in the LH language network but also in their RH homologs in all of the youngest children (ages 4 through 6). The proportion of participants showing significant RH activation decreases over age, with over 60\% of adults lacking any significant RH activation. A whole-brain correlation analysis revealed an age-related decrease in language activation only in the RH homolog of Broca’s area. This correlation was independent of task difficulty. We conclude that, while language is left-lateralized throughout life, the RH contribution to language processing is also strong early in life and decreases through childhood. Importantly, this early RH language activation may represent a developmental mechanism for recovery following early LH injury.},
  eprint = {32900940},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7NFI6RQB\\1905590117.html},
  keywords = {brain,development,fMRI,language,lateralization},
  langid = {english}
}

@article{opsommerDeterminationNerveConduction1999,
  title = {Determination of Nerve Conduction Velocity of {{C}}-Fibres in Humans from Thermal Thresholds to Contact Heat (Thermode) and from Evoked Brain Potentials to Radiant Heat ({{CO2}} Laser)},
  author = {Opsommer, E. and Masquelier, E. and Plaghki, L.},
  date = {1999-10-01},
  journaltitle = {Neurophysiologie Clinique/Clinical Neurophysiology},
  shortjournal = {Neurophysiologie Clinique/Clinical Neurophysiology},
  volume = {29},
  pages = {411--422},
  issn = {0987-7053},
  doi = {10.1016/S0987-7053(00)87265-2},
  url = {http://www.sciencedirect.com/science/article/pii/S0987705300872652},
  urldate = {2020-10-12},
  abstract = {This study was designed to estimate and compare nerve conduction velocity (NCV) of cutaneous heat-sensitive C-fibres obtained using two methods. The first is a method based on reaction times to different rates of temperature change produced by a large contact thermode (Thermotest®). The second is a novel method based on ultra-late-evoked brain potentials to CO2 laser stimuli with tiny beam sections ({$<$} 0.25 mm2), allowing selective and direct activation of very slow conducting afferents. Both methods were applied on three sites of the right leg (foot, knee and thigh) of ten healthy subjects. When based on the reaction times to contact heat, NCV estimations were 0.4 ± 0.22 m/s for the proximal segment (knee-thigh) and 0.6 ± 0.23 m/s for the distal segment (foot-knee). When based on the difference in latency of the ultra-late positivity of laser-evoked brain potentials, NCV estimations were respectively 1.4 ± 0.77 m/s and 1.2 ± 0.55 m/s. For both methods, the difference in NCV between proximal and distal limb segments was not significant. Although both methods give NCV estimations within the range of C-fibres, the systematic difference between NCV obtained from each method may result from the activation of subpopulations of C-fibres with different NCV depending on the method of stimulation (low-threshold thermal receptors by the thermode and thermal nociceptors by the CO2 laser). Considering the difficulty of investigating peripheral fibres with slow conduction velocities (C-fibres) in humans, the methods used in the present study may be useful tools in both experimental and clinical situations. Résumé Cette étude a été conçue pour déterminer et comparer la vitesse de conduction nerveuse (NCV) différences cutanées activées par la chaleur utilisant deux différentes méthodes de stimulation. La première méthode utilise une large thermode de contact (Thermotest®) et l'estimation de la NCV est basée sur les temps de réaction à différentes rampes d'échauffement. La seconde méthode est nouvelle et basée sur des potentiels cérébraux ultratardifs évoqués par un stimulateur laser CO2 utilisant un faisceau de fin diamètre ({$<$} 0,25 mm2) permettant d'activer sélectivement et directement des fibres afférentes à conduction lente. Trois sites de la jambe droite (pied, genou et hanche) sont stimulés avec ces deux méthodes chez dix sujets sains. Les estimations de la NCV (m/s) calculées à partir des temps de réaction à la chaleur de contact sont 0,4 ± 0,22 m/s pour le segment proximal (genou-hanche) et 0,6 ± 0,23 m/s pour le segment distal (pied-genou). Elles sont respectivement de 1,4 ± 0,77 m/s et 1,2 ± 0,55 m/s lorsque estimées à partir de la différence de latence de pics positifs ultratardifs des potentiels cérébraux évoqués par la chaleur radiante. Pour les deux méthodes, la différence en NCV entre segments proximaux et distaux n'est pas significative. Bien que ces deux méthodes fournissent des estimations de la NCV dans l'étendue de celles des fibres C, il existe une différence systématique qui pourrait résulter de l'activation de sous-populations de fibres C ayant des NCV différentes dépendant de la méthode de stimulation (récepteurs thermique à seuil faible par la thermode et récepteurs thermiques nociceptifs par le laser CO2). Étant donné la difficulté d'évaluer la vitesse de conduction nerveuse des fibres afférentes à conduction lente (fibres C) chez l'homme, les méthodes décrites ici pourraient être des outils intéressants tant pour le laboratoire de recherche qu'en clinique.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\E4MPHS57\\S0987705300872652.html},
  keywords = {C-fibres,CO laser,fibres C,laser CO,laser evoked potentials,nerve conduction velocity,potentiels évoqués laser,thermode,vitesse de conduction nerveuse},
  langid = {english},
  number = {5}
}

@article{oreganSensorimotorAccountVision2001,
  title = {A Sensorimotor Account of Vision and Visual Consciousness},
  author = {O'Regan, J. Kevin and Noë, Alva},
  date = {2001-10},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {24},
  pages = {939--973},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X01000115},
  url = {https://www.cambridge.org/core/product/identifier/S0140525X01000115/type/journal_article},
  urldate = {2020-10-13},
  abstract = {Many current neurophysiological, psychophysical, and psychological approaches to vision rest on the idea that when we see, the brain produces an internal representation of the world. The activation of this internal representation is assumed to give rise to the experience of seeing. The problem with this kind of approach is that it leaves unexplained how the existence of such a detailed internal representation might produce visual consciousness. An alternative proposal is made here. We propose that seeing is a way of acting. It is a particular way of exploring the environment. Activity in internal representations does not generate the experience of seeing. The outside world serves as its own, external, representation. The experience of seeing occurs when the organism masters what we call the governing laws of sensorimotor contingency. The advantage of this approach is that it provides a natural and principled way of accounting for visual consciousness, and for the differences in the perceived quality of sensory experience in the different sensory modalities. Several lines of empirical evidence are brought forward in support of the theory, in particular: evidence from experiments in sensorimotor adaptation, visual “filling in,” visual stability despite eye movements, change blindness, sensory substitution, and color perception.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BXJLUN8B\\O'Regan and Noë - 2001 - A sensorimotor account of vision and visual consci.pdf},
  langid = {english},
  number = {5}
}

@article{orrMouthingFingeringSupports2020,
  title = {Mouthing and Fingering Supports Vocal Behaviors Development},
  author = {Orr, Edna},
  date = {2020-06-04},
  journaltitle = {Early Child Development and Care},
  volume = {0},
  pages = {1--14},
  publisher = {{Routledge}},
  issn = {0300-4430},
  doi = {10.1080/03004430.2020.1756792},
  url = {https://doi.org/10.1080/03004430.2020.1756792},
  urldate = {2020-12-01},
  abstract = {The current study explored the link between mouthing and fingering and vocal behaviours directed to objects and caregivers. Nine infants were tracked from the ages of 8–16 months by video recording their mouthing and fingering vignettes and vocal behaviours and vocal behaviours resulting in a total of 2,061 coded behaviours. Microanalysis revealed that the infants tended to accompany almost 25\% of their mouthing and fingering vignettes with vocalization, a tendency that was found not associated with the attainment of linguistic milestones. Rather, it was found that the combination of mouthing and fingering link to most of the vocal behaviours directed to objects and caregivers. The benefits of mouthing and fingering to vocal behaviours development are discussed in terms of the level of exposure to vocal outputs and information integration skills provided by both exploratory forms.},
  annotation = {\_eprint: https://doi.org/10.1080/03004430.2020.1756792},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FSGR4ACD\\Orr - 2020 - Mouthing and fingering supports vocal behaviors de.pdf},
  keywords = {Exploration,fingering,language development,mouthing,vocal behaviours},
  number = {0}
}

@article{ortegaHearingNonsignersUse2019,
  title = {Hearing Non-Signers Use Their Gestures to Predict Iconic Form-Meaning Mappings at First Exposure to Signs - {{ScienceDirect}}},
  author = {Ortega, G. and Schiefner, A. and Ozyurek, A.},
  date = {2019},
  journaltitle = {Cognition},
  volume = {191},
  doi = {10.1016/j.cognition.2019.06.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027719301696},
  urldate = {2020-03-24},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9NRYWPQJ\\S0010027719301696.html},
  number = {103996}
}

@article{ortegaSystematicMappingsSemantic2020,
  title = {Systematic Mappings between Semantic Categories and Types of Iconic Representations in the Manual Modality: {{A}} Normed Database of Silent Gesture},
  shorttitle = {Systematic Mappings between Semantic Categories and Types of Iconic Representations in the Manual Modality},
  author = {Ortega, Gerardo and Özyürek, Aslı},
  date = {2020-02-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {52},
  pages = {51--67},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01204-6},
  url = {https://doi.org/10.3758/s13428-019-01204-6},
  urldate = {2020-06-23},
  abstract = {An unprecedented number of empirical studies have shown that iconic gestures—those that mimic the sensorimotor attributes of a referent—contribute significantly to language acquisition, perception, and processing. However, there has been a lack of normed studies describing generalizable principles in gesture production and in comprehension of the mappings of different types of iconic strategies (i.e., modes of representation; Müller, 2013). In Study 1 we elicited silent gestures in order to explore the implementation of different types of iconic representation (i.e., acting, representing, drawing, and personification) to express concepts across five semantic domains. In Study 2 we investigated the degree of meaning transparency (i.e., iconicity ratings) of the gestures elicited in Study 1. We found systematicity in the gestural forms of 109 concepts across all participants, with different types of iconicity aligning with specific semantic domains: Acting was favored for actions and manipulable objects, drawing for nonmanipulable objects, and personification for animate entities. Interpretation of gesture–meaning transparency was modulated by the interaction between mode of representation and semantic domain, with some couplings being more transparent than others: Acting yielded higher ratings for actions, representing for object-related concepts, personification for animate entities, and drawing for nonmanipulable entities. This study provides mapping principles that may extend to all forms of manual communication (gesture and sign). This database includes a list of the most systematic silent gestures in the group of participants, a notation of the form of each gesture based on four features (hand configuration, orientation, placement, and movement), each gesture’s mode of representation, iconicity ratings, and professionally filmed videos that can be used for experimental and clinical endeavors.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IDNVLLTJ\\Ortega and Özyürek - 2020 - Systematic mappings between semantic categories an.pdf},
  langid = {english},
  number = {1}
}

@article{ortegaTypeIconicityMatters2017,
  title = {Type of Iconicity Matters in the Vocabulary Development of Signing Children},
  author = {Ortega, Gerardo and Sümer, Beyza and Özyürek, Aslı},
  date = {2017},
  journaltitle = {Developmental Psychology},
  volume = {53},
  pages = {89--99},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-0599(Electronic),0012-1649(Print)},
  doi = {10.1037/dev0000161},
  abstract = {Recent research on signed as well as spoken language shows that the iconic features of the target language might play a role in language development. Here, we ask further whether different types of iconic depictions modulate children’s preferences for certain types of sign-referent links during vocabulary development in sign language. Results from a picture description task indicate that lexical signs with 2 possible variants are used in different proportions by deaf signers from different age groups. While preschool and school-age children favored variants representing actions associated with their referent (e.g., a writing hand for the sign PEN), adults preferred variants representing the perceptual features of those objects (e.g., upward index finger representing a thin, elongated object for the sign PEN). Deaf parents interacting with their children, however, used action- and perceptual-based variants in equal proportion and favored action variants more than adults signing to other adults. We propose that when children are confronted with 2 variants for the same concept, they initially prefer action-based variants because they give them the opportunity to link a linguistic label to familiar schemas linked to their action/motor experiences. Our results echo findings showing a bias for action-based depictions in the development of iconic co-speech gestures suggesting a modality bias for such representations during development. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BCIXN6MR\\Ortega et al. - 2017 - Type of iconicity matters in the vocabulary develo.pdf;C\:\\Users\\u668173\\Zotero\\storage\\36QYLMLI\\2016-53086-001.html},
  keywords = {Childhood Development,Communication,Gestures,Language Development,Sign Language,Vocabulary},
  number = {1}
}

@article{ortegaTypesIconicityCombinatorial2020,
  title = {Types of Iconicity and Combinatorial Strategies Distinguish Semantic Categories in Silent Gesture across Cultures},
  author = {Ortega, Gerardo and Özyürek, Asli},
  date = {2020-03},
  journaltitle = {Language and Cognition},
  volume = {12},
  pages = {84--113},
  publisher = {{Cambridge University Press}},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2019.28},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/types-of-iconicity-and-combinatorial-strategies-distinguish-semantic-categories-in-silent-gesture-across-cultures/79A4488F54C5BFCDE9953CD9240183FE},
  urldate = {2020-04-28},
  abstract = {In this study we explore whether different types of iconic gestures (i.e., acting, drawing, representing) and their combinations are used systematically to distinguish between different semantic categories in production and comprehension. In Study 1, we elicited silent gestures from Mexican and Dutch participants to represent concepts from three semantic categories: actions, manipulable objects, and non-manipulable objects. Both groups favoured the acting strategy to represent actions and manipulable objects; while non-manipulable objects were represented through the drawing strategy. Actions elicited primarily single gestures whereas objects elicited combinations of different types of iconic gestures as well as pointing. In Study 2, a different group of participants were shown gestures from Study 1 and were asked to guess their meaning. Single-gesture depictions for actions were more accurately guessed than for objects. Objects represented through two-gesture combinations (e.g., acting + drawing) were more accurately guessed than objects represented with a single gesture. We suggest iconicity is exploited to make direct links with a referent, but when it lends itself to ambiguity, individuals resort to combinatorial structures to clarify the intended referent. Iconicity and the need to communicate a clear signal shape the structure of silent gestures and this in turn supports comprehension.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7P7FUTQE\\Ortega and Özyürek - 2020 - Types of iconicity and combinatorial strategies di.pdf;C\:\\Users\\u668173\\Zotero\\storage\\9KYKA5C8\\79A4488F54C5BFCDE9953CD9240183FE.html},
  keywords = {combinatorial structure,emerging sign language,iconicity,language emergence,silent gesture},
  langid = {english},
  number = {1}
}

@article{osborneBeatGesturesPostural2017,
  title = {Beat Gestures and Postural Control in Youth at Ultrahigh Risk for Psychosis},
  author = {Osborne, K. Juston and Bernard, Jessica A. and Gupta, Tina and Dean, Derek J. and Millman, Zachary and Vargas, Teresa and Ristanovic, Ivanka and Schiffman, Jason and Mittal, Vijay A.},
  date = {2017-07-01},
  journaltitle = {Schizophrenia Research},
  shortjournal = {Schizophrenia Research},
  volume = {185},
  pages = {197--199},
  issn = {0920-9964},
  doi = {10.1016/j.schres.2016.11.028},
  url = {http://www.sciencedirect.com/science/article/pii/S0920996416305217},
  urldate = {2020-07-09},
  abstract = {Beat gestures, rhythmic hand movements that co-occur with speech, appear to be uniquely associated with the cerebellum in healthy individuals. This behavior may also have relevance for psychosis-risk youth, a group characterized by cerebellar dysfunction. This study examined beat gesture frequency and postural sway (a sensitive index of cerebellar functioning) in youth at ultrahigh risk (UHR) for psychosis. Results indicated that decreased beat gesture frequency, but not self-regulatory movement, is associated with elevated postural sway, suggesting that beat gestures may be an important biomarker in this critical population.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SH7DPRUI\\Osborne et al. - 2017 - Beat gestures and postural control in youth at ult.pdf;C\:\\Users\\u668173\\Zotero\\storage\\WMX7F456\\S0920996416305217.html},
  keywords = {Cerebellum,Gesture,Postural sway,Prodrome,Psychosis},
  langid = {english}
}

@article{ostlingVisualIconicitySign2018,
  title = {Visual {{Iconicity Across Sign Languages}}: {{Large}}-{{Scale Automated Video Analysis}} of {{Iconic Articulators}} and {{Locations}}},
  shorttitle = {Visual {{Iconicity Across Sign Languages}}},
  author = {Östling, Robert and Börstell, Carl and Courtaux, Servane},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {9},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.00725},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00725/full},
  urldate = {2020-12-29},
  abstract = {We use automatic processing of 120 000 sign videos in 31 different sign languages to show a cross-linguistic pattern for two types of form--meaning relationships in the visual modality. First, we demonstrate that the degree of inherent plurality of concepts, based on individual ratings by non-signers, strongly correlates with the number of hands used in the sign forms encoding the same concepts across sign languages. Second, we show that the semantics of a concept is strongly associated with its place of articulation in a way predicted by non-signers' intuitions. The implications of our results are both theoretical and methodological. With regard to theoretical implications, we corroborate previous research by demonstrating and quantifying, using a much larger material than previously available, the iconic nature of languages in the visual modality. As for the methodological implications, we show how automatic methods are, in fact, useful for performing large-scale analysis of sign language data, to a high level of accuracy, as indicated by our manual error analysis.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KZIN72AW\\Östling et al. - 2018 - Visual Iconicity Across Sign Languages Large-Scal.pdf},
  keywords = {automated video processing,Iconicity,Lexical Plurality,location,semantics,sign language,two-handed signs,typology},
  langid = {english}
}

@article{otterRoadmapComputationPersistent2017,
  title = {A Roadmap for the Computation of Persistent Homology},
  author = {Otter, Nina and Porter, Mason A. and Tillmann, Ulrike and Grindrod, Peter and Harrington, Heather A.},
  date = {2017-12},
  journaltitle = {EPJ Data Science},
  shortjournal = {EPJ Data Sci.},
  volume = {6},
  pages = {17},
  issn = {2193-1127},
  doi = {10.1140/epjds/s13688-017-0109-5},
  url = {http://arxiv.org/abs/1506.08903},
  urldate = {2020-03-11},
  abstract = {Persistent homology (PH) is a method used in topological data analysis (TDA) to study qualitative features of data that persist across multiple scales. It is robust to perturbations of input data, independent of dimensions and coordinates, and provides a compact representation of the qualitative features of the input. The computation of PH is an open area with numerous important and fascinating challenges. The field of PH computation is evolving rapidly, and new algorithms and software implementations are being updated and released at a rapid pace. The purposes of our article are to (1) introduce theory and computational methods for PH to a broad range of computational scientists and (2) provide benchmarks of state-of-the-art implementations for the computation of PH. We give a friendly introduction to PH, navigate the pipeline for the computation of PH with an eye towards applications, and use a range of synthetic and real-world data sets to evaluate currently available open-source implementations for the computation of PH. Based on our benchmarking, we indicate which algorithms and implementations are best suited to different types of data sets. In an accompanying tutorial, we provide guidelines for the computation of PH. We make publicly available all scripts that we wrote for the tutorial, and we make available the processed version of the data sets used in the benchmarking.},
  archiveprefix = {arXiv},
  eprint = {1506.08903},
  eprinttype = {arxiv},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XP2TJHTG\\Otter et al. - 2017 - A roadmap for the computation of persistent homolo.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZQBRPFJR\\1506.html},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Quantitative Methods},
  number = {1}
}

@article{owerkowiczContributionGularPumping1999,
  title = {Contribution of Gular Pumping to Lung Ventilation in Monitor Lizards},
  author = {Owerkowicz, Tomasz and Farmer, Colleen G. and Hicks, James W. and Brainerd, Elizabeth L.},
  date = {1999},
  journaltitle = {Science},
  pages = {1661--1663},
  abstract = {A controversial hypothesis has proposed that lizards are subject to a speed-dependent axial constraint that prevents effective lung ventilation during mod-erate- and high-speed locomotion. This hypothesis has been challenged by results demonstrating that monitor lizards (genus Varanus) experience no axial constraint. Evidence presented here shows that, during locomotion, varanids use a positive pressure gular pump to assist lung ventilation. Disabling the gular pump reveals that the axial constraint is present in varanids but it is masked by gular pumping under normal conditions. These Þndings support the predic-tion that the axial constraint may be found in other tetrapods that breathe by costal aspiration and locomote with a lateral undulatory gait. When lizards walk and run, they generally use a lateral undulatory gait in which their bodies flex from side to side with each stride. It has been proposed that these lat-eral flexions prevent effective lung ventila-},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QCUZLGJD\\Owerkowicz et al. - 1999 - Contribution of gular pumping to lung ventilation .pdf;C\:\\Users\\u668173\\Zotero\\storage\\SAL3CTJP\\summary.html}
}

@article{ozcaliskanDoesLanguageShape2016,
  title = {Does Language Shape Silent Gesture?},
  author = {Özçalışkan, Şeyda and Lucero, Ché and Goldin-Meadow, Susan},
  date = {2016-03-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {148},
  pages = {10--18},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.12.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027715301153},
  urldate = {2019-08-26},
  abstract = {Languages differ in how they organize events, particularly in the types of semantic elements they express and the arrangement of those elements within a sentence. Here we ask whether these cross-linguistic differences have an impact on how events are represented nonverbally; more specifically, on how events are represented in gestures produced without speech (silent gesture), compared to gestures produced with speech (co-speech gesture). We observed speech and gesture in 40 adult native speakers of English and Turkish (N=20/per language) asked to describe physical motion events (e.g., running down a path)—a domain known to elicit distinct patterns of speech and co-speech gesture in English- and Turkish-speakers. Replicating previous work (Kita \& Özyürek, 2003), we found an effect of language on gesture when it was produced with speech—co-speech gestures produced by English-speakers differed from co-speech gestures produced by Turkish-speakers. However, we found no effect of language on gesture when it was produced on its own—silent gestures produced by English-speakers were identical in how motion elements were packaged and ordered to silent gestures produced by Turkish-speakers. The findings provide evidence for a natural semantic organization that humans impose on motion events when they convey those events without language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YBUH3TR5\\Özçalışkan et al. - 2016 - Does language shape silent gesture.pdf;C\:\\Users\\u668173\\Zotero\\storage\\BSUWUPFT\\S0010027715301153.html},
  keywords = {Cross-linguistic differences,Gesture,Language and cognition,Motion events}
}

@incollection{ozcaliskanThereIconicGesture2011,
  title = {Is There an Iconic Gesture Spurt at 26 Months?},
  booktitle = {Integrating Gestures: {{The}} Interdisciplinary Nature of Gesture},
  author = {Özçalişkan, Şeyda and Goldin-Meadow, Susan},
  date = {2011},
  pages = {163--174},
  publisher = {{John Benjamins Publishing Company}},
  location = {{Amsterdam, Netherlands}},
  doi = {10.1075/gs.4.14ozc},
  abstract = {Previous research has shown that children understand the iconicity of a gesture at 26 months. Here we ask when children begin to display an appreciation of iconicity in the gestures they produce. We observed spontaneous gesture in 40 children interacting with their parents from 14 to 34 months of age and found that children increased their production of iconic gestures over time. At 26 months, they not only produced significantly more iconic gestures (tokens) than at any previous time point, but they also conveyed significantly more different meanings with those iconic gestures (types). We found similar increases in the iconic gestures that the children's parents produced, suggesting that parents either were sensitive to changes in their children's iconic gestures or perhaps were responsible for those changes. Overall, the results suggest that the 26-month age period is a turning point for children's grasp of the iconicity of a symbol. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZPN234Z3\\2011-03609-012.html},
  isbn = {978-90-272-2845-1 978-90-272-8720-5},
  keywords = {Age Differences,Childhood Development,Developmental Stages,Gestures,Nonverbal Communication},
  series = {Gesture Studies}
}

@article{paddenPatternedIconicitySign2013,
  title = {Patterned Iconicity in Sign Language Lexicons},
  author = {Padden, Carol A. and Meir, Irit and Hwang, So-One and Lepic, Ryan and Seegers, Sharon and Sampson, Tory},
  date = {2013-01-01},
  journaltitle = {Gesture},
  volume = {13},
  pages = {287--308},
  publisher = {{John Benjamins}},
  issn = {1568-1475, 1569-9773},
  doi = {10.1075/gest.13.3.03pad},
  url = {https://www.jbe-platform.com/content/journals/10.1075/gest.13.3.03pad},
  urldate = {2020-04-28},
  abstract = {Iconicity is an acknowledged property of both gesture and sign language. In contrast to the familiar definition of iconicity as a correspondence between individual forms and their referents, we explore iconicity as a shared property among groups of signs, in what we call patterned iconicity. In this paper, we focus on iconic strategies used by hearing silent gesturers and by signers of three unrelated sign languages in an elicitation task featuring pictures of hand-held manufactured tools. As in previous gesture literature, we find that silent gesturers largely prefer a handling strategy, though some use an instrument strategy, in which the handshape represents the shape of the tool. There are additional differences in use of handling and instrument strategies for hand-held tools across the different sign languages, suggesting typological differences in iconic patterning. Iconic patterning in each of the three sign languages demonstrates how gestural iconic resources are organized in the grammars of sign languages.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CNVAKU3G\\gest.13.3.html},
  langid = {english},
  number = {3}
}

@article{pagancanovasQuantifyingSpeechgestureRelation2020,
  title = {Quantifying the Speech-Gesture Relation with Massive Multimodal Datasets: {{Informativity}} in Time Expressions},
  shorttitle = {Quantifying the Speech-Gesture Relation with Massive Multimodal Datasets},
  author = {Pagán Cánovas, Cristóbal and Valenzuela, Javier and Alcaraz Carrión, Daniel and Olza, Inés and Ramscar, Michael},
  editor = {Perlman, Marcus},
  date = {2020-06-02},
  journaltitle = {PLOS ONE},
  volume = {15},
  pages = {e0233892},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0233892},
  url = {https://dx.plos.org/10.1371/journal.pone.0233892},
  urldate = {2020-06-05},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MXLHQGGB\\Pagán Cánovas et al. - 2020 - Quantifying the speech-gesture relation with massi.pdf},
  langid = {english},
  number = {6}
}

@article{palmerCorticospinalProjectionsUpper1992,
  title = {Corticospinal Projections to Upper Limb Motoneurones in Humans.},
  author = {Palmer, E and Ashby, P},
  date = {1992-03},
  journaltitle = {The Journal of Physiology},
  shortjournal = {J Physiol},
  volume = {448},
  pages = {397--412},
  issn = {0022-3751},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1176206/},
  urldate = {2020-10-29},
  abstract = {1. Magnetic stimulation was applied over the motor cortex in forty-five normal human subjects and peristimulus time histograms (PSTHs) of the discharges of single motor units were used to record changes in the firing probability of individual spinal motoneurones of contralateral upper limb muscles. Recordings were obtained from 153 motor units from fourteen upper limb muscles. 2. For the majority of motor units the initial effect was a short latency facilitation. The estimated central conduction velocities and the rise times of the underlying excitatory postsynaptic potentials (EPSPs) were compatible with monosynaptic facilitation by a fast corticospinal pathway. In some motor units the initial effect was a short latency inhibition. Other units showed no statistically significant changes in firing probability. The proportion of the tested motor units in each of these categories depended on the muscle. All of the sampled units of first dorsal interosseous (1DI) showed short latency facilitation, as did the majority of units in the forearm and the biceps brachii. More than half of the sampled motor units of triceps brachii and deltoid showed either no effect or were inhibited. 3. To compare the net short latency actions of the neurones activated by magnetic stimulation on various motoneurone pools, the magnitude of the short latency facilitation or inhibition in a given motor unit was normalized to the magnitude of the short latency facilitation in the 1DI motor unit of the same subject at the same stimulus intensity, and these data were pooled for a number of subjects. 4. 1DI motoneurones received strong net facilitation (estimated mean EPSP amplitude 2.9 +/- 0.2 mV), the motoneurones of forearm muscles and biceps brachii received weaker net facilitation and triceps brachii and deltoid received no net effect. 5. It is concluded that the short latency corticospinal projections to upper limb motoneurones in humans have a distinct pattern which is similar to that in other primates.},
  eprint = {1593472},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TZIXGUJH\\Palmer and Ashby - 1992 - Corticospinal projections to upper limb motoneuron.pdf},
  pmcid = {PMC1176206}
}

@book{paoloLinguisticBodiesContinuity2018,
  title = {Linguistic {{Bodies}}: {{The Continuity}} between {{Life}} and {{Language}}},
  shorttitle = {Linguistic {{Bodies}}},
  author = {Paolo, Ezequiel A. Di and Cuffari, Elena Clare and Jaegher, Hanne De},
  date = {2018-11-06},
  publisher = {{MIT Press}},
  abstract = {A novel theoretical framework for an embodied, non-representational approach to language that extends and deepens enactive theory, bridging the gap between sensorimotor skills and language.Linguistic Bodies offers a fully embodied and fully social treatment of human language without positing mental representations. The authors present the first coherent, overarching theory that connects dynamical explanations of action and perception with language. Arguing from the assumption of a deep continuity between life and mind, they show that this continuity extends to language. Expanding and deepening enactive theory, they offer a constitutive account of language and the co-emergent phenomena of personhood, reflexivity, social normativity, and ideality. Language, they argue, is not something we add to a range of existing cognitive capacities but a new way of being embodied. Each of us is a linguistic body in a community of other linguistic bodies. The book describes three distinct yet entangled kinds of human embodiment, organic, sensorimotor, and intersubjective; it traces the emergence of linguistic sensitivities and introduces the novel concept of linguistic bodies; and it explores the implications of living as linguistic bodies in perpetual becoming, applying the concept of linguistic bodies to questions of language acquisition, parenting, autism, grammar, symbol, narrative, and gesture, and to such ethical concerns as microaggression, institutional speech, and pedagogy.},
  eprint = {_rVyDwAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-262-03816-4},
  keywords = {Language Arts & Disciplines / Linguistics / General,Psychology / Cognitive Psychology & Cognition},
  langid = {english},
  pagetotal = {428}
}

@article{parkBreathingCoupledVoluntary2020,
  title = {Breathing Is Coupled with Voluntary Action and the Cortical Readiness Potential},
  author = {Park, Hyeong-Dong and Barnoud, Coline and Trang, Henri and Kannape, Oliver A. and Schaller, Karl and Blanke, Olaf},
  date = {2020-12},
  journaltitle = {Nature Communications},
  volume = {11},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13967-9},
  url = {http://www.nature.com/articles/s41467-019-13967-9},
  urldate = {2020-09-18},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MMPPYSBQ\\Park et al. - 2020 - Breathing is coupled with voluntary action and the.pdf},
  langid = {english},
  number = {1}
}

@article{parkLipMovementsEntrain2016,
  title = {Lip Movements Entrain the Observers’ Low-Frequency Brain Oscillations to Facilitate Speech Intelligibility},
  author = {Park, Hyojin and Kayser, Christoph and Thut, Gregor and Gross, Joachim},
  editor = {King, Andrew J},
  date = {2016-05-05},
  journaltitle = {eLife},
  volume = {5},
  pages = {e14521},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.14521},
  url = {https://doi.org/10.7554/eLife.14521},
  urldate = {2020-09-17},
  abstract = {During continuous speech, lip movements provide visual temporal signals that facilitate speech processing. Here, using MEG we directly investigated how these visual signals interact with rhythmic brain activity in participants listening to and seeing the speaker. First, we investigated coherence between oscillatory brain activity and speaker’s lip movements and demonstrated significant entrainment in visual cortex. We then used partial coherence to remove contributions of the coherent auditory speech signal from the lip-brain coherence. Comparing this synchronization between different attention conditions revealed that attending visual speech enhances the coherence between activity in visual cortex and the speaker’s lips. Further, we identified a significant partial coherence between left motor cortex and lip movements and this partial coherence directly predicted comprehension accuracy. Our results emphasize the importance of visually entrained and attention-modulated rhythmic brain activity for the enhancement of audiovisual speech processing.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DESLSKIC\\Park et al. - 2016 - Lip movements entrain the observers’ low-frequency.pdf},
  keywords = {electroencephalography,language,lip movements,magnetoencephalography,oscillations,speech}
}

@article{parrellBridgingDynamicalSystems2019,
  title = {Bridging {{Dynamical Systems}} and {{Optimal Trajectory Approaches}} to {{Speech Motor Control With Dynamic Movement Primitives}}},
  author = {Parrell, Benjamin and Lammert, Adam C.},
  date = {2019},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {10},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02251},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02251/full},
  urldate = {2021-02-26},
  abstract = {Current models of speech motor control rely on either trajectory-based control (DIVA, GEPPETO, ACT) or a dynamical systems approach based on feedback control (Task Dynamics, FACTS). While both approaches have provided insights into the speech motor system, it is difficult to connect these findings across models given the distinct theoretical and computational bases of the two approaches. We propose a new extension of the most widely used dynamical systems approach, Task Dynamics, that incorporates many of the strengths of trajectory-based approaches, providing a way to bridge the theoretical divide between what have been two separate approaches to understanding speech motor control. The Task Dynamics (TD) model posits that speech gestures are governed by point attractor dynamics consistent with a critically damped harmonic oscillator. Kinematic trajectories associated with such gestures should therefore be consistent with a second-order dynamical system, possibly modified by blending with temporally overlapping gestures or altering oscillator parameters. This account of observed kinematics is powerful and theoretically appealing, but may be insufficient to account for deviations from predicted kinematics – i.e., changes produced in response to some external perturbations to the jaw, changes in control during acquisition and development, or effects of word/syllable frequency. Optimization, such as would be needed to minimize articulatory effort, is also incompatible with the current TD model, though the idea that the speech production systems economizes effort has a long history and, importantly, also plays a critical role in current theories of domain-general human motor control. To address these issues, we use Dynamic Movement Primitives (DMPs) to expand a dynamical systems framework for speech motor control to allow modification of kinematic trajectories by incorporating a simple, learnable forcing term into existing point attractor dynamics. We show that integration of DMPs with task- based point-attractor dynamics enhances the potential explanatory power of TD in a number of critical ways, including the ability to account for external forces in planning and optimizing both kinematic and dynamic movement costs. At the same time, this approach preserves the successes of Task Dynamics in handling multi-gesture planning and coordination.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YP4X55NP\\Parrell and Lammert - 2019 - Bridging Dynamical Systems and Optimal Trajectory .pdf},
  keywords = {dynamical systems,optimal control (models),Speech Motor Control,Task dynamics,trajectory control},
  langid = {english}
}

@article{parrellSpatiotemporalCouplingSpeech2014,
  title = {Spatiotemporal Coupling between Speech and Manual Motor Actions},
  author = {Parrell, Benjamin and Goldstein, Louis and Lee, Sungbok and Byrd, Dani},
  date = {2014-01-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {42},
  pages = {1--11},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2013.11.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447013000776},
  urldate = {2019-04-16},
  abstract = {Much evidence has been found for pervasive links between the manual and speech motor systems, including evidence from infant development, deictic pointing, and repetitive tapping and speaking tasks. We expand on the last of these paradigms to look at intra- and cross-modal effects of emphatic stress, as well as the effects of coordination in the absence of explicit rhythm. In this study, subjects repeatedly tapped their finger and synchronously repeated a single spoken syllable. On each trial, subjects placed an emphatic stress on one finger tap or one spoken syllable. Results show that both movement duration and magnitude are affected by emphatic stress regardless of whether that stress is in the same domain (e.g., effects on the oral articulators when a spoken repetition is stressed) or across domains (e.g., effects on the oral articulators when a tap is stressed). Though the size of the effects differs between intra-and cross-domain emphases, the implementation of stress affects both motor domains, indicating a tight connection. This close coupling is seen even in the absence of stress, though it is highlighted under stress. The results of this study support the idea that implementation of prosody is not domain-specific but relies on general aspects of the motor system.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UZIY6KZ5\\Parrell et al. - 2014 - Spatiotemporal coupling between speech and manual .pdf;C\:\\Users\\u668173\\Zotero\\storage\\BRJQDEAI\\S0095447013000776.html}
}

@article{parrFacialExpressionRecognition2009,
  title = {Facial Expression Recognition in Rhesus Monkeys, {{Macaca}} Mulatta},
  author = {Parr, Lisa A. and Heintz, Matthew},
  date = {2009-06},
  journaltitle = {Animal Behaviour},
  shortjournal = {Anim Behav},
  volume = {77},
  pages = {1507--1513},
  issn = {0003-3472},
  doi = {10.1016/j.anbehav.2009.02.024},
  abstract = {The ability to recognize and accurately interpret facial expressions is critically important for nonhuman primates that rely on these nonverbal signals for social communication. Despite this, little is known about how nonhuman primates, particularly monkeys, discriminate between facial expressions. In the present study, seven rhesus monkeys were required to discriminate four categories of conspecific facial expressions using a matching-to-sample task. In experiment 1, the matching pair showed identical photographs of facial expressions, paired with every other expression type as the nonmatch. The identity of the nonmatching stimulus monkey differed from the one in the sample. Subjects performed above chance on session 1, with no difference in performance across the four expression types. In experiment 2, the identity of all three monkeys differed in each trial, and a neutral portrait was also included as the nonmatching stimulus. Monkeys discriminated expressions across individual identity when the non-match was a neutral stimulus, but they had difficulty when the nonmatch was another expression type. We analysed the degree to which specific feature redundancy could account for these error patterns using a multidimensional scaling analysis which plotted the perceived dissimilarity between expression dyads along a two-dimensional axis. One axis appeared to represent mouth shape, stretched open versus funnelled, while the other appeared to represent a combination of lip retraction and mouth opening. These features alone, however, could not account for overall performance and suggest that monkeys do not rely solely on distinctive features to discriminate among different expressions.},
  eprint = {20228886},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XVCMXK6I\\Parr and Heintz - 2009 - Facial expression recognition in rhesus monkeys, M.pdf},
  langid = {english},
  number = {6},
  pmcid = {PMC2836777}
}

@article{parrPerceptualBiasesMultimodal2004,
  title = {Perceptual Biases for Multimodal Cues in Chimpanzee ({{Pan}} Troglodytes) Affect Recognition},
  author = {Parr, Lisa A.},
  date = {2004},
  journaltitle = {Animal Cognition},
  volume = {7},
  pages = {171--178},
  publisher = {{Springer}},
  location = {{Germany}},
  issn = {1435-9456(Electronic),1435-9448(Print)},
  doi = {10.1007/s10071-004-0207-1},
  abstract = {The ability of organisms to discriminate social signals, such as affective displays, using different sensory modalities is important for social communication. However, a major problem for understanding the evolution and integration of multimodal signals is determining how humans and animals attend to different sensory modalities, and these different modalities contribute to the perception and categorization of social signals. Using a matching-to-sample procedure, chimpanzees discriminated videos of conspecifics' facial expressions that contained only auditory or only visual cues by selecting one of two facial expression photographs that matched the expression category represented by the sample. Other videos were edited to contain incongruent sensory cues, i.e., visual features of one expression but auditory features of another. In these cases, subjects were free to select the expression that matched either the auditory or visual modality, whichever was more salient for that expression type. Results showed that chimpanzees were able to discriminate facial expressions using only auditory or visual cues, and when these modalities were mixed. However, in these latter trials, depending on the expression category, clear preferences for either the visual or auditory modality emerged. Pant-hoots and play faces were discriminated preferentially using the auditory modality, while screams were discriminated preferentially using the visual modality. Therefore, depending on the type of expressive display, the auditory and visual modalities were differentially salient in ways that appear consistent with the ethological importance of that display's social function. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8BAT685B\\2004-16216-005.html},
  keywords = {Animal Communication,Animal Social Behavior,Chimpanzees,Classification (Cognitive Process),Cues,Emotional Content,Facial Expressions,Intersensory Processes},
  number = {3}
}

@article{partanCommunicationGoesMultimodal1999,
  title = {Communication {{Goes Multimodal}}},
  author = {Partan, S. R. and Marler, P.},
  date = {1999-02-26},
  journaltitle = {Science},
  volume = {283},
  pages = {1272--1273},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.283.5406.1272},
  url = {https://science.sciencemag.org/content/283/5406/1272},
  urldate = {2019-11-15},
  abstract = {{$<$}p{$>$} Communication depends on the simultaneous receipt of multiple sensory stimuli. The Perspective by Partan and Marler in this week9s issue postulates a new classification system for multimodal sensory signals. Combinations of sensory signals are classified according to the behavioral responses they elicit. {$<$}/p{$>$}},
  eprint = {10084931},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EZ48YMV6\\1272.html},
  langid = {english},
  number = {5406}
}

@article{partanCommunicationGoesMultimodal1999a,
  title = {Communication {{Goes Multimodal}}},
  author = {Partan, S. R. and Marler, P.},
  date = {1999-02-26},
  journaltitle = {Science},
  volume = {283},
  pages = {1272--1273},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.283.5406.1272},
  url = {https://science.sciencemag.org/content/283/5406/1272},
  urldate = {2020-05-29},
  abstract = {{$<$}p{$>$} Communication depends on the simultaneous receipt of multiple sensory stimuli. The Perspective by Partan and Marler in this week9s issue postulates a new classification system for multimodal sensory signals. Combinations of sensory signals are classified according to the behavioral responses they elicit. {$<$}/p{$>$}},
  eprint = {10084931},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CLZC53BA\\1272.html},
  langid = {english},
  number = {5406}
}

@article{partanIssuesClassificationMultimodal2005,
  title = {Issues in the Classification of Multimodal Communication Signals},
  author = {Partan, S. R. and Marler, P.},
  date = {2005-08},
  journaltitle = {The American Naturalist},
  shortjournal = {Am Nat},
  volume = {166},
  pages = {231--245},
  issn = {1537-5323},
  doi = {10.1086/431246},
  abstract = {Communication involves complex behavior in multiple sensory channels, or "modalities." We provide an overview of multimodal communication and its costs and benefits, place examples of signals and displays from an array of taxa, sensory systems, and functions into our signal classification system, and consider issues surrounding the categorization of multimodal signals. The broadest level of classification is between signals with redundant and nonredundant components, with finer distinctions in each category. We recommend that researchers gather information on responses to each component of a multimodal signal as well as the response to the signal as a whole. We discuss the choice of categories, whether to categorize signals on the basis of the signal or the response, and how to classify signals if data are missing. The choice of behavioral assay may influence the outcome, as may the context of the communicative event. We also consider similarities and differences between multimodal and unimodal composite signals and signals that are sequentially, rather than simultaneously, multimodal.},
  eprint = {16032576},
  eprinttype = {pmid},
  keywords = {Animal Communication,Animals,Biological Evolution,Classification,Sensation},
  langid = {english},
  number = {2}
}

@article{partanSingleMultichannelSignal2002,
  title = {Single and Multichannel Signal Composition: {{Facial}} Expression and Vocalizations of Rhesus Macaques ({{Macaca Mulatta}})},
  shorttitle = {{{SINGLE AND MULTICHANNEL SIGNAL COMPOSITION}}},
  author = {Partan, S.},
  date = {2002-01-01},
  journaltitle = {Behaviour},
  volume = {139},
  pages = {993--1027},
  publisher = {{Brill}},
  issn = {0005-7959, 1568-539X},
  doi = {10.1163/15685390260337877},
  url = {https://brill.com/view/journals/beh/139/8/article-p993_1.xml},
  urldate = {2020-12-07},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EFSZTPK2\\Partan - 2002 - SINGLE AND MULTICHANNEL SIGNAL COMPOSITION FACIAL.pdf;C\:\\Users\\u668173\\Zotero\\storage\\LWE5N8R3\\article-p993_1.html},
  langid = {english},
  number = {8}
}

@article{partanTenUnansweredQuestions2013,
  title = {Ten Unanswered Questions in Multimodal Communication},
  author = {Partan, S. R.},
  date = {2013},
  journaltitle = {Behavioral Ecology and Sociobiology},
  shortjournal = {Behav Ecol Sociobiol},
  volume = {67},
  pages = {1523--1539},
  issn = {0340-5443},
  doi = {10.1007/s00265-013-1565-y},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3742419/},
  urldate = {2020-10-08},
  abstract = {The study of multimodal communication has become an active and vibrant field. This special issue of Behavioral Ecology and Sociobiology brings together new developments in this rapidly expanding area. In this final contribution to the special issue, I look to the future and discuss ten questions in need of further work, touching on issues ranging from theoretical modeling and the evolution of behavior to molecular mechanisms and the development of behavior. In particular, I emphasize that the use of multimodal communication allows animals to switch between sensory channels when one channel becomes too noisy, and suggest that a better understanding of this process may help us both to understand the evolution of multisensory signaling and to predict the success of species facing environmental changes that affect signaling channels, such as urbanization and climate change. An expanded section is included on the effects of climate change on animal communication across sensory channels, urging researchers to pursue this topic due to the rapidity with which the environment is currently transforming.},
  eprint = {23956487},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9P4RZHZ3\\Partan - 2013 - Ten unanswered questions in multimodal communicati.pdf},
  number = {9},
  pmcid = {PMC3742419}
}

@thesis{paschalidouEffortGesturalInteractions2017,
  title = {Effort in Gestural Interactions with Imaginary Objects in {{Hindustani Dhrupad}} Vocal Music},
  author = {Paschalidou, P.-S.},
  date = {2017},
  institution = {{Durham University}},
  url = {http://etheses.dur.ac.uk/12308/},
  urldate = {2020-10-23},
  abstract = {Physical effort has often been regarded as a key factor of expressivity in music performance. Nevertheless, systematic experimental approaches to the subject have been rare. In North Indian classical (Hindustani) vocal music, singers often engage with melodic ideas during improvisation by manipulating intangible, imaginary objects with their hands, such as through stretching, pulling, pushing, throwing etc. The above observation suggests that some patterns of change in acoustic features allude to interactions that real objects through their physical properties can afford. The present study reports on the exploration of the relationships between movement and sound by accounting for the physical effort that such interactions require in the Dhrupad genre of Hindustani vocal improvisation. The work follows a mixed methodological approach, combining qualitative and quantitative methods to analyse interviews, audio-visual material and movement data. Findings indicate that despite the flexibility in the way a Dhrupad vocalist might use his/her hands while singing, there is a certain degree of consistency by which performers associate effort levels with melody and types of gestural interactions with imaginary objects. However, different schemes of cross-modal associations are revealed for the vocalists analysed, that depend on the pitch space organisation of each particular melodic mode (rāga), the mechanical requirements of voice production, the macro-structure of the ālāp improvisation and morphological cross-domain analogies. Results further suggest that a good part of the variance in both physical effort and gesture type can be explained through a small set of sound and movement features. Based on the findings, I argue that gesturing in Dhrupad singing is guided by: the know-how of humans in interacting with and exerting effort on real objects of the environment, the movement–sound relationships transmitted from teacher to student in the oral music training context and the mechanical demands of vocalisation.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZS6VETZH\\PASCHALIDOU - 2017 - Effort in gestural interactions with imaginary obj.pdf;C\:\\Users\\u668173\\Zotero\\storage\\6BYPE4SR\\12308.html;C\:\\Users\\u668173\\Zotero\\storage\\YKXZHFSG\\12308.html},
  type = {Doctoral}
}

@article{patelExperimentalEvidenceSynchronization2009,
  title = {Experimental Evidence for Synchronization to a Musical Beat in a Nonhuman Animal},
  author = {Patel, Aniruddh D. and Iversen, John R. and Bregman, Micah R. and Schulz, Irena},
  date = {2009-05-26},
  journaltitle = {Current biology: CB},
  shortjournal = {Curr Biol},
  volume = {19},
  pages = {827--830},
  issn = {1879-0445},
  doi = {10.1016/j.cub.2009.03.038},
  abstract = {The tendency to move in rhythmic synchrony with a musical beat (e.g., via head bobbing, foot tapping, or dance) is a human universal [1] yet is not commonly observed in other species [2]. Does this ability reflect a brain specialization for music cognition, or does it build on neural circuitry that ordinarily serves other functions? According to the "vocal learning and rhythmic synchronization" hypothesis [3], entrainment to a musical beat relies on the neural circuitry for complex vocal learning, an ability that requires a tight link between auditory and motor circuits in the brain [4, 5]. This hypothesis predicts that only vocal learning species (such as humans and some birds, cetaceans, and pinnipeds, but not nonhuman primates) are capable of synchronizing movements to a musical beat. Here we report experimental evidence for synchronization to a beat in a sulphur-crested cockatoo (Cacatua galerita eleonora). By manipulating the tempo of a musical excerpt across a wide range, we show that the animal spontaneously adjusts the tempo of its rhythmic movements to stay synchronized with the beat. These findings indicate that synchronization to a musical beat is not uniquely human and suggest that animal models can provide insights into the neurobiology and evolution of human music [6].},
  eprint = {19409790},
  eprinttype = {pmid},
  keywords = {Adult,Animals,Auditory Perception,Behavior; Animal,Birds,Child,Dancing,Humans,Male,Music,Periodicity,Time Perception},
  langid = {english},
  number = {10}
}

@article{patelStudyingSynchronizationMusical2009,
  title = {Studying Synchronization to a Musical Beat in Nonhuman Animals},
  author = {Patel, Aniruddh D. and Iversen, John R. and Bregman, Micah R. and Schulz, Irena},
  date = {2009-07},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann N Y Acad Sci},
  volume = {1169},
  pages = {459--469},
  issn = {1749-6632},
  doi = {10.1111/j.1749-6632.2009.04581.x},
  abstract = {The recent discovery of spontaneous synchronization to music in a nonhuman animal (the sulphur-crested cockatoo Cacatua galerita eleonora) raises several questions. How does this behavior differ from nonmusical synchronization abilities in other species, such as synchronized frog calls or firefly flashes? What significance does the behavior have for debates over the evolution of human music? What kinds of animals can synchronize to musical rhythms, and what are the key methodological issues for research in this area? This paper addresses these questions and proposes some refinements to the "vocal learning and rhythmic synchronization hypothesis."},
  eprint = {19673824},
  eprinttype = {pmid},
  keywords = {Animals,Auditory Perception,Birds,Music,Periodicity,Primates},
  langid = {english}
}

@book{patteeLAWSLANGUAGELIFE2012,
  title = {{{LAWS}}, {{LANGUAGE}} and {{LIFE}}: {{Howard Pattee}}’s Classic Papers on the Physics of Symbols with Contemporary Commentary},
  shorttitle = {{{LAWS}}, {{LANGUAGE}} and {{LIFE}}},
  author = {Pattee, Howard Hunt and Rączaszek-Leonardi, Joanna},
  date = {2012-12-09},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Howard Pattee is a physicist who for many years has taken his own path in studying the physics of symbols, which is now a foundation for biosemiotics. By extending von Neumann’s logical requirements for self-replication, to the physical requirements of symbolic instruction at the molecular level, he concludes that a form of quantum measurement is necessary for life. He explains why all non-dynamic symbolic and informational controls act as special (allosteric) constraints on dynamical systems. Pattee also points out that symbols do not exist in isolation but in coordinated symbol systems we call languages. Such insights turn out to be necessary to situate biosemiotics as an objective scientific endeavor. By proposing a way to relate quiescent symbolic constraints to dynamics, Pattee’s work builds a bridge between physical, biological, and psychological models that are based on dynamical systems theory. Pattee’s work awakes new interest in cognitive scientists, where his recognition of the necessary separation—the epistemic cut—between the subject and object provides a basis for a complementary third way of relating the purely symbolic, computational models of cognition and the purely dynamic, non-representational models. This selection of Pattee’s papers also addresses several other fields, including hierarchy theory, artificial life, self-organization, complexity theory, and the complementary epistemologies of the physical and biological sciences.},
  eprint = {raEQodcVdYQC},
  eprinttype = {googlebooks},
  isbn = {978-94-007-5161-3},
  keywords = {Philosophy / Epistemology,Philosophy / Language,Philosophy / Reference,Psychology / Cognitive Psychology & Cognition,Science / History,Science / Life Sciences / Anatomy & Physiology,Science / Life Sciences / Biology,Science / Life Sciences / General,Science / Life Sciences / Molecular Biology,Science / Physics / General},
  langid = {english},
  pagetotal = {338}
}

@article{paxtonArgumentDisruptsInterpersonal2013,
  title = {Argument Disrupts Interpersonal Synchrony},
  author = {Paxton, A. and Dale, Rick},
  date = {2013-11-01},
  journaltitle = {Quarterly Journal of Experimental Psychology},
  shortjournal = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  pages = {2092--2102},
  issn = {1747-0218},
  doi = {10.1080/17470218.2013.853089},
  url = {https://doi.org/10.1080/17470218.2013.853089},
  urldate = {2019-11-30},
  abstract = {Research on interpersonal convergence and synchrony characterizes the way in which interacting individuals come to have more similar affect, behaviour, and cognition over time. Although its dynamics have been explored in many settings, convergence during conflict has been almost entirely overlooked. We present a simple but ecologically valid study comparing how different situational contexts that highlight affiliation and argument impact interpersonal convergence of body movement and to what degree emotional states affect convergence in both conversational settings. Using linear mixed-effect models, we found that in-phase bodily synchrony decreases significantly during argument. However, affective changes did not significantly predict changes in levels of interpersonal synchrony, suggesting that differences in affect valences between affiliation and argument cannot solely explain our results.},
  langid = {english},
  number = {11}
}

@article{paxtonCaseIntersectionalityEcological,
  title = {The Case for Intersectionality in Ecological Psychology},
  author = {Paxton, A. and Blau, J. and Weston, M. L.}
}

@article{paxtonInterpersonalMovementSynchrony2017,
  title = {Interpersonal Movement Synchrony Responds to High- and Low-{{Level}} Conversational Constraints},
  author = {Paxton, A. and Dale, R.},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2017.01135},
  url = {https://www.jair.org/index.php/jair/article/view/10536},
  urldate = {2019-06-21},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\B2NMR4A4\\10536.html}
}

@article{paxtonInterpersonalMovementSynchrony2017a,
  title = {Interpersonal {{Movement Synchrony Responds}} to {{High}}- and {{Low}}-{{Level Conversational Constraints}}},
  author = {Paxton, Alexandra and Dale, Rick},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.01135},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01135/full},
  urldate = {2020-12-05},
  abstract = {Much work on communication and joint action conceptualizes interaction as a dynamical system. Under this view, dynamic properties of interaction should be shaped by the context in which the interaction is taking place. Here we explore interpersonal movement coordination or synchrony---the degree to which individuals move in similar ways over time---as one such context-sensitive property. Studies of coordination have typically investigated how these dynamics are influenced by either high-level constraints (i.e., slow-changing factors) or low-level constraints (i.e., fast-changing factors like movement). Focusing on nonverbal communication behaviors during naturalistic conversation, we analyzed how interacting participants' head movement dynamics were shaped simultaneously by high-level constraints (i.e., conversation type; friendly conversations or arguments) and low-level constraints (i.e., perceptual stimuli; non-informative visual stimuli vs. informative visual stimuli). We found that high- and low-level constraints interacted non-additively to affect interpersonal movement dynamics, highlighting the context sensitivity of interaction and supporting the view of joint action as a complex adaptive system.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7T829TUP\\Paxton and Dale - 2017 - Interpersonal Movement Synchrony Responds to High-.pdf},
  keywords = {conversation,cross-recurrence quantification analysis,dual-task performance,interpersonal coordination,Joint Action,Movement dynamics,synchrony,working memory},
  langid = {english}
}

@article{paxtonMultimodalNetworksInterpersonal,
  title = {Multimodal {{Networks}} of {{Interpersonal Interaction}} and {{Conversational Contexts}}},
  author = {Paxton, Alexandra and Dale, Rick},
  pages = {7},
  abstract = {In interpersonal interaction, the terms synchrony or alignment refer to the way in which communication channels like speech or body movement become intertwined over time, both across interlocutors and within a single individual. A recent trend in alignment research has targeted multimodal alignment, exploring how various communication channels affect one another over time (e.g., Louwerse et al., 2012). While existing research has made significant progress in mapping multimodal alignment during task-based or positively valenced interactions, little is known about the dynamics of multimodal alignment during conflict. We visualize multimodal alignment during naturalistic affiliative and argumentative interactions as networks based on analyses of body movement and speech. Broadly, we find that conversational contexts strongly impact the ways in which interlocutors’ movement and speech systems self-organize interpersonally and intrapersonally.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5BD477BP\\Paxton and Dale - Multimodal Networks of Interpersonal Interaction a.pdf},
  langid = {english}
}

@article{paxtonNetworkAnalysisMultimodal2014,
  title = {Network {{Analysis}} of {{Multimodal}}, {{Multiscale Coordination}} in {{Dyadic Problem Solving}}},
  author = {Paxton, Alexanra and Abney, Drew H. and Kello, Christopher T. and Dale, Rick K.},
  date = {2014},
  journaltitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {36},
  issn = {1069-7977},
  url = {https://escholarship.org/uc/item/7xz2z06w},
  urldate = {2020-09-17},
  abstract = {Author(s): Paxton, Alexanra; Abney, Drew H.; Kello, Christopher T.; Dale, Rick K.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Q3LPAJUW\\Paxton et al. - 2014 - Network Analysis of Multimodal, Multiscale Coordin.pdf;C\:\\Users\\u668173\\Zotero\\storage\\WHWV7S5Y\\7xz2z06w.html},
  langid = {english},
  number = {36}
}

@article{pearceAuditoryExpectationInformation2012,
  title = {Auditory Expectation: The Information Dynamics of Music Perception and Cognition},
  shorttitle = {Auditory Expectation},
  author = {Pearce, Marcus T. and Wiggins, Geraint A.},
  date = {2012-10},
  journaltitle = {Topics in Cognitive Science},
  shortjournal = {Top Cogn Sci},
  volume = {4},
  pages = {625--652},
  issn = {1756-8765},
  doi = {10.1111/j.1756-8765.2012.01214.x},
  abstract = {Following in a psychological and musicological tradition beginning with Leonard Meyer, and continuing through David Huron, we present a functional, cognitive account of the phenomenon of expectation in music, grounded in computational, probabilistic modeling. We summarize a range of evidence for this approach, from psychology, neuroscience, musicology, linguistics, and creativity studies, and argue that simulating expectation is an important part of understanding a broad range of human faculties, in music and beyond.},
  eprint = {22847872},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DNIJ9H2D\\Pearce and Wiggins - 2012 - Auditory expectation the information dynamics of .pdf},
  keywords = {Anticipation; Psychological,Auditory Perception,Cognition,Creativity,Emotions,Esthetics,Humans,Information Theory,Learning,Memory,Models; Theoretical,Music,Pitch Perception},
  langid = {english},
  number = {4}
}

@thesis{pearceConstructionEvaluationStatistical2005,
  title = {The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition},
  author = {Pearce, M. T.},
  date = {2005-12},
  institution = {{City University London}},
  url = {https://openaccess.city.ac.uk/id/eprint/8459/},
  urldate = {2020-12-03},
  abstract = {The prevalent approach to developing cognitive models of music perception and composition is to construct systems of symbolic rules and constraints on the basis of extensive music-theoretic and music-analytic knowledge. The thesis proposed in this dissertation is that statistical models which acquire knowledge through the induction of regularities in corpora of existing music can, if examined with appropriate methodologies, provide significant insights into the cognitive processing involved in music perception and composition. This claim is examined in three stages. First, a number of statistical modelling techniques drawn from the fields of data compression, statistical language modelling and machine learning are subjected to empirical evaluation in the context of sequential prediction of pitch structure in unseen melodies. This investigation results in a collection of modelling strategies which together yield significant performance improvements over existing methods. In the second stage, these statistical systems are used to examine observed patterns of expectation collected in previous psychological research on melody perception. In contrast to previous accounts of this data, the results demonstrate that these patterns of expectation can be accounted for in terms of the induction of statistical regularities acquired through exposure to music. In the final stage of the present research, the statistical systems developed in the first stage are used to examine the intrinsic computational demands of the task of composing a stylistically successful melody The results suggest that the systems lack the degree of expressive power needed to consistently meet the demands of the task. In contrast to previous research, however, the methodological framework developed for the evaluation of computational models of composition enables a detailed empirical examination and comparison of such models which facilitates the identification and resolution of their weaknesses.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VN9MVNCX\\Pearce - 2005 - The construction and evaluation of statistical mod.pdf;C\:\\Users\\u668173\\Zotero\\storage\\DFSQ8UH7\\8459.html;C\:\\Users\\u668173\\Zotero\\storage\\WXWR7IVE\\8459.html},
  langid = {english},
  type = {doctoral}
}

@article{pearceExpectationMelodyInfluence2006,
  title = {Expectation in {{Melody}}: {{The Influence}} of {{Context}} and {{Learning}}},
  shorttitle = {Expectation in {{Melody}}},
  author = {Pearce, Marcus T. and Wiggins, Geraint A.},
  date = {2006-07-01},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  volume = {23},
  pages = {377--405},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.23.5.377},
  url = {/mp/article/23/5/377/62287/Expectation-in-Melody-The-Influence-of-Context-and},
  urldate = {2020-12-03},
  abstract = {The Implication-Realization (IR) theory (Narmour, 1990) posits two cognitive systems involved in the generation of melodic expectations: The first consists of a limited number of symbolic rules that are held to be innate and universal; the second reflects the top-down influences of acquired stylistic knowledge. Aspects of both systems have been implemented as quantitative models in research which has yielded empirical support for both components of the theory (Cuddy \& Lunny, 1995; Krumhansl, 1995a, 1995b; Schellenberg, 1996, 1997). However, there is also evidence that the implemented bottom-up rules constitute too inflexible a model to account for the influence of the musical experience of the listener and the melodic context in which expectations are elicited. A theory is presented, according to which both bottom-up and top-down descriptions of observed patterns of melodic expectation may be accounted for in terms of the induction of statistical regularities in existing musical repertoires. A computational model that embodies this theory is developed and used to reanalyze existing experimental data on melodic expectancy. The results of three experiments with increasingly complex melodic stimuli demonstrate that this model is capable of accounting for listeners’ expectations as well as or better than the two-factor model of Schellenberg (1997).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JW6QBLBN\\Pearce and Wiggins - 2006 - Expectation in Melody The Influence of Context an.pdf;C\:\\Users\\u668173\\Zotero\\storage\\92PXYFA7\\Expectation-in-Melody-The-Influence-of-Context-and.html},
  langid = {english},
  number = {5}
}

@article{pearceUnsupervisedStatisticalLearning2010,
  title = {Unsupervised Statistical Learning Underpins Computational, Behavioural, and Neural Manifestations of Musical Expectation},
  author = {Pearce, Marcus T. and Ruiz, María Herrojo and Kapasi, Selina and Wiggins, Geraint A. and Bhattacharya, Joydeep},
  date = {2010-03},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  volume = {50},
  pages = {302--313},
  issn = {1095-9572},
  doi = {10.1016/j.neuroimage.2009.12.019},
  abstract = {The ability to anticipate forthcoming events has clear evolutionary advantages, and predictive successes or failures often entail significant psychological and physiological consequences. In music perception, the confirmation and violation of expectations are critical to the communication of emotion and aesthetic effects of a composition. Neuroscientific research on musical expectations has focused on harmony. Although harmony is important in Western tonal styles, other musical traditions, emphasizing pitch and melody, have been rather neglected. In this study, we investigated melodic pitch expectations elicited by ecologically valid musical stimuli by drawing together computational, behavioural, and electrophysiological evidence. Unlike rule-based models, our computational model acquires knowledge through unsupervised statistical learning of sequential structure in music and uses this knowledge to estimate the conditional probability (and information content) of musical notes. Unlike previous behavioural paradigms that interrupt a stimulus, we devised a new paradigm for studying auditory expectation without compromising ecological validity. A strong negative correlation was found between the probability of notes predicted by our model and the subjectively perceived degree of expectedness. Our electrophysiological results showed that low-probability notes, as compared to high-probability notes, elicited a larger (i) negative ERP component at a late time period (400-450 ms), (ii) beta band (14-30 Hz) oscillation over the parietal lobe, and (iii) long-range phase synchronization between multiple brain regions. Altogether, the study demonstrated that statistical learning produces information-theoretic descriptions of musical notes that are proportional to their perceived expectedness and are associated with characteristic patterns of neural activity.},
  eprint = {20005297},
  eprinttype = {pmid},
  keywords = {Adult,Aged,Auditory Perception,Beta Rhythm,Computer Simulation,Cortical Synchronization,Electroencephalography,Evoked Potentials,Female,Humans,Learning,Male,Mental Processes,Middle Aged,Models; Neurological,Music,Periodicity,Probability,Time Factors,Young Adult},
  langid = {english},
  number = {1}
}

@article{pearceUnsupervisedStatisticalLearning2010a,
  title = {Unsupervised Statistical Learning Underpins Computational, Behavioural, and Neural Manifestations of Musical Expectation},
  author = {Pearce, Marcus T. and Ruiz, María Herrojo and Kapasi, Selina and Wiggins, Geraint A. and Bhattacharya, Joydeep},
  date = {2010-03-01},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {50},
  pages = {302--313},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2009.12.019},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811909013068},
  urldate = {2020-12-17},
  abstract = {The ability to anticipate forthcoming events has clear evolutionary advantages, and predictive successes or failures often entail significant psychological and physiological consequences. In music perception, the confirmation and violation of expectations are critical to the communication of emotion and aesthetic effects of a composition. Neuroscientific research on musical expectations has focused on harmony. Although harmony is important in Western tonal styles, other musical traditions, emphasizing pitch and melody, have been rather neglected. In this study, we investigated melodic pitch expectations elicited by ecologically valid musical stimuli by drawing together computational, behavioural, and electrophysiological evidence. Unlike rule-based models, our computational model acquires knowledge through unsupervised statistical learning of sequential structure in music and uses this knowledge to estimate the conditional probability (and information content) of musical notes. Unlike previous behavioural paradigms that interrupt a stimulus, we devised a new paradigm for studying auditory expectation without compromising ecological validity. A strong negative correlation was found between the probability of notes predicted by our model and the subjectively perceived degree of expectedness. Our electrophysiological results showed that low-probability notes, as compared to high-probability notes, elicited a larger (i) negative ERP component at a late time period (400–450~ms), (ii) beta band (14–30~Hz) oscillation over the parietal lobe, and (iii) long-range phase synchronization between multiple brain regions. Altogether, the study demonstrated that statistical learning produces information-theoretic descriptions of musical notes that are proportional to their perceived expectedness and are associated with characteristic patterns of neural activity.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JNED9J2S\\S1053811909013068.html},
  langid = {english},
  number = {1}
}

@thesis{pearsonGestureKarnatakMusic2016,
  title = {Gesture in {{Karnatak Music}}: {{Pedagogy}} and {{Musical Structure}} in {{South India}}},
  shorttitle = {Gesture in {{Karnatak Music}}},
  author = {Pearson, L.},
  date = {2016},
  institution = {{Durham University}},
  url = {http://etheses.dur.ac.uk/11782/},
  urldate = {2020-10-23},
  abstract = {This thesis presents an examination of gesture in Karnatak music, the art music of South India. The topic is approached from two perspectives; the first considers Karnatak music structure from a gestural perspective, looking both at the music itself and at the gestures that create it, while the second enquires into the role played by physical gesture in vocal pedagogy. The broader aims of the thesis are to provide insight into the musical structure of the Karnatak style, and to contribute to wider discourses on connections between music and movement. An interdisciplinary approach to the research is taken, drawing on theories and methods from the fields of ethnomusicology, embodied music cognition, and gesture studies.  The first part of the thesis opens with a discussion of differences between practical and theoretical conceptions of the Karnatak style. I argue for the significance in practice of svara-gamaka units and longer motifs formed of chains of such units, and also consider the gestural qualities of certain motifs and their contribution to bhāva (mood). Subsequently, I present a joint musical and motoric analysis of a section of Karnatak violin performance, seeking to elucidate the dynamic processes that form the style. The second part of the thesis enquires into the role played by hand gestures produced by teachers and students in vocal lessons, looking at what is indexed by the gestures and how such indexing contributes to the pedagogic process. This part of the thesis also considers how gestures contribute to the formation and maintenance of common ground between teacher and student. The final chapter brings the two strands of this thesis together to discuss the connections that exist between musical and physical gesture in Karnatak music.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QMULA53U\\PEARSON - 2016 - Gesture in Karnatak Music Pedagogy and Musical St.pdf;C\:\\Users\\u668173\\Zotero\\storage\\SJYYIS62\\11782.html;C\:\\Users\\u668173\\Zotero\\storage\\UPXWC3XA\\11782.html},
  type = {Doctoral}
}

@article{pearsonGestureSonicEvent2013,
  title = {Gesture and the {{Sonic Event}} in {{Karnatak Music}}},
  author = {Pearson, Lara},
  date = {2013-10-24},
  journaltitle = {Empirical Musicology Review},
  volume = {8},
  pages = {2--14},
  issn = {1559-5749},
  doi = {10.18061/emr.v8i1.3918},
  url = {http://emusicology.org/article/view/3918},
  urldate = {2019-09-20},
  abstract = {This paper presents an analysis of the relationship between gesture and music in the context of a Karnatak vocal lesson recorded in Tamil Nadu, South India in September 2011. The study aims to examine instances of correspondence between gesture and sonic event that occur during the lesson. Through this analysis the paper aims to contribute to the wider debate on the factors that determine gesture. Shape and trajectory are used in this study as means of describing and comparing gestures. The teacher’s hand movements are tracked and traced rendering the gestures as static shapes in still images, and developing lines in moving images. The correspondences found between gestures and sonic features are discussed in relation to the physical movement required to produce the music. In addition, the circumstances in which correspondence is not found are analyzed and the extent to which the dynamic form of gesture is also influenced by the phrase as a whole is emphasized.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TCRJQNPC\\Pearson - 2013 - Gesture and the Sonic Event in Karnatak Music.pdf;C\:\\Users\\u668173\\Zotero\\storage\\X4RGCFSX\\3918.html},
  keywords = {gesture,movement,shape,South Indian music},
  langid = {english},
  number = {1}
}

@article{peckreClarifyingExpandingSocial2019,
  title = {Clarifying and Expanding the Social Complexity Hypothesis for Communicative Complexity},
  author = {Peckre, Louise and Kappeler, Peter M. and Fichtel, Claudia},
  date = {2019-01},
  journaltitle = {Behavioral Ecology and Sociobiology},
  volume = {73},
  issn = {0340-5443, 1432-0762},
  doi = {10.1007/s00265-018-2605-4},
  url = {http://link.springer.com/10.1007/s00265-018-2605-4},
  urldate = {2020-09-02},
  abstract = {Variation in communicative complexity has been conceptually and empirically attributed to social complexity, with animals living in more complex social environments exhibiting more signals and/or more complex signals than animals living in simpler social environments. As compelling as studies highlighting a link between social and communicative variables are, this hypothesis remains challenged by operational problems, contrasting results, and several weaknesses of the associated tests. Specifically, how to best operationalize social and communicative complexity remains debated; alternative hypotheses, such as the role of a species’ ecology, morphology, or phylogenetic history, have been neglected; and the actual ways in which variation in signaling is directly affected by social factors remain largely unexplored. In this review, we address these three issues and propose an extension of the Bsocial complexity hypothesis for communicative complexity\^ that resolves and acknowledges the above factors. We specifically argue for integrating the inherently multimodal nature of communication into a more comprehensive framework and for acknowledging the social context of derived signals and the potential of audience effects. By doing so, we believe it will be possible to generate more accurate predictions about which specific social parameters may be responsible for selection on new or more complex signals, as well as to uncover potential adaptive functions that are not necessarily apparent from studying communication in only one modality.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PEZ3N7RX\\Peckre et al. - 2019 - Clarifying and expanding the social complexity hyp.pdf},
  langid = {english},
  number = {1}
}

@article{peetersElectrophysiologicalKinematicCorrelates2015,
  title = {Electrophysiological and {{Kinematic Correlates}} of {{Communicative Intent}} in the {{Planning}} and {{Production}} of {{Pointing Gestures}} and {{Speech}}},
  author = {Peeters, David and Chu, Mingyuan and Holler, Judith and Hagoort, Peter and Özyürek, Aslı},
  date = {2015-08-18},
  journaltitle = {Journal of Cognitive Neuroscience},
  volume = {27},
  pages = {2352--2368},
  publisher = {{MIT Press}},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_00865},
  url = {https://doi.org/10.1162/jocn_a_00865},
  urldate = {2021-03-03},
  abstract = {In everyday human communication, we often express our communicative intentions by manually pointing out referents in the material world around us to an addressee, often in tight synchronization with referential speech. This study investigated whether and how the kinematic form of index finger pointing gestures is shaped by the gesturer's communicative intentions and how this is modulated by the presence of concurrently produced speech. Furthermore, we explored the neural mechanisms underpinning the planning of communicative pointing gestures and speech. Two experiments were carried out in which participants pointed at referents for an addressee while the informativeness of their gestures and speech was varied. Kinematic and electrophysiological data were recorded online. It was found that participants prolonged the duration of the stroke and poststroke hold phase of their gesture to be more communicative, in particular when the gesture was carrying the main informational burden in their multimodal utterance. Frontal and P300 effects in the ERPs suggested the importance of intentional and modality-independent attentional mechanisms during the planning phase of informative pointing gestures. These findings contribute to a better understanding of the complex interplay between action, attention, intention, and language in the production of pointing gestures, a communicative act core to human interaction.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NWPAED85\\Peeters et al. - 2015 - Electrophysiological and Kinematic Correlates of C.pdf;C\:\\Users\\u668173\\Zotero\\storage\\WYWM8RHS\\jocn_a_00865.html},
  number = {12}
}

@article{pellecchiaConcurrentCognitiveTask2005,
  title = {Concurrent Cognitive Task Modulates Coordination Dynamics},
  author = {Pellecchia, Geraldine L. and Shockley, Kevin and Turvey, M. T.},
  date = {2005},
  journaltitle = {Cognitive Science},
  volume = {29},
  pages = {531--557},
  publisher = {{Lawrence Erlbaum}},
  location = {{US}},
  issn = {1551-6709(Electronic),0364-0213(Print)},
  doi = {10.1207/s15516709cog0000_12},
  abstract = {Does a concurrent cognitive task affect the dynamics of bimanual rhythmic coordination? In-phase coordination was performed under manipulations of phase detuning and movement frequency and either singly or in combination with an arithmetic task. Predicted direction-specific shifts in stable relative phase from 0° due to detuning and movement frequency were amplified by the cognitive task. Nonlinear cross-recurrence analysis suggested that this cognitive influence on the locations of the stable points or attractors of coordination entailed a magnification of attractor noise without a reduction in attractor strength. An approximation to these findings was achieved through parameter changes in a motion equation in relative phase. Results are discussed in terms of dual-task performance as limited resources, dynamics rather than chronometrics, and reparameterization rather than degradation. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\E6EBIDDD\\2005-11173-002.html},
  keywords = {Cognitive Processes,Motor Coordination,Rhythm,Task Complexity},
  number = {4}
}

@article{pellecchiaConcurrentCognitiveTask2005a,
  title = {Concurrent {{Cognitive Task Modulates Coordination Dynamics}}},
  author = {Pellecchia, Geraldine L. and Shockley, Kevin and Turvey, M. T.},
  date = {2005},
  journaltitle = {Cognitive Science},
  volume = {29},
  pages = {531--557},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog0000_12},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_12},
  urldate = {2020-10-15},
  abstract = {Does a concurrent cognitive task affect the dynamics of bimanual rhythmic coordination? In-phase coordination was performed under manipulations of phase detuning and movement frequency and either singly or in combination with an arithmetic task. Predicted direction-specific shifts in stable relative phase from 0° due to detuning and movement frequency were amplified by the cognitive task. Nonlinear cross-recurrence analysis suggested that this cognitive influence on the locations of the stable points or attractors of coordination entailed a magnification of attractor noise without a reduction in attractor strength. An approximation to these findings was achieved through parameter changes in a motion equation in relative phase. Results are discussed in terms of dual-task performance as limited resources, dynamics rather than chronometrics, and reparameterization rather than degradation.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog0000\_12},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3SZANJWV\\Pellecchia et al. - 2005 - Concurrent Cognitive Task Modulates Coordination D.pdf;C\:\\Users\\u668173\\Zotero\\storage\\I98JWG6S\\s15516709cog0000_12.html},
  keywords = {Cognition,Coordination,Dynamics},
  langid = {english},
  number = {4}
}

@article{penaRhythmYourLips2016,
  title = {Rhythm on {{Your Lips}}},
  author = {Peña, Marcela and Langus, Alan and Gutiérrez, César and Huepe-Artigas, Daniela and Nespor, Marina},
  date = {2016},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {7},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.01708},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01708/full},
  urldate = {2020-12-07},
  abstract = {The Iambic-Trochaic Law (ITL) accounts for speech rhythm, grouping of sounds as either Iambs – if alternating in duration – or Trochees – if alternating in pitch and/or intensity. The two different rhythms signal word order, one of the basic syntactic properties of language. We investigated the extent to which Iambic and Trochaic phrases could be auditorily and visually recognized, when visual stimuli engage lip reading. Our results show both rhythmic patterns were recognized from both, auditory and visual stimuli, suggesting that speech rhythm has a multimodal representation. We further explored whether participants could match Iambic and Trochaic phrases across the two modalities. We found that participants auditorily familiarized with Trochees, but not with Iambs, were more accurate in recognizing visual targets, while participants visually familiarized with Iambs, but not with Trochees, were more accurate in recognizing auditory targets. The latter results suggest an asymmetric processing of speech rhythm: in auditory domain, the changes in either pitch or intensity are better perceived and represented than changes in duration, while in the visual domain the changes in duration are better processed and represented than changes in pitch, raising important questions about domain general and specialized mechanisms for speech rhythm processing.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NNCEKM8S\\Peña et al. - 2016 - Rhythm on Your Lips.pdf},
  keywords = {Iambic-trochaic law,Language,lip reading,Speech Perception,Visual Perception},
  langid = {english}
}

@article{pereiraChimpanzeeLipsmacksConfirm2020,
  title = {Chimpanzee Lip-Smacks Confirm Primate Continuity for Speech-Rhythm Evolution},
  author = {Pereira, André S. and Kavanagh, Eithne and Hobaiter, Catherine and Slocombe, Katie E. and Lameira, Adriano R.},
  date = {2020-05-27},
  journaltitle = {Biology Letters},
  shortjournal = {Biology Letters},
  volume = {16},
  pages = {20200232},
  publisher = {{Royal Society}},
  doi = {10.1098/rsbl.2020.0232},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsbl.2020.0232},
  urldate = {2020-06-10},
  abstract = {Speech is a human hallmark, but its evolutionary origins continue to defy scientific explanation. Recently, the open–close mouth rhythm of 2–7 Hz (cycles/second) characteristic of all spoken languages has been identified in the orofacial signals of several nonhuman primate genera, including orangutans, but evidence from any of the African apes remained missing. Evolutionary continuity for the emergence of speech is, thus, still inconclusive. To address this empirical gap, we investigated the rhythm of chimpanzee lip-smacks across four populations (two captive and two wild). We found that lip-smacks exhibit a speech-like rhythm at approximately 4 Hz, closing a gap in the evidence for the evolution of speech-rhythm within the primate order. We observed sizeable rhythmic variation within and between chimpanzee populations, with differences of over 2 Hz at each level. This variation did not result, however, in systematic group differences within our sample. To further explore the phylogenetic and evolutionary perspective on this variability, inter-individual and inter-population analyses will be necessary across primate species producing mouth signals at speech-like rhythm. Our findings support the hypothesis that speech recruited ancient primate rhythmic signals and suggest that multi-site studies may still reveal new windows of understanding about these signals' use and production along the evolutionary timeline of speech.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SNEBKM9Z\\Pereira et al. - 2020 - Chimpanzee lip-smacks confirm primate continuity f.pdf;C\:\\Users\\u668173\\Zotero\\storage\\BIIR2ABN\\rsbl.2020.html},
  number = {5}
}

@article{pereiraChimpanzeeLipsmacksConfirm2020a,
  title = {Chimpanzee Lip-Smacks Confirm Primate Continuity for Speech-Rhythm Evolution},
  author = {Pereira, André S. and Kavanagh, Eithne and Hobaiter, Catherine and Slocombe, Katie E. and Lameira, Adriano R.},
  date = {2020-05-27},
  journaltitle = {Biology Letters},
  shortjournal = {Biology Letters},
  volume = {16},
  pages = {20200232},
  publisher = {{Royal Society}},
  doi = {10.1098/rsbl.2020.0232},
  url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsbl.2020.0232},
  urldate = {2020-10-30},
  abstract = {Speech is a human hallmark, but its evolutionary origins continue to defy scientific explanation. Recently, the open–close mouth rhythm of 2–7 Hz (cycles/second) characteristic of all spoken languages has been identified in the orofacial signals of several nonhuman primate genera, including orangutans, but evidence from any of the African apes remained missing. Evolutionary continuity for the emergence of speech is, thus, still inconclusive. To address this empirical gap, we investigated the rhythm of chimpanzee lip-smacks across four populations (two captive and two wild). We found that lip-smacks exhibit a speech-like rhythm at approximately 4 Hz, closing a gap in the evidence for the evolution of speech-rhythm within the primate order. We observed sizeable rhythmic variation within and between chimpanzee populations, with differences of over 2 Hz at each level. This variation did not result, however, in systematic group differences within our sample. To further explore the phylogenetic and evolutionary perspective on this variability, inter-individual and inter-population analyses will be necessary across primate species producing mouth signals at speech-like rhythm. Our findings support the hypothesis that speech recruited ancient primate rhythmic signals and suggest that multi-site studies may still reveal new windows of understanding about these signals' use and production along the evolutionary timeline of speech.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\58G8VM8H\\Pereira et al. - 2020 - Chimpanzee lip-smacks confirm primate continuity f.pdf;C\:\\Users\\u668173\\Zotero\\storage\\3DBSYINH\\rsbl.2020.html;C\:\\Users\\u668173\\Zotero\\storage\\7VEV2TZH\\rsbl.2020.html},
  number = {5}
}

@book{perez-pereiraLanguageDevelopmentSocial1999,
  title = {Language {{Development}} and {{Social Interaction}} in {{Blind Children}}},
  author = {Perez-Pereira, M. and Conti-Ramsden, G.},
  date = {1999},
  publisher = {{Psychology Press}},
  location = {{New York}}
}

@article{perezConsciousProcessingNarrative2020,
  title = {Conscious Processing of Narrative Stimuli Synchronizes Heart Rate between Individuals},
  author = {Perez, Pauline and Madsen, Jens and Banellis, Leah and Turker, Basak and Raimondo, Federico and Perlbarg, Vincent and Valente, Melanie and Nierat, Marie-Cecile and Puybasset, Louis and Naccache, Lionel and Similowski, Thomas and Cruse, Damian and Parra, Lucas C. and Sitt, Jacobo},
  date = {2020-05-28},
  journaltitle = {bioRxiv},
  pages = {2020.05.26.116079},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.05.26.116079},
  url = {https://www.biorxiv.org/content/10.1101/2020.05.26.116079v1},
  urldate = {2020-05-29},
  abstract = {{$<$}p{$>$}Heart rate has natural fluctuations that are typically ascribed to autonomic function. Recent evidence suggests that conscious processing can affect the timing of the heartbeat. We hypothesized that heart rate is modulated by conscious processing and therefore dependent on attentional focus. To test this we leverage the observation that neural processes can be synchronized between subjects by presenting an identical narrative stimulus. As predicted, we find significant inter-subject correlation of the heartbeat (ISC-HR) when subjects are presented with an auditory or audiovisual narrative. Consistent with the conscious processing hypothesis, we find that ISC-HR is reduced when subjects are distracted from the narrative, and that higher heart rate synchronization predicts better recall of the narrative. Finally, patients with disorders of consciousness who are listening to a story have lower ISC, as compared to healthy individuals, and that individual ISC-HR might predict a patients9 prognosis. We conclude that heart rate fluctuations are partially driven by conscious processing, depend on attentional state, and may represent a simple metric to assess conscious state in unresponsive patients.{$<$}/p{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PZDGNWCX\\Perez et al. - 2020 - Conscious processing of narrative stimuli synchron.pdf;C\:\\Users\\u668173\\Zotero\\storage\\96LMU5TC\\2020.05.26.html},
  langid = {english}
}

@article{perichRethinkingBrainwideInteractions2020,
  title = {Rethinking Brain-Wide Interactions through Multi-Region ‘Network of Networks’ Models},
  author = {Perich, Matthew G and Rajan, Kanaka},
  date = {2020-12-01},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {65},
  pages = {146--151},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2020.11.003},
  url = {http://www.sciencedirect.com/science/article/pii/S0959438820301707},
  urldate = {2020-12-03},
  abstract = {The neural control of behavior is distributed across many functionally and anatomically distinct brain regions even in small nervous systems. While classical neuroscience models treated these regions as a set of hierarchically isolated nodes, the brain comprises a recurrently interconnected network in which each region is intimately modulated by many others. Uncovering these interactions is now possible through experimental techniques that access large neural populations from many brain regions simultaneously. Harnessing these large-scale datasets, however, requires new theoretical approaches. Here, we review recent work to understand brain-wide interactions using multi-region ‘network of networks’ models and discuss how they can guide future experiments. We also emphasize the importance of multi-region recordings, and posit that studying individual components in isolation will be insufficient to understand the neural basis of behavior.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TQ9RBUBH\\S0959438820301707.html},
  langid = {english}
}

@article{perkellSpeechMotorControl1997,
  title = {Speech Motor Control: {{Acoustic}} Goals, Saturation Effects, Auditory Feedback and Internal Models},
  shorttitle = {Speech Motor Control},
  author = {Perkell, Joseph and Matthies, Melanie and Lane, Harlan and Guenther, Frank and Wilhelms-Tricarico, Reiner and Wozniak, Jane and Guiod, Peter},
  date = {1997-08-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {22},
  pages = {227--250},
  issn = {0167-6393},
  doi = {10.1016/S0167-6393(97)00026-5},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639397000265},
  urldate = {2021-02-27},
  abstract = {A theoretical overview and supporting data are presented about the control of the segmental component of speech production. Findings of “motor-equivalent” trading relations between the contributions of two constrictions to the same acoustic transfer function provide preliminary support for the idea that segmental control is based on acoustic or auditory-perceptual goals. The goals are determined partly by non-linear, quantal relations (called “saturation effects”) between motor commands and articulatory movements and between articulation and sound. Since processing times would be too long to allow the use of auditory feedback for closed-loop error correction in achieving acoustic goals, the control mechanism must use a robust “internal model” of the relation between articulation and the sound output that is learned during speech acquisition. Studies of the speech of cochlear implant and bilateral acoustic neuroma patients provide evidence supporting two roles for auditory feedback in adults: maintenance of the internal model, and monitoring the acoustic environment to help assure intelligibility by guiding relatively rapid adjustments in “postural” parameters underlying average sound level, speaking rate and the amount of prosodically-based inflection of F0 and SPL. Zusammenfassung Dies ist ein mit Daten unterstützter theoretischer Überblick über die Kontrolle der segmentellen Sprachproduktionskomponente. Die Existenz von “motorisch equivalenten” Austauschbeziehungen zwischen zwei Vokaltraktverängungen, die beide zur einer Lautübertragungsfunktion beitragen, unterstützt in vorläufiger Weise die Idee, daβ segmentelle Kontrolle auf akustischen oder auditorisch-perzeptiven Zielen basiert. Die Ziele werden teilweise durch nichtlineare, gequantelte Beziehungen (sogenannte Sättigungseffekte) zwischen Bewegungskommandos und artikulatorischer Bewegung und zwischen Artikulation und Schall determiniert. Da die direkte auditorische Rückkopplung zur Fehlerberichtigung beim Ausführen akustischer Ziele zu lange dauern würde, muβ der Kontollmechanismus ein robustes “internes Modell”, das während der Spracherlernung erstellt wird, verwenden, welches die Beziehung zwischen Artikulation und akustischer Ausgabe beinhaltet. Sprachstudien an Patienten mit cochlearen Implantaten und bilateralem akustischem Neurom weisen auf zwei Rollen der auditiven Rückkopplung bei Erwachsenen hin: zum einen, die Erhaltung des internen Modells, und zum anderen, die Beobachtung und Berücksichtigung der akustischen Umgebung, die, um Verständlichkeit sicherzustellen, relativ schnelle Anpassungen von “Haltungsparametern” anleitet, welche die durchschnittliche Sprachlautstärke, Sprechgeschwindigkeit und prosodische bedingte Flexion von Grundfrequenz und Lautstärke bestimmen. Résumé Une analyse théorique du contrôle des composantes segmentales de la parole est proposée, qui s'appuie sur un certain nombre de résultats expérimentaux. La mise en évidence de phénomènes d'équivalence motrice entre deux constrictions, dont les variations se compensent pour préserver la même fonction de transfert du conduit vocal, est interprétée comme une première corroboration de l'hypothèse selon laquelle le contrôle segmental est orienté vers des objectifs acoustiques ou audio-perceptifs. Nous proposons que ces objectifs soient partiellement déterminés par des relations quantiques (appelées “effets de saturation”) entre les commandes motrices et les mouvements articulatoires, et entre l'articulation et le son. Considérant que les temps de traitement qui seraient nécessaires à l'exploitation en boucle fermée du feedback auditif seraient trop longs, nous proposons que la réalisation des objectifs acoustiques soit assurée via un contrôle mettant en jeu un “modèle interne” robuste, qui serait appris par le locuteur au cours de la phase d'apprentissage de la parole, et qui rendrait compte des relations entre l'articulation et le son. Des études menées sur des patients équipés d'implants cochléaires ou souffrant de neurinomes acoustiques bilatéraux ont fourni un certain nombre d'évidences en faveur d'un double rôle du feedback auditif chez l'adulte: (1) préserver et actualiser le modèle interne; (2) prendre en compte l'environnement acoustique pour contribuer à assurer l'intelligibilité de la parole, en guidant des ajustements rapides des paramètres posturaux agissant sur le niveau sonore moyen de la parole, le débit d'élocution, et les inflexions de F0 et de niveau sonore associées à des facteurs prosodiques.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DDK6FDHE\\S0167639397000265.html},
  keywords = {Acoustic goals,Auditory feedback,Cochlear implants,Intelligibility,Internal model,Motor equivalence,Quantal mechanisms,Saturation effects,Speech motor programming},
  langid = {english},
  number = {2}
}

@article{perlHumanNonolfactoryCognition2019,
  title = {Human Non-Olfactory Cognition Phase-Locked with Inhalation},
  author = {Perl, Ofer and Ravia, Aharon and Rubinson, Mica and Eisen, Ami and Soroka, Timna and Mor, Nofar and Secundo, Lavi and Sobel, Noam},
  date = {2019-05},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {3},
  pages = {501--512},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0556-z},
  abstract = {Olfactory stimulus acquisition is perfectly synchronized with inhalation, which tunes neuronal ensembles for incoming information. Because olfaction is an ancient sensory system that provided a template for brain evolution, we hypothesized that this link persisted, and therefore nasal inhalations may also tune the brain for acquisition of non-olfactory information. To test this, we measured nasal airflow and electroencephalography during various non-olfactory cognitive tasks. We observed that participants spontaneously inhale at non-olfactory cognitive task onset and that such inhalations shift brain functional network architecture. Concentrating on visuospatial perception, we observed that nasal inhalation drove increased task-related brain activity in specific task-related brain regions and resulted in improved performance accuracy in the visuospatial task. Thus, mental processes with no link to olfaction are nevertheless phase-locked with nasal inhalation, consistent with the notion of an olfaction-based template in the evolution of human brain function.},
  eprint = {31089297},
  eprinttype = {pmid},
  keywords = {Adult,Brain Waves,Cerebral Cortex,Connectome,Exhalation,Female,Humans,Inhalation,Language,Male,Nasal Cavity,Nerve Net,Pattern Recognition; Visual,Space Perception,Task Performance and Analysis,Thinking,Time Factors,Young Adult},
  langid = {english},
  number = {5}
}

@article{perlmanGorillasMayUse2017,
  title = {Gorillas May Use Their Laryngeal Air Sacs for Whinny-Type Vocalizations and Male Display},
  author = {Perlman, Marcus and Salmi, Roberta},
  date = {2017-07-01},
  journaltitle = {Journal of Language Evolution},
  shortjournal = {Journal of Language Evolution},
  volume = {2},
  pages = {126--140},
  publisher = {{Oxford Academic}},
  issn = {2058-4571},
  doi = {10.1093/jole/lzx012},
  url = {https://academic.oup.com/jole/article/2/2/126/3869489},
  urldate = {2020-09-19},
  abstract = {Abstract.  Great apes and siamangs—but not humans—possess laryngeal air sacs, suggesting that they were lost over hominin evolution. The absence of air sacs in},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QHHS89FR\\Perlman and Salmi - 2017 - Gorillas may use their laryngeal air sacs for whin.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XH9M58ZG\\3869489.html},
  langid = {english},
  number = {2}
}

@article{perlmanIconicityCanGround2015,
  title = {Iconicity Can Ground the Creation of Vocal Symbols},
  author = {Perlman, Marcus and Dale, Rick and Lupyan, Gary},
  date = {2015},
  journaltitle = {Royal Society Open Science},
  shortjournal = {Royal Society Open Science},
  volume = {2},
  pages = {150152},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.150152},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.150152},
  urldate = {2020-12-22},
  abstract = {Studies of gestural communication systems find that they originate from spontaneously created iconic gestures. Yet, we know little about how people create vocal communication systems, and many have suggested that vocalizations do not afford iconicity beyond trivial instances of onomatopoeia. It is unknown whether people can generate vocal communication systems through a process of iconic creation similar to gestural systems. Here, we examine the creation and development of a rudimentary vocal symbol system in a laboratory setting. Pairs of participants generated novel vocalizations for 18 different meanings in an iterative ‘vocal’ charades communication game. The communicators quickly converged on stable vocalizations, and naive listeners could correctly infer their meanings in subsequent playback experiments. People's ability to guess the meanings of these novel vocalizations was predicted by how close the vocalization was to an iconic ‘meaning template’ we derived from the production data. These results strongly suggest that the meaningfulness of these vocalizations derived from iconicity. Our findings illuminate a mechanism by which iconicity can ground the creation of vocal symbols, analogous to the function of iconicity in gestural communication systems.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KW7A3PQR\\Perlman et al. - Iconicity can ground the creation of vocal symbols.pdf;C\:\\Users\\u668173\\Zotero\\storage\\IFSV2Q3H\\rsos.html},
  number = {8}
}

@incollection{perrierMotorEquivalenceSpeech2015,
  title = {Motor {{Equivalence}} in {{Speech Production}}},
  booktitle = {The {{Handbook}} of {{Speech Production}}},
  author = {Perrier, Pascal and Fuchs, Susanne},
  date = {2015},
  pages = {223--247},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118584156.ch11},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118584156.ch11},
  urldate = {2019-08-08},
  abstract = {In this chapter, the concepts of “motor equivalence” and “degrees of freedom” are first described and illustrated with a few examples of motor tasks in general and of speech production tasks in particular. After this, methods used to investigate experimentally motor equivalence phenomena in speech production are presented. These are mainly paradigms that perturb the perception-action loop during on-going speech, either by limiting the degrees of freedom of the speech motor system, or by changing the physical conditions of speech production or by modifying the feedback information. Examples are provided for each of these approaches. Implications of these studies for a better understanding of speech production and its interactions with speech perception are presented in a final section of the chapter. Implications are mainly related to characterizing the mechanisms underlying interarticulatory coordination and to the analysis of speech production goals.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MVB5X94U\\Perrier and Fuchs - 2015 - Motor Equivalence in Speech Production.pdf;C\:\\Users\\u668173\\Zotero\\storage\\AA9TEB2L\\9781118584156.html},
  isbn = {978-1-118-58415-6},
  keywords = {auditory feedback perturbations,motor equivalence,motor goals,motor perturbations,speech coordination,speech motor control},
  langid = {english}
}

@article{perryEvolutionaryOriginMammalian2010,
  title = {The Evolutionary Origin of the Mammalian Diaphragm},
  author = {Perry, Steven F. and Similowski, Thomas and Klein, Wilfried and Codd, Jonathan R.},
  date = {2010-04-15},
  journaltitle = {Respiratory Physiology \& Neurobiology},
  shortjournal = {Respiratory Physiology \& Neurobiology},
  volume = {171},
  pages = {1--16},
  issn = {1569-9048},
  doi = {10.1016/j.resp.2010.01.004},
  url = {http://www.sciencedirect.com/science/article/pii/S1569904810000054},
  urldate = {2020-08-27},
  abstract = {The comparatively low compliance of the mammalian lung results in an evolutionary dilemma: the origin and evolution of this bronchoalveolar lung into a high-performance gas-exchange organ results in a high work of breathing that cannot be achieved without the coupled evolution of a muscular diaphragm. However, despite over 400 years of research into respiratory biology, the origin of this exclusively mammalian structure remains elusive. Here we examine the basic structure of the body wall muscles in vertebrates and discuss the mechanics of costal breathing and functional significance of accessory breathing muscles in non-mammalian amniotes. We then critically examine the mammalian diaphragm and compare hypotheses on its ontogenetic and phylogenetic origin. A closer look at the structure and function across various mammalian groups reveals the evolutionary significance of collateral functions of the diaphragm as a visceral organizer and its role in producing high intra-abdominal pressure.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4C2BDWYX\\S1569904810000054.html},
  keywords = {Breathing mechanics,Diaphragm,Lung,Mammal},
  langid = {english},
  number = {1}
}

@article{persegolEvidenceEntrainmentBreathing1991,
  title = {Evidence for the Entrainment of Breathing by Locomotor Pattern in Human},
  author = {Perségol, L. and Jordan, M. and Viala, D.},
  date = {1991},
  journaltitle = {Journal De Physiologie},
  shortjournal = {J. Physiol. (Paris)},
  volume = {85},
  pages = {38--43},
  issn = {0021-7948},
  abstract = {In human, it has been shown that interactions between locomotor and respiratory patterns may lead to locomotor-respiratory couplings termed entrainment. In order to prove that this coupling is really an entrainment, we tried to show that it obeys one of the expected rules, i.e. that it evolves and is not present for all imposed locomotor frequencies. For that purpose, seventeen healthy volunteers were asked to run on a treadmill at 14 different locomotor rates (instead of 2 or 3 in previous works) for 40 s. All the subjects did not exhibit the same coupling and different relationships could be obtained: the most commonly observed was 2:1 (2 locomotor activities for a respiratory one) but other forms could appear (4:1 and even 5:2 or 3:2). When the coupling evolution was followed in the same subject, it did not appear for all locomotor frequencies but only for locomotor periods close to harmonics of respiratory ones (absolute coordination). On both sides of these values, it progressively evolved to relative coordination and to the lack of coordination. When two forms of absolute coordination were observed in a same subject, the phase relationships followed the rules of the entrainment. Compared to data obtained in quadrupeds, these results suggest that the entrainment of breathing frequency by the locomotor activity is due to central interactions between the respiratory and locomotor pattern generators and does not depend on a chemical regulation avoided here by short locomotor sequences.},
  eprint = {1941642},
  eprinttype = {pmid},
  keywords = {Breathing Exercises,Exercise,Humans,Locomotion,Respiration},
  langid = {english},
  number = {1}
}

@article{petroneRelationsSubglottalPressure2017,
  title = {Relations among Subglottal Pressure, Breathing, and Acoustic Parameters of Sentence-Level Prominence in {{German}}},
  author = {Petrone, Caterina and Fuchs, Susanne and Koenig, Laura L.},
  date = {2017-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {141},
  pages = {1715--1725},
  issn = {0001-4966},
  doi = {10.1121/1.4976073},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.4976073},
  urldate = {2019-05-05},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\73SSTAQS\\Petrone et al. - 2017 - Relations among subglottal pressure, breathing, an.pdf;C\:\\Users\\u668173\\Zotero\\storage\\BCEL8HVU\\1.html},
  number = {3}
}

@article{petroneRelationsSubglottalPressure2017a,
  title = {Relations among Subglottal Pressure, Breathing, and Acoustic Parameters of Sentence-Level Prominence in {{German}}},
  author = {Petrone, Caterina and Fuchs, Susanne and Koenig, Laura L.},
  date = {2017-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {141},
  pages = {1715--1725},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.4976073},
  url = {https://asa.scitation.org/doi/full/10.1121/1.4976073},
  urldate = {2020-09-10},
  abstract = {This study investigates whether acoustic correlates of prominence are related to actions of the respiratory system resulting in local changes of subglottal pressure (Psub). Simultaneous recordings were made of acoustics; intraoral pressure (Pio), as an estimate of Psub; and thoracic and abdominal volume changes. Ten German speakers read sentences containing a verb ending with /t/ followed by a noun starting with /t/. These /t\#t/ sequences were typically realized as one /t:/ with a long intraoral pressure plateau. Sentence-level prominence was manipulated by shifting the position of contrastive focus within the sentences. The slope and peak values of Pio within the /t\#t/ sequence were used to estimate differences in Psub across focus positions. Results show that prominence production is related to changes in the slope and maximum value of the pressure plateau. While pressure increases led to higher intensity, the increases did not relate to f0, hence, suggesting that local f0 changes primarily reflect laryngeal activity. Finally, strong individual differences were observed in the respiratory data. These findings confirm past reports of local Psub increases corresponding to sentence-level prominence. Speaker-specific activations of the respiratory system are interpreted in terms of motor equivalence, with laryngeal mechanisms also appearing to contribute to Psub changes.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3NFI27P6\\Petrone et al. - 2017 - Relations among subglottal pressure, breathing, an.pdf;C\:\\Users\\u668173\\Zotero\\storage\\VD3CDRNW\\1.html},
  number = {3}
}

@article{pettersenActivityPatternsNeck2005,
  title = {The Activity Patterns of Neck Muscles in Professional Classical Singing},
  author = {Pettersen, V. and Westgaard, R. H.},
  date = {2005-06},
  journaltitle = {Journal of Voice: Official Journal of the Voice Foundation},
  volume = {19},
  pages = {238--251},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2004.02.006},
  abstract = {The study aimed to characterize the activity patterns of neck muscles during classical singing. Muscle usage during inhalation and phonation and the relationship to changes in pitch and vocal loudness was of particular interest. Five professional opera singers (2 males, 3 females) participated. Surface electromyographic activity (EMG) was recorded from the upper trapezius (TR), the sternocleidomastoideus (STM), and the scalenus (SC) muscles and the muscles in the posterior neck region (PN). EMG activity in TR and STM was lowered by EMG biofeedback (BF), and the possible effect of lowered EMG activity in these muscles on the EMG activity of SC and PN was analyzed. A strain gauge sensor recorded the chest circumference of the thorax. Three singing tasks were performed. Each task was performed three times with variation in vocal loudness and pitch. After the first performance of the singing tasks, the BF session was carried out. Thereafter muscle activity was recorded in repeat performances of the same tasks, and the EMG amplitude of all muscles was compared before and after BF. We conclude that STM and SC showed correlated activity patterns during inhalation and phonation by classical singers. Second, substantial muscle activity was observed in PN during inhalation and phonation. BF performed on TR and STM had a secondary effect of lowering EMG activity in SC and PN. The activity of all neck muscles was markedly elevated when singing in the highest pitch. There was no consistent task-based difference in EMG amplitude for the other singing tasks.},
  eprint = {15907438},
  eprinttype = {pmid},
  keywords = {Adult,Biofeedback; Psychology,Electromyography,Female,Humans,Male,Muscle; Skeletal,Neck Muscles,Occupations,Phonation,Professional Competence,Respiration,Voice,Voice Quality},
  langid = {english},
  number = {2}
}

@article{pettersenMuscleActivityProfessional2004,
  title = {Muscle Activity in Professional Classical Singing: A Study on Muscles in the Shoulder, Neck and Trunk},
  shorttitle = {Muscle Activity in Professional Classical Singing},
  author = {Pettersen, V. and Westgaard, R. H.},
  date = {2004-06-01},
  journaltitle = {Logopedics Phoniatrics Vocology},
  volume = {29},
  pages = {56--65},
  publisher = {{Taylor \& Francis}},
  issn = {1401-5439},
  doi = {10.1080/14015430410031661},
  url = {https://doi.org/10.1080/14015430410031661},
  urldate = {2020-10-29},
  abstract = {This study aimed to examine whether changes in the activity of shoulder and neck muscles have consequences for the activation of primary breathing muscles. It further aimed to compare muscle loading levels of professional and student singers. Four professional opera singers participated in the study. Previous unpublished recordings of 4 to 16 student singers and one opera singer were included to allow comparison of EMG loading levels between student and professional singers. Electromyographic (EMG) recordings of trapezius (TR), sternocleidomastoideus (STM), intercostals (INT), rectus abdominis (RC) and the lateral abdominal muscles (OBL) were performed. EMG biofeedback (BF) was performed on TR and STM to lower the activity in these two muscles and the potential change in EMG activity of INT, RC and OBL were examined. Three singing tasks were performed: aria, sustained tones and extreme tones. Each task was performed three times with variation in volume or pitch. Following the first performance of the singing tasks, the BF session was carried out and muscle activity recorded in a repeat performance of the same tasks. The EMG activity levels of all muscles were compared before and after BF. We found no significant effect of reduced TR/STM activity on the activation of INT, RC and OBL. Professional opera singers activated the TR, INT, RC and OBL muscles to higher levels than the student singers did. Another finding was large inter-subject variation in muscle usage, showing an idiosyncratic composition of the muscle contribution to subglottal pressure.},
  annotation = {\_eprint: https://doi.org/10.1080/14015430410031661},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WQW33SPA\\14015430410031661.html},
  keywords = {abdominal muscles,biofeedback (BF),electromyography (EMG),EMG loading,intercostals,rectus abdominis,sternocleidomastoideus,trapezius},
  number = {2}
}

@article{pettersenNeckShoulderMuscle2005,
  title = {Neck and Shoulder Muscle Activity and Thorax Movement in Singing and Speaking Tasks with Variation in Vocal Loudness and Pitch},
  author = {Pettersen, V. and Bjørkøy, K. and Torp, H. and Westgaard, R. H.},
  date = {2005-12},
  journaltitle = {Journal of Voice: Official Journal of the Voice Foundation},
  shortjournal = {J Voice},
  volume = {19},
  pages = {623--634},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2004.08.007},
  abstract = {The aim of this study was to examine respiratory phasing and loading levels of sternocleidomastoideus (STM), scalenus (SC), and upper trapezius (TR) muscles in vocalization tasks with variation in vocal loudness and pitch. Eight advanced singing students, aged 22 to 28 years, participated. Surface electromyographic (EMG) activity was recorded from STM, SC, and TR. Thorax movement was detected by two strain gauge sensors placed around the upper (upper TX) and lower (lower TX) thorax. A glissando and simplified singing and speaking tasks were performed. Sustained vowels /a:-i-ae-o:/ were sung in a glissando from lowest to highest pitch (mixed voice/falsetto) back to lowest pitch and in short singing sequences at comfortable, low, and high pitches. The same vowels were spoken softly and loudly for about the same length. The subjects inhaled between the vowels. It was concluded that the inspiratory phased STM and SC muscles produced a counterforce to compression of upper TX at high pitches in glissando. STM and SC were activated to higher levels during phonation than in inhalation. As breathing demands were reduced, STM and SC activity was lowered and the respiratory phasing of peak amplitude changed to inhalation. TR contributed to exhalation in demanding singing with long breathing cycles, but it was less active in singing tasks with short breathing cycles and was essentially inactive in simplified speaking tasks.},
  eprint = {16301107},
  eprinttype = {pmid},
  keywords = {Adult,Electromyography,Female,Humans,Male,Muscle; Skeletal,Music,Neck Muscles,Phonation,Respiration,Shoulder,Sound Spectrography,Thorax,Videotape Recording,Voice,Voice Quality},
  langid = {english},
  number = {4}
}

@article{pettersenPreliminaryFindingsClassical2006,
  title = {Preliminary {{Findings}} on the {{Classical Singer}}’s {{Use}} of the {{Pectoralis Major Muscle}}},
  author = {Pettersen, V.},
  date = {2006},
  journaltitle = {Folia Phoniatrica et Logopaedica},
  shortjournal = {FPL},
  volume = {58},
  pages = {427--439},
  publisher = {{Karger Publishers}},
  issn = {1021-7762, 1421-9972},
  doi = {10.1159/000095003},
  url = {https://www.karger.com/Article/FullText/95003},
  urldate = {2020-10-29},
  abstract = {\emph{Objective: }This study aims to further characterize the muscle activity that influences the posture and breathing utilized for classical singing. The activity in the pectoralis major (PC) muscle and the phasing of PC activity to sternocleidomastoideus (STM) activity and upper thorax (UTX) movement were investigated. \emph{Material and Methods: }Seven professional classical singers (3 sopranos, 1 mezzo, 1 tenor and 2 baritones) and 8 advanced classical singing students (4 sopranos, 1 mezzo, 1 tenor and 2 baritones) participated. Electromyographic activity was recorded from the PC and STM muscles on the right side. UTX movement was traced with a strain gauge sensor placed around the upper thorax. Different arias, freely chosen by the singers from their professional repertoire, served as singing tasks. All subjects performed their task 3 times with variation in vocal loudness (normal, forte, piano). \emph{Results: }It was observed that a majority of the singers activated the PC during inhalation and that {$>$}50\% of the singers activated it during parts of phonation. In general, however, the activity of the PC was relatively low during phonation. \emph{Conclusion: }This study shows that the PC, in idiosyncratic patterns, could be involved in the inspiratory effort recruited when positioning the UTX during inhalation and phonation.},
  eprint = {17108700},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BKWKNBCY\\95003.html},
  langid = {english},
  number = {6}
}

@article{pfordresherAuditoryFeedbackMusic2003,
  title = {Auditory Feedback in Music Performance: {{Evidence}} for a Dissociation of Sequencing and Timing.},
  shorttitle = {Auditory Feedback in Music Performance},
  author = {Pfordresher, Peter Q.},
  date = {2003},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {29},
  pages = {949--964},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.29.5.949},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.29.5.949},
  urldate = {2020-09-22},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9ASRAZ7Q\\Pfordresher - 2003 - Auditory feedback in music performance Evidence f.pdf},
  langid = {english},
  number = {5}
}

@article{pfordresherCoordinationPerceptionAction2006,
  title = {Coordination of Perception and Action in Music Performance},
  author = {Pfordresher, Peter Q.},
  date = {2006},
  journaltitle = {Advances in Cognitive Psychology},
  volume = {2},
  pages = {183--198},
  publisher = {{Vizja Press \& IT Ltd.}},
  location = {{Poland}},
  issn = {1895-1171(Print)},
  doi = {10.2478/v10053-008-0054-8},
  abstract = {This review summarizes recent research on the way in which music performance may rely on the perception of sounds that accompany actions (termed auditory feedback). Alterations of auditory feedback can profoundly disrupt performance, though not all alterations cause disruption and different alterations generate different types of disruption. Recent results have revealed a basic distinction between the role of feedback contents (musical pitch) and the degree to which feedback onsets are synchronized with actions. These results further suggest a theoretical framework for the coordination of actions with feedback in which perception and action share a common representation of sequence structure. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\A45SYPZD\\Pfordresher - 2006 - Coordination of perception and action in music per.pdf;C\:\\Users\\u668173\\Zotero\\storage\\GGLYSS7K\\2007-04959-006.html},
  keywords = {Auditory Stimulation,Music,Music Perception,Pitch (Frequency)},
  number = {2-3}
}

@article{phillips-silverVestibularInfluenceAuditory2008,
  title = {Vestibular Influence on Auditory Metrical Interpretation},
  author = {Phillips-Silver, Jessica and Trainor, Laurel J.},
  date = {2008-06},
  journaltitle = {Brain and Cognition},
  shortjournal = {Brain Cogn},
  volume = {67},
  pages = {94--102},
  issn = {1090-2147},
  doi = {10.1016/j.bandc.2007.11.007},
  abstract = {When we move to music we feel the beat, and this feeling can shape the sound we hear. Previous studies have shown that when people listen to a metrically ambiguous rhythm pattern, moving the body on a certain beat--adults, by actively bouncing themselves in synchrony with the experimenter, and babies, by being bounced passively in the experimenter's arms--can bias their auditory metrical representation so that they interpret the pattern in a corresponding metrical form [Phillips-Silver, J., \& Trainor, L. J. (2005). Feeling the beat: Movement influences infant rhythm perception. Science, 308, 1430; Phillips-Silver, J., \& Trainor, L. J. (2007). Hearing what the body feels: Auditory encoding of rhythmic movement. Cognition, 105, 533-546]. The present studies show that in adults, as well as in infants, metrical encoding of rhythm can be biased by passive motion. Furthermore, because movement of the head alone affected auditory encoding whereas movement of the legs alone did not, we propose that vestibular input may play a key role in the effect of movement on auditory rhythm processing. We discuss possible cortical and subcortical sites for the integration of auditory and vestibular inputs that may underlie the interaction between movement and auditory metrical rhythm perception.},
  eprint = {18234407},
  eprinttype = {pmid},
  keywords = {Adult,Auditory Perception,Humans,Movement,Music,Pattern Recognition; Physiological,Periodicity,Vestibule; Labyrinth},
  langid = {english},
  number = {1}
}

@article{pikaTakingTurnsBridging2018,
  title = {Taking Turns: Bridging the Gap between Human and Animal Communication},
  shorttitle = {Taking Turns},
  author = {Pika, Simone and Wilkinson, Ray and Kendrick, Kobin H. and Vernes, Sonja C.},
  date = {2018-06-13},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {285},
  pages = {20180598},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2018.0598},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2018.0598},
  urldate = {2020-05-18},
  abstract = {Language, humans’ most distinctive trait, still remains a ‘mystery’ for evolutionary theory. It is underpinned by a universal infrastructure—cooperative turn-taking—which has been suggested as an ancient mechanism bridging the existing gap between the articulate human species and their inarticulate primate cousins. However, we know remarkably little about turn-taking systems of non-human animals, and methodological confounds have often prevented meaningful cross-species comparisons. Thus, the extent to which cooperative turn-taking is uniquely human or represents a homologous and/or analogous trait is currently unknown. The present paper draws attention to this promising research avenue by providing an overview of the state of the art of turn-taking in four animal taxa—birds, mammals, insects and anurans. It concludes with a new comparative framework to spur more research into this research domain and to test which elements of the human turn-taking system are shared across species and taxa.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\W83M2UMT\\Pika et al. - 2018 - Taking turns bridging the gap between human and a.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XCA63SWN\\rspb.2018.html},
  number = {1880}
}

@book{pikovskySynchronizationUniversalConcept2001,
  title = {Synchronization: {{A Universal Concept}} in {{Nonlinear Sciences}}},
  author = {Pikovsky, A and Kurths, J and Rosenblum, M},
  date = {2001},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, Mass}},
  url = {https://books.google.nl/books?hl=en&lr=&id=FuIv845q3QUC&oi=fnd&pg=PP1&dq=pikovsky+synchronization&ots=RM-tIgHgU7&sig=PHOyMBVgtLIao8rdkP5bCTVKwo0#v=onepage&q=pikovsky%20synchronization&f=false},
  urldate = {2019-07-11},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8IGPB5M9\\books.html}
}

@software{pinheiroNlmeLinearNonlinear2019,
  title = {Nlme: {{Linear}} and Nonlinear Mixed Effects Models},
  author = {Pinheiro, J. and Bates, D. and DebRoy, S. and Sarkar, D. and R Team, R. C.},
  date = {2019},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\F38LDCK6\\Cayuela - Modelos lineales mixtos en R.pdf}
}

@article{pisanskiCanBlindPersons2016,
  title = {Can Blind Persons Accurately Assess Body Size from the Voice?},
  author = {Pisanski, Katarzyna and Oleszkiewicz, Anna and Sorokowska, Agnieszka},
  date = {2016-04-30},
  journaltitle = {Biology Letters},
  shortjournal = {Biology Letters},
  volume = {12},
  pages = {20160063},
  doi = {10.1098/rsbl.2016.0063},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsbl.2016.0063},
  urldate = {2019-11-30},
  abstract = {Vocal tract resonances provide reliable information about a speaker's body size that human listeners use for biosocial judgements as well as speech recognition. Although humans can accurately assess men's relative body size from the voice alone, how this ability is acquired remains unknown. In this study, we test the prediction that accurate voice-based size estimation is possible without prior audiovisual experience linking low frequencies to large bodies. Ninety-one healthy congenitally or early blind, late blind and sighted adults (aged 20–65) participated in the study. On the basis of vowel sounds alone, participants assessed the relative body sizes of male pairs of varying heights. Accuracy of voice-based body size assessments significantly exceeded chance and did not differ among participants who were sighted, or congenitally blind or who had lost their sight later in life. Accuracy increased significantly with relative differences in physical height between men, suggesting that both blind and sighted participants used reliable vocal cues to size (i.e. vocal tract resonances). Our findings demonstrate that prior visual experience is not necessary for accurate body size estimation. This capacity, integral to both nonverbal communication and speech perception, may be present at birth or may generalize from broader cross-modal correspondences.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Z6GEIHL2\\Pisanski et al. - 2016 - Can blind persons accurately assess body size from.pdf;C\:\\Users\\u668173\\Zotero\\storage\\FR5XWK37\\rsbl.2016.html},
  number = {4}
}

@article{pisanskiReturnOzVoice2014,
  title = {Return to {{Oz}}: Voice Pitch Facilitates Assessments of Men's Body Size},
  shorttitle = {Return to {{Oz}}},
  author = {Pisanski, Katarzyna and Fraccaro, Paul J. and Tigue, Cara C. and O'Connor, Jillian J. M. and Feinberg, David R.},
  date = {2014-08},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {40},
  pages = {1316--1331},
  issn = {1939-1277},
  doi = {10.1037/a0036956},
  abstract = {Listeners associate low voice pitch (fundamental frequency and/or harmonics) and formants (vocal-tract resonances) with large body size. Although formants reliably predict size within sexes, pitch does not reliably predict size in groups of same-sex adults. Voice pitch has therefore long been hypothesized to confound within-sex size assessment. Here we performed a knockout test of this hypothesis using whispered and 3-formant sine-wave speech devoid of pitch. Listeners estimated the relative size of men with above-chance accuracy from voiced, whispered, and sine-wave speech. Critically, although men's pitch and physical height were unrelated, the accuracy of listeners' size assessments increased in the presence rather than absence of pitch. Size assessments based on relatively low pitch yielded particularly high accuracy (70\%-80\%). Results of Experiment 2 revealed that amplitude, noise, and signal degradation of unvoiced speech could not explain this effect; listeners readily perceived formant shifts in manipulated whispered speech. Rather, in Experiment 3, we show that the denser harmonic spectrum provided by low pitch allowed for better resolution of formants, aiding formant-based size assessment. These findings demonstrate that pitch does not confuse body size assessment as has been previously suggested, but instead facilitates accurate size assessment by providing a carrier signal for vocal-tract resonances.},
  eprint = {24933617},
  eprinttype = {pmid},
  keywords = {Adolescent,Adult,Body Size,Female,Humans,Male,Pitch Perception,Speech Perception,Voice,Young Adult},
  langid = {english},
  number = {4}
}

@article{pisanskiVoiceModulationWindow2016,
  title = {Voice Modulation: {{A}} Window into the Origins of Human Vocal Control?},
  shorttitle = {Voice {{Modulation}}},
  author = {Pisanski, Katarzyna and Cartei, Valentina and McGettigan, Carolyn and Raine, Jordan and Reby, David},
  date = {2016-04-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {20},
  pages = {304--318},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2016.01.002},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(16)00020-6},
  urldate = {2019-10-17},
  eprint = {26857619},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CIL5832Q\\S1364-6613(16)00020-6.html},
  keywords = {formant scaling,fundamental frequency,nonverbal vocal communication,source–filter theory,speech evolution},
  langid = {english},
  number = {4}
}

@online{PitchAccentTrajectories,
  title = {Pitch {{Accent Trajectories}} across {{Different Conditions}} of {{Visibility}} and {{Information Structure}} - {{Evidence}} from {{Spontaneous Dyadic Interaction}}},
  url = {https://pub.uni-bielefeld.de/record/2936369},
  urldate = {2019-08-08},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3RXUK7T3\\2936369.html}
}

@article{pittmanAmazonMechanicalTurk2016,
  title = {Amazon’s {{Mechanical Turk}} a {{Digital Sweatshop}}? {{Transparency}} and {{Accountability}} in {{Crowdsourced Online Research}}},
  shorttitle = {Amazon’s {{Mechanical Turk}} a {{Digital Sweatshop}}?},
  author = {Pittman, Matthew and Sheehan, Kim},
  date = {2016-10-01},
  journaltitle = {Journal of Media Ethics},
  volume = {31},
  pages = {260--262},
  publisher = {{Routledge}},
  issn = {2373-6992},
  doi = {10.1080/23736992.2016.1228811},
  url = {https://doi.org/10.1080/23736992.2016.1228811},
  urldate = {2020-07-09},
  annotation = {\_eprint: https://doi.org/10.1080/23736992.2016.1228811},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IKNZX5CE\\Pittman and Sheehan - 2016 - Amazon’s Mechanical Turk a Digital Sweatshop Tran.pdf;C\:\\Users\\u668173\\Zotero\\storage\\392BN5AV\\23736992.2016.html},
  number = {4}
}

@article{platzWhenEyeListens2012,
  title = {When the {{Eye Listens}}: {{A Meta}}-Analysis of {{How Audio}}-Visual {{Presentation Enhances}} the {{Appreciation}} of {{Music Performance}}},
  shorttitle = {When the {{Eye Listens}}},
  author = {Platz, Friedrich and Kopiez, Reinhard},
  date = {2012-09-01},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  volume = {30},
  pages = {71--83},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.1525/mp.2012.30.1.71},
  url = {/mp/article/30/1/71/62562/When-the-Eye-Listens-A-Meta-analysis-of-How-Audio},
  urldate = {2020-12-07},
  abstract = {the visual component of music performance as experienced in a live concert is of central importance for the appreciation of music performance. However, up until now the influence of the visual component on the evaluation of music performance has remained unquantified in terms of effect size estimations. Based on a meta-analysis of 15 aggregated studies on audio-visual music perception (total N = 1,298), we calculated the average effect size of the visual component in music performance appreciation by subtracting ratings for the audio-only condition from those for the audio-visual condition. The outcome focus was on evaluation ratings such as liking, expressiveness, or overall quality of musical performances. For the first time, this study reveals an average medium effect size of 0.51 standard deviations — Cohen's d; 95\% CI (0.42, 0.59) — for the visual component. Consequences for models of intermodal music perception and experimental planning are addressed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8DXV5V5W\\When-the-Eye-Listens-A-Meta-analysis-of-How-Audio.html},
  langid = {english},
  number = {1}
}

@article{poeppelSpeechRhythmsTheir2020,
  title = {Speech Rhythms and Their Neural Foundations},
  author = {Poeppel, David and Assaneo, M. Florencia},
  date = {2020-05-06},
  journaltitle = {Nature Reviews Neuroscience},
  pages = {1--13},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-020-0304-4},
  url = {https://www.nature.com/articles/s41583-020-0304-4},
  urldate = {2020-05-08},
  abstract = {Syllables play a central role in speech production and perception. In this Review, Poeppel and Assaneo outline how a simple biophysical model of the speech production system as an oscillator explains the remarkably stable rhythmic structure of spoken language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I7MIA2RQ\\Poeppel and Assaneo - 2020 - Speech rhythms and their neural foundations.pdf},
  langid = {english}
}

@article{poeppelSpeechRhythmsTheir2020a,
  title = {Speech Rhythms and Their Neural Foundations},
  author = {Poeppel, David and Assaneo, M. Florencia},
  date = {2020-05-06},
  journaltitle = {Nature Reviews Neuroscience},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0304-4},
  url = {http://www.nature.com/articles/s41583-020-0304-4},
  urldate = {2020-05-08},
  abstract = {The recognition of spoken language has typically been studied by focusing on either words or their constituent elements (for example, low-level features or phonemes). More recently{$\mkern1mu$}, the ‘temporal mesoscale’ of speech has been explored, specifically regularities in the envelope of the acoustic signal that correlate with syllabic information and that play a central role in production and perception processes. The temporal structure of speech at this scale is remarkably stable across languages, with a preferred range of rhythmicity of 2– 8\,Hz. Importantly, this rhythmicity is required by the processes underlying the construction of intelligible speech. A lot of current work focuses on audio-m otor interactions in speech, highlighting behavioural and neural evidence that demonstrates how properties of perceptual and motor systems, and their relation, can underlie the mesoscale speech rhythms. The data invite the hypothesis that the speech motor cortex is best modelled as a neural oscillator, a conjecture that aligns well with current proposals highlighting the fundamental role of neural oscillations in perception and cognition. The findings also show motor theories (of speech) in a different light, placing new mechanistic constraints on accounts of the action–perception interface.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VKT6RFNX\\Poeppel and Assaneo - 2020 - Speech rhythms and their neural foundations.pdf},
  langid = {english}
}

@article{poggiConductorIntensityGestures2020,
  title = {The Conductor’s Intensity Gestures},
  author = {Poggi, Isabella and D’Errico, Francesca and Ansani, Alessandro},
  date = {2020-10-21},
  journaltitle = {Psychology of Music},
  shortjournal = {Psychology of Music},
  pages = {0305735620963179},
  publisher = {{SAGE Publications Ltd}},
  issn = {0305-7356},
  doi = {10.1177/0305735620963179},
  url = {https://doi.org/10.1177/0305735620963179},
  urldate = {2020-10-27},
  abstract = {This work is aimed at outlining a repertoire of conductors’ gestures. In this perspective, it presents two studies that investigate a specific subset of the body signals of orchestra and choir conductors, namely, the gestures for musical intensity. First, an observational qualitative study, based on a systematic coding of a corpus of fragments from orchestra concerts and rehearsals, singled out 21 gestures, in which either the gesture as a whole or some aspects of it conveyed indications for forte, piano, crescendo, or diminuendo; some are symbolic gestures, used either with the same meaning as in everyday interaction or with one specific of conductors; others are iconic gestures, both directly or indirectly iconic. Second, in a perception study, a questionnaire submitted to 77 participants tested if 8 gestures of intensity out of the 21 singled out by the coding study are in fact shared and understood, and whether they are better interpreted by music experts than by laypeople. Results showed that the tested gestures are fairly comprehensible, not only by experts but also by non-expert participants, probably due, for some gestures, to their high level of iconicity, and for others to their closeness to everyday gestures.},
  langid = {english}
}

@article{polletHowDiverseAre2019,
  title = {How {{Diverse Are}} the {{Samples Used}} in the {{Journals}} ‘{{Evolution}} \& {{Human Behavior}}’ and ‘{{Evolutionary Psychology}}’?},
  author = {Pollet, Thomas V. and Saxton, Tamsin K.},
  date = {2019-09-01},
  journaltitle = {Evolutionary Psychological Science},
  shortjournal = {Evolutionary Psychological Science},
  volume = {5},
  pages = {357--368},
  issn = {2198-9885},
  doi = {10.1007/s40806-019-00192-2},
  url = {https://doi.org/10.1007/s40806-019-00192-2},
  urldate = {2020-07-22},
  abstract = {Psychologists regularly draw inferences about populations based on data from small samples of people, and so have long been interested in how well those samples generalise to wider populations. There is a consensus that psychology probably relies too much on samples from Western, Educated, Industrialised, Rich and Democratic (WEIRD) societies and among those from university students. Online surveys might be used to increase sample diversity, although online sampling still reaches only a restricted range of participants. Studies from evolutionary psychology often seek to uncover aspects of evolved universal characteristics, and so might demonstrate a particular interest in the use of diverse samples. Here, we empirically examine the samples used in the 2015–2016 volumes of ‘Evolution \& Human Behavior’ (104 articles) and ‘Evolutionary Psychology’ (76 articles). Our database consists of 311 samples of humans (median sample size\,=\,186). The majority of samples were either online or student samples (70\% of samples), followed by other adult Western samples (19\%). Two hundred fifty-three (81\%) of the samples were classified as ‘Western’ (Europe/North America/Australia). The remaining samples were predominantly from Asia (N\,=\,37; 12\%, mostly Japan). Only a small fraction of the samples were taken from Latin American and Caribbean (N\,=\,8) or African (N\,=\,6) countries. The median sample size did not differ significantly between continents, but online samples (both paid and unpaid) were typically larger than samples sourced offline. It seems that the samples used are more diverse than those that have been reported in reviews of the literature from social and developmental psychology, perhaps because evolutionary psychology has a greater inherent need to test hypotheses about an evolved and universal human nature. However, it is also apparent that the majority of samples within contemporary evolutionary psychology research remain WEIRD.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TYCNQYE8\\Pollet and Saxton - 2019 - How Diverse Are the Samples Used in the Journals ‘.pdf},
  langid = {english},
  number = {3}
}

@article{pollickApeGesturesLanguage2007,
  title = {Ape Gestures and Language Evolution},
  author = {Pollick, Amy S. and de Waal, Frans B. M.},
  date = {2007-05-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {104},
  pages = {8184--8189},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0702624104},
  url = {https://www.pnas.org/content/104/19/8184},
  urldate = {2020-12-07},
  abstract = {The natural communication of apes may hold clues about language origins, especially because apes frequently gesture with limbs and hands, a mode of communication thought to have been the starting point of human language evolution. The present study aimed to contrast brachiomanual gestures with orofacial movements and vocalizations in the natural communication of our closest primate relatives, bonobos (Pan paniscus) and chimpanzees (Pan troglodytes). We tested whether gesture is the more flexible form of communication by measuring the strength of association between signals and specific behavioral contexts, comparing groups of both the same and different ape species. Subjects were two captive bonobo groups, a total of 13 individuals, and two captive chimpanzee groups, a total of 34 individuals. The study distinguished 31 manual gestures and 18 facial/vocal signals. It was found that homologous facial/vocal displays were used very similarly by both ape species, yet the same did not apply to gestures. Both within and between species gesture usage varied enormously. Moreover, bonobos showed greater flexibility in this regard than chimpanzees and were also the only species in which multimodal communication (i.e., combinations of gestures and facial/vocal signals) added to behavioral impact on the recipient.},
  eprint = {17470779},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HE382FCZ\\Pollick and Waal - 2007 - Ape gestures and language evolution.pdf;C\:\\Users\\u668173\\Zotero\\storage\\TXPX4INF\\8184.html},
  keywords = {bonobo,chimpanzee,communication,multimodal},
  langid = {english},
  number = {19}
}

@article{pontzerControlFunctionArm2009,
  title = {Control and Function of Arm Swing in Human Walking and Running},
  author = {Pontzer, Herman and Holloway, John H. and Raichlen, David A. and Lieberman, Daniel E.},
  date = {2009-02-15},
  journaltitle = {Journal of Experimental Biology},
  volume = {212},
  pages = {523--534},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.024927},
  url = {https://jeb.biologists.org/content/212/4/523},
  urldate = {2020-09-19},
  abstract = {Skip to Next Section We investigated the control and function of arm swing in human walking and running to test the hypothesis that the arms act as passive mass dampers powered by movement of the lower body, rather than being actively driven by the shoulder muscles. We measured locomotor cost, deltoid muscle activity and kinematics in 10 healthy adult subjects while walking and running on a treadmill in three experimental conditions: control; no arms (arms folded across the chest); and arm weights (weights worn at the elbow). Decreasing and increasing the moment of inertia of the upper body in no arms and arm weights conditions, respectively, had corresponding effects on head yaw and on the phase differences between shoulder and pelvis rotation, consistent with the view of arms as mass dampers. Angular acceleration of the shoulders and arm increased with torsion of the trunk and shoulder, respectively, but angular acceleration of the shoulders was not inversely related to angular acceleration of the pelvis or arm. Restricting arm swing in no arms trials had no effect on locomotor cost. Anterior and posterior portions of the deltoid contracted simultaneously rather than firing alternately to drive the arm. These results support a passive arm swing hypothesis for upper body movement during human walking and running, in which the trunk and shoulders act primarily as elastic linkages between the pelvis, shoulder girdle and arms, the arms act as passive mass dampers which reduce torso and head rotation, and upper body movement is primarily powered by lower body movement.},
  eprint = {19181900},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TA36RRRT\\Pontzer et al. - 2009 - Control and function of arm swing in human walking.pdf;C\:\\Users\\u668173\\Zotero\\storage\\69JLZ3WT\\523.html},
  keywords = {arm swing,passive dynamics,running,tuned mass dampers,walking},
  langid = {english},
  number = {4}
}

@article{portLanguageSocialInstitution2010,
  title = {Language as a {{Social Institution}}: {{Why Phonemes}} and {{Words Do Not Live}} in the {{Brain}}},
  shorttitle = {Language as a {{Social Institution}}},
  author = {Port, Robert F.},
  date = {2010-10-29},
  journaltitle = {Ecological Psychology},
  volume = {22},
  pages = {304--326},
  issn = {1040-7413},
  doi = {10.1080/10407413.2010.517122},
  url = {https://doi.org/10.1080/10407413.2010.517122},
  urldate = {2020-02-25},
  abstract = {It is proposed that a language, in a rich, high-dimensional form, is part of the cultural environment of the child learner. A language is the product of a community of speakers who develop its phonological, lexical, and phrasal patterns over many generations. The language emerges from the joint behavior of many agents in the community acting as a complex adaptive system. Its form only roughly approximates the low-dimensional structures that our traditional phonology highlights. Those who study spoken language have attempted to approach it as an internal knowledge structure rather than as a communal institution or set of conventions for coordination of activity. We also find it very difficult to avoid being deceived into seeing language in the form employed by our writing system as letters, words, and sentences. But our writing system is a further set of conventions that approximate the high-dimensional spoken language in a consistent and regularized graphical form.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6R3DRNQ9\\Port - 2010 - Language as a Social Institution Why Phonemes and.pdf;C\:\\Users\\u668173\\Zotero\\storage\\AZJU3YBG\\10407413.2010.html},
  number = {4}
}

@article{pouwAcousticInformationUpper2020,
  title = {Acoustic Information about Upper Limb Movement in Voicing},
  author = {Pouw, W. and Paxton, A. and Harrison, S. J. and Dixon, J. A.},
  date = {2020-05-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {117},
  pages = {11364--11367},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2004163117},
  url = {https://www.pnas.org/content/early/2020/05/05/2004163117},
  urldate = {2020-05-26},
  abstract = {We show that the human voice has complex acoustic qualities that are directly coupled to peripheral musculoskeletal tensioning of the body, such as subtle wrist movements. In this study, human vocalizers produced a steady-state vocalization while rhythmically moving the wrist or the arm at different tempos. Although listeners could only hear and not see the vocalizer, they were able to completely synchronize their own rhythmic wrist or arm movement with the movement of the vocalizer which they perceived in the voice acoustics. This study corroborates recent evidence suggesting that the human voice is constrained by bodily tensioning affecting the respiratory–vocal system. The current results show that the human voice contains a bodily imprint that is directly informative for the interpersonal perception of another’s dynamic physical states.},
  eprint = {32393618},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FXE7ICIZ\\Pouw et al. - 2020 - Acoustic information about upper limb movement in .pdf;C\:\\Users\\u668173\\Zotero\\storage\\B3ILSP8W\\2004163117.html},
  keywords = {hand gesture,interpersonal synchrony,motion tracking,vocalization acoustics},
  langid = {english},
  number = {12}
}

@inproceedings{pouwAcousticSpecificationUpper2019,
  title = {Acoustic Specification of Upper Limb Movement in Voicing},
  booktitle = {Proceedings of the 6th Meeting of {{Gesture}} and {{Speech}} in {{Interaction}}},
  author = {Pouw, W. and Paxton, A. and Harrison, S. J. and Dixon, J. A.},
  date = {2019},
  pages = {75--80},
  publisher = {{Universitaetsbibliothek Paderborn}},
  location = {{Paderborn, Germany}},
  doi = {10.17619/UNIPB/1-812},
  url = {https://psyarxiv.com/5rcdu/},
  urldate = {2019-05-05},
  abstract = {Hand gestures communicate complex information to listeners through the visual information created by movement. In a recent study, however, we found that there are also direct biomechanical effects of high-impetus upper limb movement on voice acoustics. Here we explored whether listeners could detect information about movement in voice acoustics of another person. In this exploratory study, participants listened to a recording of a vocalizer who was simultaneously producing low- (wrist movement) or high- (arm movement) impetus movements at three different tempos. Listeners were asked to synchronize their own movement (wrist or arm movement) with that of the vocalizer. Listeners coupled with the frequency of the vocalizer arm (but not wrist) movements, and showed phase-coupling with vocalizer arm (but not wrist) movements. However, we found that this synchronization occurred regardless of whether the listener was moving their wrist or arm. This study shows that, in principle, there is acoustic specification of arm movements in voicing, but not wrist movements. These results, if replicated, provide novel insight into the possible interpersonal functions of gesture acoustics, which may lie in communicating bodily states. The second part of the paper is a pre-registration for the confirmatory study that will assess the research question in a larger sample with more diverse and naturalistic stimuli.},
  eventtitle = {{{GESPIN}} 6},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\B75VYI5G\\Pouw et al. - 2019 - Acoustic specification of upper limb movement in v.pdf;C\:\\Users\\u668173\\Zotero\\storage\\BUY5LQSE\\5rcdu.html}
}

@article{pouwAreGestureSpeech2017,
  title = {Are Gesture and Speech Mismatches Produced by an Integrated Gesture-Speech System? {{A}} More Dynamically Embodied Perspective Is Needed for Understanding Gesture-Related Learning},
  shorttitle = {Are Gesture and Speech Mismatches Produced by an Integrated Gesture-Speech System?},
  author = {Pouw, W. and van Gog, T. and Zwaan, R. A. and Paas, F.},
  date = {2017},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {40},
  publisher = {{Cambridge University Press.}},
  issn = {0140-525X},
  doi = {10.1017/S0140525X15003039},
  url = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3176700},
  urldate = {2020-09-10},
  abstract = {Author: Pouw, Wim et al.; Genre: Journal Article; Published online: 2017; Open Access; Title: Are gesture and speech mismatches produced by an integrated{$<$}br/{$>$}gesture-speech system? A more dynamically embodied perspective is needed for understanding gesture-related learning},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZCGKMJ9L\\Pouw et al. - 2017 - Are gesture and speech mismatches produced by an i.pdf;C\:\\Users\\u668173\\Zotero\\storage\\7TTQH94T\\ViewItemOverviewPage.html},
  langid = {english},
  options = {useprefix=true}
}

@article{pouwEnergyFlowsGesturespeech2019,
  title = {Energy Flows in Gesture-Speech Physics: {{Exploratory}} Findings and Pre-Registration of Confirmatory Analysis},
  shorttitle = {Energy Flows in Gesture-Speech Physics},
  author = {Pouw, W. and Harrison, S. J. and Esteve-Gibert, N. and Dixon, J. A.},
  date = {2019},
  doi = {10.31234/osf.io/c7456},
  url = {https://psyarxiv.com/c7456/},
  urldate = {2019-10-13},
  abstract = {A well-known phenomenon of multimodal language is the synchronous coupling of prosodic contours in speech with salient kinematic changes in co-speech hand-gesture motions. Invariably, such coupling has been rendered by psychologists to require a dedicated neural-cognitive mechanism preplanning speech and gesture trajectories. Recently, in a continuous vocalization task, it was found that acoustic peaks unintentionally appear in vocalizations when gesture motions reach peaks in physical impetus, suggesting a biomechanical basis for gesture-speech synchrony (Pouw, Harrison, \& Dixon, 2019). However, from this rudimentary study it is still difficult to draw strong conclusions about gesture-speech dynamics in (more) complex speech and the precise biomechanical nature of these effects. Here we assess how the timing of physical impetus of a gesture relates to its effect on acoustic parameters of mono-syllabic consonant-vowel (CV) vocalization(/pa/). Furthermore, we assess how chest-wall kinematics is affected by gesturing, and whether this modulates the effect of gestures on acoustics. In the current exploratory analysis, we analyze a subset (N = 4) of an already collected dataset (N = 36), which serves as the basis for a pre-registration of the confirmatory analyses yet to be completed. Here we provide exploratory evidence that gestures affect acoustics (amplitude envelope and F0) as well as chest-wall kinematics during mono-syllabic vocalizations. These effects are more extreme when a gesture’s peak impetus occurs closer to the center of the vowel vocalization event. If the current findings can be replicated in confirmatory fashion, there is a more compelling case to be made that gesture-speech physics is important facet of multimodal synchrony.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\J4JHP6YV\\Pouw et al. - 2019 - Energy flows in gesture-speech physics Explorator.pdf;C\:\\Users\\u668173\\Zotero\\storage\\RTTD5PQQ\\c7456.html}
}

@article{pouwEnergyFlowsGesturespeech2020,
  title = {Energy Flows in Gesture-Speech Physics: {{The}} Respiratory-Vocal System and Its Coupling with Hand Gestures},
  shorttitle = {Energy Flows in Gesture-Speech Physics},
  author = {Pouw, W. and Harrison, S. J. and Esteve-Gibert, N. and Dixon, J. A.},
  date = {2020-09-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {148},
  pages = {1231--1247},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/10.0001730},
  url = {https://asa.scitation.org/doi/full/10.1121/10.0001730},
  urldate = {2020-09-10},
  abstract = {Expressive moments in communicative hand gestures often align with emphatic stress in speech. It has recently been found that acoustic markers of emphatic stress arise naturally during steady-state phonation when upper-limb movements impart physical impulses on the body, most likely affecting acoustics via respiratory activity. In this confirmatory study, participants (N\,=\,29) repeatedly uttered consonant-vowel (/pa/) mono-syllables while moving in particular phase relations with speech, or not moving the upper limbs. This study shows that respiration-related activity is affected by (especially high-impulse) gesturing when vocalizations occur near peaks in physical impulse. This study further shows that gesture-induced moments of bodily impulses increase the amplitude envelope of speech, while not similarly affecting the Fundamental Frequency (F0). Finally, tight relations between respiration-related activity and vocalization were observed, even in the absence of movement, but even more so when upper-limb movement is present. The current findings expand a developing line of research showing that speech is modulated by functional biomechanical linkages between hand gestures and the respiratory system. This identification of gesture-speech biomechanics promises to provide an alternative phylogenetic, ontogenetic, and mechanistic explanatory route of why communicative upper limb movements co-occur with speech in humans.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SIMT5S67\\Pouw et al. - 2020 - Energy flows in gesture-speech physics The respir.pdf;C\:\\Users\\u668173\\Zotero\\storage\\YRABRDLX\\10.html},
  number = {3}
}

@article{pouwEntrainmentModulationGesture2019,
  title = {Entrainment and Modulation of Gesture–Speech Synchrony under Delayed Auditory Feedback},
  author = {Pouw, W. and Dixon, J. A.},
  date = {2019},
  journaltitle = {Cognitive Science},
  volume = {43},
  pages = {e12721},
  issn = {1551-6709},
  doi = {10.1111/cogs.12721},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12721},
  urldate = {2019-04-16},
  abstract = {Gesture–speech synchrony re-stabilizes when hand movement or speech is disrupted by a delayed feedback manipulation, suggesting strong bidirectional coupling between gesture and speech. Yet it has also been argued from case studies in perceptual–motor pathology that hand gestures are a special kind of action that does not require closed-loop re-afferent feedback to maintain synchrony with speech. In the current pre-registered within-subject study, we used motion tracking to conceptually replicate McNeill's () classic study on gesture–speech synchrony under normal and 150 ms delayed auditory feedback of speech conditions (NO DAF vs. DAF). Consistent with, and extending McNeill's original results, we obtain evidence that (a) gesture-speech synchrony is more stable under DAF versus NO DAF (i.e., increased coupling effect), (b) that gesture and speech variably entrain to the external auditory delay as indicated by a consistent shift in gesture-speech synchrony offsets (i.e., entrainment effect), and (c) that the coupling effect and the entrainment effect are co-dependent. We suggest, therefore, that gesture–speech synchrony provides a way for the cognitive system to stabilize rhythmic activity under interfering conditions.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5VSDHX34\\Pouw and Dixon - 2019 - Entrainment and Modulation of Gesture–Speech Synch.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8B7CACLJ\\cogs.html},
  keywords = {Cross-wavelet analysis,Delayed auditory feedback,Hand-gesture,Speech,Synchrony},
  langid = {english},
  number = {3}
}

@article{pouwGestureNetworksIntroducing2019,
  title = {Gesture Networks: {{Introducing}} Dynamic Time Warping and Network Analysis for the Kinematic Study of Gesture Ensembles},
  author = {Pouw, W. and Dixon, J. A.},
  date = {2019},
  journaltitle = {Discourse Processes},
  volume = {57},
  pages = {301--319},
  doi = {10.1080/0163853X.2019.1678967},
  url = {https://psyarxiv.com/hbnt2},
  urldate = {2019-08-07},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JYA5DTB6\\hbnt2.html},
  number = {4}
}

@article{pouwGesturespeechPhysicsBiomechanical2019,
  title = {Gesture-Speech Physics: {{The}} Biomechanical Basis of the Emergence of Gesture-Speech Synchrony},
  author = {Pouw, W. and Harrison, S. J. and Dixon, J. A.},
  date = {2019},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {149},
  pages = {391--404},
  doi = {10.1037/xge0000646},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\455PZW9L\\default.html},
  number = {2}
}

@article{pouwGesturespeechPhysicsFluent2020,
  title = {Gesture-Speech Physics in Fluent Speech and Rhythmic Upper Limb Movements},
  author = {Pouw, W. and de Jonge-Hoekstra, L. and Harrison, S. J. and Paxton, A. and Dixon, J. A.},
  date = {2020-06-11T06:28:45},
  journaltitle = {Annals of the New York Academy of Sciences},
  publisher = {{PsyArXiv}},
  doi = {10.1111/nyas.14532},
  url = {https://psyarxiv.com/359te/},
  urldate = {2020-06-25},
  abstract = {Communicative hand gestures are often coordinated with prosodic aspects of speech, and salient moments of gestural movement (e.g., quick changes in speed) often co-occur with salient moments in speech (e.g., near peaks in fundamental frequency and intensity). A common understanding is that such gesture and speech coordination is culturally and cognitively acquired, rather than having a biological basis. Recently, however, the biomechanical physical coupling of arm movements to speech movements has been identified as a potentially important factor in understanding the emergence of gesture-speech coordination. Specifically, in the case of steady-state vocalization and mono-syllable utterances, forces produced during gesturing are transferred onto the tensioned body, leading to changes in respiratory-related activity and thereby affecting vocalization F0 and intensity. In the current experiment (N = 37), we extend this previous line of work to show that gesture-speech physics impacts fluent speech, too. Compared with non-movement, participants who are producing fluent self-formulated speech, while rhythmically moving their limbs, demonstrate heightened F0 and amplitude envelope, and such effects are more pronounced for higher-impulse arm versus lower-impulse wrist movement. We replicate that acoustic peaks arise especially during moments of peak-impulse (i.e., the beat) of the movement, namely around deceleration phases of the movement. Finally, higher deceleration rates of higher-mass arm movements were related to higher peaks in acoustics. These results confirm a role for physical-impulses of gesture affecting the speech system. We discuss the implications of  gesture-speech physics for understanding of the emergence of communicative gesture, both ontogenetically and phylogenetically.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\J2JR8U9U\\Pouw et al. - 2020 - Gesture-speech physics in fluent speech and rhythm.pdf;C\:\\Users\\u668173\\Zotero\\storage\\M8LBPMC5\\359te.html},
  options = {useprefix=true}
}

@report{pouwGesturespeechSynchronyCase2020,
  title = {Gesture-Speech Synchrony in the Case of Deafferentation: {{The}} Role of Visual Control and Biomechanics},
  shorttitle = {Gesture-Speech Synchrony in the Case of Deafferentation},
  author = {Pouw, Wim and Harrison, Steven J. and Dixon, James A.},
  date = {2020-11-16T09:10:41},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/7g3es},
  url = {https://psyarxiv.com/7g3es/},
  urldate = {2020-11-17},
  abstract = {In fluent speech, moments of acoustic prominence are tightly coordinated with peaks in the movement profile of hand gestures (e.g., speed of a gesture). This gesture-speech coordination has been found to operate on continuous bidirectional feedback of upper-limb movement. Here, we investigated the gesture-speech coordination of a person with deafferentation, the well-studied case of IW. Although IW has lost both his primary source of information about body position (i.e., proprioception) and touch, his gesture-speech coordination has been reported to be largely unaffected temporally and semantically, even if his vision is blocked. This is surprising as, without vision, his object-directed actions (e.g., grasping a cup) almost completely break down. Given differences in control in IW of gestures vs. actions when vision is unavailable, it has been suggested that communicative gesture operates under separate neural-cognitive constraints. In the current kinematic-acoustic study, we reanalyzed the classic 1998 and 2002 gesture experiments with IW (McNeill, 2005). Extending previous findings, we show that the micro-scale gesture-speech synchrony is compromised when vision is blocked, despite macro-scale coherence. Finally, we show a biomechanical linkage which could explain why IW’s gesture-speech capabilities in the absence of visual information is only mildly compromised.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8BZ5QGQV\\Pouw et al. - 2020 - Gesture-speech synchrony in the case of deafferent.pdf},
  keywords = {Behavioral Neuroscience,Clinical Neuroscience,Cognitive Psychology,Neuroscience,Physiology,Social and Behavioral Sciences}
}

@software{pouwMaterialsTutorialGespin20192019,
  title = {Materials {{Tutorial Gespin2019}} - {{Using}} Video-Based Motion Tracking to Quantify Speech-Gesture Synchrony},
  author = {Pouw, W. and Trujillo, J. P.},
  date = {2019},
  url = {10.17605/OSF.IO/RXB8J}
}

@article{pouwMoreEmbeddedExtended2014,
  title = {Toward a More Embedded/Extended Perspective on the Cognitive Function of Gestures},
  author = {Pouw, W. and de Nooijer, Jacqueline A. and van Gog, Tamara and Zwaan, Rolf A. and Paas, Fred},
  date = {2014},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00359},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00359/full},
  urldate = {2020-07-06},
  abstract = {Gestures are often considered to be demonstrative of the embodied nature of the mind (Hostetter \& Alibali, 2008). In this article we review current theories and research targeted at the intra-cognitive role of gestures. We ask the question how can gestures support internal cognitive processes of the gesturer? We suggest that extant theories are in a sense disembodied, because they focus solely on embodiment in terms of the sensorimotor neural precursors of gestures. As a result, current theories on the intra-cognitive role of gestures are lacking in explanatory scope to address how gestures-as-bodily-acts fulfill a cognitive function. On the basis of recent theoretical appeals that focus on the possibly embedded/extended cognitive role of gestures (Clark, 2013), we suggest that gestures are external physical tools of the cognitive system that replace and support otherwise solely internal cognitive processes. That is gestures provide the cognitive system with a stable external physical and visual presence that can provide a platform to think on. We show that there is a considerable amount of overlap between the way the human cognitive system has been found to use its environment, and how gestures are used during cognitive processes. Lastly, we provide several suggestions of how to investigate the embedded/extended perspective of the cognitive function of gestures.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YVH7DQUE\\Pouw et al. - 2014 - Toward a more embeddedextended perspective on the.pdf},
  keywords = {Cognition,embedded cognition,Embodied Cognition,extended cognition,Gestures},
  langid = {english},
  options = {useprefix=true}
}

@report{pouwMultiscaleKinematicAnalysis2020,
  title = {Multiscale Kinematic Analysis Reveals Structural Properties of Change in Evolving Manual Languages in the Lab},
  author = {Pouw, Wim and Dingemanse, Mark and Motamedi, Yasamin and Ozyurek, Asli},
  date = {2020-06-08T14:41:03},
  institution = {{OSF Preprints}},
  doi = {10.31219/osf.io/heu24},
  url = {https://osf.io/heu24/},
  urldate = {2020-12-20},
  abstract = {Reverse engineering how language emerged is a daunting interdisciplinary project. Experimental cognitive science has contributed to this effort by eliciting in the lab constraints likely playing a role for language emergence; constraints such as iterated transmission of communicative tokens between agents. Since such constraints played out over long phylogenetic time and involved vast populations, a crucial challenge for iterated language learning paradigms is to extend its limits. In the current approach we perform a multiscale quantification of kinematic changes of an evolving silent gesture system. Silent gestures consist of complex multi-articulatory movement that have so far proven elusive to quantify in a structural and reproducable way, and is primarily studied through human coders meticulously interpreting the referential content of gestures. Here we reanalyzed video data from a silent gesture iterated learning experiment (Motamedi et al. 2019), which originally showed increases in systematicity of gestural form over language transmissions. We applied a signal-based approach, first utilizing computer vision techniques to quantify kinematics from videodata. Then we performed a multiscale kinematic analysis showing that over generations of language users, silent gestures became more efficient and less complex in their kinematics. We further detect systematicity of the communicative tokens’s interrelations which proved itself as a proxy of systematicity obtained via human observation data. Thus we demonstrate the potential for a signal-based approach of language evolution in complex multi-articulatory  gestures.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MH48PALE\\Pouw et al. - 2020 - Multiscale kinematic analysis reveals structural p.pdf},
  keywords = {Cognitive Psychology,Linguistics,Psycholinguistics and Neurolinguistics,Psychology,Social and Behavioral Sciences}
}

@article{pouwPhysicalBasisGesturespeech2018,
  title = {The Physical Basis of Gesture-Speech Synchrony: {{Exploratory}} Study and Pre-Registration},
  author = {Pouw, W. and Harrison, S. and Dixon, J.},
  date = {2018},
  journaltitle = {PsyArXiv},
  url = {https://psyarxiv.com/9fzsv},
  urldate = {2019-08-09},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QQU48XH5\\9fzsv.html}
}

@article{pouwQuantificationGesturespeechSynchrony2019,
  title = {The Quantification of Gesture-Speech Synchrony: {{A}} Tutorial and Validation of Multi-Modal Data Acquisition Using Device-Based and Video-Based Motion Tracking},
  author = {Pouw, W. and Trujillo, J. and Dixon, J.A.},
  date = {2019},
  journaltitle = {Behavior Research Methods},
  doi = {10.3758/s13428-019-01271-9},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\TAEEFQA8\\jm3hk.html}
}

@inproceedings{pouwQuantifyingGesturespeechSynchrony2019,
  title = {Quantifying Gesture-Speech Synchrony},
  booktitle = {Proceedings of the 6th Meeting of {{Gesture}} and {{Speech}} in {{Interaction}}},
  author = {Pouw, W. and Dixon, J. A.},
  date = {2019},
  pages = {68--74},
  publisher = {{Universitaetsbibliothek Paderborn}},
  location = {{Paderborn}},
  doi = {10.17619/UNIPB/1-812},
  eventtitle = {{{GESPIN}} 6}
}

@article{pouwRhythmicInteractionWhen2020,
  title = {Rhythmic Interaction When Communication Goes Multi-Modal},
  author = {Pouw, Wim and Proksch, Shannon and Drijvers, Linda and Gamba, Marco and Holler, Judith and Kello, Chris and Schaefer, Rebecca and Wiggins, Geraint},
  date = {2020-12-18T16:09:09},
  publisher = {{OSF Preprints}},
  doi = {10.31219/osf.io/psmhn},
  url = {https://osf.io/psmhn/},
  urldate = {2021-03-03},
  abstract = {It is now widely known that much animal communication is conducted over several modalities, e.g., acoustic and visual, either simultaneously or sequentially. Despite this awareness, students of synchrony and rhythm interaction in animal communication have traditionally focused on a single modality at a time in their analyses. This paper reviews our current knowledge of non-human and human rhythm interaction in multi-modal communication at several levels of analysis. We begin by examining how multimodality can be defined as a scale-free phenomenon. Then we review which communicative rhythms are sustained by unique characteristics of processes at several levels. We consider communicative rhythms that are sustained by neural-cognitive, peripheral bodily, and social interactive constraints. Potential adaptiveness of multi-modal interaction, both within and between individuals, can be inferred.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YWB7MWIK\\Pouw et al. - 2020 - Rhythmic interaction when communication goes multi.pdf},
  keywords = {Biology,Cross-species,Life Sciences,Multimodal Communication,Multimodal Signaling,Physical Sciences and Mathematics,Psychology,Rhythm,Social and Behavioral Sciences}
}

@article{pouwSemanticallyRelatedGestures2021,
  title = {Semantically Related Gestures Move Alike: {{Towards}} a Distributional Semantics of Gesture Kinematics},
  shorttitle = {Semantically Related Gestures Move Alike},
  author = {Pouw, W. and Wit, J. and Bögels, S. and Rasenberg, M. and Milivojevic, B. and Ozyurek, A.},
  date = {2021-01-27T11:25:42},
  journaltitle = {Proceedings  of  the  23rd  International  Conference  on  Human-Computer  Interaction},
  publisher = {{Springer}},
  location = {{Washington, DC}},
  doi = {10.31219/osf.io/pgq6m},
  url = {https://osf.io/pgq6m/},
  urldate = {2021-02-18},
  abstract = {Most manual communicative gestures that humans produce cannot be looked up in a dictionary, as these manual gestures inherit their meaning in large part from the communicative context and are not conventionalized. However, it is understudied to what extent the communicative signal as such — bodily postures in movement, or kinematics — can inform about gesture semantics. Can we construct, in principle, a distribution-based semantics of gesture kinematics, similar to how word vectorization methods in NLP (Natural language Processing) are now widely used to study semantic properties in text and speech? For such a project to get off the ground, we need to know the extent to which semantically similar gestures are more likely to be kinematically similar. In study 1 we assess whether semantic word2vec distances between the conveyed concepts participants were explicitly instructed to convey in silent gestures, relate to the kinematic distances of these gestures as obtained from Dynamic Time Warping (DTW). In a second director-matcher dyadic study we assess kinematic similarity between spontaneous co-speech gestures produced between interacting participants. Participants were asked before and after they interacted how they would name the objects. The semantic distances between the resulting names were related to the gesture kinematic distances of gestures that were made in the context of conveying those objects in the interaction. We find that the gestures’ semantic relatedness is reliably predictive of kinematic relatedness across these highly divergent studies, which suggests that the development of an NLP method of deriving semantic relatedness from kinematics is a promising avenue for future developments in automated multimodal recognition. Deeper implications for statistical learning processes in multimodal language are discussed.},
  eventtitle = {{{HCII2021}}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\72ZMBTCF\\Pouw et al. - 2021 - Semantically related gestures move alike Towards .pdf},
  keywords = {Communication,comparison,gesture semantics,manual gesture kinematics,NLP,Social and Behavioral Sciences,time series}
}

@article{pouwStabilizingSpeechProduction2018,
  title = {Stabilizing {{Speech Production}} through {{Gesture}}-{{Speech Coordination}}},
  author = {Pouw, W. and De Jonge-Hoekstra, L. and Dixon, J.},
  date = {2018-12-17T16:48:41},
  doi = {10.31234/osf.io/arzne},
  url = {https://psyarxiv.com/arzne/},
  urldate = {2019-09-20},
  abstract = {Hand-gestures are seamlessly coordinated with speech. Yet, there is only anecdotal support for gestures’ functional role in speech production. Here we explore temporal aspects of speech production when people use hand gesture. We performed exploratory analyses with a naturalistic German-speaking sample from The Bielefeld Speech and Gesture Alignment Corpus (SaGA), which consisted of 67 minutes of narration data and over 500 gesture events (N = 6). We found that the rhythmic timing of speech (defined as the mean and standard deviations of speech onset intervals) is highly correlated with the likelihood of gesturing. Furthermore, we utilized deep learning methods to track gesture motion, and extracted the amplitude envelope of speech, so as to gauge the degree of (continuous) gesture-speech synchrony. We then performed a continuous time-series analysis (recurrence quantification analysis; RQA) to index how temporal properties of speech change when gesture and speech are more or less synchronized. Our analyses revealed that when gesture and speech were more synchronized, the temporal structure of speech was more ordered and less complex, as indexed by classic measures of dynamic temporal stability (e.g., Entropy, Ratio of \%Determinism/Recurrence). We suggest that a fundamental gesture-speech relation is rooted in entrainment, which yields stability in the temporal structure of speech.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\USEMMUQM\\Pouw et al. - 2018 - Stabilizing Speech Production through Gesture-Spee.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5LEH8GKW\\arzne.html}
}

@report{pouwTimingConversationDynamically2020,
  title = {Timing in Conversation Is Dynamically Adjusted Turn by Turn: {{Evidence}} for Lag-1 Negatively Autocorrelated Turn Taking Times in Telephone Conversation},
  shorttitle = {Timing in Conversation Is Dynamically Adjusted Turn by Turn},
  author = {Pouw, Wim and Holler, Judith},
  date = {2020-12-09T17:19:34},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/b98da},
  url = {https://psyarxiv.com/b98da/},
  urldate = {2020-12-12},
  abstract = {Conversational turn taking in humans has been deemed an exquisite feat of timing. Speakers tend to anticipate another speaker’s turn ending so as to rapidly initiate the next turn as a response. This rapid response often takes only around 200ms which is considerably less time than it would take to plan and initiate a next turn in response to a turn-final silence. The timing mechanism has been heavily debated, and revolves around questions such as who is doing the timing, and how to model such a timing mechanism. Here we replicate a basic phenomenon obtained in non-communicative timing research on human rhythmic tapping abilities. We show that turn transition times in phone conversations behave similarly as rhythmic tapping to a metronome. We show that there is serial dependence between turn transition times (TTT’s), such that TTT’s are lag-1 negatively autocorrelated, suggesting that there is a joint correction mechanism operating at the level of the dyad in TTT’s during telephone conversations. This finding, if replicated, has major implications for models describing turn taking, and confirms the joint anticipatory nature of human conversational dynamics. Future research is needed to see how pervasive serial dependencies in TTT’s are, such as for example in richer communicative face-to-face contexts where visual signals affect conversational timing.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\E5RZKQHG\\Pouw and Holler - 2020 - Timing in conversation is dynamically adjusted tur.pdf},
  keywords = {autocorrelation,Cognitive Psychology,psycholinguistics,rhythm,Social and Behavioral Sciences,timing,turn taking}
}

@article{preuschoftMechanismsAcquisitionHabitual2004,
  title = {Mechanisms for the Acquisition of Habitual Bipedality: Are There Biomechanical Reasons for the Acquisition of Upright Bipedal Posture?},
  shorttitle = {Mechanisms for the Acquisition of Habitual Bipedality},
  author = {Preuschoft, Holger},
  date = {2004},
  journaltitle = {Journal of Anatomy},
  volume = {204},
  pages = {363--384},
  issn = {1469-7580},
  doi = {10.1111/j.0021-8782.2004.00303.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0021-8782.2004.00303.x},
  urldate = {2020-11-20},
  abstract = {Morphology and biomechanics are linked by causal morphogenesis (‘Wolff's law’) and the interplay of mutations and selection (Darwin's ‘survival of the fittest’). Thus shape-based selective pressures can be determined. In both cases we need to know which biomechanical factors lead to skeletal adaptation, and which ones exert selective pressures on body shape. Each bone must be able to sustain the greatest regularly occurring loads. Smaller loads are unlikely to lead to adaptation of morphology. The highest loads occur primarily in posture and locomotion, simply because of the effect of body weight (or its multiple). In the skull, however, it is biting and chewing that result in the greatest loads. Body shape adapted for an arboreal lifestyle also smooths the way towards bipedality. Hindlimb dominance, length of the limbs in relation to the axial skeleton, grasping hands and feet, mass distribution (especially of the limb segments), thoracic shape, rib curvatures, and the position of the centre of gravity are the adaptations to arboreality that also pre-adapt for bipedality. Five divergent locomotor/morphological types have evolved from this base: arm-swinging in gibbons, forelimb-dominated slow climbing in orang-utans, quadrupedalism/climbing in the African apes, an unknown mix of climbing and bipedal walking in australopithecines, and the remarkably endurant bipedal walking of humans. All other apes are also facultative bipeds, but it is the biomechanical characteristics of bipedalism in orang-utans, the most arboreal great ape, which is closest to that in humans. If not evolutionary accident, what selective factor can explain why two forms adopted bipedality? Most authors tend to connect bipedal locomotion with some aspect of progressively increasing distance between trees because of climatic changes. More precise factors, in accordance with biomechanical requirements, include stone-throwing, thermoregulation or wading in shallow water. Once bipedality has been acquired, development of typical human morphology can readily be explained as adaptations for energy saving over long distances. A paper in this volume shows that load-carrying ability was enhanced from australopithecines to Homo ergaster (early African H. erectus), supporting an earlier proposition that load-carrying was an essential factor in human evolution.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0021-8782.2004.00303.x},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UTKAYVIG\\Preuschoft - 2004 - Mechanisms for the acquisition of habitual bipedal.pdf;C\:\\Users\\u668173\\Zotero\\storage\\QY5ND5Z3\\j.0021-8782.2004.00303.html},
  keywords = {biomechanics of arboreality,biomechanics of walking,human evolution,selective pressure,trunk shape},
  langid = {english},
  number = {5}
}

@article{priceDegeneracyCognitiveAnatomy2002,
  title = {Degeneracy and Cognitive Anatomy},
  author = {Price, Cathy J. and Friston, Karl J.},
  date = {2002-10-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends Cogn Sci},
  volume = {6},
  pages = {416--421},
  issn = {1879-307X},
  doi = {10.1016/s1364-6613(02)01976-9},
  abstract = {Cognitive models indicate that there are multiple ways of completing the same task. This implicit degeneracy cannot be revealed by functional imaging studies of normal subjects if more than one of the sufficient neural systems is activated. Nor can it be detected by neuropsychological studies of patients because their performance might not be impaired when only one degenerate system is damaged. We propose that degenerate sets of sufficient neural systems can only be identified by an iterative approach that integrates the lesion-deficit model and functional imaging studies of normal and neurologically damaged subjects.},
  eprint = {12413574},
  eprinttype = {pmid},
  langid = {english},
  number = {10}
}

@article{priceMotionDoesMatter2006,
  title = {Motion Does Matter: An Examination of Speech-Based Text Entry on the Move},
  shorttitle = {Motion Does Matter},
  author = {Price, Kathleen J. and Lin, Min and Feng, Jinjuan and Goldman, Rich and Sears, Andrew and Jacko, Julie A.},
  date = {2006-03-01},
  journaltitle = {Universal Access in the Information Society},
  shortjournal = {Univ Access Inf Soc},
  volume = {4},
  pages = {246--257},
  issn = {1615-5297},
  doi = {10.1007/s10209-005-0006-8},
  url = {https://doi.org/10.1007/s10209-005-0006-8},
  urldate = {2019-10-17},
  abstract = {Desktop interaction solutions are often inappropriate for mobile devices due to small screen size and portability needs. Speech recognition can improve interactions by providing a relatively hands-free solution that can be used in various situations. While mobile systems are designed to be transportable, few have examined the effects of motion on mobile interactions. This paper investigates the effect of motion on automatic speech recognition (ASR) input for mobile devices. Speech recognition error rates (RER) have been examined with subjects walking or seated, while performing text input tasks and the effect of ASR enrollment conditions on RER. The obtained results suggest changes in user training of ASR systems for mobile and seated usage.},
  keywords = {Automatic speech recognition,Mobile device,Motion,Speech recognition errors,Speech-based data entry},
  langid = {english},
  number = {3}
}

@book{prietoDevelopmentProsodyFirst2018,
  title = {The {{Development}} of {{Prosody}} in {{First Language Acquisition}} ({{Trends}} in {{Language Acquisition Research}})},
  author = {Prieto, P. and Esteve-Gibert, N.},
  date = {2018},
  volume = {23},
  publisher = {{John Benjamins}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EB53FMQX\\ref=sr_1_1.html},
  series = {Trends in {{Language Acquisition Research}}}
}

@article{prieurOriginsGesturesLanguage2019,
  title = {The Origins of Gestures and Language: History, Current Advances and Proposed Theories},
  shorttitle = {The Origins of Gestures and Language},
  author = {Prieur, Jacques and Barbu, Stéphanie and Blois-Heulin, Catherine and Lemasson, Alban},
  date = {2019-12-18},
  journaltitle = {Biological Reviews of the Cambridge Philosophical Society},
  shortjournal = {Biol Rev Camb Philos Soc},
  issn = {1469-185X},
  doi = {10.1111/brv.12576},
  abstract = {Investigating in depth the mechanisms underlying human and non-human primate intentional communication systems (involving gestures, vocalisations, facial expressions and eye behaviours) can shed light on the evolutionary roots of language. Reports on non-human primates, particularly great apes, suggest that gestural communication would have been a crucial prerequisite for the emergence of language, mainly based on the evidence of large communication repertoires and their associated multifaceted nature of intentionality that are key properties of language. Such research fuels important debates on the origins of gestures and language. We review here three non-mutually exclusive processes that can explain mainly great apes' gestural acquisition and development: phylogenetic ritualisation, ontogenetic ritualisation, and learning via social negotiation. We hypothesise the following scenario for the evolutionary origins of gestures: gestures would have appeared gradually through evolution via signal ritualisation following the principle of derived activities, with the key involvement of emotional expression and processing. The increasing level of complexity of socioecological lifestyles and associated daily manipulative activities might then have enabled the acquisition and development of different interactional strategies throughout the life cycle. Many studies support a multimodal origin of language. However, we stress that the origins of language are not only multimodal, but more broadly multicausal. We propose a multicausal theory of language origins which better explains current findings. It postulates that primates' communicative signalling is a complex trait continually shaped by a cost-benefit trade-off of signal production and processing of interactants in relation to four closely interlinked categories of evolutionary and life cycle factors: species, individual and context-related characteristics as well as behaviour and its characteristics. We conclude by suggesting directions for future research to improve our understanding of the evolutionary roots of gestures and language.},
  eprint = {31854102},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XDV6ZT32\\Prieur et al. - 2019 - The origins of gestures and language history, cur.pdf},
  keywords = {communication behaviours,emotional and intentional signalling,language evolution,multifactoriality,multimodality},
  langid = {english}
}

@article{prieurOriginsGesturesLanguage2019a,
  title = {The Origins of Gestures and Language: History, Current Advances and Proposed Theories},
  shorttitle = {The Origins of Gestures and Language},
  author = {Prieur, Jacques and Barbu, Stéphanie and Blois-Heulin, Catherine and Lemasson, Alban},
  date = {2019-12-18},
  journaltitle = {Biological Reviews of the Cambridge Philosophical Society},
  shortjournal = {Biol Rev Camb Philos Soc},
  issn = {1469-185X},
  doi = {10.1111/brv.12576},
  abstract = {Investigating in depth the mechanisms underlying human and non-human primate intentional communication systems (involving gestures, vocalisations, facial expressions and eye behaviours) can shed light on the evolutionary roots of language. Reports on non-human primates, particularly great apes, suggest that gestural communication would have been a crucial prerequisite for the emergence of language, mainly based on the evidence of large communication repertoires and their associated multifaceted nature of intentionality that are key properties of language. Such research fuels important debates on the origins of gestures and language. We review here three non-mutually exclusive processes that can explain mainly great apes' gestural acquisition and development: phylogenetic ritualisation, ontogenetic ritualisation, and learning via social negotiation. We hypothesise the following scenario for the evolutionary origins of gestures: gestures would have appeared gradually through evolution via signal ritualisation following the principle of derived activities, with the key involvement of emotional expression and processing. The increasing level of complexity of socioecological lifestyles and associated daily manipulative activities might then have enabled the acquisition and development of different interactional strategies throughout the life cycle. Many studies support a multimodal origin of language. However, we stress that the origins of language are not only multimodal, but more broadly multicausal. We propose a multicausal theory of language origins which better explains current findings. It postulates that primates' communicative signalling is a complex trait continually shaped by a cost-benefit trade-off of signal production and processing of interactants in relation to four closely interlinked categories of evolutionary and life cycle factors: species, individual and context-related characteristics as well as behaviour and its characteristics. We conclude by suggesting directions for future research to improve our understanding of the evolutionary roots of gestures and language.},
  eprint = {31854102},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7AZS82NH\\Prieur et al. - 2019 - The origins of gestures and language history, cur.pdf},
  keywords = {communication behaviours,emotional and intentional signalling,language evolution,multifactoriality,multimodality},
  langid = {english}
}

@article{profetaBernsteinLevelsMovement2018,
  title = {Bernstein’s Levels of Movement Construction: {{A}} Contemporary Perspective},
  shorttitle = {Bernstein’s Levels of Movement Construction},
  author = {Profeta, Vitor L. S. and Turvey, Michael T.},
  date = {2018-02-01},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {57},
  pages = {111--133},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2017.11.013},
  url = {http://www.sciencedirect.com/science/article/pii/S0167945717305717},
  urldate = {2020-03-31},
  abstract = {Explanation of how goal-directed movements are made manifest is the ultimate aim of the field classically referred to as “motor control”. Essential to the sought-after explanation is comprehension of the supporting functional architecture. Seven decades ago, the Russian physiologist and movement scientist Nikolai A. Bernstein proposed a hierarchical model to explain the construction of movements. In his model, the levels of the hierarchy share a common language (i.e., they are commensurate) and perform complementing functions to bring about dexterous movements. The science of the control and coordination of movement in the phylum Craniata has made considerable progress in the intervening seven decades. The contemporary body of knowledge about each of Bernstein’s hypothesized functional levels is both more detailed and more sophisticated. A natural consequence of this progress, however, is the relatively independent theoretical development of a given level from the other levels. In this essay, we revisit each level of Bernstein’s hierarchy from the joint perspectives of (a) the ecological approach to perception-action and (b) dynamical systems theory. We review a substantial and relevant body of literature produced in different areas of study that are accommodated by this ecological-dynamical version of Bernstein’s levels. Implications for the control and coordination of movement and the challenges to producing a unified theory are discussed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BWJJTZXU\\S0167945717305717.html},
  keywords = {Bernstein,Control,Coordination,Movement construction,Synergy},
  langid = {english}
}

@article{provasiRhythmPerceptionProduction2014,
  title = {Rhythm Perception, Production, and Synchronization during the Perinatal Period},
  author = {Provasi, Joëlle and Anderson, David I. and Barbu-Roth, Marianne},
  date = {2014},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  volume = {5},
  pages = {1048},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.01048},
  abstract = {Sensori-motor synchronization (SMS) is the coordination of rhythmic movement with an external rhythm. It plays a central role in motor, cognitive, and social behavior. SMS is commonly studied in adults and in children from four years of age onward. Prior to this age, the ability has rarely been investigated due to a lack of available methods. The present paper reviews what is known about SMS in young children, infants, newborns, and fetuses. The review highlights fetal and infant perception of rhythm and cross modal perception of rhythm, fetal, and infant production of rhythm and cross modal production of rhythm, and the contexts in which production of rhythm can be observed in infants. A primary question is whether infants, even newborns, can modify their spontaneous rhythmical motor behavior in response to external rhythmical stimulation. Spontaneous sucking, crying, and leg movements have been studied in the presence or absence of rhythmical auditory stimulation. Findings suggest that the interaction between movement and sound is present at birth and that SMS can be observed in special conditions and within a narrow range of tempi, particularly near the infant's own spontaneous motor tempo. The discussion centers on the fundamental role of SMS in interaction and communication at the beginning of life.},
  eprint = {25278929},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RUU5EK45\\Provasi et al. - 2014 - Rhythm perception, production, and synchronization.pdf},
  keywords = {infants,rhythm,sensorimotor synchronization,synchrony},
  langid = {english},
  pmcid = {PMC4166894}
}

@article{provineLaughingTicklingEvolution2004,
  title = {Laughing, {{Tickling}}, and the {{Evolution}} of {{Speech}} and {{Self}}},
  author = {Provine, Robert R.},
  date = {2004-12},
  journaltitle = {Current Directions in Psychological Science},
  volume = {13},
  pages = {215--218},
  issn = {0963-7214, 1467-8721},
  doi = {10.1111/j.0963-7214.2004.00311.x},
  url = {http://journals.sagepub.com/doi/10.1111/j.0963-7214.2004.00311.x},
  urldate = {2020-02-25},
  abstract = {Laughter is an instinctive, contagious, stereotyped, unconsciously controlled, social play vocalization that is unusual in solitary settings. Laughter punctuates speech and is not typically humor related, speakers often laugh more often than their audience, and male speakers are the best laugh getters. Laughter evolved from the labored breathing of physical play, with the characteristic ‘‘pant-pant’’ laugh of chimpanzees and derivative ‘‘ha-ha’’ of humans signaling (‘‘ritualizing’’) its rowdy origin. Laughter reveals that breath control is why humans can speak and chimpanzees cannot. The evolution of bipedality in human ancestors freed the thorax of its support role in quadrupedal locomotion, a critical step in uncoupling breathing from running, providing humans with the flexible breath control necessary for speech and our characteristic laugh. Tickle, an ancient laughter stimulus, is a means of communication between preverbal infants and mothers, and between friends, family, and lovers. Because you cannot tickle yourself, tickle involves a neurological self /nonself discrimination, providing the most primitive social scenario.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\M8LTM7W7\\Provine - 2004 - Laughing, Tickling, and the Evolution of Speech an.pdf},
  langid = {english},
  number = {6}
}

@article{puckreeAbdominalMotorUnit1998,
  title = {Abdominal Motor Unit Activity during Respiratory and Nonrespiratory Tasks},
  author = {Puckree, T. and Cerny, F. and Bishop, B.},
  date = {1998-05},
  journaltitle = {Journal of Applied Physiology (Bethesda, Md.: 1985)},
  volume = {84},
  pages = {1707--1715},
  issn = {8750-7587},
  doi = {10.1152/jappl.1998.84.5.1707},
  abstract = {Abdominal muscles serve multiple roles, but the functional organization of their motoneurons remains unclear. To gain insight, we recorded single motor unit potentials from the internal oblique (IO) and transversus abdominis (TA) muscles of three standing subjects during quiet breathing, a leg lift, and an expiratory threshold load. Inspiratory airflow, recorded from a pneumotachometer, provided tidal volumes and respiratory cycle timing. Fine wires, implanted under ultrasonic imaging, detected single motor unit potentials that were visually distinguished by their spike morphology. From the number of spikes, firing profiles, times of occurrence in the respiratory cycle, and their onset, instantaneous, mean, and peak firing frequencies we deduced that 1) breathing patterns varied across tasks, 2) different motor units were recruited for each task with essentially no overlap, 3) their firing displayed prominent expiratory activity during each task, and 4) the recruitment levels and discharge patterns of IO and TA were different. We conclude that the IO and TA motor pools receive a strong central respiratory drive, yet each pool receives its own distinct, task-dependent synaptic input.},
  eprint = {9572821},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PT2V7IW6\\Puckree et al. - 1998 - Abdominal motor unit activity during respiratory a.pdf},
  keywords = {Abdominal Muscles,Action Potentials,Adult,Electrodes; Implanted,Electrophysiology,Humans,Male,Middle Aged,Respiration,Tidal Volume},
  langid = {english},
  number = {5}
}

@article{pullerRespiratoryFunctionSpeech1988,
  title = {Respiratory Function in Speech and Song, by Thomas j. Hixon and Collaborators, 433 Pp, Hard Cover, College-Hill Press, Boston, Ma, 1987, \$32.00},
  author = {Puller, Dennis},
  date = {1988},
  journaltitle = {The Laryngoscope},
  volume = {98},
  pages = {689--689},
  issn = {1531-4995},
  doi = {10.1288/00005537-198806000-00026},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1288/00005537-198806000-00026},
  urldate = {2020-11-06},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1288/00005537-198806000-00026},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\B2ZAMUDF\\00005537-198806000-00026.html},
  langid = {english},
  number = {6}
}

@article{putzarDevelopmentVisualFeature2007,
  title = {The Development of Visual Feature Binding Processes after Visual Deprivation in Early Infancy},
  author = {Putzar, Lisa and Hötting, Kirsten and Rösler, Frank and Röder, Brigitte},
  date = {2007-09-01},
  journaltitle = {Vision Research},
  shortjournal = {Vision Research},
  volume = {47},
  pages = {2616--2626},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2007.07.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0042698907002921},
  urldate = {2020-12-05},
  abstract = {Higher visual functions were investigated in patients treated for bilateral congenital cataracts in two experiments. Participants were asked to detect either real or illusory contours (Kanizsa squares in Experiment 1 or one of four different Kanizsa contours in Experiment 2) among distractor items. Compared to normally sighted participants matched for age, gender and education, cataract patients treated after the age of 5–6 months took relatively longer to detect Kanizsa figures (Experiments 1 and 2) and they had higher miss rates (Experiment 2). The present results suggest that the ability of visual feature binding depends on early visual input and is permanently impaired if patterned vision is prevented in early infancy for 5 months or more.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\694GK69Y\\Putzar et al. - 2007 - The development of visual feature binding processe.pdf;C\:\\Users\\u668173\\Zotero\\storage\\FNBB27KK\\S0042698907002921.html},
  keywords = {Congenital cataract,Illusory contours,Visual deprivation,Visual development},
  langid = {english},
  number = {20}
}

@inproceedings{quinnEvolvingCommunicationDedicated2001,
  title = {Evolving Communication without Dedicated Communication Channels. {{Paper}} Presented at The},
  booktitle = {Advances in {{Artificial Life}}: 6th {{European Conference}} on {{Artificial Life}} ({{ECAL}} 2001},
  author = {Quinn, Matt},
  date = {2001},
  abstract = {Abstract. Artificial Life models have consistently implemented com-munication as an exchange of signals over dedicated and functionally isolated channels. I argue that such a feature prevents models from pro-viding a satisfactory account of the origins of communication and present a model in which there are no dedicated channels. Agents controlled by neural networks and equipped with proximity sensors and wheels are pre-sented with a co-ordinated movement task. It is observed that functional, but non-communicative, behaviours which evolve in the early stages of the simulation both make possible, and form the basis of, the commu-nicative behaviour which subsequently evolves. 1},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WSJJZTJ3\\Quinn - 2001 - Evolving communication without dedicated communica.pdf;C\:\\Users\\u668173\\Zotero\\storage\\NCBPJ9ET\\summary.html}
}

@article{raczaszek-leonardiReconcilingSymbolicDynamic2008,
  title = {Reconciling Symbolic and Dynamic Aspects of Language: {{Toward}} a Dynamic Psycholinguistics},
  shorttitle = {Reconciling Symbolic and Dynamic Aspects of Language},
  author = {Rączaszek-Leonardi, J. and Kelso, S.J.A.},
  date = {2008-08},
  journaltitle = {New Ideas in Psychology},
  shortjournal = {New Ideas in Psychology},
  volume = {26},
  pages = {193--207},
  issn = {0732118X},
  doi = {10.1016/j.newideapsych.2007.07.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0732118X07000487},
  urldate = {2020-12-27},
  abstract = {The present paper examines natural language as a dynamical system. The oft-expressed view of language as ‘‘a static system of symbols’’ is here seen as an element of a larger system that embraces the mutuality of symbols and dynamics. Following along the lines of the theoretical biologist H.H. Pattee, the relation between symbolic and dynamic aspects of language is expressed within a more general framework that deals with the role of information in biological systems. In this framework, symbols are seen as information-bearing entities that emerge under pressures of communicative needs and that serve as concrete constraints on development and communication. In an attempt to identify relevant dynamic aspects of such a system, one has to take into account events that happen on different time scales: evolutionary language change (i.e., a diachronic aspect), processes of communication (language use) and language acquisition. Acknowledging the role of dynamic processes in shaping and sustaining the structures of natural language calls for a change in methodology. In particular, a purely synchronic analysis of a system of symbols as ‘‘meaning-containing entities’’ is not sufficient to obtain answers to certain recurring problems in linguistics and the philosophy of language. A more encompassing research framework may be the one designed specifically for studying informationally based coupled dynamical systems (coordination dynamics) in which processes of self-organization take place over different time scales.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IXIPPT4E\\Rączaszek-Leonardi and Scott Kelso - 2008 - Reconciling symbolic and dynamic aspects of langua.pdf},
  langid = {english},
  number = {2}
}

@article{raderRoleSpeechGestureSynchrony2015,
  title = {The {{Role}} of {{Speech}}-{{Gesture Synchrony}} in {{Clipping Words From}} the {{Speech Stream}}: {{Evidence From Infant Pupil Responses}}},
  shorttitle = {The {{Role}} of {{Speech}}-{{Gesture Synchrony}} in {{Clipping Words From}} the {{Speech Stream}}},
  author = {Rader, Nancy de Villiers and Zukow-Goldring, Patricia},
  date = {2015-10-02},
  journaltitle = {Ecological Psychology},
  volume = {27},
  pages = {290--299},
  publisher = {{Routledge}},
  issn = {1040-7413},
  doi = {10.1080/10407413.2015.1086226},
  url = {https://doi.org/10.1080/10407413.2015.1086226},
  urldate = {2020-11-19},
  abstract = {How do young infants discover that a segment of the sound stream refers to a particular aspect of the visual world around them? Speakers do not enunciate each word separately, even to infants; rather, whattheysayrunstogether. To relate an object (say, an apple) to its referent, infants must notice the interactant's target of attention (the apple) and at the same time single out the word that refers to it as the other person speaks (Lookattheapple!). We contend that caregivers through their actions assist by directing and educating an infant's attention, particularly through the use of a show gesture. The onset/offset, rhythm, tempo, and duration of these show gestures are synchronous with the saying of the words referring to the target objects. Our prior research using eye tracking found that show gestures lead an infant to look at the object presented as the word for it is uttered and that show gestures facilitate word learning. In this research we tested the hypothesis that show gestures also lead to enhanced attentional processing as measured through pupil dilation. Comparing pupil diameters while words were introduced with a show, static, or asynchronous dynamic gesture, we found that pupil dilation occurred for the show gesture condition and was positively correlated with word learning.},
  annotation = {\_eprint: https://doi.org/10.1080/10407413.2015.1086226},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\54RHBI74\\10407413.2015.html},
  number = {4}
}

@article{radPsychologyHomoSapiens2018,
  title = {Toward a Psychology of {{Homo}} Sapiens: {{Making}} Psychological Science More Representative of the Human Population},
  shorttitle = {Toward a Psychology of {{Homo}} Sapiens},
  author = {Rad, Mostafa Salari and Martingano, Alison Jane and Ginges, Jeremy},
  date = {2018-11-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {115},
  pages = {11401--11405},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1721165115},
  url = {https://www.pnas.org/content/115/45/11401},
  urldate = {2020-07-22},
  abstract = {Two primary goals of psychological science should be to understand what aspects of human psychology are universal and the way that context and culture produce variability. This requires that we take into account the importance of culture and context in the way that we write our papers and in the types of populations that we sample. However, most research published in our leading journals has relied on sampling WEIRD (Western, educated, industrialized, rich, and democratic) populations. One might expect that our scholarly work and editorial choices would by now reflect the knowledge that Western populations may not be representative of humans generally with respect to any given psychological phenomenon. However, as we show here, almost all research published by one of our leading journals, Psychological Science, relies on Western samples and uses these data in an unreflective way to make inferences about humans in general. To take us forward, we offer a set of concrete proposals for authors, journal editors, and reviewers that may lead to a psychological science that is more representative of the human condition.},
  eprint = {30397114},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\II43ZVXL\\Rad et al. - 2018 - Toward a psychology of Homo sapiens Making psycho.pdf;C\:\\Users\\u668173\\Zotero\\storage\\M2BNWVF5\\11401.html},
  keywords = {cognition,culture,diversity,methodology,psychological science},
  langid = {english},
  number = {45}
}

@article{radPsychologyHomoSapiens2018a,
  title = {Toward a Psychology of {{{\emph{Homo}}}}{\emph{ Sapiens}} : {{Making}} Psychological Science More Representative of the Human Population},
  shorttitle = {Toward a Psychology of {{{\emph{Homo}}}}{\emph{ Sapiens}}},
  author = {Rad, Mostafa Salari and Martingano, Alison Jane and Ginges, Jeremy},
  date = {2018-11-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {115},
  pages = {11401--11405},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1721165115},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1721165115},
  urldate = {2020-07-09},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\48Q2EJFT\\Rad et al. - 2018 - Toward a psychology of Homo sapiens  Makin.pdf},
  langid = {english},
  number = {45}
}

@book{rahaimMusickingBodiesGesture2012,
  title = {Musicking bodies: Gesture and voice in hindustani music},
  shorttitle = {Musicking bodies},
  author = {Rahaim, Matthew},
  date = {2012-12-01},
  publisher = {{Wesleyan University Press}},
  url = {https://experts.umn.edu/en/publications/musicking-bodies-gesture-and-voice-in-hindustani-music-2},
  urldate = {2020-10-23},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EQRIZQUU\\musicking-bodies-gesture-and-voice-in-hindustani-music-2.html},
  isbn = {978-0-8195-7325-4},
  langid = {English (US)}
}

@article{raineHumanRoarsCommunicate2019,
  title = {Human Roars Communicate Upper-Body Strength More Effectively than Do Screams or Aggressive and Distressed Speech},
  author = {Raine, Jordan and Pisanski, Katarzyna and Bond, Rod and Simner, Julia and Reby, David},
  date = {2019-03-04},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  pages = {e0213034},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0213034},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0213034},
  urldate = {2019-10-17},
  abstract = {Despite widespread evidence that nonverbal components of human speech (e.g., voice pitch) communicate information about physical attributes of vocalizers and that listeners can judge traits such as strength and body size from speech, few studies have examined the communicative functions of human nonverbal vocalizations (such as roars, screams, grunts and laughs). Critically, no previous study has yet to examine the acoustic correlates of strength in nonverbal vocalisations, including roars, nor identified reliable vocal cues to strength in human speech. In addition to being less acoustically constrained than articulated speech, agonistic nonverbal vocalizations function primarily to express motivation and emotion, such as threat, and may therefore communicate strength and body size more effectively than speech. Here, we investigated acoustic cues to strength and size in roars compared to screams and speech sentences produced in both aggressive and distress contexts. Using playback experiments, we then tested whether listeners can reliably infer a vocalizer’s actual strength and height from roars, screams, and valenced speech equivalents, and which acoustic features predicted listeners’ judgments. While there were no consistent acoustic cues to strength in any vocal stimuli, listeners accurately judged inter-individual differences in strength, and did so most effectively from aggressive voice stimuli (roars and aggressive speech). In addition, listeners more accurately judged strength from roars than from aggressive speech. In contrast, listeners’ judgments of height were most accurate for speech stimuli. These results support the prediction that vocalizers maximize impressions of physical strength in aggressive compared to distress contexts, and that inter-individual variation in strength may only be honestly communicated in vocalizations that function to communicate threat, particularly roars. Thus, in continuity with nonhuman mammals, the acoustic structure of human aggressive roars may have been selected to communicate, and to some extent exaggerate, functional cues to physical formidability.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MPXMFGRN\\Raine et al. - 2019 - Human roars communicate upper-body strength more e.pdf;C\:\\Users\\u668173\\Zotero\\storage\\BJ2YTZ3P\\article.html},
  keywords = {Acoustics,Bioacoustics,Hand strength,Physiological parameters,Speech,Speech signal processing,Verbal communication,Vocalization},
  langid = {english},
  number = {3}
}

@article{ramusCorrelatesLinguisticRhythm2000,
  title = {Correlates of Linguistic Rhythm in the Speech Signal},
  author = {Ramus, Franck and Nespor, Marina and Mehler, Jacques},
  date = {2000-04-14},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {75},
  pages = {AD3-AD30},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(00)00101-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027700001013},
  urldate = {2019-09-28},
  abstract = {Spoken languages have been classified by linguists according to their rhythmic properties, and psycholinguists have relied on this classification to account for infants’ capacity to discriminate languages. Although researchers have measured many speech signal properties, they have failed to identify reliable acoustic characteristics for language classes. This paper presents instrumental measurements based on a consonant/vowel segmentation for eight languages. The measurements suggest that intuitive rhythm types reflect specific phonological properties, which in turn are signaled by the acoustic/phonetic properties of speech. The data support the notion of rhythm classes and also allow the simulation of infant language discrimination, consistent with the hypothesis that newborns rely on a coarse segmentation of speech. A hypothesis is proposed regarding the role of rhythm perception in language acquisition.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PL9745Z6\\Ramus et al. - 2000 - Correlates of linguistic rhythm in the speech sign.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8NJPFQUW\\S0010027700001013.html},
  keywords = {Language acquisition,Language discrimination,Phonological bootstrapping,Prosody,Speech rhythm,Syllable structure},
  number = {1}
}

@article{rasenbergAlignmentMultimodalInteraction2020,
  title = {Alignment in {{Multimodal Interaction}}: {{An Integrative Framework}}},
  shorttitle = {Alignment in {{Multimodal Interaction}}},
  author = {Rasenberg, Marlou and Özyürek, Asli and Dingemanse, Mark},
  date = {2020},
  journaltitle = {Cognitive Science},
  volume = {44},
  pages = {e12911},
  issn = {1551-6709},
  doi = {10.1111/cogs.12911},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12911},
  urldate = {2021-01-06},
  abstract = {When people are engaged in social interaction, they can repeat aspects of each other’s communicative behavior, such as words or gestures. This kind of behavioral alignment has been studied across a wide range of disciplines and has been accounted for by diverging theories. In this paper, we review various operationalizations of lexical and gestural alignment. We reveal that scholars have fundamentally different takes on when and how behavior is considered to be aligned, which makes it difficult to compare findings and draw uniform conclusions. Furthermore, we show that scholars tend to focus on one particular dimension of alignment (traditionally, whether two instances of behavior overlap in form), while other dimensions remain understudied. This hampers theory testing and building, which requires a well-defined account of the factors that are central to or might enhance alignment. To capture the complex nature of alignment, we identify five key dimensions to formalize the relationship between any pair of behavior: time, sequence, meaning, form, and modality. We show how assumptions regarding the underlying mechanism of alignment (placed along the continuum of priming vs. grounding) pattern together with operationalizations in terms of the five dimensions. This integrative framework can help researchers in the field of alignment and related phenomena (including behavior matching, mimicry, entrainment, and accommodation) to formulate their hypotheses and operationalizations in a more transparent and systematic manner. The framework also enables us to discover unexplored research avenues and derive new hypotheses regarding alignment.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12911},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KZIZW7LJ\\Rasenberg et al. - 2020 - Alignment in Multimodal Interaction An Integrativ.pdf;C\:\\Users\\u668173\\Zotero\\storage\\Q5CFWS44\\cogs.html},
  keywords = {Accommodation,Alignment,Behavior matching,Co-speech gestures,Entrainment,Mimicry,Social interaction},
  langid = {english},
  number = {11}
}

@inproceedings{rasenbergLexicalGesturalAlignment,
  title = {Lexical and Gestural Alignment in Interaction and the Emergence of Novel Shared Symbols},
  booktitle = {Evolang13},
  author = {Rasenberg, Marlou and Dingemanse, Mark and Özyürek, Asli},
  editor = {Ravignani, A. and Barbieri, C. and Flaherty, M. and Jadoul, Y. and Lattenkamp, E. and Little, M. and Martins, M. and Mudd, K. and Verhoef, T.},
  pages = {356--358},
  eventtitle = {The {{Evolution}} of {{Language}}: {{Proceedings}} of the 13th {{International Conference}}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EKF9U3A4\\Rasenberg et al. - LEXICAL AND GESTURAL ALIGNMENT IN INTERACTION AND .pdf},
  langid = {english}
}

@article{rasslerCoordinationBreathingFinger1996,
  title = {Coordination between {{Breathing}} and {{Finger Tracking}} in {{Man}}},
  author = {Raßler, Beate and Ebert, Dietrich and Waurick, Siegfried and Junghans, Reinhard},
  date = {1996-03-01},
  journaltitle = {Journal of Motor Behavior},
  volume = {28},
  pages = {48--57},
  publisher = {{Routledge}},
  issn = {0022-2895},
  doi = {10.1080/00222895.1996.9941732},
  url = {https://doi.org/10.1080/00222895.1996.9941732},
  urldate = {2020-09-19},
  abstract = {Arm and leg movements are known to produce temporal pattern changes of breathing. This can be interpreted as coordination, as defined by von Holst (1939). The aim of the present study was to find whether breathing exerts an influence in a reverse direction on a nonrespiratory movement as well. A pursuit tracking test was used, and test individuals (N = 19) were instructed to track a visually presented step function by flexion or extension of their right index finger. Velocity and precision of the step responses proved to be dependent on their relation to the breathing time course; the differences between inspiratory and expiratory responses were smaller than those within each half-cycle. The movements were performed more rapidly and more precisely in about the middle of each half-cycle than immediately after the respiratory phase transition or during the second half of each inspiration or expiration. Discontinuous short-lasting motor actions exerted a coordinative influence on respiration comparable with that of periodical events: Breaths coinciding with step responses were shortened, preferably when the preset step was given early in the inspiration. It was hypothesized that the reciprocal effect between both motor actions changes periodically. In the first part of each respiratory half-cycle, the respiratory rhythm exerts only a weak influence on additional movements, but it can be altered easily by simultaneous motor processes. Toward the respiratory phase-switching, the respiratory rhythm behaves more stably against coordinative influences and becomes capable of impairing an additional movement.},
  annotation = {\_eprint: https://doi.org/10.1080/00222895.1996.9941732},
  eprint = {12529223},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WTIVP644\\00222895.1996.html},
  keywords = {breathing control,central coordination,motor control,pursuit tracking},
  number = {1}
}

@article{ratnovskyMechanicsRespiratoryMuscles2008,
  title = {Mechanics of Respiratory Muscles},
  author = {Ratnovsky, Anat and Elad, David and Halpern, Pinchas},
  date = {2008-11},
  journaltitle = {Respiratory Physiology \& Neurobiology},
  volume = {163},
  pages = {82--89},
  issn = {15699048},
  doi = {10.1016/j.resp.2008.04.019},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1569904808001134},
  urldate = {2020-07-22},
  abstract = {Lung ventilation is a mechanical process in which the respiratory muscles are acting in concert to remove air in and out of the lungs. Any alteration in the performance of the respiratory muscle may reduce the effectiveness of ventilation. Thus, early diagnosis of their weakness is vital for treatment and rehabilitation. Different techniques, which are based on different measurement protocols, can be utilized for evaluation of respiratory muscle strength. Respiratory muscle strength can be assessed using pressure measurement either from the mouth or from the nostril during quasi-static breathing. However, it estimates only global performance of respiratory muscles. Techniques that are based on electromyography measurements during muscle contraction (EMG) enable the differentiation between the different respiratory muscles. Along with the above clinical and physiological techniques for assessment of respiratory muscle strength and endurance, mechanical and mathematical models of the chest wall were developed in the last few decades for analysis of chest wall movements and the contribution of its components to respiration. In this review, the different methods and the models utilized for evaluation of respiratory muscles function will be discussed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Y52HPEGA\\Ratnovsky et al. - 2008 - Mechanics of respiratory muscles.pdf},
  langid = {english},
  number = {1-3}
}

@article{ravignaniBreathingVoiceSynchronized2020,
  title = {Breathing, Voice, and Synchronized Movement},
  author = {Ravignani, Andrea and Kotz, Sonja A.},
  date = {2020-09-22},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {117},
  pages = {23223--23224},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2011402117},
  url = {https://www.pnas.org/content/117/38/23223},
  urldate = {2020-12-02},
  abstract = {When humans vocalize along with rhythmic arm movements, the recorded voice shows the same rhythmic pattern as these movements (1). When others listen to this voice, their rhythmic movements synchronize with those of the unseen vocalizer (1). Pouw et al.’s (1) outstanding multimodal research connects voice and gesture, and—we suggest—even more so breathing and movement. Pouw et al. (1) report rhythmic modulation of fundamental frequency, which is the rate at which vocal folds vibrate. They attribute this rhythmicity to voice modulation. However, many mechanisms can give the voice a rhythmic character; the most common one is breathing. Try exhaling while rhythmically moving your arm; the sound of your breath will become rhythmic. Could breathing alone explain Pouw et al.’s results (1, 2) (Fig. 1)? Their data may show that muscular contractions for movement propagate …  [↵][1]1To whom correspondence may be addressed. Email: andrea.ravignani\{at\}gmail.com or sonja.kotz\{at\}maastrichtuniversity.nl.  [1]: \#xref-corresp-1-1},
  eprint = {32967065},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RH6GJCXT\\Ravignani and Kotz - 2020 - Breathing, voice, and synchronized movement.pdf;C\:\\Users\\u668173\\Zotero\\storage\\L8SYR6P6\\23223.html},
  langid = {english},
  number = {38}
}

@article{ravignaniEvolutionSpeechRhythm2018,
  title = {Evolution of Speech Rhythm: A Cross-Species Perspective},
  shorttitle = {Evolution of Speech Rhythm},
  author = {Ravignani, Andrea and Dalla Bella, Simone and Falk, Simone and Kello, Chris and Noriega, Florencia and Kotz, Sonja},
  date = {2018},
  doi = {10.7287/peerj.preprints.27539v1},
  url = {https://peerj.com/preprints/27539},
  urldate = {2019-05-07},
  abstract = {Cognition and communication, at the core of human speech rhythm, do not leave a fossil record. However, if the purpose is to understand the origin and evolution of speech rhythm, alternative methods are available. A powerful tool is comparative approach: studying the presence or absence of cognitive/behavioral traits in other species, drawing conclusions on which traits are shared between species, and which are recent human inventions. Here we apply this approach to traits related to human speech rhythm. Many species exhibit temporal structure in their vocalizations but little is known about the range of rhythmic structures perceived and produced, their biological and developmental bases, and communicative functions. We review the literatures on human and non-human studies of rhythm in speech and animal vocalizations to survey similarities and differences. We report important links between vocal perception and motor coordination, and the differentiation of rhythm based on hierarchical temporal structure. We extend this review to quantitative techniques useful for computing rhythmic structure in acoustic sequences and hence facilitating cross-species research. While still far from a full comparative cross-species perspective of speech rhythm, we are closer to fitting missing pieces of the puzzle.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EYVX4IU5\\Ravignani et al. - Evolution of speech rhythm a cross-species perspe.pdf},
  langid = {english}
}

@article{ravignaniInteractiveRhythmsSpecies2019,
  title = {Interactive Rhythms across Species: The Evolutionary Biology of Animal Chorusing and Turn-Taking},
  shorttitle = {Interactive Rhythms across Species},
  author = {Ravignani, Andrea and Verga, Laura and Greenfield, Michael D.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  volume = {1453},
  pages = {12--21},
  issn = {1749-6632},
  doi = {10.1111/nyas.14230},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14230},
  urldate = {2019-10-13},
  abstract = {The study of human language is progressively moving toward comparative and interactive frameworks, extending the concept of turn-taking to animal communication. While such an endeavor will help us understand the interactive origins of language, any theoretical account for cross-species turn-taking should consider three key points. First, animal turn-taking must incorporate biological studies on animal chorusing, namely how different species coordinate their signals over time. Second, while concepts employed in human communication and turn-taking, such as intentionality, are still debated in animal behavior, lower level mechanisms with clear neurobiological bases can explain much of animal interactive behavior. Third, social behavior, interactivity, and cooperation can be orthogonal, and the alternation of animal signals need not be cooperative. Considering turn-taking a subset of chorusing in the rhythmic dimension may avoid overinterpretation and enhance the comparability of future empirical work.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZITD6ZMI\\Ravignani et al. - 2019 - Interactive rhythms across species the evolutiona.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ID8J4AYB\\nyas.html},
  keywords = {bioacoustics,cooperation,interaction,language evolution,speech rhythm,synchrony},
  langid = {english},
  number = {1}
}

@article{ravignaniMusicalEvolutionLab2016,
  title = {Musical Evolution in the Lab Exhibits Rhythmic Universals},
  author = {Ravignani, Andrea and Delgado, Tania and Kirby, Simon},
  date = {2016-12-19},
  journaltitle = {Nature Human Behaviour},
  volume = {1},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0007},
  url = {https://www.nature.com/articles/s41562-016-0007},
  urldate = {2020-03-25},
  abstract = {The authors asked human participants to listen to and imitate randomly generated drumming sequences from each other. Participants turned initially random sequences into rhythmically structured patterns that are characterized by all six statistical universals found in world music.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UGKMX294\\Ravignani et al. - 2016 - Musical evolution in the lab exhibits rhythmic uni.pdf;C\:\\Users\\u668173\\Zotero\\storage\\IT6U5FD7\\s41562-016-0007.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{ravignaniRhythmSpeechAnimal2019,
  title = {Rhythm in Speech and Animal Vocalizations: A Cross‐species Perspective},
  author = {Ravignani, A and Dalla Bella, S and Falk, S and Kello, C. T. and Noriega, F. and Kotz, S.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  doi = {10.1111/nyas.14166},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/epdf/10.1111/nyas.14166},
  urldate = {2019-08-30},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VGFKQD2P\\nyas.html}
}

@article{ravignaniRhythmSynchronyAnimal2019,
  title = {Rhythm and Synchrony in Animal Movement and Communication},
  author = {Ravignani, Andrea},
  date = {2019-02-01},
  journaltitle = {Current Zoology},
  shortjournal = {Curr Zool},
  volume = {65},
  pages = {77--81},
  publisher = {{Oxford Academic}},
  doi = {10.1093/cz/zoy087},
  url = {https://academic.oup.com/cz/article/65/1/77/5201041},
  urldate = {2020-12-05},
  abstract = {Animal communication and motoric behavior develop over time. Often, this temporal dimension has communicative relevance and is organized according to structural},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BYD6H52K\\Ravignani - 2019 - Rhythm and synchrony in animal movement and commun.pdf;C\:\\Users\\u668173\\Zotero\\storage\\7725PM9X\\5201041.html},
  langid = {english},
  number = {1}
}

@article{ravivLargerCommunitiesCreate2019,
  title = {Larger Communities Create More Systematic Languages},
  author = {Raviv, Limor and Meyer, Antje and Lev-Ari, Shiri},
  date = {2019-07-24},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {286},
  pages = {20191262},
  doi = {10.1098/rspb.2019.1262},
  url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspb.2019.1262},
  urldate = {2019-12-11},
  abstract = {Understanding worldwide patterns of language diversity has long been a goal for evolutionary scientists, linguists and philosophers. Research over the past decade has suggested that linguistic diversity may result from differences in the social environments in which languages evolve. Specifically, recent work found that languages spoken in larger communities typically have more systematic grammatical structures. However, in the real world, community size is confounded with other social factors such as network structure and the number of second languages learners in the community, and it is often assumed that linguistic simplification is driven by these factors instead. Here, we show that in contrast to previous assumptions, community size has a unique and important influence on linguistic structure. We experimentally examine the live formation of new languages created in the laboratory by small and larger groups, and find that larger groups of interacting participants develop more systematic languages over time, and do so faster and more consistently than small groups. Small groups also vary more in their linguistic behaviours, suggesting that small communities are more vulnerable to drift. These results show that community size predicts patterns of language diversity, and suggest that an increase in community size might have contributed to language evolution.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QA5QVPTR\\rspb.2019.html},
  number = {1907}
}

@movie{rawlenceManWhoLost1997,
  title = {The {{Man Who Lost His Body}}},
  editor = {Rawlence, Chris},
  year = {1997, 16th October},
  publisher = {{BBC Horizon}},
  editortype = {director},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3RE6SZCW\\the-man-who-lost-his-body.html},
  langid = {english}
}

@book{redfordHandbookSpeechProduction2015,
  title = {The Handbook of Speech Production},
  author = {Redford, Melissa A.},
  date = {2015},
  publisher = {{Wiley Blackwell}},
  location = {{West Sussex}}
}

@article{redmondLeadingLimbPreference2004,
  title = {Leading Limb Preference during Brachiation in the Gibbon Family Member, {{Hylobates}} Syndactylus (Siamangs): {{A}} Study of the Effects of Singing on Lateralisation},
  shorttitle = {Leading Limb Preference during Brachiation in the Gibbon Family Member, {{Hylobates}} Syndactylus (Siamangs)},
  author = {Redmond, John and Lamperez, Al},
  date = {2004-10-01},
  journaltitle = {Laterality},
  volume = {9},
  pages = {381--396},
  publisher = {{Routledge}},
  issn = {1357-650X},
  doi = {10.1080/13576500342000211},
  url = {https://doi.org/10.1080/13576500342000211},
  urldate = {2020-09-19},
  abstract = {Individual‐level lateralisations are common among vertebrates, however population‐level preferences are usually reserved for specific tasks. In humans, handedness is thought to be related to the hemispheric processing of specific aspects of language, including speech. Although nonhuman primates do not possess speech, gibbons are known to produce elaborate vocal displays often referred to as song. To investigate the evolutionary effects of singing on hand preference, this study examined leading limb preference during brachiation in the large‐bodied, melodious gibbon, Hylobates syndactylus (siamangs). A total of 13 male and 12 female siamangs were observed in captive and semi‐captive settings for leading limb preference during vocal and nonvocal behaviour. No significant results were found for the effect of vocalisation at the group level, although individual‐level data indicated a trend towards a right shift in the vocal condition, especially for females, where 8 out of 12 demonstrated a greater reliance on a right leading limb during the vocal versus nonvocal conditions. Males were not found to possess significant preferences in either the vocal or nonvocal conditions alone, however when condition data were combined, the number of individual males lateralised was significantly greater than predicted by chance. These findings support individual, but not population‐level, effects for vocalisation on leading limb preference in siamangs, and emphasise the importance of testing for sex difference in handedness and lateralisation research.},
  annotation = {\_eprint: https://doi.org/10.1080/13576500342000211},
  eprint = {15513236},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\42L5EDH2\\13576500342000211.html},
  number = {4}
}

@article{remediosMonkeyDrummingReveals2009,
  title = {Monkey Drumming Reveals Common Networks for Perceiving Vocal and Nonvocal Communication Sounds},
  author = {Remedios, Ryan and Logothetis, Nikos K. and Kayser, Christoph},
  date = {2009-10-20},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {106},
  pages = {18010--18015},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0909756106},
  url = {https://www.pnas.org/content/106/42/18010},
  urldate = {2020-12-12},
  abstract = {Salient sounds such as those created by drumming can serve as means of nonvocal acoustic communication in addition to vocal sounds. Despite the ubiquity of drumming across human cultures, its origins and the brain regions specialized in processing such signals remain unexplored. Here, we report that an important animal model for vocal communication, the macaque monkey, also displays drumming behavior, and we exploit this finding to show that vocal and nonvocal communication sounds are represented by overlapping networks in the brain's temporal lobe. Observing social macaque groups, we found that these animals use artificial objects to produce salient periodic sounds, similar to acoustic gestures. Behavioral tests confirmed that these drumming sounds attract the attention of listening monkeys similarly as conspecific vocalizations. Furthermore, in a preferential looking experiment, drumming sounds influenced the way monkeys viewed their conspecifics, suggesting that drumming serves as a multimodal signal of social dominance. Finally, by using high-resolution functional imaging we identified those brain regions preferentially activated by drumming sounds or by vocalizations and found that the representations of both these communication sounds overlap in caudal auditory cortex and the amygdala. The similar behavioral responses to drumming and vocal sounds, and their shared neural representation, suggest a common origin of primate vocal and nonvocal communication systems and support the notion of a gestural origin of speech and music.},
  eprint = {19805199},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WHXAL66M\\Remedios et al. - 2009 - Monkey drumming reveals common networks for percei.pdf},
  keywords = {gestures,speech,temporal lobe},
  langid = {english},
  number = {42}
}

@article{rendallLiftingCurtainWizard2007,
  title = {Lifting the Curtain on the {{Wizard}} of {{Oz}}: Biased Voice-Based Impressions of Speaker Size},
  shorttitle = {Lifting the Curtain on the {{Wizard}} of {{Oz}}},
  author = {Rendall, Drew and Vokey, John R. and Nemeth, Christie},
  date = {2007-10},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {33},
  pages = {1208--1219},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.33.5.1208},
  abstract = {The consistent, but often wrong, impressions people form of the size of unseen speakers are not random but rather point to a consistent misattribution bias, one that the advertising, broadcasting, and entertainment industries also routinely exploit. The authors report 3 experiments examining the perceptual basis of this bias. The results indicate that, under controlled experimental conditions, listeners can make relative size distinctions between male speakers using reliable cues carried in voice formant frequencies (resonant frequencies, or timbre) but that this ability can be perturbed by discordant voice fundamental frequency (F-sub-0, or pitch) differences between speakers. The authors introduce 3 accounts for the perceptual pull that voice F-sub-0 can exert on our routine (mis)attributions of speaker size and consider the role that voice F-sub-0 plays in additional voice-based attributions that may or may not be reliable but that have clear size connotations.},
  eprint = {17924818},
  eprinttype = {pmid},
  keywords = {Adult,Amplifiers; Electronic,Cues,Female,Humans,Literature,Male,Speech Perception,Voice Quality},
  langid = {english},
  number = {5}
}

@article{reppActionCanAffect2016,
  title = {Action {{Can Affect Auditory Perception}}:},
  shorttitle = {Action {{Can Affect Auditory Perception}}},
  author = {Repp, Bruno H. and Knoblich, Günther},
  date = {2016-05-06},
  journaltitle = {Psychological Science},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  issn = {1467-9280},
  url = {https://journals.sagepub.com/doi/10.1111/j.1467-9280.2007.01839.x},
  urldate = {2020-11-03},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\J6JQHL46\\j.1467-9280.2007.01839.html},
  langid = {english}
}

@article{reppRhythmicMovementAttracted2004,
  title = {Rhythmic Movement Is Attracted More Strongly to Auditory than to Visual Rhythms},
  author = {Repp, Bruno H. and Penel, Amandine},
  date = {2004-08},
  journaltitle = {Psychological Research},
  shortjournal = {Psychol Res},
  volume = {68},
  pages = {252--270},
  issn = {0340-0727},
  doi = {10.1007/s00426-003-0143-8},
  abstract = {People often move in synchrony with auditory rhythms (e.g., music), whereas synchronization of movement with purely visual rhythms is rare. In two experiments, this apparent attraction of movement to auditory rhythms was investigated by requiring participants to tap their index finger in synchrony with an isochronous auditory (tone) or visual (flashing light) target sequence while a distractor sequence was presented in the other modality at one of various phase relationships. The obtained asynchronies and their variability showed that auditory distractors strongly attracted participants' taps, whereas visual distractors had much weaker effects, if any. This asymmetry held regardless of the spatial congruence or relative salience of the stimuli in the two modalities. When different irregular timing patterns were imposed on target and distractor sequences, participants' taps tended to track the timing pattern of auditory distractor sequences when they were approximately in phase with visual target sequences, but not the reverse. These results confirm that rhythmic movement is more strongly attracted to auditory than to visual rhythms. To the extent that this is an innate proclivity, it may have been an important factor in the evolution of music.},
  eprint = {12955504},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Adult,Attention,Auditory Perception,Feedback,Female,Humans,Male,Middle Aged,Photic Stimulation,Psychoacoustics,Psychomotor Performance,Time Perception,Visual Perception},
  langid = {english},
  number = {4}
}

@article{reppSensorimotorSynchronizationAdaptively2008,
  title = {Sensorimotor Synchronization with Adaptively Timed Sequences},
  author = {Repp, Bruno H. and Keller, Peter E.},
  date = {2008-06},
  journaltitle = {Human Movement Science},
  shortjournal = {Hum Mov Sci},
  volume = {27},
  pages = {423--456},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2008.02.016},
  abstract = {Most studies of human sensorimotor synchronization require participants to coordinate actions with computer-controlled event sequences that are unresponsive to their behavior. In the present research, the computer was programmed to carry out phase and/or period correction in response to asynchronies between taps and tones, and thereby to modulate adaptively the timing of the auditory sequence that human participants were synchronizing with, as a human partner might do. In five experiments the computer's error correction parameters were varied over a wide range, including "uncooperative" settings that a human synchronization partner could not (or would not normally) adopt. Musically trained participants were able to maintain synchrony in all these situations, but their behavior varied systematically as a function of the computer's parameter settings. Computer simulations were conducted to infer the human participants' error correction parameters from statistical properties of their behavior (means, standard deviations, auto- and cross-correlations). The results suggest that participants maintained a fixed gain of phase correction as long as the computer was cooperative, but changed their error correction strategies adaptively when faced with an uncooperative computer.},
  eprint = {18405989},
  eprinttype = {pmid},
  keywords = {Algorithms,Attention,Cognition,Computers,Humans,Interpersonal Relations,Music,Psychomotor Performance,Reaction Time,Time Perception,User-Computer Interface},
  langid = {english},
  number = {3}
}

@article{reppSensorimotorSynchronizationReview2005,
  title = {Sensorimotor Synchronization: {{A}} Review of the Tapping Literature},
  shorttitle = {Sensorimotor Synchronization},
  author = {Repp, Bruno H.},
  date = {2005-12-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {12},
  pages = {969--992},
  issn = {1531-5320},
  doi = {10.3758/BF03206433},
  url = {https://doi.org/10.3758/BF03206433},
  urldate = {2019-04-02},
  abstract = {Sensorimotor synchronization (SMS), the rhythmic coordination of perception and action, occurs in many contexts, but most conspicuously in music performance and dance. In the laboratory, it is most often studied in the form of finger tapping to a sequence of auditory stimuli. This review summarizes theories and empirical findings obtained with the tapping task. Its eight sections deal with the role of intention, rate limits, the negative mean asynchrony, variability, models of error correction, perturbation studies, neural correlates of SMS, and SMS in musical contexts. The central theoretical issue is considered to be how best to characterize the perceptual information and the internal processes that enable people to achieve and maintain SMS. Recent research suggests that SMS is controlled jointly by two error correction processes (phase correction and period correction) that differ in their degrees of cognitive control and may be associated with different brain circuits. They exemplify the general distinction between subconscious mechanisms of action regulation and conscious processes involved in perceptual judgment and action planning.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EYLWBHB4\\Repp - 2005 - Sensorimotor synchronization A review of the tapp.pdf},
  keywords = {Auditory Sequence,Experimental Brain Research,Music Perception,Phase Correction,Target Tone},
  langid = {english},
  number = {6}
}

@online{RespirationWingBeatUltrasonic,
  title = {Respiration, {{Wing}}-{{Beat}} and {{Ultrasonic Pulse Emission}} in an {{Echo}}-{{Locating Bat}} | {{Journal}} of {{Experimental Biology}}},
  url = {https://jeb.biologists.org/content/56/1/37},
  urldate = {2020-01-23},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QSGQKIY8\\37.html}
}

@online{RespiratoryMuscleActivity,
  title = {Respiratory Muscle Activity in Relation to Vocalization in Flying Bats. | {{Journal}} of {{Experimental Biology}}},
  url = {https://jeb.biologists.org/content/198/1/175},
  urldate = {2020-01-23},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7PLEZBXS\\175.html}
}

@article{richardsonEffectsVisualVerbal2005,
  title = {Effects of Visual and Verbal Interaction on Unintentional Interpersonal Coordination},
  author = {Richardson, Michael J. and Marsh, Kerry L. and Schmidt, R. C.},
  date = {2005-02},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {31},
  pages = {62--79},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.31.1.62},
  abstract = {Previous research has demonstrated that people's movements can become unintentionally coordinated during interpersonal interaction. The current study sought to uncover the degree to which visual and verbal (conversation) interaction constrains and organizes the rhythmic limb movements of coactors. Two experiments were conducted in which pairs of participants completed an interpersonal puzzle task while swinging handheld pendulums with instructions that minimized intentional coordination but facilitated either visual or verbal interaction. Cross-spectral analysis revealed a higher degree of coordination for conditions in which the pairs were visually coupled. In contrast, verbal interaction alone was not found to provide a sufficient medium for unintentional coordination to occur, nor did it enhance the unintentional coordination that emerged during visual interaction. The results raise questions concerning differences between visual and verbal informational linkages during interaction and how these differences may affect interpersonal movement production and its coordination.},
  eprint = {15709863},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ALUFZ92P\\Richardson et al. - 2005 - Effects of visual and verbal interaction on uninte.pdf},
  keywords = {Extremities,Female,Humans,Intention,Interpersonal Relations,Male,Movement,Periodicity,Verbal Behavior,Visual Perception},
  langid = {english},
  number = {1}
}

@software{richardsonPolhemusApplicationsExample2009,
  title = {Polhemus Applications and Example Code},
  author = {Richardson, M.},
  date = {2009},
  url = {http://xkiwilabs.com/softwa re-toolboxes/}
}

@article{richardsonRockingTogetherDynamics2007,
  title = {Rocking Together: Dynamics of Intentional and Unintentional Interpersonal Coordination},
  shorttitle = {Rocking Together},
  author = {Richardson, Michael J. and Marsh, Kerry L. and Isenhower, Robert W. and Goodman, Justin R. L. and Schmidt, R. C.},
  date = {2007-12},
  journaltitle = {Human Movement Science},
  shortjournal = {Hum Mov Sci},
  volume = {26},
  pages = {867--891},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2007.07.002},
  abstract = {The current study investigated the interpersonal coordination that occurred between two people when sitting side-by-side in rocking chairs. In two experiments participant pairs rocked in chairs that had the same or different natural periods. By instructing pairs to coordinate their movements inphase or antiphase, Experiment 1 investigated whether the stable patterns of intentional interpersonal coordination were consistent with the dynamics of within person interlimb coordination. By instructing the participants to rock at their own preferred tempo, Experiment 2 investigated whether the rocking chair movements of visually coupled individuals would become unintentionally coordinated. The degree to which the participants fixated on the movements of their co-actor was also manipulated to examine whether visual focus modulates the strength of interpersonal coordination. As expected, the patterns of coordination observed in both experiments demonstrated that the intentional and unintentional interpersonal coordination of rocking chair movements is constrained by the self-organizing dynamics of a coupled oscillator system. The results of the visual focus manipulations indicate that the stability of a visual interpersonal coupling is mediated by attention and the degree to which an individual is able to detect information about a co-actor's movements.},
  eprint = {17765345},
  eprinttype = {pmid},
  keywords = {Cooperative Behavior,Humans,Intention,Interpersonal Relations,Models; Biological,Movement,Visual Perception},
  langid = {english},
  number = {6}
}

@article{ripperdaSpeedingDetectionNoniconic2020,
  title = {Speeding up the Detection of Non-Iconic and Iconic Gestures ({{SPUDNIG}}): {{A}} Toolkit for the Automatic Detection of Hand Movements and Gestures in Video Data},
  shorttitle = {Speeding up the Detection of Non-Iconic and Iconic Gestures ({{SPUDNIG}})},
  author = {Ripperda, Jordy and Drijvers, Linda and Holler, Judith},
  date = {2020-08-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {52},
  pages = {1783--1794},
  issn = {1554-3528},
  doi = {10.3758/s13428-020-01350-2},
  url = {https://doi.org/10.3758/s13428-020-01350-2},
  urldate = {2020-12-29},
  abstract = {In human face-to-face communication, speech is frequently accompanied by visual signals, especially communicative hand gestures. Analyzing these visual signals requires detailed manual annotation of video data, which is often a labor-intensive and time-consuming process. To facilitate this process, we here present SPUDNIG (SPeeding Up the Detection of Non-iconic and Iconic Gestures), a tool to automatize the detection and annotation of hand movements in video data. We provide a detailed description of how SPUDNIG detects hand movement initiation and termination, as well as open-source code and a short tutorial on an easy-to-use graphical user interface (GUI) of our tool. We then provide a proof-of-principle and validation of our method by comparing SPUDNIG’s output to manual annotations of gestures by a human coder. While the tool does not entirely eliminate the need of a human coder (e.g., for false positives detection), our results demonstrate that SPUDNIG can detect both iconic and non-iconic gestures with very high accuracy, and could successfully detect all iconic gestures in our validation dataset. Importantly, SPUDNIG’s output can directly be imported into commonly used annotation tools such as ELAN and ANVIL. We therefore believe that SPUDNIG will be highly relevant for researchers studying multimodal communication due to its annotations significantly accelerating the analysis of large video corpora.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JQFPH7KV\\Ripperda et al. - 2020 - Speeding up the detection of non-iconic and iconic.pdf},
  langid = {english},
  number = {4}
}

@article{risueno-segoviaThetaSynchronizationPhonatory2020,
  title = {Theta Synchronization of Phonatory and Articulatory Systems in Marmoset Monkey Vocal Production},
  author = {Risueno-Segovia, Cristina and Hage, Steffen R.},
  date = {2020-09-03},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2020.08.019},
  url = {http://www.sciencedirect.com/science/article/pii/S0960982220311738},
  urldate = {2020-09-10},
  abstract = {Human speech shares a 3–8-Hz theta rhythm across all languages [1, 2, 3]. According to the frame/content theory of speech evolution, this rhythm corresponds to syllabic rates derived from natural mandibular-associated oscillations [4]. The underlying pattern originates from oscillatory movements of articulatory muscles [4, 5] tightly linked to periodic vocal fold vibrations [4, 6, 7]. Such phono-articulatory rhythms have been proposed as one of the crucial preadaptations for human speech evolution [3, 8, 9]. However, the evolutionary link in phono-articulatory rhythmicity between vertebrate vocalization and human speech remains unclear. From the phonatory perspective, theta oscillations might be phylogenetically preserved throughout all vertebrate clades [10, 11, 12]. From the articulatory perspective, theta oscillations are present in non-vocal lip smacking [1, 13, 14], teeth chattering [15], vocal lip smacking [16], and clicks and faux-speech [17] in non-human primates, potential evolutionary precursors for speech rhythmicity [1, 13]. Notably, a universal phono-articulatory rhythmicity similar to that in human speech is considered to be absent in non-human primate vocalizations, typically produced with sound modulations lacking concomitant articulatory movements [1, 9, 18]. Here, we challenge this view by investigating the coupling of phonatory and articulatory systems in marmoset vocalizations. Using quantitative measures of acoustic call structure, e.g., amplitude envelope, and call-associated articulatory movements, i.e., inter-lip distance, we show that marmosets display speech-like bi-motor rhythmicity. These oscillations are synchronized and phase locked at theta rhythms. Our findings suggest that oscillatory rhythms underlying speech production evolved early in the primate lineage, identifying marmosets as a suitable animal model to decipher the evolutionary and neural basis of coupled phono-articulatory movements.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2Z3QDZIB\\S0960982220311738.html},
  keywords = {articulation,human speech evolution,language,marmosets,phee call,phonation,phono-articulatory oscillations,theta rhythm,vocal segmentation,vocalization},
  langid = {english}
}

@article{robertsCommunicativeIntentionsWild2013,
  title = {Communicative Intentions in Wild Chimpanzees: Persistence and Elaboration in Gestural Signalling},
  shorttitle = {Communicative Intentions in Wild Chimpanzees},
  author = {Roberts, Anna Ilona and Vick, Sarah-Jane and Buchanan-Smith, Hannah M.},
  date = {2013-03},
  journaltitle = {Animal Cognition},
  shortjournal = {Anim Cogn},
  volume = {16},
  pages = {187--196},
  issn = {1435-9456},
  doi = {10.1007/s10071-012-0563-1},
  abstract = {We examine evidence for communicative intent during conspecific interactions in wild chimpanzees (Budongo Forest, Uganda), focusing on persistence in gestural communication. Previous research indicates that great apes have large gestural repertoires and produce gestural communication in a flexible and intentional manner, including the production of gesture sequences. Although there is a lack of consensus on the form and function of sequences, there is some evidence that sequences are produced when signallers fail to receive any response from a recipient. Here, we provide first systematic evidence for communicative persistence in wild chimpanzees. Rather than examining only the presence or absence of a response, we used the most commonly observed response to assign meanings to gestures and examined sequence production in relation to response congruency. Chimpanzees ceased communication if successful, but persevered when unsuccessful. Chimpanzees repeated gestures when a response partially matched their goal but substituted the original gesture when a response was incongruent. Persistence was also mediated by recipient intent to respond, with more sequences produced within competitive than affiliative contexts. Gestures within sequences were homogenous in semantic meaning and signallers continued until the response matched the assigned meaning of the initial gesture. Gestural sequence production was not primarily affective; gesture intensity (in terms of modality) did not increase within sequences. Chimpanzee gestural sequences emerged to achieve specific outcomes; given variability in recipient behaviour following initial gestures, signallers were flexible in their persistence towards these goals.},
  eprint = {23053796},
  eprinttype = {pmid},
  keywords = {Animal Communication,Animals,Animals; Wild,Female,Gestures,Intention,Male,Pan troglodytes},
  langid = {english},
  number = {2}
}

@article{robertsEffectsProcessingSequence2015,
  title = {The Effects of Processing and Sequence Organization on the Timing of Turn Taking: A Corpus Study},
  shorttitle = {The Effects of Processing and Sequence Organization on the Timing of Turn Taking},
  author = {Roberts, Seán G. and Torreira, Francisco and Levinson, Stephen C.},
  date = {2015},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00509},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00509/full},
  urldate = {2020-09-21},
  abstract = {The timing of turn taking in conversation is extremely rapid given the cognitive demands on speakers to comprehend, plan and execute turns in real time. Findings from psycholinguistics predict that the timing of turn taking is influenced by demands on processing, such as word frequency or syntactic complexity. An alternative view comes from the field of conversation analysis, which predicts that the rules of turn-taking and sequence organization may dictate the variation in gap durations (e.g. the functional role of each turn in communication). In this paper, we estimate the role of these two different kinds of factors in determining the speed of turn-taking in conversation. We use the Switchboard corpus of English telephone conversation, already richly annotated for syntactic structure speech act sequences, and segmental alignment. To this we add further information including Floor Transfer Offset (the amount of time between the end of one turn and the beginning of the next), word frequency, concreteness, and surprisal values. We then apply a novel statistical framework ('random forests') to show that these two dimensions are interwoven together with indexical properties of the speakers as explanatory factors determining the speed of response. We conclude that an explanation of the of the timing of turn taking will require insights from both processing and sequence organisation.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I2YPDZHV\\Roberts et al. - 2015 - The effects of processing and sequence organizatio.pdf},
  keywords = {concreteness,Frequency,processing,Random forests,Sequence organisation,surprisal,turn-taking},
  langid = {english}
}

@article{robertsSocialEcologicalComplexity,
  title = {Social and Ecological Complexity Is Associated with Gestural Repertoire Size of Wild Chimpanzees},
  author = {Roberts, Sam George Bradley and Roberts, Anna Ilona},
  journaltitle = {Integrative Zoology},
  volume = {n/a},
  issn = {1749-4877},
  doi = {10.1111/1749-4877.12423},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1749-4877.12423},
  urldate = {2019-11-30},
  abstract = {Increasing our understanding of primate gestural communication can provide new insights into language evolution. A key question in primate communication is the association between the social relationships of primates and their repertoire of gestures. Such analyses can reveal how primates use their repertoire of gestural communication to maintain their networks of family and friends, much as humans use language to maintain their social networks. In this study we examined the association between the repertoire of gestures (overall, manual and bodily gestures, gestures of different modalities) and social bonds (presence of reciprocated grooming), coordinated behaviours (travel, resting, co-feeding), and the complexity of ecology (e.g. noise, illumination) and sociality (party size, audience), in wild East African chimpanzees (Pan troglodytes schweinfurthii). A larger repertoire size of manual, visual gestures was associated with the presence of a relationship based on reciprocated grooming and increases in social complexity. A smaller repertoire of manual tactile gestures occurred when relationship was based on reciprocated grooming. A smaller repertoire of bodily gestures occurred between partners who jointly travelled for longer. Whereas gesture repertoire size was associated with social complexity, complex ecology also influenced repertoire size. The evolution of a large repertoire of manual, visual gestures may have been a key factor that enabled for larger social groups to emerge during evolution. Thus, the evolution of the larger brains in hominins may have co-occurred with an increase in the cognitive complexity underpinning gestural communication and this in turn may have enabled hominins to live in more complex social groups. This article is protected by copyright. All rights reserved.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ANUHUNCE\\1749-4877.html},
  keywords = {chimpanzee,ecology,gesture,repertoire size,social network analysis,sociality},
  langid = {english},
  number = {n/a}
}

@article{roch-levecqProductionBasicEmotions2006,
  title = {Production of Basic Emotions by Children with Congenital Blindness: {{Evidence}} for the Embodiment of Theory of Mind},
  shorttitle = {Production of Basic Emotions by Children with Congenital Blindness},
  author = {Roch‐Levecq, Anne-Catherine},
  date = {2006},
  journaltitle = {British Journal of Developmental Psychology},
  volume = {24},
  pages = {507--528},
  issn = {2044-835X},
  doi = {10.1348/026151005X50663},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/026151005X50663},
  urldate = {2019-11-22},
  abstract = {Children with congenital blindness are delayed in understanding other people's minds. The present study examined whether this delay was related to a more primitive form of inter-subjectivity by which infants draw correspondence between parental mirroring of the infant's display and proprioceptive sensations. Twenty children with congenital blindness and 20 typically-developing sighted children aged between 4 and 12 years were administered a series of tasks examining false belief and emotion understanding and production. The blind children scored lower on the false belief tasks and did not convey emotions facially to adult observers as accurately as sighted participants. The adults' ratings of the children's expressions were correlated with the children's scores on the false belief tasks. It is suggested that understanding people's minds might be anchored in primitive embodied forms of relatedness.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VMDL76VZ\\Roch‐Levecq - 2006 - Production of basic emotions by children with cong.pdf;C\:\\Users\\u668173\\Zotero\\storage\\HJA4M7WY\\026151005X50663.html},
  langid = {english},
  number = {3}
}

@article{rochet-capellanSpeechFocusPosition2008,
  title = {The Speech Focus Position Effect on Jaw–Finger Coordination in a Pointing Task},
  author = {Rochet-Capellan, A. and Laboissière, R. and Galván, A. and Schwartz, J.},
  date = {2008-12-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {51},
  pages = {1507--1521},
  doi = {10.1044/1092-4388(2008/07-0173)},
  url = {https://jslhr.pubs.asha.org/doi/full/10.1044/1092-4388%282008/07-0173%29},
  urldate = {2019-04-18},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\22HUYGHV\\Rochet-Capellan Amélie et al. - 2008 - The Speech Focus Position Effect on Jaw–Finger Coo.pdf;C\:\\Users\\u668173\\Zotero\\storage\\6CD9L55U\\07-0173).html},
  number = {6}
}

@article{rochet-capellanTakeBreathTake2014,
  title = {Take a Breath and Take the Turn: How Breathing Meets Turns in Spontaneous Dialogue},
  shorttitle = {Take a Breath and Take the Turn},
  author = {Rochet-Capellan, A. and Fuchs, S.},
  date = {2014-12-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0399},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4240966/},
  urldate = {2019-11-16},
  abstract = {Physiological rhythms are sensitive to social interactions and could contribute to defining social rhythms. Nevertheless, our knowledge of the implications of breathing in conversational turn exchanges remains limited. In this paper, we addressed the idea that breathing may contribute to timing and coordination between dialogue partners. The relationships between turns and breathing were analysed in unconstrained face-to-face conversations involving female speakers. No overall relationship between breathing and turn-taking rates was observed, as breathing rate was specific to the subjects' activity in dialogue (listening versus taking the turn versus holding the turn). A general inter-personal coordination of breathing over the whole conversation was not evident. However, specific coordinative patterns were observed in shorter time-windows when participants engaged in taking turns. The type of turn-taking had an effect on the respective coordination in breathing. Most of the smooth and interrupted turns were taken just after an inhalation, with specific profiles of alignment to partner breathing. Unsuccessful attempts to take the turn were initiated late in the exhalation phase and with no clear inter-personal coordination. Finally, breathing profiles at turn-taking were different than those at turn-holding. The results support the idea that breathing is actively involved in turn-taking and turn-holding.},
  eprint = {25385777},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GWEBTNTH\\Rochet-Capellan and Fuchs - 2014 - Take a breath and take the turn how breathing mee.pdf},
  number = {1658},
  pmcid = {PMC4240966}
}

@article{roheNeuralDynamicsHierarchical2019,
  title = {The Neural Dynamics of Hierarchical {{Bayesian}} Causal Inference in Multisensory Perception},
  author = {Rohe, Tim and Ehlis, Ann-Christine and Noppeney, Uta},
  date = {2019-04-23},
  journaltitle = {Nature Communications},
  volume = {10},
  pages = {1907},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09664-2},
  url = {https://www.nature.com/articles/s41467-019-09664-2},
  urldate = {2020-12-05},
  abstract = {Transforming the barrage of sensory signals into a coherent multisensory percept relies on solving the binding problem – deciding whether signals come from a common cause and should be integrated or, instead, segregated. Human observers typically arbitrate between integration and segregation consistent with Bayesian Causal Inference, but the neural mechanisms remain poorly understood. Here, we presented people with audiovisual sequences that varied in the number of flashes and beeps, then combined Bayesian modelling and EEG representational similarity analyses. Our data suggest that the brain initially represents the number of flashes and beeps independently. Later, it computes their numbers by averaging the forced-fusion and segregation estimates weighted by the probabilities of common and independent cause models (i.e. model averaging). Crucially, prestimulus oscillatory alpha power and phase correlate with observers’ prior beliefs about the world’s causal structure that guide their arbitration between sensory integration and segregation.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MST4XUCY\\Rohe et al. - 2019 - The neural dynamics of hierarchical Bayesian causa.pdf;C\:\\Users\\u668173\\Zotero\\storage\\94LL9HZ6\\s41467-019-09664-2.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@article{rohrerAvoidingSpuriousSubmovement2003,
  title = {Avoiding Spurious Submovement Decompositions: A Globally Optimal Algorithm},
  shorttitle = {Avoiding Spurious Submovement Decompositions},
  author = {Rohrer, Brandon and Hogan, Neville},
  date = {2003-09-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybern.},
  volume = {89},
  pages = {190--199},
  issn = {1432-0770},
  doi = {10.1007/s00422-003-0428-4},
  url = {https://doi.org/10.1007/s00422-003-0428-4},
  urldate = {2020-01-31},
  abstract = {Evidence for the existence of discrete submovements underlying continuous human movement has motivated many attempts to “extract” them. Although they produce visually convincing results, all of the methodologies that have been employed are prone to produce spurious decompositions. Examples of potential failures are given. A branch-and-bound algorithm for submovement extraction, capable of global nonlinear minimization (and hence capable of avoiding spurious decompositions), is developed and demonstrated.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZJISG8WJ\\Rohrer and Hogan - 2003 - Avoiding spurious submovement decompositions a gl.pdf},
  langid = {english},
  number = {3}
}

@article{rombergStatisticalLearningLanguage2010,
  title = {Statistical Learning and Language Acquisition},
  author = {Romberg, Alexa R. and Saffran, Jenny R.},
  date = {2010},
  journaltitle = {WIREs Cognitive Science},
  volume = {1},
  pages = {906--914},
  issn = {1939-5086},
  doi = {10.1002/wcs.78},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.78},
  urldate = {2021-01-23},
  abstract = {Human learners, including infants, are highly sensitive to structure in their environment. Statistical learning refers to the process of extracting this structure. A major question in language acquisition in the past few decades has been the extent to which infants use statistical learning mechanisms to acquire their native language. There have been many demonstrations showing infants' ability to extract structures in linguistic input, such as the transitional probability between adjacent elements. This paper reviews current research on how statistical learning contributes to language acquisition. Current research is extending the initial findings of infants' sensitivity to basic statistical information in many different directions, including investigating how infants represent regularities, learn about different levels of language, and integrate information across situations. These current directions emphasize studying statistical language learning in context: within language, within the infant learner, and within the environment as a whole. WIREs Cogn Sci 2010 1 906–914 This article is categorized under: Linguistics {$>$} Language Acquisition Psychology {$>$} Language},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.78},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\N627642D\\Romberg and Saffran - 2010 - Statistical learning and language acquisition.pdf;C\:\\Users\\u668173\\Zotero\\storage\\B9K9Y9GS\\wcs.html},
  langid = {english},
  number = {6}
}

@online{rongFrankMocapFastMonocular2020,
  title = {{{FrankMocap}}: {{Fast Monocular 3D Hand}} and {{Body Motion Capture}} by {{Regression}} and {{Integration}}},
  shorttitle = {{{FrankMocap}}},
  author = {Rong, Yu and Shiratori, Takaaki and Joo, Hanbyul},
  date = {2020-08-19},
  url = {http://arxiv.org/abs/2008.08324},
  urldate = {2021-03-02},
  abstract = {Although the essential nuance of human motion is often conveyed as a combination of body movements and hand gestures, the existing monocular motion capture approaches mostly focus on either body motion capture only ignoring hand parts or hand motion capture only without considering body motion. In this paper, we present FrankMocap, a motion capture system that can estimate both 3D hand and body motion from in-the-wild monocular inputs with faster speed (9.5 fps) and better accuracy than previous work. Our method works in near real-time (9.5 fps) and produces 3D body and hand motion capture outputs as a unified parametric model structure. Our method aims to capture 3D body and hand motion simultaneously from challenging in-the-wild monocular videos. To construct FrankMocap, we build the state-of-the-art monocular 3D "hand" motion capture method by taking the hand part of the whole body parametric model (SMPL-X). Our 3D hand motion capture output can be efficiently integrated to monocular body motion capture output, producing whole body motion results in a unified parrametric model structure. We demonstrate the state-of-the-art performance of our hand motion capture system in public benchmarks, and show the high quality of our whole body motion capture result in various challenging real-world scenes, including a live demo scenario.},
  archiveprefix = {arXiv},
  eprint = {2008.08324},
  eprinttype = {arxiv},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PP7HTD6J\\Rong et al. - 2020 - FrankMocap Fast Monocular 3D Hand and Body Motion.pdf;C\:\\Users\\u668173\\Zotero\\storage\\T3JVXY8S\\2008.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{roschWaveletCompGuidedTour2014,
  title = {{{WaveletComp}} 1.1: {{A}} Guided Tour through the {{R}} Package},
  author = {Rosch, Angi and Schmidbauer, Harald},
  date = {2014},
  pages = {59},
  abstract = {WaveletComp is an R package for continuous wavelet-based analysis of univariate and bivariate time series. Wavelet functions are implemented in WaveletComp such that a wide range of intermediate and final results are easily accessible. The null hypothesis that there is no (joint) periodicity in the series is tested via p-values obtained from simulation, where the model to be simulated can be chosen from a wide variety of options. The reconstruction, and thus filtering, of a given series from its wavelet decomposition, subject to a range of possible constraints, is also possible. WaveletComp provides extended plotting functionality — which objects should be added to a plot (for example, the ridge of wavelet power, contour lines indicating significant periodicity, arrows indicating the leading/lagging series), which kind and degree of smoothing is desired in wavelet coherence plots, which color palette to use, how to define the layout of the time axis (using POSIXct conventions), and others. Technically, we have developed vector- and matrix-based implementations of algorithms to reduce computation time. Easy and intuitive handling was given high priority.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FXBWTB7H\\Rosch and Schmidbauer - WaveletComp 1.1 A guided tour through the R packa.pdf},
  langid = {english}
}

@article{rosenbaumProblemSerialOrder2007,
  title = {The Problem of Serial Order in Behavior: {{Lashley}}'s Legacy},
  shorttitle = {The Problem of Serial Order in Behavior},
  author = {Rosenbaum, David A. and Cohen, Rajal G. and Jax, Steven A. and Weiss, Daniel J. and van der Wel, Robrecht},
  date = {2007-08},
  journaltitle = {Human Movement Science},
  shortjournal = {Hum Mov Sci},
  volume = {26},
  pages = {525--554},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2007.04.001},
  abstract = {In a prescient paper Karl Lashley (1951) rejected reflex chaining accounts of the sequencing of behavior and argued instead for a more cognitive account in which behavioral sequences are typically controlled with central plans. An important feature of such plans, according to Lashley, is that they are hierarchical. Lashley offered several sources of evidence for the hierarchical organization for behavioral plans, and others afterward provided more evidence for this hypothesis. We briefly review that evidence here and then shift from a focus on the structure of plans (Lashley's point of concentration) to the processes by which plans are formed in real time. Two principles emerge from the studies we review. One is that plans are not formed from scratch for each successive movement sequence but instead are formed by making whatever changes are needed to distinguish the movement sequence to be performed next from the movement sequence that has just been performed. This plan-modification view is supported by two phenomena discovered in our laboratory: the parameter remapping effect, and the handpath priming effect. The other principle we review is that even single movements appear to be controlled with hierarchically organized plans. At the top level are the starting and goal postures. At the lower level are the intermediate states comprising the transition from the starting posture to the goal posture. The latter principle is supported by another phenomenon discovered in our lab, the end-state comfort effect, and by a computational model of motor planning which accounts for a large number of motor phenomena. Interestingly, the computational model hearkens back to a classical method of generating cartoon animations that relies on the production of keyframes first and the production of interframes (intermediate frames) second.},
  eprint = {17698232},
  eprinttype = {pmid},
  keywords = {Animals,Attention,Biomechanical Phenomena,Hand Strength,Handwriting,Humans,Mental Recall,Motor Cortex,Movement,Neurons,Orientation,Posture,Practice; Psychological,Psychomotor Performance,Psychophysics,Serial Learning,Time Perception},
  langid = {english},
  number = {4},
  options = {useprefix=true}
}

@article{rosenbaumProblemSerialOrder2007a,
  title = {The Problem of Serial Order in Behavior: {{Lashley}}’s Legacy},
  shorttitle = {The Problem of Serial Order in Behavior},
  author = {Rosenbaum, David A. and Cohen, Rajal G. and Jax, Steven A. and Weiss, Daniel J. and van der Wel, Robrecht},
  date = {2007-08},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {26},
  pages = {525--554},
  issn = {01679457},
  doi = {10.1016/j.humov.2007.04.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167945707000280},
  urldate = {2020-12-27},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DWESJHMP\\Rosenbaum et al. - 2007 - The problem of serial order in behavior Lashley’s.pdf},
  langid = {english},
  number = {4},
  options = {useprefix=true}
}

@article{roubeauElectromyographicActivityStrap1997,
  title = {Electromyographic Activity of Strap and Cricothyroid Muscles in Pitch Change},
  author = {Roubeau, B. and Chevrie-Muller, C. and Lacau Saint Guily, J.},
  date = {1997-05},
  journaltitle = {Acta Oto-Laryngologica},
  volume = {117},
  pages = {459--464},
  issn = {0001-6489},
  doi = {10.3109/00016489709113421},
  abstract = {The EMG activity of the cricothyroid muscle (CT) and the three extrinsic laryngeal muscles (thyohyoid, TH; sternothyroid, ST, and sternohyoid, SH) were recorded throughout the voice range of one female and one male subject, both untrained singers. The voice range was examined using rising and falling glissandos (production of a sustained sound with progressive and continuous variation of fundamental frequency). Muscle activity was observed at various pitches during the glissandos. The strap muscle activity during the production of glissandos appears to be synergistic. At the lowest frequency, the CT is inactive but strap muscles (TH, ST, SH) are active. As frequency increases, strap muscle activity decreases while the CT controls frequency in the middle of the range. At higher frequencies the strap muscles once again become active. This activity might depend on the vocal vibratory mechanism involved. The role of the strap muscles at high pitches is a widely debated point but it seems that in some way they control the phenomena relevant to the rising pitch. The phasic-type strap muscle activity contrasts with the tonic-type activity of the CT. The CT closely controls the frequency, while the straps are not directly linked to the pitch but rather to the evolution of the frequency of voice production (speaking voice, singing voice, held notes, glissandos, trillo, vibrato, etc.).},
  eprint = {9199535},
  eprinttype = {pmid},
  keywords = {Adult,Electromyography,Female,Humans,Laryngeal Muscles,Male,Muscle Contraction,Neck Muscles,Phonation,Voice},
  langid = {english},
  number = {3}
}

@article{royVocalControlCommon2011,
  title = {Vocal Control by the Common Marmoset in the Presence of Interfering Noise},
  author = {Roy, Sabyasachi and Miller, Cory T. and Gottsch, Dane and Wang, Xiaoqin},
  date = {2011-11-01},
  journaltitle = {Journal of Experimental Biology},
  volume = {214},
  pages = {3619--3629},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.056101},
  url = {https://jeb.biologists.org/content/214/21/3619},
  urldate = {2020-12-12},
  abstract = {Skip to Next Section The natural environment is inherently noisy with acoustic interferences. It is, therefore, beneficial for a species to modify its vocal production to effectively communicate in the presence of interfering noises. Non-human primates have been traditionally considered to possess limited voluntary vocal control, but little is known about their ability to modify vocal behavior when encountering interfering noises. Here we tested the ability of the common marmoset (Callithrix jacchus) to control the initiation of vocalizations and maintain vocal interactions between pairs in an acoustic environment in which the length and predictability (periodic or random aperiodic occurrences) of interfering noise bursts were varied. Despite the presence of interfering noise, the marmosets continued to engage in antiphonal calling behavior. Results showed that the overwhelming majority of calls were initiated during silence gaps even when the length of the silence gap following each noise burst was unpredictable. During the periodic noise conditions, as the length of the silence gap decreased, the latency between the end of noise burst and call onset decreased significantly. In contrast, when presented with aperiodic noise bursts, the marmosets chose to call predominantly during long (4 and 8 s) over short (2 s) silence gaps. In the 8 s periodic noise conditions, a marmoset pair either initiated both calls of an antiphonal exchange within the same silence gap or exchanged calls in two consecutive silence gaps. Our findings provide compelling evidence that common marmosets are capable of modifying their vocal production according to the dynamics of their acoustic environment during vocal communication.},
  eprint = {21993791},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8LRAWRMG\\Roy et al. - 2011 - Vocal control by the common marmoset in the presen.pdf},
  langid = {english},
  number = {21}
}

@article{royWalkingMultisensoryBeat2017,
  title = {Walking to a Multisensory Beat},
  author = {Roy, Charlotte and Lagarde, Julien and Dotov, Dobromir and Dalla Bella, Simone},
  date = {2017-04-01},
  journaltitle = {Brain and Cognition},
  shortjournal = {Brain and Cognition},
  volume = {113},
  pages = {172--183},
  issn = {0278-2626},
  doi = {10.1016/j.bandc.2017.02.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0278262616301877},
  urldate = {2020-12-07},
  abstract = {Living in a complex and multisensory environment demands constant interaction between perception and action. In everyday life it is common to combine efficiently simultaneous signals coming from different modalities. There is evidence of a multisensory benefit in a variety of laboratory tasks (temporal judgement, reaction time tasks). It is less clear if this effect extends to ecological tasks, such as walking. Furthermore, benefits of multimodal stimulation are linked to temporal properties such as the temporal window of integration and temporal recalibration. These properties have been examined in tasks involving single, non-repeating stimulus presentations. Here we investigate the same temporal properties in the context of a rhythmic task, namely audio-tactile stimulation during walking. The effect of audio-tactile rhythmic cues on gait variability and the ability to synchronize to the cues was studied in young adults. Participants walked with rhythmic cues presented at different stimulus-onset asynchronies. We observed a multisensory benefit by comparing audio-tactile to unimodal stimulation. Moreover, both the temporal window of integration and temporal recalibration mediated the response to multimodal stimulation. In sum, rhythmic behaviours obey the same principles as temporal discrimination and detection behaviours and thus can also benefit from multimodal stimulation.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BD73PKQK\\S0278262616301877.html},
  keywords = {Gait,Multisensory benefit,Rhythm,Sensorimotor synchronization,Temporal recalibration,Temporal window of integration},
  langid = {english}
}

@article{rucinskaSocialEnactivePerspectives2019,
  title = {Social and {{Enactive Perspectives}} on {{Pretending}} | {{Avant}}},
  author = {Rucińska, Z.},
  date = {2019},
  journaltitle = {Avant},
  volume = {X},
  url = {http://avant.edu.pl/en/2019-03-15},
  urldate = {2020-02-20},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HQRBYRKS\\2019-03-15.html},
  langid = {american},
  number = {3}
}

@online{ruiterProductionGestureSpeech2000,
  title = {The Production of Gesture and Speech},
  author = {de Ruiter, Jan Peter},
  date = {2000-08},
  doi = {10.1017/CBO9780511620850.018},
  url = {/core/books/language-and-gesture/production-of-gesture-and-speech/7703D35DC0D8F631AD0E7525AB363841},
  urldate = {2019-05-04},
  abstract = {{$<$}div class="abstract" data-abstract-type="normal"{$><$}p{$>$}Introduction{$<$}/p{$><$}p{$>$}Research topics in the field of speech-related gesture that have received considerable attention are the function of gesture, its synchronization with speech, and its semiotic properties. While the findings of these studies often have interesting implications for theories about the processing of gesture in the human brain, few studies have addressed this issue in the framework of information processing.{$<$}/p{$><$}p{$>$}In this chapter, I will present a general processing architecture for gesture production. It can be used as a starting point for investigating the processes and representations involved in gesture and speech. For convenience, I will use the term ‘model'when referring to ‘processing architecture’ throughout this chapter.{$<$}/p{$><$}p{$>$}Since the use of information-processing models is not believed by every gesture researcher to be an appropriate way of investigating gesture (see, e.g., McNeill 1992), I will first argue that information-processing models are essential theoretical tools for understanding the processing involved in gesture and speech. I will then proceed to formulate a new model for the production of gesture and speech, called the Sketch Model. It is an extension of Levelt's (1989) model for speech production. The modifications and additions to Levelt's model are discussed in detail. At the end of the section, the working of the Sketch Model is demonstrated, using a number of illustrative gesture/speech fragments as examples.{$<$}/p{$><$}p{$>$}Subsequently, I will compare the Sketch Model with both McNeill's (1992) growth-point theory and with the information-processing model by Krauss, Chen \& Gottesman (this volume). While the Sketch Model and the model by Krauss et al. are formulated within the same framework, they are based on fundamentally different assumptions.{$<$}/p{$><$}/div{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9FX5U9BF\\7703D35DC0D8F631AD0E7525AB363841.html},
  langid = {english},
  organization = {{Language and Gesture}}
}

@article{ruiterProjectingEndSpeaker2006,
  title = {Projecting the {{End}} of a {{Speaker}}'s {{Turn}}: {{A Cognitive Cornerstone}} of {{Conversation}}},
  shorttitle = {Projecting the {{End}} of a {{Speaker}}'s {{Turn}}},
  author = {de Ruiter, Jan-Peter and Mitterer, Holger. and Enfield, N. J.},
  date = {2006},
  journaltitle = {Language},
  volume = {82},
  pages = {515--535},
  issn = {1535-0665},
  doi = {10.1353/lan.2006.0130},
  url = {http://muse.jhu.edu/content/crossref/journals/language/v082/82.3de_ruiter.pdf},
  urldate = {2020-09-21},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\SFSZ3PSY\\Ruiter et al. - 2006 - Projecting the End of a Speaker's Turn A Cognitiv.pdf},
  langid = {english},
  number = {3}
}

@article{runesonKinematicSpecificationDynamics1983,
  title = {Kinematic {{Specification}} of {{Dynamics}} as an {{Informational Basis}} for {{Person}}-and-{{Action Perception}}: {{Expectation}}, {{Gender Recognition}}, and {{Deceptive Intention}}},
  author = {Runeson, Sverker and Frykholm, Gunilla},
  date = {1983},
  journaltitle = {Journal of Experimental Psychology: General},
  pages = {31},
  abstract = {The widespread conviction that perceiving another person must rest on ambiguous and fakeable information is challenged. Arguingfrom biomechanical necessities inherent in maintaining balance and coping with reactive impulses, we show that the detailed kinematic pattern is specific to an acting person's anatomical makeup and to the working of his or her motor control system. In this way information is potentially available about gender, identity, expectations, intentions, and what the person is in fact doing. We invoke the lawfulness of human movement, as elucidated by recent advances in motor control theory, to demonstrate the virtual impossibility of performing truly deceptive movements and to argue in general terms for the specification power inherent in human kinematics. The outcome of the analysis is subsumed under a principle of kinematic specification of dynamics (KSD), which states that movements specify the causal factors of events. Generally, a linked multiple degrees-of-freedom system does not exhibit substitutability; a change in one of its "input" factors cannot substitute for, or cancel, the multivariable effects of a change in another factor.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DTGJMNFR\\Runeson and Frykholm - Kinematic Specification of Dynamics as an Informat.pdf},
  langid = {english}
}

@article{rusiewiczEffectsProsodyPosition2013,
  title = {Effects of Prosody and Position on the Timing of Deictic Gestures},
  author = {Rusiewicz, H. L. and Susan, S. and Iverson, J. and Szuminsky, N.},
  date = {2013-04-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {56},
  pages = {458--470},
  doi = {10.1044/1092-4388(2012/11-0283)},
  url = {https://jslhr.pubs.asha.org/doi/full/10.1044/1092-4388%282012/11-0283%29},
  urldate = {2019-04-18},
  abstract = {Purpose       In this study, the authors investigated the hypothesis that the perceived tight temporal          synchrony of speech and gesture is evidence of an integrated spoken language and manual          gesture communication system. It was hypothesized that experimental manipulations          of the spoken response would affect the timing of deictic gestures.                     Method       The authors manipulated syllable position and contrastive stress in compound words          in multiword utterances by using a repeated-measures design to investigate the degree          of synchronization of speech and pointing gestures produced by 15 American English          speakers. Acoustic measures were compared with the gesture movement recorded via capacitance.                     Results       Although most participants began a gesture before the target word, the temporal parameters          of the gesture changed as a function of syllable position and prosody. Syllables with          contrastive stress in the 2nd position of compound words were the longest in duration          and also most consistently affected the timing of gestures, as measured by several          dependent measures.                     Conclusion       Increasing the stress of a syllable significantly affected the timing of a corresponding          gesture, notably for syllables in the 2nd position of words that would not typically          be stressed. The findings highlight the need to consider the interaction of gestures          and spoken language production from a motor-based perspective of coordination.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EUJRIGW6\\Rusiewicz Heather Leavy et al. - 2013 - Effects of Prosody and Position on the Timing of D.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ER26DFTE\\11-0283).html},
  number = {2}
}

@incollection{rusiewiczSetTime2018,
  title = {Set in Time},
  booktitle = {The {{Development}} of Prosody in First Language Acquisition},
  author = {Rusiewicz, H. L. and Esteve-Gibert, N.},
  date = {2018},
  pages = {103}
}

@article{ryanEnergyCallingSelection1988,
  title = {Energy, {{Calling}}, and {{Selection}}},
  author = {Ryan, Michael J.},
  date = {1988},
  journaltitle = {American Zoologist},
  volume = {28},
  pages = {885--898},
  publisher = {{Oxford University Press}},
  issn = {0003-1569},
  abstract = {Acoustic signals often mediate the mating process and are under selection through the action of female choice. Acoustic signalling requires relatively large amounts of energy input, but metabolic energy is coupled to acoustic energy inefficiently. Although not necessarily a cause and effect relationship, females often prefer signals with more energy. Females may prefer more intense calls, more complicated calls, or calls produced at a greater repetition rate. I discuss various evolutionary changes that could increase acoustic energy received by the female and examine how these changes are influenced by other factors inherent to communication systems: signal radiation, species recognition, sexual selection, the physiology of the receptor system, and environmental bioacoustics. I conclude that these factors constrain the ability of the animal to maximize energy received by the female. I then consider how two hypotheses, the good genes hypothesis and the runaway sexual selection hypothesis, attempt to explain the evolution of female choice for signals with greater energy content.},
  eprint = {3883387},
  eprinttype = {jstor},
  number = {3}
}

@article{sacksSimplestSystematicsOrganization1974,
  title = {A {{Simplest}} Systematics for the Organization of Turn-Taking for Conversation},
  author = {Sacks, H. and Schegloff, E. A. and Jefferson, G.},
  date = {1974},
  journaltitle = {Language},
  volume = {50},
  pages = {696--735},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4T9JR35H\\1974 - A Simplest Systematics for the Organization of Tur.pdf},
  langid = {english},
  number = {4}
}

@article{saffranAbsolutePitchInfant2001,
  title = {Absolute Pitch in Infant Auditory Learning: {{Evidence}} for Developmental Reorganization},
  shorttitle = {Absolute Pitch in Infant Auditory Learning},
  author = {Saffran, Jenny R. and Griepentrog, Gregory J.},
  date = {2001},
  journaltitle = {Developmental Psychology},
  volume = {37},
  pages = {74--85},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-0599(Electronic),0012-1649(Print)},
  doi = {10.1037/0012-1649.37.1.74},
  abstract = {To what extent do infants represent the absolute pitches of complex auditory stimuli? Two experiments with 8-month-old infants examined the use of absolute and relative pitch cues in a tone-sequence statistical learning task. The results suggest that, given unsegmented stimuli that do not conform to the rules of musical composition, infants are more likely to track patterns of absolute pitches than of relative pitches. A 3rd experiment tested adults with or without musical training on the same statistical learning tasks used in the infant experiments. Unlike the infants, adult listeners relied primarily on relative pitch cues. These results suggest a shift from an initial focus on absolute pitch to the eventual dominance of relative pitch, which, it is argued, is more useful for both music and speech processing. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RH8ZH5H4\\2000-14218-007.html},
  keywords = {Age Differences,Auditory Stimulation,Cues,Learning,Pitch Discrimination},
  number = {1}
}

@article{saffranStatisticalLearning8MonthOld1996,
  title = {Statistical {{Learning}} by 8-{{Month}}-{{Old Infants}}},
  author = {Saffran, Jenny R. and Aslin, Richard N. and Newport, Elissa L.},
  date = {1996-12-13},
  journaltitle = {Science},
  volume = {274},
  pages = {1926--1928},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.274.5294.1926},
  url = {https://science.sciencemag.org/content/274/5294/1926},
  urldate = {2021-01-23},
  abstract = {Learners rely on a combination of experience-independent and experience-dependent mechanisms to extract information from the environment. Language acquisition involves both types of mechanisms, but most theorists emphasize the relative importance of experience-independent mechanisms. The present study shows that a fundamental task of language acquisition, segmentation of words from fluent speech, can be accomplished by 8-month-old infants based solely on the statistical relationships between neighboring speech sounds. Moreover, this word segmentation was based on statistical learning from only 2 minutes of exposure, suggesting that infants have access to a powerful mechanism for the computation of statistical properties of the language input.},
  eprint = {8943209},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2IDENMPK\\1926.html},
  langid = {english},
  number = {5294}
}

@article{sainburgLatentSpaceVisualization2019,
  title = {Latent Space Visualization, Characterization, and Generation of Diverse Vocal Communication Signals},
  author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q.},
  date = {2019-12-11},
  journaltitle = {bioRxiv},
  pages = {870311},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/870311},
  url = {https://www.biorxiv.org/content/10.1101/870311v1},
  urldate = {2021-02-19},
  abstract = {{$<$}h3{$>$}ABSTRACT{$<$}/h3{$>$} {$<$}p{$>$}Animals produce vocalizations that range in complexity from a single repeated call to hundreds of unique vocal elements patterned in sequences unfolding over hours. Characterizing complex vocalizations can require considerable effort and a deep intuition about each species’ vocal behavior. Even with a great deal of experience, human characterizations of animal communication can be affected by human perceptual biases. We present here a set of computational methods that center around projecting animal vocalizations into low dimensional latent representational spaces that are directly learned from data. We apply these methods to diverse datasets from over 20 species, including humans, bats, songbirds, mice, cetaceans, and nonhuman primates, enabling high-powered comparative analyses of unbiased acoustic features in the communicative repertoires across species. Latent projections uncover complex features of data in visually intuitive and quantifiable ways. We introduce methods for analyzing vocalizations as both discrete sequences and as continuous latent variables. Each method can be used to disentangle complex spectro-temporal structure and observe long-timescale organization in communication. Finally, we show how systematic sampling from latent representational spaces of vocalizations enables comprehensive investigations of perceptual and neural representations of complex and ecologically relevant acoustic feature spaces.{$<$}/p{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UK8N2DWX\\Sainburg et al. - 2019 - Latent space visualization, characterization, and .pdf;C\:\\Users\\u668173\\Zotero\\storage\\LYQKH2IZ\\870311v1.html},
  langid = {english}
}

@article{saltzmanDynamicalApproachGestural1989,
  title = {A Dynamical Approach to Gestural Patterning in Speech Production},
  author = {Saltzman, Elliot L. and Munhall, Kevin G.},
  date = {1989},
  journaltitle = {Ecological Psychology},
  volume = {1},
  pages = {333--382},
  publisher = {{Lawrence Erlbaum}},
  location = {{US}},
  issn = {1532-6969(Electronic),1040-7413(Print)},
  doi = {10.1207/s15326969eco0104_2},
  abstract = {Reviews a task-dynamic model for coordination and control of speech articulators. An extension of this model is described in which invariant speech units are identified with context-independent sets of parameters in a dynamical system having 2 functionally distinct but interacting levels. The intergestural level is defined according to a set of activation coordinates; the interarticulator level is defined according to both model articulator and tract-variable coordinates. Coproduction effects in speech are described in terms of blending dynamics defined among a set of temporally overlapping active units; relative timing of speech gestures is formulated in terms of serial dynamics that shape temporal patterning of onsets and offsets in unit activations. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2SEKF3CJ\\1990-30025-001.html},
  keywords = {Articulation (Speech),Gestures,Models},
  number = {4}
}

@inproceedings{saltzmanTaskdynamicToolkitModeling2008,
  title = {A Task-Dynamic Toolkit for Modeling the Effects of Prosodic Structure on Articulation},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Speech Prosody}}},
  author = {Saltzman, Elliot and Nam, Hosung and Krivokapic, Jelena and Goldstein, Louis},
  date = {2008},
  pages = {175--184},
  location = {{Campina, Brazil}},
  eventtitle = {Speech {{Prosody}} 2009},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9AXZRZSU\\Saltzman et al. - A task-dynamic toolkit for modeling the effects of.pdf},
  langid = {english}
}

@article{samuelPerceptualLearningSpeech2009,
  title = {Perceptual Learning for Speech},
  author = {Samuel, Arthur G. and Kraljic, Tanya},
  date = {2009-08},
  journaltitle = {Attention, Perception, \& Psychophysics},
  volume = {71},
  pages = {1207--1218},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/APP.71.6.1207},
  url = {http://www.springerlink.com/index/10.3758/APP.71.6.1207},
  urldate = {2019-05-07},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BJFJMQS5\\Samuel and Kraljic - 2009 - Perceptual learning for speech.pdf},
  langid = {english},
  number = {6}
}

@article{sanbornEmergentMotorVocalCoordination2015,
  title = {Emergent {{Motor}}-{{Vocal Coordination}} in {{Pre}}-Linguistic {{Infants}}},
  author = {Sanborn, Sarah},
  date = {2015-07-21},
  journaltitle = {Doctoral Dissertations},
  url = {https://opencommons.uconn.edu/dissertations/816},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WYH9XZ74\\816.html}
}

@article{sandlerBodyEvidenceNature2018,
  title = {The {{Body}} as {{Evidence}} for the {{Nature}} of {{Language}}},
  author = {Sandler, Wendy},
  date = {2018-10-29},
  journaltitle = {Frontiers in Psychology},
  volume = {9},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.01782},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2018.01782/full},
  urldate = {2020-06-08},
  abstract = {Taking its cue from sign languages, this paper proposes that the recruitment and composition of body actions provide evidence for key properties of language and its emergence. Adopting the view that compositionality is the fundamental organizing property of language, we show first that actions of the hands, face, head, and torso in sign languages directly reflect linguistic components, and illuminate certain aspects of compositional organization among them that are relevant for all languages, signed and spoken. Studies of emerging sign languages strengthen the approach by showing that the gradual recruitment of bodily articulators for linguistic functions directly maps the way in which a new language increases in complexity and efficiency over time. While compositional communication is almost exclusively restricted to humans, it is not restricted to language. In the spontaneous, intense emotional displays of athletes, different emotional states are correlated with actions of particular face and body features and feature groupings. These findings indicate a much more ancient communicative compositional capacity, and support a paradigm that includes visible body actions in the quest for core linguistic properties and their origins.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\E3763AJ7\\Sandler - 2018 - The Body as Evidence for the Nature of Language.pdf},
  langid = {english}
}

@article{satoAllAspectsLearning2020,
  title = {Do All Aspects of Learning Benefit from Iconicity? {{Evidence}} from Motion Capture},
  shorttitle = {Do All Aspects of Learning Benefit from Iconicity?},
  author = {Sato, Asha and Schouwstra, Marieke and Flaherty, Molly and Kirby, Simon},
  date = {2020-03},
  journaltitle = {Language and Cognition},
  volume = {12},
  pages = {36--55},
  publisher = {{Cambridge University Press}},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2019.37},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/do-all-aspects-of-learning-benefit-from-iconicity-evidence-from-motion-capture/55EDF990ED0E81A85100F8F01988B7C2},
  urldate = {2020-03-18},
  abstract = {Recent work suggests that not all aspects of learning benefit from an iconicity advantage (Ortega, 2017). We present the results of an artificial sign language learning experiment testing the hypothesis that iconicity may help learners to learn mappings between forms and meanings, whilst having a negative impact on learning specific features of the form. We used a 3D camera (Microsoft Kinect) to capture participants’ gestures and quantify the accuracy with which they reproduce the target gestures in two conditions. In the iconic condition, participants were shown an artificial sign language consisting of congruent gesture–meaning pairs. In the arbitrary condition, the language consisted of non-congruent gesture–meaning pairs. We quantified the accuracy of participants’ gestures using dynamic time warping (Celebi et. al., 2013). Our results show that participants in the iconic condition learn mappings more successfully than participants in the arbitrary condition, but there is no difference in the accuracy with which participants reproduce the forms. While our work confirms that iconicity helps to establish form–meaning mappings, our study did not give conclusive evidence about the effect of iconicity on production; we suggest that iconicity may only have an impact on learning forms when these are complex.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LKDEMR8W\\Sato et al. - 2020 - Do all aspects of learning benefit from iconicity.pdf;C\:\\Users\\u668173\\Zotero\\storage\\GPZL9V5I\\55EDF990ED0E81A85100F8F01988B7C2.html},
  keywords = {Artificial Sign Language learning,iconicity,motion capture},
  langid = {english},
  number = {1}
}

@article{sauterCrossculturalRecognitionBasic2010,
  title = {Cross-Cultural Recognition of Basic Emotions through Nonverbal Emotional Vocalizations},
  author = {Sauter, Disa A. and Eisner, Frank and Ekman, Paul and Scott, Sophie K.},
  date = {2010-02-09},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {107},
  pages = {2408--2412},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0908239106},
  url = {https://www.pnas.org/content/107/6/2408},
  urldate = {2019-10-22},
  abstract = {Emotional signals are crucial for sharing important information, with conspecifics, for example, to warn humans of danger. Humans use a range of different cues to communicate to others how they feel, including facial, vocal, and gestural signals. We examined the recognition of nonverbal emotional vocalizations, such as screams and laughs, across two dramatically different cultural groups. Western participants were compared to individuals from remote, culturally isolated Namibian villages. Vocalizations communicating the so-called “basic emotions” (anger, disgust, fear, joy, sadness, and surprise) were bidirectionally recognized. In contrast, a set of additional emotions was only recognized within, but not across, cultural boundaries. Our findings indicate that a number of primarily negative emotions have vocalizations that can be recognized across cultures, while most positive emotions are communicated with culture-specific signals.},
  eprint = {20133790},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\L8ZW7D4H\\Sauter et al. - 2010 - Cross-cultural recognition of basic emotions throu.pdf;C\:\\Users\\u668173\\Zotero\\storage\\6PBF8UUR\\2408.html},
  keywords = {affect,communication,universality,vocal signals},
  langid = {english},
  number = {6}
}

@article{savageMusicCoevolvedSystem2020,
  title = {Music as a Coevolved System for Social Bonding},
  author = {Savage, P. E. and Loui, P. and Tarr, B. and Schachner, A. and Glowacki, L. and Mithen, S. and Fitch, W. T.},
  date = {2020},
  journaltitle = {Behavioral and Brain Sciences},
  pages = {1--36},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X20000333},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/music-as-a-coevolved-system-for-social-bonding/F1ACB3586FD3DD5965E56021F506BC4F},
  urldate = {2020-09-11},
  abstract = {Why do humans make music? Theories of the evolution of musicality have focused mainly on the value of music for specific adaptive contexts such as mate selection, parental care, coalition signaling, and group cohesion. Synthesizing and extending previous proposals, we argue that social bonding is an overarching function that unifies all of these theories, and that musicality enabled social bonding at larger scales than grooming and other bonding mechanisms available in ancestral primate societies. We combine cross-disciplinary evidence from archaeology, anthropology, biology, musicology, psychology, and neuroscience into a unified framework that accounts for the biological and cultural evolution of music. We argue that the evolution of musicality involves gene-culture coevolution, through which proto-musical behaviors that initially arose and spread as cultural inventions had feedback effects on biological evolution due to their impact on social bonding. We emphasize the deep links between production, perception, prediction, and social reward arising from repetition, synchronization, and harmonization of rhythms and pitches, and summarize empirical evidence for these links at the levels of brain networks, physiological mechanisms, and behaviors across cultures and across species. Finally, we address potential criticisms and make testable predictions for future research, including neurobiological bases of musicality and relationships between human music, language, animal song, and other domains. The music and social bonding (MSB) hypothesis provides the most comprehensive theory to date of the biological and cultural evolution of music.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WXEGY2P6\\Savage et al. - Music as a coevolved system for social bonding.pdf;C\:\\Users\\u668173\\Zotero\\storage\\V5IHJII6\\F1ACB3586FD3DD5965E56021F506BC4F.html},
  keywords = {comparative,cooperation,cultural evolution,harmony,language,music,prediction,reward,synchrony,vocal learning},
  langid = {english}
}

@article{scarrExaminingTemporomandibularJoint2017,
  title = {Examining the Temporo-Mandibular Joint from a Biotensegrity Perspective: {{A}} Change in Thinking},
  shorttitle = {Examining the Temporo-Mandibular Joint from a Biotensegrity Perspective},
  author = {Scarr, Graham and Harrison, Helen},
  date = {2017-01-01},
  journaltitle = {Journal of Applied Biomedicine},
  volume = {15},
  pages = {55--62},
  publisher = {{Journal of Applied Biomedicine}},
  issn = {1214021X, 12140287},
  doi = {10.1016/j.jab.2016.10.002},
  url = {http://jab.zsf.jcu.cz/doi/10.1016/j.jab.2016.10.002.html},
  urldate = {2020-06-11},
  abstract = {The temporo-mandibular joint is a characteristic feature of mammalian development, and essential to mastication and speech, yet it causes more problems than any other joint in the body and remains the least understood. While it is generally accepted that the normal joint is loaded under compression, the problems and controversies surrounding this view remain unresolved and the disparity in opinion over its treatment continues. Although difficulties in the acquisition of reliable information have undoubtedly contributed to this situation, it is now considered that deficits in neural control and shortcomings in the underlying biomechanical theory and analysis have also played a part, and that a re-assessment from a different perspective could resolve these.Biotensegrity considers the TMJ from this position, where the mandible is suspended within a tensioned network that extends over a much wider anatomical field than is generally recognized and significant motion control is contained within the structure itself. It is an evolutionary-conserved arrangement that enables the system to rapidly respond to changing functional demands and provides a more complete model of joint physiology that can be used to guide further research.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DX2T2JPH\\jab-201701-0008_examining-the-temporo-mandibular-joint-from-a-biotensegrity-perspective-a-chang.html},
  langid = {english},
  number = {1}
}

@article{schaeferAuditoryRhythmicCueing2014,
  title = {Auditory Rhythmic Cueing in Movement Rehabilitation: Findings and Possible Mechanisms},
  shorttitle = {Auditory Rhythmic Cueing in Movement Rehabilitation},
  author = {Schaefer, Rebecca S.},
  date = {2014-12-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0402},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4240969/},
  urldate = {2020-12-16},
  abstract = {Moving to music is intuitive and spontaneous, and music is widely used to support movement, most commonly during exercise. Auditory cues are increasingly also used in the rehabilitation of disordered movement, by aligning actions to sounds such as a metronome or music. Here, the effect of rhythmic auditory cueing on movement is discussed and representative findings of cued movement rehabilitation are considered for several movement disorders, specifically post-stroke motor impairment, Parkinson's disease and Huntington's disease. There are multiple explanations for the efficacy of cued movement practice. Potentially relevant, non-mutually exclusive mechanisms include the acceleration of learning; qualitatively different motor learning owing to an auditory context; effects of increased temporal skills through rhythmic practices and motivational aspects of musical rhythm. Further considerations of rehabilitation paradigm efficacy focus on specific movement disorders, intervention methods and complexity of the auditory cues. Although clinical interventions using rhythmic auditory cueing do not show consistently positive results, it is argued that internal mechanisms of temporal prediction and tracking are crucial, and further research may inform rehabilitation practice to increase intervention efficacy.},
  eprint = {25385780},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Y63ACQKV\\Schaefer - 2014 - Auditory rhythmic cueing in movement rehabilitatio.pdf},
  number = {1658},
  pmcid = {PMC4240969}
}

@article{schaeferDecomposingRhythmProcessing2011,
  title = {Decomposing Rhythm Processing: Electroencephalography of Perceived and Self-Imposed Rhythmic Patterns},
  shorttitle = {Decomposing Rhythm Processing},
  author = {Schaefer, Rebecca S. and Vlek, Rutger J. and Desain, Peter},
  date = {2011-03},
  journaltitle = {Psychological Research},
  shortjournal = {Psychol Res},
  volume = {75},
  pages = {95--106},
  issn = {1430-2772},
  doi = {10.1007/s00426-010-0293-4},
  abstract = {Perceiving musical rhythms can be considered a process of attentional chunking over time, driven by accent patterns. A rhythmic structure can also be generated internally, by placing a subjective accent pattern on an isochronous stimulus train. Here, we investigate the event-related potential (ERP) signature of actual and subjective accents, thus disentangling low-level perceptual processes from the cognitive aspects of rhythm processing. The results show differences between accented and unaccented events, but also show that different types of unaccented events can be distinguished, revealing additional structure within the rhythmic pattern. This structure is further investigated by decomposing the ERP into subcomponents, using principal component analysis. In this way, the processes that are common for perceiving a pattern and self-generating it are isolated, and can be visualized for the tasks separately. The results suggest that top-down processes have a substantial role in the cerebral mechanisms of rhythm processing, independent of an externally presented stimulus.},
  eprint = {20574661},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BW45HDSF\\Schaefer et al. - 2011 - Decomposing rhythm processing electroencephalogra.pdf},
  keywords = {Acoustic Stimulation,Attention,Auditory Perception,Brain Mapping,Cerebral Cortex,Electroencephalography,Evoked Potentials; Auditory,Humans,Music,Periodicity},
  langid = {english},
  number = {2},
  pmcid = {PMC3036830}
}

@article{schaeferIntuitiveVisualizationsPitch2016,
  title = {Intuitive Visualizations of Pitch and Loudness in Speech},
  author = {Schaefer, Rebecca S. and Beijer, Lilian J. and Seuskens, Wiel and Rietveld, Toni C. M. and Sadakata, Makiko},
  date = {2016-04},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {23},
  pages = {548--555},
  issn = {1531-5320},
  doi = {10.3758/s13423-015-0934-0},
  abstract = {Visualizing acoustic features of speech has proven helpful in speech therapy; however, it is as yet unclear how to create intuitive and fitting visualizations. To better understand the mappings from speech sound aspects to visual space, a large web-based experiment (n\,=\,249) was performed to evaluate spatial parameters that may optimally represent pitch and loudness of speech. To this end, five novel animated visualizations were developed and presented in pairwise comparisons, together with a static visualization. Pitch and loudness of speech were each mapped onto either the vertical (y-axis) or the size (z-axis) dimension, or combined (with size indicating loudness and vertical position indicating pitch height) and visualized as an animation along the horizontal dimension (x-axis) over time. The results indicated that firstly, there is a general preference towards the use of the y-axis for both pitch and loudness, with pitch ranking higher than loudness in terms of fit. Secondly, the data suggest that representing both pitch and loudness combined in a single visualization is preferred over visualization in only one dimension. Finally, the z-axis, although not preferred, was evaluated as corresponding better to loudness than to pitch. This relation between sound and visual space has not been reported previously for speech sounds, and elaborates earlier findings on musical material. In addition to elucidating more general mappings between auditory and visual modalities, the findings provide us with a method of visualizing speech that may be helpful in clinical applications such as computerized speech therapy, or other feedback-based learning paradigms.},
  eprint = {26370217},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FXGFGFY9\\Schaefer et al. - 2016 - Intuitive visualizations of pitch and loudness in .pdf},
  keywords = {Adult,Aged,Audio-visual processing,Feedback learning,Female,Humans,Loudness Perception,Male,Middle Aged,Pitch Perception,Speech Perception,Speech therapy,Visual Perception,Visualizing sound,Young Adult},
  langid = {english},
  number = {2},
  pmcid = {PMC4828474}
}

@article{schaeferMovingMusicEffects2014,
  title = {Moving to Music: {{Effects}} of Heard and Imagined Musical Cues on Movement-Related Brain Activity},
  shorttitle = {Moving to {{Music}}},
  author = {Schaefer, Rebecca S. and Morcom, Alexa M. and Roberts, Neil and Overy, Katie},
  date = {2014-09-26},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front Hum Neurosci},
  volume = {8},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2014.00774},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176038/},
  urldate = {2020-12-07},
  abstract = {Music is commonly used to facilitate or support movement, and increasingly used in movement rehabilitation. Additionally, there is some evidence to suggest that music imagery, which is reported to lead to brain signatures similar to music perception, may also assist movement. However, it is not yet known whether either imagined or musical cueing changes the way in which the motor system of the human brain is activated during simple movements. Here, functional magnetic resonance imaging was used to compare neural activity during wrist flexions performed to either heard or imagined music with self-pacing of the same movement without any cueing. Focusing specifically on the motor network of the brain, analyses were performed within a mask of BA4, BA6, the basal ganglia (putamen, caudate, and pallidum), the motor nuclei of the thalamus, and the whole cerebellum. Results revealed that moving to music compared with self-paced movement resulted in significantly increased activation in left cerebellum VI. Moving to imagined music led to significantly more activation in pre-supplementary motor area (pre-SMA) and right globus pallidus, relative to self-paced movement. When the music and imagery cueing conditions were contrasted directly, movements in the music condition showed significantly more activity in left hemisphere cerebellum VII and right hemisphere and vermis of cerebellum IX, while the imagery condition revealed more significant activity in pre-SMA. These results suggest that cueing movement with actual or imagined music impacts upon engagement of motor network regions during the movement, and suggest that heard and imagined cues can modulate movement in subtly different ways. These results may have implications for the applicability of auditory cueing in movement rehabilitation for different patient populations.},
  eprint = {25309407},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7IAZST6L\\Schaefer et al. - 2014 - Moving to Music Effects of Heard and Imagined Mus.pdf},
  pmcid = {PMC4176038}
}

@article{schafferRoleIntonationCue1983,
  title = {The Role of Intonation as a Cue to Turn Taking in Conversation},
  author = {Schaffer, Deborah},
  date = {1983-07-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {11},
  pages = {243--257},
  issn = {0095-4470},
  doi = {10.1016/S0095-4470(19)30825-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447019308253},
  urldate = {2020-09-21},
  abstract = {The sociolinguistic literature concerning turn taking in conversation is extensive, but also limited in certain respects. Most studies have focused on kinesic behaviors and semantic/pragmatic devices for maintaining the flow of conversations, while few have investigated the role of prosody, especially in non-face-to-face turn taking. Moreover, most researchers describe the production of conversational behaviors rather than their perception by conversationalists. The present study attempts to fill these gaps through a series of listening tests incorporating both face-to-face (FF)and non-face-to-face (NFF) conversational excerpts, in order to discover how intonation is used as a perceptual cue for turn taking. Utterances isolated from these conversations were used to construct two test tapes; each set was also filtered so that the utterances were unintelligible but retained some prosodic information, notably intonation. Subjects then made turn beginning and turn end judgments for each item on the four resulting tapes. The findings show a great amount of variability in listener use of intonation as a cue to both FF and NFF speaker status, with rising fundamental frequency the strongest cue (to turn ends) in both conditions. The results also illustrate the highly interactive nature of prosody and other types of cues (e.g. syntactic and contextual information).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MBIVDHUL\\S0095447019308253.html},
  langid = {english},
  number = {3}
}

@article{schelChimpanzeeAlarmCall2013,
  title = {Chimpanzee {{Alarm Call Production Meets Key Criteria}} for {{Intentionality}}},
  author = {Schel, Anne Marijke and Townsend, Simon W. and Machanda, Zarin and Zuberbühler, Klaus and Slocombe, Katie E.},
  date = {2013-10-16},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  pages = {e76674},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0076674},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0076674},
  urldate = {2020-12-07},
  abstract = {Determining the intentionality of primate communication is critical to understanding the evolution of human language. Although intentional signalling has been claimed for some great ape gestural signals, comparable evidence is currently lacking for their vocal signals. We presented wild chimpanzees with a python model and found that two of three alarm call types exhibited characteristics previously used to argue for intentionality in gestural communication. These alarm calls were: (i) socially directed and given to the arrival of friends, (ii) associated with visual monitoring of the audience and gaze alternations, and (iii) goal directed, as calling only stopped when recipients were safe from the predator. Our results demonstrate that certain vocalisations of our closest living relatives qualify as intentional signals, in a directly comparable way to many great ape gestures. We conclude that our results undermine a central argument of gestural theories of language evolution and instead support a multimodal origin of human language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\U6PCSDE3\\Schel et al. - 2013 - Chimpanzee Alarm Call Production Meets Key Criteri.pdf;C\:\\Users\\u668173\\Zotero\\storage\\MCVT49SF\\article.html},
  keywords = {Acoustic signals,Animal behavior,Animal communication,Animal sociality,Chimpanzees,Primates,Snakes,Vocalization},
  langid = {english},
  number = {10}
}

@article{schembriComparingActionGestures2005,
  title = {Comparing Action Gestures and Classifier Verbs of Motion: Evidence from {{Australian Sign Language}}, {{Taiwan Sign Language}}, and Nonsigners' Gestures without Speech},
  shorttitle = {Comparing Action Gestures and Classifier Verbs of Motion},
  author = {Schembri, Adam and Jones, Caroline and Burnham, Denis},
  date = {2005},
  journaltitle = {Journal of Deaf Studies and Deaf Education},
  shortjournal = {J Deaf Stud Deaf Educ},
  volume = {10},
  pages = {272--290},
  issn = {1081-4159},
  doi = {10.1093/deafed/eni029},
  abstract = {Recent research into signed languages indicates that signs may share some properties with gesture, especially in the use of space in classifier constructions. A prediction of this proposal is that there will be similarities in the representation of motion events by sign-naive gesturers and by native signers of unrelated signed languages. This prediction is tested for deaf native signers of Australian Sign Language (Auslan), deaf signers of Taiwan Sign Language (TSL), and hearing nonsigners using the Verbs of Motion Production task from the Test Battery for American Sign Language (ASL) Morphology and Syntax. Results indicate that differences between the responses of nonsigners, Auslan signers, and TSL signers and the expected ASL responses are greatest with handshape units; movement and location units appear to be very similar. Although not definitive, these data are consistent with the claim that classifier constructions are blends of linguistic and gestural elements.},
  eprint = {15858072},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CBXERQKJ\\Schembri et al. - 2005 - Comparing action gestures and classifier verbs of .pdf},
  keywords = {Adolescent,Adult,Case-Control Studies,Deafness,Female,Gestures,Humans,Male,Middle Aged,Multivariate Analysis,Nonverbal Communication,Sign Language,Visual Perception},
  langid = {english},
  number = {3}
}

@article{schererVocalCommunicationEmotion2003,
  title = {Vocal Communication of Emotion: {{A}} Review of Research Paradigms},
  shorttitle = {Vocal Communication of Emotion},
  author = {Scherer, Klaus R},
  date = {2003-04-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {40},
  pages = {227--256},
  issn = {0167-6393},
  doi = {10.1016/S0167-6393(02)00084-5},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639302000845},
  urldate = {2019-10-22},
  abstract = {The current state of research on emotion effects on voice and speech is reviewed and issues for future research efforts are discussed. In particular, it is suggested to use the Brunswikian lens model as a base for research on the vocal communication of emotion. This approach allows one to model the complete process, including both encoding (expression), transmission, and decoding (impression) of vocal emotion communication. Special emphasis is placed on the conceptualization and operationalization of the major elements of the model (i.e., the speaker’s emotional state, the listener’s attribution, and the mediating acoustic cues). In addition, the advantages and disadvantages of research paradigms for the induction or observation of emotional expression in voice and speech and the experimental manipulation of vocal cues are discussed, using pertinent examples drawn from past and present research. Zusammenfassung Der Aufsatz gibt einen umfassenden Überblick über den Forschungsstand zum Thema der Beeinflussung von Stimme und Sprechweise durch Emotionen des Sprechers. Allgemein wird vorgeschlagen, die Forschung zur vokalen Kommunikation der Emotionen am Brunswik’schen Linsenmodell zu orientieren. Dieser Ansatz erlaubt den gesamten Kommunikationsprozess zu modellieren, von der Enkodierung (Ausdruck), über die Transmission (Übertragung), bis zur Dekodierung (Eindruck). Besondere Aufmerksamkeit gilt den Problemen der Konzeptualisierung und Operationalisierung der zentralen Elemente des Modells (z.B., dem Emotionszustand des Sprechers, den Inferenzprozessen des Hörers, und den zugrundeliegenden vokalen Hinweisreizen). Anhand ausgewählter Beispiele empirischer Untersuchungen werden die Vor- und Nachteile verschiedener Forschungsparadigmen zur Induktion und Beobachtung des emotionalen Stimmausdrucks sowie zur experimentellen Manipulation vokaler Hinweisreize diskutiert. Résumé L’état actuel de la recherche sur l’effet des émotions d’un locuteur sur la voix et la parole est décrit et des approches prometteuses pour le futur identifiées. En particulier, le modèle de perception de Brunswik (dit “de la lentille” est proposé) comme paradigme pour la recherche sur la communication vocale des émotions. Ce modèle permet la modélisation du processus complet, de l’encodage (expression) par la transmission au décodage (impression). La conceptualisation et l’opérationalization des éléments centraux du modèle (l’état émotionnel du locuteur, l’inférence de cet état par l’auditeur, et les indices auditifs) sont discuté en détail. De plus, en analysant des exemples de la recherche dans le domaine, les avantages et désavantages de différentes méthodes pour l’induction et l’observation de l’expression émotionnelle dans la voix et la parole et pour la manipulation expérimentale de différents indices vocaux sont évoqués.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XSGGSSSV\\Scherer - 2003 - Vocal communication of emotion A review of resear.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8ZBAMLDD\\S0167639302000845.html},
  keywords = {Acoustic markers of emotion,Emotion induction,Emotion simulation,Evaluation of emotion effects on voice and speech,Expression of emotion,Perception/decoding,Speaker moods and attitudes,Speech technology,Stress effects on voice,Theories of emotion,Vocal communication},
  langid = {english},
  number = {1}
}

@article{schippersMappingInformationFlow2010,
  title = {Mapping the Information Flow from One Brain to Another during Gestural Communication},
  author = {Schippers, M. B. and Roebroeck, A. and Renken, R. and Nanetti, L. and Keysers, C.},
  date = {2010-05-18},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {107},
  pages = {9388--9393},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1001791107},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1001791107},
  urldate = {2020-09-27},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YF2YV5JD\\Schippers et al. - 2010 - Mapping the information flow from one brain to ano.pdf},
  langid = {english},
  number = {20}
}

@article{schmidtBodilySynchronizationUnderlying2014,
  title = {Bodily Synchronization Underlying Joke Telling},
  author = {Schmidt, R. C. and Nie, Lin and Franco, Alison and Richardson, Michael J.},
  date = {2014},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {8},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2014.00633},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2014.00633/full},
  urldate = {2019-05-07},
  abstract = {Advances in video and time series analysis have greatly enhanced our ability to study the bodily synchronization that occurs in natural interactions. Past research has demonstrated that the behavioral synchronization involved in social interactions is similar to dynamical synchronization found generically in nature. The present study investigated how the bodily synchronization in a joke telling task is spread across different nested temporal scales. Pairs of participants enacted knock-knock jokes and times series of their bodily activity were recorded. Coherence and relative phase analyses were used to evaluate the synchronization of bodily rhythms for the whole trial as well as at the subsidiary time scales of the whole joke, the setup of the punch line, the two-person exchange and the utterance. The analyses revealed greater than chance entrainment of the joke teller’s and joke responder’s movements at all time scales and that the relative phasing of the teller’s movements led those of the responder at the longer time scales. Moreover, this entrainment was greater when visual information about the partner’s movements was present but was decreased particularly at the shorter time scales when explicit gesturing in telling the joke was performed. In short, the results demonstrate that a complex interpersonal bodily “dance” occurs during structured conversation interactions and that this “dance” is constructed from a set of rhythms associated with the nested behavioral structure of the interaction.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2WA6HV7G\\Schmidt et al. - 2014 - Bodily synchronization underlying joke telling.pdf},
  keywords = {Motor movements,sensorimotor synchronization,social coordination,social interaction,Spectral decomposition},
  langid = {english}
}

@article{schmidtEvaluatingDynamicsUnintended1997,
  title = {Evaluating the {{Dynamics}} of {{Unintended Interpersonal Coordination}}},
  author = {Schmidt, R. C. and O'Brien, Beth},
  date = {1997-09-01},
  journaltitle = {Ecological Psychology},
  volume = {9},
  pages = {189--206},
  publisher = {{Routledge}},
  issn = {1040-7413},
  doi = {10.1207/s15326969eco0903_2},
  url = {https://doi.org/10.1207/s15326969eco0903_2},
  urldate = {2020-12-05},
  abstract = {Past research has shown that interpersonal interactions are characterized by a tacit coordination of motor movements of the participants and has suggested that the emergent synchrony might be explained by a coupled oscillator dynamic. This study investigates whether unintended between-person coordination can be demonstrated in a laboratory task that will allow an evaluation of whether such dynamical processes are involved. Ten pairs of participants performed a simple rhythmic task in which they had visual information about each other's movements but had no goal to coordinate. A cross-spectral analysis of the movements revealed higher coherence and a distribution of relative phase angles that was dominated by values near 0 deg. and 180 deg. These results support the hypothesis that dynamical organizing principles are involved in natural interpersonal synchrony.},
  annotation = {\_eprint: https://doi.org/10.1207/s15326969eco0903\_2},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4LTCU2XD\\s15326969eco0903_2.html},
  number = {3}
}

@article{schonerTimingClocksDynamical2002,
  title = {Timing, {{Clocks}}, and {{Dynamical Systems}}},
  author = {Schöner, Gregor},
  date = {2002-02},
  journaltitle = {Brain and Cognition},
  volume = {48},
  pages = {31--51},
  issn = {02782626},
  doi = {10.1006/brcg.2001.1302},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0278262601913028},
  urldate = {2020-05-19},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IE9EE4Y7\\Schöner - 2002 - Timing, Clocks, and Dynamical Systems.pdf},
  langid = {english},
  number = {1}
}

@article{schouwstraTemporalStructureEmerging2017,
  title = {Temporal {{Structure}} in {{Emerging Language}}: {{From Natural Data}} to {{Silent Gesture}}},
  shorttitle = {Temporal {{Structure}} in {{Emerging Language}}},
  author = {Schouwstra, Marieke},
  date = {2017},
  journaltitle = {Cognitive Science},
  volume = {41},
  pages = {928--940},
  issn = {1551-6709},
  doi = {10.1111/cogs.12441},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12441},
  urldate = {2020-03-06},
  abstract = {Many human languages have complex grammatical machinery devoted to temporality, but very little is known about how this came about. This paper investigates how people convey temporal information when they cannot use any conventional languages they know. In a laboratory experiment, adult participants were asked to convey information about simple events taking place at a given time, in spoken language and in silent gesture (i.e., using only gesture and no speech). It was shown that in spoken language, participants formed utterances according to the rules of their native language (Dutch), but in silent gesture, the temporal information was presented initially, and structurally separately, from the other information in the utterance. The experimental results are consistent with findings from natural systems emerging in situations of communicative stress: unsupervised adult second language learning and homesign. This confirms that presenting temporal information separately and initially (directly mirroring how temporal and propositional information can be represented semantically) is a robust strategy to talk about past and future when only sparse communicative means are available.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12441},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\R67K2959\\Schouwstra - 2017 - Temporal Structure in Emerging Language From Natu.pdf;C\:\\Users\\u668173\\Zotero\\storage\\RRB2ZMJY\\cogs.html},
  keywords = {Language evolution,Semantics,Silent gesture,Temporality,Tense,Word order},
  langid = {english},
  number = {S4}
}

@article{schubotzTimePerceptionMotor2000,
  title = {Time Perception and Motor Timing: A Common Cortical and Subcortical Basis Revealed by {{fMRI}}},
  shorttitle = {Time Perception and Motor Timing},
  author = {Schubotz, R. I. and Friederici, A. D. and von Cramon, D. Y.},
  date = {2000-01},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  volume = {11},
  pages = {1--12},
  issn = {1053-8119},
  doi = {10.1006/nimg.1999.0514},
  abstract = {Though it is well known that humans perceive the temporal features of the environment incessantly, the brain mechanisms underlying temporal processing are relatively unexplored. Functional magnetic resonance imaging was used in this study to identify brain activations during sustained perceptual analysis of auditorally and visually presented temporal patterns (rhythms). Our findings show that the neural network supporting time perception involves the same brain areas that are responsible for the temporal planning and coordination of movements. These results indicate that time perception and motor timing rely on similar cerebral structures.},
  eprint = {10686112},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\B4P3RC5H\\Schubotz et al. - 2000 - Time perception and motor timing a common cortica.pdf},
  keywords = {Adult,Analysis of Variance,Auditory Perception,Behavior,Brain Mapping,Cerebral Cortex,Female,Humans,Magnetic Resonance Imaging,Male,Movement,Time Factors,Time Perception,Visual Perception},
  langid = {english},
  number = {1},
  options = {useprefix=true}
}

@article{schultzSpeechRatesConverge2016,
  title = {Speech Rates Converge in Scripted Turn-Taking Conversations},
  author = {Schultz, Benjamin G. and O’brien, Irena and Phillips, Natalie and McFARLAND, David H. and Titone, Debra and Palmer, Caroline},
  date = {2016-09},
  journaltitle = {Applied Psycholinguistics},
  volume = {37},
  pages = {1201--1220},
  publisher = {{Cambridge University Press}},
  issn = {0142-7164, 1469-1817},
  doi = {10.1017/S0142716415000545},
  url = {https://www.cambridge.org/core/journals/applied-psycholinguistics/article/abs/speech-rates-converge-in-scripted-turntaking-conversations/8FFF4A40AE5A1213C7F64C6144F6284B},
  urldate = {2020-12-09},
  abstract = {When speakers engage in conversation, acoustic features of their utterances sometimes converge. We examined how the speech rate of participants changed when a confederate spoke at fast or slow rates during readings of scripted dialogues. A beat-tracking algorithm extracted the periodic relations between stressed syllables (beats) from acoustic recordings. The mean interbeat interval (IBI) between successive stressed syllables was compared across speech rates. Participants’ IBIs were smaller in the fast condition than in the slow condition; the difference between participants’ and the confederate's IBIs decreased across utterances. Cross-correlational analyses demonstrated mutual influences between speakers, with greater impact of the confederate on participants’ beat rates than vice versa. Beat rates converged in scripted conversations, suggesting speakers mutually entrain to one another's beat.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UPMYVI7G\\8FFF4A40AE5A1213C7F64C6144F6284B.html},
  langid = {english},
  number = {5}
}

@article{schutzHearingGesturesSeeing2016,
  title = {Hearing Gestures, Seeing Music: {{Vision}} Influences Perceived Tone Duration},
  shorttitle = {Hearing {{Gestures}}, {{Seeing Music}}},
  author = {Schutz, Michael and Lipscomb, Scott},
  date = {2016-06-25},
  journaltitle = {Perception},
  publisher = {{SAGE PublicationsSage UK: London, England}},
  doi = {10.1068/p5635},
  url = {https://journals.sagepub.com/doi/10.1068/p5635},
  urldate = {2020-12-05},
  abstract = {Percussionists inadvertently use visual information to strategically manipulate audience perception of note duration. Videos of long (L) and short (S) notes per...},
  langid = {english}
}

@article{scott-phillipsLanguageEvolutionLaboratory2010,
  title = {Language Evolution in the Laboratory},
  author = {Scott-phillips, T. C. and Kirby, S.},
  date = {2010},
  journaltitle = {Trends in Cognitive Sciences},
  volume = {14},
  pages = {411--417},
  doi = {10.1016/j.tics.2010.06.006},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CBG7FTXE\\Scott-phillips and Kirby - Author's personal copy Language evolution in the l.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ICYHGK9W\\summary.html}
}

@article{scott-phillipsMeaningAnimalHuman2015,
  title = {Meaning in Animal and Human Communication},
  author = {Scott-Phillips, Thomas C.},
  date = {2015-05},
  journaltitle = {Animal Cognition},
  volume = {18},
  pages = {801--805},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-015-0845-5},
  url = {http://link.springer.com/10.1007/s10071-015-0845-5},
  urldate = {2020-09-02},
  abstract = {What is meaning? While traditionally the domain of philosophy and linguistics, this question, and others related to it, is critical for cognitive and comparative approaches to communication. This short essay provides a concise and accessible description of how the term meaning can and should be used, how it relates to ‘intentional communication’, and what would constitute good evidence of meaning in animal communication, in the sense that is relevant for comparisons with human language.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CZW8ZSKJ\\Scott-Phillips - 2015 - Meaning in animal and human communication.pdf},
  langid = {english},
  number = {3}
}

@book{seikelAnatomyPhysiologySpeech2019,
  title = {Anatomy \& {{Physiology}} for {{Speech}}, {{Language}}, and {{Hearing}}},
  author = {Seikel, John A. and Drumright, David G. and Hudock, Daniel J.},
  date = {2019-12},
  publisher = {{Plural Publishing, Incorporated}},
  abstract = {"Anatomy \& Physiology for Speech, Language, and Hearing, Sixth Edition provides a solid foundation in anatomical and physiological principles relevant to communication sciences and disorders. This bestselling textbook beloved by instructors and students integrates clinical information with everyday experiences to reveal how anatomy and physiology relate to the speech, language, and hearing systems. Combining comprehensive coverage with abundant, beautiful full-color illustrations and a strong practical focus, the text makes complex material approachable even for students with little or no background in anatomy and physiology"--},
  eprint = {ulXJxQEACAAJ},
  eprinttype = {googlebooks},
  isbn = {978-1-63550-279-4},
  langid = {english},
  pagetotal = {912}
}

@article{selenDeliberationMotorSystem2012,
  title = {Deliberation in the {{Motor System}}: {{Reflex Gains Track Evolving Evidence Leading}} to a {{Decision}}},
  shorttitle = {Deliberation in the {{Motor System}}},
  author = {Selen, Luc P. J. and Shadlen, Michael N. and Wolpert, Daniel M.},
  date = {2012-02-15},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {32},
  pages = {2276--2286},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5273-11.2012},
  url = {https://www.jneurosci.org/content/32/7/2276},
  urldate = {2021-03-03},
  abstract = {Both decision making and sensorimotor control require real-time processing of noisy information streams. Historically these processes were thought to operate sequentially: cognitive processing leads to a decision, and the outcome is passed to the motor system to be converted into action. Recently, it has been suggested that the decision process may provide a continuous flow of information to the motor system, allowing it to prepare in a graded fashion for the probable outcome. Such continuous flow is supported by electrophysiology in nonhuman primates. Here we provide direct evidence for the continuous flow of an evolving decision variable to the motor system in humans. Subjects viewed a dynamic random dot display and were asked to indicate their decision about direction by moving a handle to one of two targets. We probed the state of the motor system by perturbing the arm at random times during decision formation. Reflex gains were modulated by the strength and duration of motion, reflecting the accumulated evidence in support of the evolving decision. The magnitude and variance of these gains tracked a decision variable that explained the subject's decision accuracy. The findings support a continuous process linking the evolving computations associated with decision making and sensorimotor control.},
  eprint = {22396403},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\T7XSY4KW\\Selen et al. - 2012 - Deliberation in the Motor System Reflex Gains Tra.pdf;C\:\\Users\\u668173\\Zotero\\storage\\BTY2I6DI\\2276.html},
  langid = {english},
  number = {7}
}

@article{sellAdaptationsHumansAssessing2010,
  title = {Adaptations in Humans for Assessing Physical Strength from the Voice},
  author = {Sell, Aaron and Bryant, Gregory A. and Cosmides, Leda and Tooby, John and Sznycer, Daniel and von Rueden, Christopher and Krauss, Andre and Gurven, Michael},
  date = {2010-11-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {277},
  pages = {3509--3518},
  doi = {10.1098/rspb.2010.0769},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2010.0769},
  urldate = {2019-10-17},
  abstract = {Recent research has shown that humans, like many other animals, have a specialization for assessing fighting ability from visual cues. Because it is probable that the voice contains cues of strength and formidability that are not available visually, we predicted that selection has also equipped humans with the ability to estimate physical strength from the voice. We found that subjects accurately assessed upper-body strength in voices taken from eight samples across four distinct populations and language groups: the Tsimane of Bolivia, Andean herder-horticulturalists and United States and Romanian college students. Regardless of whether raters were told to assess height, weight, strength or fighting ability, they produced similar ratings that tracked upper-body strength independent of height and weight. Male voices were more accurately assessed than female voices, which is consistent with ethnographic data showing a greater tendency among males to engage in violent aggression. Raters extracted information about strength from the voice that was not supplied from visual cues, and were accurate with both familiar and unfamiliar languages. These results provide, to our knowledge, the first direct evidence that both men and women can accurately assess men's physical strength from the voice, and suggest that estimates of strength are used to assess fighting ability.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\K4KVXIR4\\Sell et al. - 2010 - Adaptations in humans for assessing physical stren.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ARPVBSQY\\rspb.2010.html},
  number = {1699},
  options = {useprefix=true}
}

@article{semjenTimingPrecisionContinuation2000,
  title = {Timing Precision in Continuation and Synchronization Tapping},
  author = {Semjen, A. and Schulze, H. H. and Vorberg, D.},
  date = {2000},
  journaltitle = {Psychological Research},
  shortjournal = {Psychol Res},
  volume = {63},
  pages = {137--147},
  issn = {0340-0727},
  doi = {10.1007/pl00008172},
  abstract = {Wing and Kristofferson (1973) have shown that temporal precision in self-paced tapping is limited by variability in a central timekeeper and by variability arising in the peripheral motor system. Here we test an extension of the Wing-Kristofferson model to synchronization with periodic external events that was proposed by Vorberg and Wing (1994). In addition to the timekeeper and motor components, a linear phase correction mechanism is assumed which is triggered by the last or the last two synchronization errors. The model is tested in an experiment that contrasts synchronized and self-paced trapping, with response periods ranging from 200-640 ms. The variances of timekeeper and motor delays and the error correction parameters were estimated from the auto-covariance functions of the inter-response intervals in continuation and the asynchronies in synchronization. Plausible estimates for all parameters were obtained when equal motor variance was assumed for synchronization and continuation. Timekeeper variance increased with metronome period, but more steeply during continuation than during synchronization, suggesting that internal timekeeping processes are stabilized by periodic external signals. First-order error correction became more important as the metronome period increased, whereas the contribution of second-order error correction decreased. It is concluded that the extended two-level model accounts well for both synchronization and continuation performance.},
  eprint = {10946587},
  eprinttype = {pmid},
  keywords = {Adolescent,Adult,Female,Humans,Male,Middle Aged,Models; Theoretical,Movement,Periodicity,Time Factors},
  langid = {english},
  number = {2}
}

@article{senghasChildrenCreatingCore2004,
  title = {Children Creating Core Properties of Language: {{Evidence}} from an Emerging Sign Language in Nicaragua},
  shorttitle = {Children {{Creating Core Properties}} of {{Language}}},
  author = {Senghas, Ann and Kita, Sotaro and Özyürek, Asli},
  date = {2004-09-17},
  journaltitle = {Science},
  volume = {305},
  pages = {1779--1782},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1100199},
  url = {https://science.sciencemag.org/content/305/5691/1779},
  urldate = {2019-11-30},
  abstract = {A new sign language has been created by deaf Nicaraguans over the past 25 years, providing an opportunity to observe the inception of universal hallmarks of language. We found that in their initial creation of the language, children analyzed complex events into basic elements and sequenced these elements into hierarchically structured expressions according to principles not observed in gestures accompanying speech in the surrounding language. Successive cohorts of learners extended this procedure, transforming Nicaraguan signing from its early gestural form into a linguistic system. We propose that this early segmentation and recombination reflect mechanisms with which children learn, and thereby perpetuate, language. Thus, children naturally possess learning abilities capable of giving language its fundamental structure. A sign language developed by deaf children consists of discrete units similar to those of spoken language, perhaps reflecting the fundamental organization of the brain's language centers. A sign language developed by deaf children consists of discrete units similar to those of spoken language, perhaps reflecting the fundamental organization of the brain's language centers.},
  eprint = {15375269},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8ZZMFZSD\\Senghas et al. - 2004 - Children Creating Core Properties of Language Evi.pdf;C\:\\Users\\u668173\\Zotero\\storage\\9BBN28AM\\1779.html},
  langid = {english},
  number = {5691}
}

@article{senkowskiCrossmodalBindingNeural2008,
  title = {Crossmodal Binding through Neural Coherence: Implications for Multisensory Processing},
  shorttitle = {Crossmodal Binding through Neural Coherence},
  author = {Senkowski, Daniel and Schneider, Till R. and Foxe, John J. and Engel, Andreas K.},
  date = {2008-08-01},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  volume = {31},
  pages = {401--409},
  publisher = {{Elsevier}},
  issn = {0166-2236, 1878-108X},
  doi = {10.1016/j.tins.2008.05.002},
  url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(08)00144-6},
  urldate = {2020-12-05},
  eprint = {18602171},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AT7L4JYX\\S0166-2236(08)00144-6.html},
  langid = {english},
  number = {8}
}

@online{SequenceMemoryConstraints,
  title = {Sequence {{Memory Constraints Give Rise}} to {{Language}}-{{Like Structure}} through {{Iterated Learning}}},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168532},
  urldate = {2020-01-21},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VIJ5DP3M\\article.html}
}

@article{sergioMotorCortexNeural2005,
  title = {Motor Cortex Neural Correlates of Output Kinematics and Kinetics during Isometric-Force and Arm-Reaching Tasks},
  author = {Sergio, Lauren E. and Hamel-Pâquet, Catherine and Kalaska, John F.},
  date = {2005-10},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {J. Neurophysiol.},
  volume = {94},
  pages = {2353--2378},
  issn = {0022-3077},
  doi = {10.1152/jn.00989.2004},
  abstract = {We recorded the activity of 132 proximal-arm-related neurons in caudal primary motor cortex (M1) of two monkeys while they generated either isometric forces against a rigid handle or arm movements with a heavy movable handle, in the same eight directions in a horizontal plane. The isometric forces increased in monotonic fashion in the direction of the force target. The forces exerted against the handle in the movement task were more complex, including an initial accelerating force in the direction of movement followed by a transient decelerating force opposite to the direction of movement as the hand approached the target. EMG activity of proximal-arm muscles reflected the difference in task dynamics, showing directional ramplike activity changes in the isometric task and reciprocally tuned "triphasic" patterns in the movement task. The apparent instantaneous directionality of muscle activity, when expressed in hand-centered spatial coordinates, remained relatively stable during the isometric ramps but often showed a large transient shift during deceleration of the arm movements. Single-neuron and population-level activity in M1 showed similar task-dependent changes in temporal pattern and instantaneous directionality. The momentary dissociation of the directionality of neuronal discharge and movement kinematics during deceleration indicated that the activity of many arm-related M1 neurons is not coupled only to the direction and speed of hand motion. These results also demonstrate that population-level signals reflecting the dynamics of motor tasks and of interactions with objects in the environment are available in caudal M1. This task-dynamics signal could greatly enhance the performance capabilities of neuroprosthetic controllers.},
  eprint = {15888522},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WNHQ2V4F\\Sergio et al. - 2005 - Motor cortex neural correlates of output kinematic.pdf},
  keywords = {Animals,Arm,Behavior; Animal,Biomechanical Phenomena,Electromyography,Female,Isometric Contraction,Macaca mulatta,Motor Cortex,Movement,Muscle; Skeletal,Neurons,Psychomotor Performance,Reaction Time,Task Performance and Analysis},
  langid = {english},
  number = {4}
}

@book{seyfeddinipurGestureConversationVisible2014,
  title = {From {{Gesture}} in {{Conversation}} to {{Visible Action}} as {{Utterance}}: {{Essays}} in Honor of {{Adam Kendon}}},
  shorttitle = {From {{Gesture}} in {{Conversation}} to {{Visible Action}} as {{Utterance}}},
  author = {Seyfeddinipur, M. and Gullberg, M.},
  date = {2014-08-06},
  publisher = {{John Benjamins Publishing Company}},
  abstract = {Language use is fundamentally multimodal. Speakers use their hands to point to locations, to represent content and to comment on ongoing talk; they position their bodies to show their orientation and stance in interaction; they use facial displays to comment on what is being said; and they engage in mutual gaze to establish intersubjectivity. This volume brings together studies by leading scholars from several fields on gaze and facial displays, on the relationship between gestures, sign, and language, on pointing and other conventionalized forms of manual expression, on gestures and language evolution, and on gestures in child development. The papers in this collection honor Adam Kendon whose pioneering work has laid the theoretical and methodological foundations for contemporary studies of multimodality, gestures, and utterance visible action.},
  eprint = {If2ADQAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-90-272-6927-0},
  keywords = {Language Arts & Disciplines / Linguistics / General},
  langid = {english},
  pagetotal = {389}
}

@article{sharkeyHandGesturesVisually2000,
  title = {Hand {{Gestures}} of {{Visually Impaired}} and {{Sighted Interactants}}},
  author = {Sharkey, William F. and Asamoto, Paula and Tokunaga, Christine and Haraguchi, Gail and McFaddon-Robar, Tammy},
  date = {2000-09-01},
  journaltitle = {Journal of Visual Impairment \& Blindness},
  shortjournal = {Journal of Visual Impairment \& Blindness},
  volume = {94},
  pages = {549--563},
  issn = {0145-482X},
  doi = {10.1177/0145482X0009400902},
  url = {https://doi.org/10.1177/0145482X0009400902},
  urldate = {2019-11-30},
  abstract = {This study investigated the types of gestures used, the frequency of the gestures, and the total time engaged in gestural communication by 11 visually impaired-sighted dyads; 12 sighted-sighted dyads; and 8 visually impaired-visually impaired dyads. Regardless of the type of dyad, the persons who were visually impaired used more adaptors and used gestures, emblems, and illustrators less often than did those who were sighted.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KGADQWEP\\Sharkey et al. - 2000 - Hand Gestures of Visually Impaired and Sighted Int.pdf},
  langid = {english},
  number = {9}
}

@article{sharkeyTurnTakingResources1990,
  title = {Turn‐taking Resources Employed by Congenitally Blind Conversers},
  author = {Sharkey, William F. and Stafford, Laura},
  date = {1990-06-01},
  journaltitle = {Communication Studies},
  volume = {41},
  pages = {161--182},
  issn = {1051-0974},
  doi = {10.1080/10510979009368299},
  url = {https://doi.org/10.1080/10510979009368299},
  urldate = {2019-11-30},
  abstract = {It has been suggested that blind persons lack appropriate communicative social skills. One aspect of social skills is the ability to regulate interaction smoothly. The study examined turn‐taking resources utilized by congenitally blind persons. Conversational Analysis was employed to discover the turn‐taking resources used by six congenitally blind individuals in three dyads (i.e., one male, one female and one mixed dyad). The results were compared with past research on turn‐taking resources utilized by sighted conversers. Overall, the participants utilized the majority of focal resources reported in research on sighted individuals. However, non‐vocal resources deviated from those found in previous research on sighted conversers. Specifically, tactile resources were not used; self‐adaptors, gestures and posture shifts were seldom used; mechanistic movements of the head and atypical use of facial orientation were discovered. Possible implications of these finding are discussed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\G6XQJHDE\\10510979009368299.html},
  number = {2}
}

@inproceedings{shattuck-hufnagelDimensionalizingCospeechGestures2019,
  title = {Dimensionalizing Co-Speech Gestures},
  booktitle = {Proceedings of the {{International Congress}} of {{Phonetic Sciences}} 2019},
  author = {Shattuck-Hufnagel, S. and Prieto, P.},
  date = {2019},
  pages = {5},
  location = {{Melbourne, Australia}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IHR6ZBZX\\Shattuck-Hufnagel and Prieto - Dimensionalizing co-speech gestures.pdf},
  langid = {english}
}

@article{shattuck-hufnagelProsodicCharacteristicsNonreferential2018,
  title = {The Prosodic Characteristics of Non-Referential Co-Speech Gestures in a Sample of Academic-Lecture-Style Speech},
  author = {Shattuck-Hufnagel, S. and Ren, Ada},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  volume = {9},
  doi = {10.3389/fpsyg.2018.01514},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HTSWA3AA\\fpsyg.2018.html},
  number = {1514}
}

@article{shattuck-hufnagelProsodyTutorialInvestigators1996,
  title = {A Prosody Tutorial for Investigators of Auditory Sentence Processing},
  author = {Shattuck-Hufnagel, Stefanie and Turk, Alice E.},
  date = {1996-03},
  journaltitle = {Journal of Psycholinguistic Research},
  volume = {25},
  pages = {193--247},
  issn = {0090-6905, 1573-6555},
  doi = {10.1007/BF01708572},
  url = {http://link.springer.com/10.1007/BF01708572},
  urldate = {2020-03-17},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IGUX47UT\\Shattuck-Hufnagel and Turk - 1996 - A prosody tutorial for investigators of auditory s.pdf},
  langid = {english},
  number = {2}
}

@article{sheaPersonnaliteVentilatoireOverview1992,
  title = {Personnalité Ventilatoire--an Overview},
  author = {Shea, S. A. and Guz, A.},
  date = {1992-03},
  journaltitle = {Respiration Physiology},
  shortjournal = {Respir Physiol},
  volume = {87},
  pages = {275--291},
  issn = {0034-5687},
  doi = {10.1016/0034-5687(92)90012-l},
  abstract = {An infinite number of possible combinations of tidal volume and breathing frequency, as well as pattern of airflow, can achieve the alveolar ventilation required for normal gas exchange. Individuals appear to select one particular pattern. This paper summarises our work relating to differences in the pattern of breathing between individuals when at rest and discusses the possible determinants of such individuality.},
  eprint = {1604053},
  eprinttype = {pmid},
  keywords = {Adult,Female,Humans,Male,Respiration},
  langid = {english},
  number = {3}
}

@book{sheets-johnstonePrimacyMovement2011,
  title = {The {{Primacy}} of {{Movement}}},
  author = {Sheets-Johnstone, M.},
  date = {2011},
  publisher = {{John Benjamins}},
  location = {{Amsterdam}}
}

@article{shibaFunctionalRolesSuperior1995,
  title = {Functional Roles of the Superior Laryngeal Nerve Afferents in Electrically Induced Vocalization in Anesthetized Cats},
  author = {Shiba, Keisuke and Yoshida, Koh and Miura, Takumi},
  date = {1995-03-01},
  journaltitle = {Neuroscience Research},
  shortjournal = {Neuroscience Research},
  volume = {22},
  pages = {23--30},
  issn = {0168-0102},
  doi = {10.1016/0168-0102(95)00877-V},
  url = {http://www.sciencedirect.com/science/article/pii/016801029500877V},
  urldate = {2020-11-14},
  abstract = {Our purpose was to elucidate the functional roles of the laryngeal afferents in controlling vocalization. We investigated the effects of laryngeal deafferentation (sectioning the internal branch of the superior laryngeal nerve (ISLN) on respiration and voice quality during electrically-induced vocalization in twelve ketamine anesthetized cats. Co-ordinated vocal activity was obtained by electrical stimulation to the pontine call site. After the bilateral ISLN section, the respiratory, expiratory and inspiratory durations during induced vocalization became 0.56 ± 0.15, 0.44 ± 0.12 and 0.67 ± 0.19 (mean ± S.D., n = 9) times, respectively, compared with those before the ISLN section. A decrease in respiratory duration was also observed when local anesthetics were applied to the laryngeal mucosa. The laryngeal deafferentation increased the degree of hoarseness with a decrease in the fundamental frequency. Since the laryngeal deafferentation caused a decrease in the intralaryngeal adductor activities, it was suspected that the voice quality change was partly caused by the reduction in adductor activities. It was thus concluded that feedback via laryngeal afferents plays an important role in controlling vocalization.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ERKZYQY9\\016801029500877V.html},
  keywords = {Anesthetized cat,Laryngeal afferent feedback,Respiratory duration,Superior laryngeal nerve,Vocalization,Voice quality},
  langid = {english},
  number = {1}
}

@article{shinoharaContralateralActivityHomologous2003,
  title = {Contralateral Activity in a Homologous Hand Muscle during Voluntary Contractions Is Greater in Old Adults},
  author = {Shinohara, Minoru and Keenan, Kevin G. and Enoka, Roger M.},
  date = {2003-03-01},
  journaltitle = {Journal of Applied Physiology},
  volume = {94},
  pages = {966--974},
  publisher = {{American Physiological Society}},
  issn = {8750-7587},
  doi = {10.1152/japplphysiol.00836.2002},
  url = {https://journals.physiology.org/doi/full/10.1152/japplphysiol.00836.2002},
  urldate = {2020-09-10},
  abstract = {This study compared the amount of contralateral activity produced in a homologous muscle by young (18–32 yr) and old (66–80 yr) adults when they performed unilateral isometric and anisometric contractions with a hand muscle. The subjects were not aware that the focus of the study was the contralateral activity. The tasks involved the performance of brief isometric contractions to six target forces, slowly lifting and lowering six inertial loads, and completing a set of 10 repetitions with a heavy load. The unintended force exerted by the contralateral muscle during the isometric contractions increased with target force, but the average force was greater for the old adults (means ± SD; 12.6 ± 15.3\%) compared with the young adults (6.91 ± 11.1\%). The contralateral activity also increased with load during the anisometric contractions, and the average contralateral force was greater for the old subjects (5.28 ± 6.29\%) compared with the young subjects (2.10 ± 3.19\%). Furthermore, the average contralateral force for both groups of subjects was greater during the eccentric contractions (4.17 ± 5.24\%) compared with the concentric contractions (3.20 ± 5.20\%). The rate of change in contralateral activity during the fatigue task also differed between the two groups of subjects. The results indicate that old subjects have a reduced ability to suppress unintended contralateral activity during the performance of goal-directed, unilateral tasks.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VSB94W59\\Shinohara et al. - 2003 - Contralateral activity in a homologous hand muscle.pdf;C\:\\Users\\u668173\\Zotero\\storage\\N9XG4MIU\\japplphysiol.00836.html},
  number = {3}
}

@article{shinoharaVisualProprioceptivePerceptions2020,
  title = {Visual and {{Proprioceptive Perceptions Evoke Motion}}-{{Sound Symbolism}}: {{Different Acceleration Profiles Are Associated With Different Types}} of {{Consonants}}},
  shorttitle = {Visual and {{Proprioceptive Perceptions Evoke Motion}}-{{Sound Symbolism}}},
  author = {Shinohara, Kazuko and Kawahara, Shigeto and Tanaka, Hideyuki},
  date = {2020},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  volume = {11},
  pages = {589797},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.589797},
  abstract = {A growing body of literature has shown that one perceptual modality can be systematically associated with sensation in another. However, the cross-modal relationship between linguistic sounds and motions (i.e., motion-sound symbolism) is an extremely understudied area of research. Against this background, this paper examines the cross-modal correspondences between categories of consonants on one hand and different acceleration profiles of motion stimuli on the other. In the two experiments that we conducted, we mechanically manipulated the acceleration profiles of the stimuli while holding the trajectory paths constant, thus distinguishing the effect of acceleration profiles from that of motion path shapes. The results show that different acceleration profiles can be associated with different types of consonants; in particular, movements with acceleration and deceleration tend to be associated with a class of sounds called obstruents, whereas movements without much acceleration tend to be associated with a class of sounds called sonorants. Moreover, the current experiments show that this sort of cross-modal correspondence arises even when the stimuli are not presented visually, namely, when the participants' hands were moved passively by a manipulandum. In conclusion, the present study adds an additional piece of evidence demonstrating that bodily action-based information, i.e., proprioception as a very feasible candidate, could lead to sound symbolic patterns.},
  eprint = {33281688},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6FBAPFMK\\Shinohara et al. - 2020 - Visual and Proprioceptive Perceptions Evoke Motion.pdf},
  keywords = {cross-modal correspondence,non-arbitrariness,obstruents,passive movement,sonorants},
  langid = {english},
  pmcid = {PMC7688920}
}

@article{shockleyArticulatoryConstraintsInterpersonal2007,
  title = {Articulatory Constraints on Interpersonal Postural Coordination},
  author = {Shockley, Kevin and Baker, Aimee A. and Richardson, Michael J. and Fowler, Carol A.},
  date = {2007},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  pages = {201--208},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.33.1.201},
  abstract = {Cooperative conversation has been shown to foster interpersonal postural coordination. The authors investigated whether such coordination is mediated by the influence of articulation on postural sway. In Experiment 1, talkers produced words in synchrony or in alternation, as the authors varied speaking rate and word similarity. Greater shared postural activity was found for the faster speaking rate. In Experiment 2, the authors demonstrated that shared postural activity also increases when individuals speak the same words or speak words that have similar stress patterns. However, this increase in shared postural activity is present only when participants' data are compared with those of their partner, who was present during the task, but not when compared with the data of a member of a different pair speaking the same word sequences as those of the original partner. The authors' findings suggest that interpersonal postural coordination observed during conversation is mediated by convergent speaking patterns. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KNU5IU2G\\2007-01135-014.html},
  keywords = {Articulation (Speech),Conversation,Motor Coordination,Posture},
  number = {1}
}

@article{shockleyMutualInterpersonalPostural2003,
  title = {Mutual Interpersonal Postural Constraints Are Involved in Cooperative Conversation},
  author = {Shockley, Kevin and Santana, Marie-Vee and Fowler, Carol A.},
  date = {2003-04},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {29},
  pages = {326--332},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.29.2.326},
  abstract = {The research was designed to evaluate interpersonal coordination during conversation with a new measurement tool. The experiment uses an analysis based on recurrence strategies, known as cross recurrence quantification, to evaluate the shared activity between 2 postural time series in reconstructed phase space. Pairs of participants were found to share more locations in phase space (greater recurrence) in conditions where they were conversing with one another to solve a puzzle task than in conditions in which they convened with others. The trajectories of pairs of participants also showed less divergence when they conversed with each other than when they conversed with others well. This is offered as objective evidence of interpersonal coordination of postural sway in the context of a cooperative verbal task.},
  eprint = {12760618},
  eprinttype = {pmid},
  keywords = {Adult,Analysis of Variance,Communication,Cooperative Behavior,Data Collection,Evaluation Studies as Topic,Humans,Interpersonal Relations,Kinesics,Posture,Problem Solving,Verbal Behavior},
  langid = {english},
  number = {2}
}

@article{shojaLateralizationRespiratoryControl2008,
  title = {Lateralization of the Respiratory Control Following Unilateral Cerebral Ischemia-Reperfusion Injury},
  author = {Shoja, Mohammadali M. and Tubbs, R. Shane and Jamshidi, Masoud and Shokouhi, Ghaffar and Ansarin, Khalil},
  date = {2008-02-01},
  journaltitle = {Respiratory Physiology \& Neurobiology},
  shortjournal = {Respiratory Physiology \& Neurobiology},
  volume = {160},
  pages = {204--207},
  issn = {1569-9048},
  doi = {10.1016/j.resp.2007.09.014},
  url = {http://www.sciencedirect.com/science/article/pii/S1569904807002662},
  urldate = {2020-06-10},
  abstract = {Cerebral control of respiration has been extensively studied but at present, no evidence of cerebral laterality or dominance for respiration exists. We examined the ventilatory changes following temporary (20min) occlusion of the right or left common carotid artery in rabbits. The corresponding groups of sham-operated rabbits were used as controls. The partial pressure of end-tidal carbon dioxide (PETCO2) was measured with a microstream capnograph before the operation as well as at 6h, and days 1, 4, 9 and 15 postoperation and was used to indicate the ventilatory status. The results showed that following temporary occlusion of the left common carotid artery, subjects began hypoventilation and had a progressive rise in PETCO2 on day 9 postoperation compared to the sham-operated group. However, animals that underwent occlusion of the right common carotid artery hyperventilated from as early as 6h postoperation to days 1 and 4, an effect that ceased up to day 9 postoperation. It was concluded that respiration might be under differential regulation by the two cerebral hemispheres. While the left hemispheric ischemia-reperfusion injury induced hypoventilation that of the right hemisphere resulted in hyperventilation.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\R3BBDBJU\\Shoja et al. - 2008 - Lateralization of the respiratory control followin.pdf;C\:\\Users\\u668173\\Zotero\\storage\\SH34Z4HF\\S1569904807002662.html},
  keywords = {Cerebral hemisphere,Control of ventilation,Ischemia-reperfusion injury,Laterality},
  langid = {english},
  number = {2}
}

@software{sievertPlotlyCreateInteractive2019,
  title = {Plotly: {{Create Interactive Web Graphics}} via 'Plotly.Js'},
  shorttitle = {Plotly},
  author = {Sievert, Carson and Parmer, Chris and Hocking, Toby and Chamberlain, Scott and Ram, Karthik and Corvellec, Marianne and Despouy, Pedro and Inc, Plotly Technologies},
  date = {2019-04-10},
  url = {https://CRAN.R-project.org/package=plotly},
  urldate = {2019-04-23},
  abstract = {Create interactive web graphics from 'ggplot2' graphs and/or a custom interface to the (MIT-licensed) JavaScript library 'plotly.js' inspired by the grammar of graphics.},
  keywords = {WebTechnologies},
  version = {4.9.0}
}

@online{siewCognitiveNetworkScience2019,
  title = {Cognitive {{Network Science}}: {{A Review}} of {{Research}} on {{Cognition}} through the {{Lens}} of {{Network Representations}}, {{Processes}}, and {{Dynamics}}},
  shorttitle = {Cognitive {{Network Science}}},
  author = {Siew, Cynthia S. Q. and Wulff, Dirk U. and Beckage, Nicole M. and Kenett, Yoed N.},
  date = {2019-06-17},
  volume = {2019},
  pages = {e2108423},
  publisher = {{Hindawi}},
  issn = {1076-2787},
  doi = {10.1155/2019/2108423},
  url = {https://www.hindawi.com/journals/complexity/2019/2108423/},
  urldate = {2021-01-29},
  abstract = {Network science provides a set of quantitative methods to investigate complex systems, including human cognition. Although cognitive theories in different domains are strongly based on a network perspective, the application of network science methodologies to quantitatively study cognition has so far been limited in scope. This review demonstrates how network science approaches have been applied to the study of human cognition and how network science can uniquely address and provide novel insight on important questions related to the complexity of cognitive systems and the processes that occur within those systems. Drawing on the literature in cognitive network science, with a focus on semantic and lexical networks, we argue three key points. (i) Network science provides a powerful quantitative approach to represent cognitive systems. (ii) The network science approach enables cognitive scientists to achieve a deeper understanding of human cognition by capturing how the structure, i.e., the underlying network, and processes operating on a network structure interact to produce behavioral phenomena. (iii) Network science provides a quantitative framework to model the dynamics of cognitive systems, operationalized as structural changes in cognitive systems on different timescales and resolutions. Finally, we highlight key milestones that the field of cognitive network science needs to achieve as it matures in order to provide continued insights into the nature of cognitive structures and processes.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AJLTUIVZ\\Siew et al. - 2019 - Cognitive Network Science A Review of Research on.pdf;C\:\\Users\\u668173\\Zotero\\storage\\RGZG5F8U\\2108423.html},
  langid = {english},
  organization = {{Complexity}},
  type = {Review Article}
}

@inproceedings{silvaEffectEndpointsDynamic2016,
  title = {On the {{Effect}} of {{Endpoints}} on {{Dynamic Time Warping}}},
  author = {Silva, D. F. and Batista, G. A. E. P. A. and Keogh, E.},
  date = {2016},
  pages = {10},
  location = {{San Francisco}},
  abstract = {While there exist a plethora of classification algorithms for most data types, there is an increasing acceptance that the unique properties of time series mean that the combination of nearest neighbor classifiers and Dynamic Time Warping (DTW) is very competitive across a host of domains, from medicine to astronomy to environmental sensors. While there has been significant progress in improving the efficiency and effectiveness of DTW in recent years, in this work we demonstrate that an underappreciated issue can significantly degrade the accuracy of DTW in real-world deployments. This issue has probably escaped the attention of the very active time series research community because of its reliance on static highly contrived benchmark datasets, rather than real world dynamic datasets where the problem tends to manifest itself. In essence, the issue is that DTW’s eponymous invariance to warping is only true for the main “body” of the two time series being compared. However, for the “head” and “tail” of the time series, the DTW algorithm affords no warping invariance. The effect of this is that tiny differences at the beginning or end of the time series (which may be either consequential or simply the result of poor “cropping”) will tend to contribute disproportionally to the estimated similarity, producing incorrect classifications. In this work, we show that this effect is real, and reduces the performance of the algorithm. We further show that we can fix the issue with a subtle redesign of the DTW algorithm, and that we can learn an appropriate setting for the extra parameter we introduced. We further demonstrate that our generalization is amiable to all the optimizations that make DTW tractable for large datasets.},
  eventtitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  langid = {english}
}

@article{silvaSteadystateStressOne2007,
  title = {Steady-State Stress at One Hand Magnifies the Amplitude, Stiffness, and Non-Linearity of Oscillatory Behavior at the Other Hand},
  author = {Silva, P. and Moreno, M. and Mancini, M. and Fonseca, S. and Turvey, M. T.},
  date = {2007-12-11},
  journaltitle = {Neuroscience Letters},
  shortjournal = {Neuroscience Letters},
  volume = {429},
  pages = {64--68},
  issn = {0304-3940},
  doi = {10.1016/j.neulet.2007.09.066},
  url = {http://www.sciencedirect.com/science/article/pii/S0304394007010750},
  urldate = {2019-04-18},
  abstract = {Stress at one body segment can influence rhythmic movements of non-neighboring body segments. The nervous, circulatory, and fascia (connective tissue) systems are potential mediators of such remote effects. Assessing them begins with a detailed description of the remote effects. Precisely, how do the rhythmic movements change? In our experiment with seven participants, left-hand oscillations of held pendulums at self-selected frequencies were examined as a function of right-hand tonic forces of 0, 10 or 20\% of the maximum voluntary contraction. We evaluated the effect of the right hand's tonic force on the amplitude and frequency, and the stiffness and friction functions of the left hand's oscillations. Our results suggest that (a) amplitude and stiffness (both linear and non-linear) increased with tonic force but frequency and friction (both linear and non-linear) did not, and (b) the stiffness increases due to right hand 10 and 20\% stress were indifferent to the initial (0\%) left-hand stiffness values. Discussion took note of how the nervous system and architectural features of the body (e.g., its network of connective tissue) may produce such effects.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\24CQGZCP\\Silva et al. - 2007 - Steady-state stress at one hand magnifies the ampl.pdf;C\:\\Users\\u668173\\Zotero\\storage\\QKBQX6EV\\S0304394007010750.html},
  keywords = {Fascia,Remote effects,Rhythmic movements,Tonic force},
  number = {1}
}

@article{singletaryMultimodalPairbondMaintenance2020,
  title = {Multimodal Pair-Bond Maintenance: {{A}} Review of Signaling across Modalities in Pair-Bonded Nonhuman Primates},
  shorttitle = {Multimodal Pair-Bond Maintenance},
  author = {Singletary, Britt and Tecot, Stacey},
  date = {2020},
  journaltitle = {American Journal of Primatology},
  volume = {82},
  pages = {e23105},
  issn = {1098-2345},
  doi = {10.1002/ajp.23105},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ajp.23105},
  urldate = {2020-09-02},
  abstract = {Only a handful of primate species exhibit the social relationship of pair-bonding. Efficient communication is critical for behavioral coordination within pair-bonds to maintain proximity and respond appropriately to extra-pair individuals, and possibly coordinate infant care. The use of complex signaling across modalities may help individuals improve communicative outcomes. We review many ways that pair-bonded species use signals to communicate and maintain bonds, though little previous research has taken a truly multimodal approach within a single species. We make a call for further investigation into pair-bonded communication using a multimodal approach to better understand how these species use all their senses to build, maintain, and advertise their bonds.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ajp.23105},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XXX49W72\\Singletary and Tecot - 2020 - Multimodal pair-bond maintenance A review of sign.pdf;C\:\\Users\\u668173\\Zotero\\storage\\3YAZGHYN\\ajp.html},
  keywords = {communication,multimodal,pair-bonds,signaling,territoriality},
  langid = {english},
  number = {3}
}

@online{sizemoreImportanceWholeTopological2018,
  title = {The Importance of the Whole: Topological Data Analysis for the Network Neuroscientist},
  shorttitle = {The Importance of the Whole},
  author = {Sizemore, Ann E. and Phillips-Cremins, Jennifer and Ghrist, Robert and Bassett, Danielle S.},
  date = {2018-06-13},
  url = {http://arxiv.org/abs/1806.05167},
  urldate = {2020-03-11},
  abstract = {The application of network techniques to the analysis of neural data has greatly improved our ability to quantify and describe these rich interacting systems. Among many important contributions, networks have proven useful in identifying sets of node pairs that are densely connected and that collectively support brain function. Yet the restriction to pairwise interactions prevents us from realizing intrinsic topological features such as cavities within the interconnection structure that may be just as crucial for proper function. To detect and quantify these topological features we must turn to methods from algebraic topology that encode data as a simplicial complex built of sets of interacting nodes called simplices. On this substrate, we can then use the relations between simplices and higher-order connectivity to expose cavities within the complex, thereby summarizing its topological nature. Here we provide an introduction to persistent homology, a fundamental method from applied topology that builds a global descriptor of system structure by chronicling the evolution of cavities as we move through a combinatorial object such as a weighted network. We detail the underlying mathematics and perform demonstrative calculations on the mouse structural connectome, electrical and chemical synapses in \textbackslash textit\{C. elegans\}, and genomic interaction data. Finally we suggest avenues for future work and highlight new advances in mathematics that appear ready for use in revealing the architecture and function of neural systems.},
  archiveprefix = {arXiv},
  eprint = {1806.05167},
  eprinttype = {arxiv},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QVFVEHUB\\Sizemore et al. - 2018 - The importance of the whole topological data anal.pdf;C\:\\Users\\u668173\\Zotero\\storage\\VCRHS85W\\1806.html},
  keywords = {55-01,Quantitative Biology - Neurons and Cognition,Quantitative Biology - Quantitative Methods},
  primaryclass = {q-bio}
}

@article{slocombeLanguageVoidNeed2011,
  title = {The Language Void: The Need for Multimodality in Primate Communication Research},
  shorttitle = {The Language Void},
  author = {Slocombe, Katie E. and Waller, Bridget M. and Liebal, Katja},
  date = {2011-05-01},
  journaltitle = {Animal Behaviour},
  shortjournal = {Animal Behaviour},
  volume = {81},
  pages = {919--924},
  issn = {0003-3472},
  doi = {10.1016/j.anbehav.2011.02.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0003347211000558},
  urldate = {2020-05-29},
  abstract = {Theories of language evolution often draw heavily on comparative evidence of the communicative abilities of extant nonhuman primates (primates). Many theories have argued exclusively for a unimodal origin of language, usually gestural or vocal. Theories are often strengthened by research on primates that indicates the absence of certain linguistic precursors in the opposing communicative modality. However, a systematic review of the primate communication literature reveals that vocal, gestural and facial signals have attracted differing theoretical and methodological approaches, rendering cross-modal comparisons problematic. The validity of the theories based on such comparisons can therefore be questioned. We propose that these a priori biases, inherent in unimodal research, highlight the need for integrated multimodal research. By examining communicative signals in concert we can both avoid methodological discontinuities as well as better understand the phylogenetic precursors to human language as part of a multimodal system.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VDH57FDP\\Slocombe et al. - 2011 - The language void the need for multimodality in p.pdf;C\:\\Users\\u668173\\Zotero\\storage\\4SZ5C4Q9\\S0003347211000558.html},
  keywords = {facial expression,gesture,language evolution,multimodal communication,primate communication,vocalization},
  langid = {english},
  number = {5}
}

@article{slonimskaRoleIconicitySimultaneity2020,
  title = {The Role of Iconicity and Simultaneity for Efficient Communication: {{The}} Case of {{Italian Sign Language}} ({{LIS}})},
  shorttitle = {The Role of Iconicity and Simultaneity for Efficient Communication},
  author = {Slonimska, Anita and Özyürek, Asli and Capirci, Olga},
  date = {2020-07-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {200},
  pages = {104246},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104246},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027720300652},
  urldate = {2020-04-25},
  abstract = {A fundamental assumption about language is that, regardless of language modality, it faces the linearization problem, i.e., an event that occurs simultaneously in the world has to be split in language to be organized on a temporal scale. However, the visual modality of signed languages allows its users not only to express meaning in a linear manner but also to use iconicity and multiple articulators together to encode information simultaneously. Accordingly, in cases when it is necessary to encode informatively rich events, signers can take advantage of simultaneous encoding in order to represent information about different referents and their actions simultaneously. This in turn would lead to more iconic and direct representation. Up to now, there has been no experimental study focusing on simultaneous encoding of information in signed languages and its possible advantage for efficient communication. In the present study, we assessed how many information units can be encoded simultaneously in Italian Sign Language (LIS) and whether the amount of simultaneously encoded information varies based on the amount of information that is required to be expressed. Twenty-three deaf adults participated in a director-matcher game in which they described 30 images of events that varied in amount of information they contained. Results revealed that as the information that had to be encoded increased, signers also increased use of multiple articulators to encode different information (i.e., kinematic simultaneity) and density of simultaneously encoded information in their production. Present findings show how the fundamental properties of signed languages, i.e., iconicity and simultaneity, are used for the purpose of efficient information encoding in Italian Sign Language (LIS).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QJ67B222\\S0010027720300652.html},
  keywords = {Efficient communication,Iconicity,Linearization problem,Sign language,Simultaneity},
  langid = {english}
}

@article{smithAnatomicalCharacteristicsUpper2003,
  title = {Anatomical {{Characteristics}} of the {{Upper Serratus Anterior}}: {{Cadaver Dissection}}},
  shorttitle = {Anatomical {{Characteristics}} of the {{Upper Serratus Anterior}}},
  author = {Smith, Russell and Nyquist-Battie, Cynthia and Clark, Mark and Rains, Julie},
  date = {2003-08-01},
  journaltitle = {Journal of Orthopaedic \& Sports Physical Therapy},
  shortjournal = {J Orthop Sports Phys Ther},
  volume = {33},
  pages = {449--454},
  publisher = {{Journal of Orthopaedic \& Sports Physical Therapy}},
  issn = {0190-6011},
  doi = {10.2519/jospt.2003.33.8.449},
  url = {https://www.jospt.org/doi/abs/10.2519/jospt.2003.33.8.449},
  urldate = {2020-04-21},
  abstract = {Study DesignA descriptive study of the anatomical characteristics of the upper serratus anterior.ObjectivesTo delineate the upper serratus anterior with comparison to classical descriptions of the anatomy of the muscle as a whole.BackgroundAlthough the serratus anterior has a major role in scapulothoracic stability, description of the separate function and anatomy of the upper, middle, and lower portions of the muscle has been limited.Methods and MeasuresBilateral anatomical dissection of 8 cadavers (3 female and 5 male) exposed 13 serratus anterior and surrounding structures for review. The number of serrations, attachment sites, length, and girth of the upper serratus anterior were measured.ResultsThe upper serratus anterior presented with dual serrations and single serrations in 7 (54\%) and 6 (46\%) of 13 observations, respectively. Attachments to both first and second ribs were noted in 6 (46\%) of the 13 observations. The remaining proximal attachments were to the second rib only, the first rib only, and dual attachments to the second and third ribs. In all cases, cranial attachments were to the superior scapular angle blending with the levator scapulae attachment. Length ranged from 4.8 to 9.0 cm (mean ± SD, 6.9 ± 1.2 cm). The girth ranged from 3.0 to 8.5 cm (mean ± SD, 6.1 ± 1.5 cm). One or more branches of the long thoracic nerve were observed to consistently innervate the upper serratus anterior fibers.ConclusionThe upper serratus anterior demonstrated wide variation in anatomy and was noted to be distinct in appearance and peripheral innervation from the middle and lower serratus anterior.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LWX5XBGH\\Smith et al. - 2003 - Anatomical Characteristics of the Upper Serratus A.pdf;C\:\\Users\\u668173\\Zotero\\storage\\54C9YN3T\\jospt.2003.33.8.html},
  number = {8}
}

@article{smithDevelopmentEmbodiedCognition2005,
  title = {The {{Development}} of {{Embodied Cognition}}: {{Six Lessons}} from {{Babies}}},
  shorttitle = {The {{Development}} of {{Embodied Cognition}}},
  author = {Smith, Linda and Gasser, Michael},
  date = {2005-01},
  journaltitle = {Artificial Life},
  volume = {11},
  pages = {13--29},
  issn = {1064-5462, 1530-9185},
  doi = {10.1162/1064546053278973},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/1064546053278973},
  urldate = {2020-10-20},
  abstract = {The embodiment hypothesis is the idea that intelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor activity. We offer six lessons for developing embodied intelligent agents suggested by research in developmental psychology. We argue that starting as a baby grounded in a physical, social, and linguistic world is crucial to the development of the flexible and inventive intelligence that characterizes humankind.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VX5XXRGC\\Smith and Gasser - 2005 - The Development of Embodied Cognition Six Lessons.pdf},
  langid = {english},
  number = {1-2}
}

@article{smithIntelligibilitySentencesRecorded2003,
  title = {Intelligibility of Sentences Recorded from the Uterus of a Pregnant Ewe and from the Fetal Inner Ear},
  author = {Smith, Sherri L. and Gerhardt, Kenneth J. and Griffiths, Scott K. and Huang, Xinyan and Abrams, Robert M.},
  year = {2003 Nov-Dec},
  journaltitle = {Audiology \& Neuro-Otology},
  shortjournal = {Audiol. Neurootol.},
  volume = {8},
  pages = {347--353},
  issn = {1420-3030},
  doi = {10.1159/000073519},
  abstract = {The intelligibility of sentences recorded from the uterus of a pregnant ewe and from the near-term fetal sheep inner ear was judged by 30 listeners. Sentences were presented to the ewe at 95 and 105 dB SPL while sequential recordings of sound with a hydrophone and a cochlear microphonic (CM) with electrodes were made. Recordings were randomized and presented to listeners to judge the intelligibility of sentences processed through the ewe and fetal inner ear. Intelligibility scores were nearly 99\% for air and uterus conditions, dropped to 73\% for CM ex utero and to 41\% for CM in utero. Results indicated that filtering provided by the tissues and fluids of the maternal abdomen did not affect sentence intelligibility significantly, but the filtering effects as the signal passed into the fetal inner ear resulted in much poorer intelligibility.},
  eprint = {14566105},
  eprinttype = {pmid},
  keywords = {Animals,Cochlear Microphonic Potentials,Ear; Inner,Female,Fetus,Humans,Male,Pregnancy,Sheep,Sound,Speech Intelligibility,Uterus,Voice Quality},
  langid = {english},
  number = {6}
}

@article{smithInteractionGlottalpulseRate2005,
  title = {The Interaction of Glottal-Pulse Rate and Vocal-Tract Length in Judgements of Speaker Size, Sex, and Age},
  author = {Smith, David R. R. and Patterson, Roy D.},
  date = {2005-11},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {118},
  pages = {3177--3186},
  issn = {0001-4966},
  doi = {10.1121/1.2047107},
  abstract = {Glottal-pulse rate (GPR) and vocal-tract length (VTL) are related to the size, sex, and age of the speaker but it is not clear how the two factors combine to influence our perception of speaker size, sex, and age. This paper describes experiments designed to measure the effect of the interaction of GPR and VTL upon judgements of speaker size, sex, and age. Vowels were scaled to represent people with a wide range of GPRs and VTLs, including many well beyond the normal range of the population, and listeners were asked to judge the size and sex/age of the speaker. The judgements of speaker size show that VTL has a strong influence upon perceived speaker size. The results for the sex and age categorization (man, woman, boy, or girl) show that, for vowels with GPR and VTL values in the normal range, judgements of speaker sex and age are influenced about equally by GPR and VTL. For vowels with abnormal combinations of low GPRs and short VTLs, the VTL information appears to decide the sex/age judgement.},
  eprint = {16334696},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Adult,Age Factors,Child,Female,Glottis,Humans,Male,Organ Size,Sex Characteristics,Speech Perception,Vocal Cords},
  langid = {english},
  number = {5},
  pmcid = {PMC2346770}
}

@article{smithInteractionsSpeechFinger1986,
  title = {Interactions between Speech and Finger Movements: An Exploration on the Dynamic Pattern Perspective},
  shorttitle = {Interactions between Speech and Finger Movements},
  author = {Smith, A. and McFarland, D. H. and Weber, C. M.},
  date = {1986-12},
  journaltitle = {Journal of Speech and Hearing Research},
  shortjournal = {J Speech Hear Res},
  volume = {29},
  pages = {471--480},
  issn = {0022-4685},
  doi = {10.1044/jshr.2904.471},
  abstract = {Under the dynamic perspective for motor control, movement is viewed as an "emergent" property, arising from cooperative relationships among limit-cycle oscillators. This is in contrast to more traditional approaches to motor control in which a central representation of movement is usually assumed to exist. The assertion that coordination of movement arises from interactions between oscillatory processes leads to some unexpected predictions. In particular, interactions between frequency and amplitude of simultaneously performed speech and manual tasks are predicted, because the organism "parameterizes" all ongoing oscillatory processes as a "total unit." Using quantitative analyses, we have demonstrated that interactions between speaking and finger tapping do occur in both the amplitude and frequency domains. Such interactions, however, do not appear to be as simple as those predicted by proponents of the dynamic pattern perspective.},
  eprint = {3795889},
  eprinttype = {pmid},
  keywords = {Female,Fingers,Humans,Male,Movement,Psychomotor Performance,Speech,Speech Acoustics,Time Factors},
  langid = {english},
  number = {4}
}

@article{smithNewHeuristicCapturing2013,
  title = {A New Heuristic for Capturing the Complexity of Multimodal Signals},
  author = {Smith, Carolynn L. and Evans, Christopher S.},
  date = {2013-09-01},
  journaltitle = {Behavioral Ecology and Sociobiology},
  shortjournal = {Behav Ecol Sociobiol},
  volume = {67},
  pages = {1389--1398},
  issn = {1432-0762},
  doi = {10.1007/s00265-013-1490-0},
  url = {https://doi.org/10.1007/s00265-013-1490-0},
  urldate = {2020-10-13},
  abstract = {Many animal signals are inherently multimodal, engaging more than one of the receiver’s sensory systems simultaneously, and it is the interaction between the two modalities that determines the signal’s function (s) and efficacy. It is hence necessary to quantify the effect of each modality relative to the other in order to fully understand animal communication. We have developed a new heuristic to aid in the identification and interpretation of the many distinct ways in which signals in multiple sensory modalities interact. Our approach represents natural variation in signal production for each modality and uses these to generate three-dimensional receiver response surface plots that map the relationships among the signal components and receiver behavior. We accommodate the extant hypotheses for the interactions between modalities, each of which makes a clear prediction about the shape of the response surface, and extend previous theory by considering new phenomena.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QTT9FRLL\\Smith and Evans - 2013 - A new heuristic for capturing the complexity of mu.pdf},
  langid = {english},
  number = {9}
}

@article{smithVestibularFeedbackMaintains2017,
  title = {Vestibular Feedback Maintains Reaching Accuracy during Body Movement},
  author = {Smith, Craig P. and Reynolds, Raymond F.},
  date = {2017},
  journaltitle = {The Journal of Physiology},
  volume = {595},
  pages = {1339--1349},
  issn = {1469-7793},
  doi = {10.1113/JP273125},
  url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/JP273125},
  urldate = {2020-06-30},
  abstract = {Key points Reaching movements can be perturbed by vestibular input, but the function of this response is unclear. Here, we applied galvanic vestibular stimulation concurrently with real body movement while subjects maintained arm position either fixed in space or fixed with respect to their body. During the fixed-in-space conditions, galvanic vestibular stimulation caused large changes in arm trajectory consistent with a compensatory response to maintain upper-limb accuracy in the face of body movement. Galvanic vestibular stimulation responses were absent during the body-fixed task, demonstrating task dependency in vestibular control of the upper limb. The results suggest that the function of vestibular-evoked arm movements is to maintain the accuracy of the upper limb during unpredictable body movement, but only when reaching in an earth-fixed reference frame. Abstract When using our arms to interact with the world, unintended body motion can introduce movement error. A mechanism that could detect and compensate for such motion would be beneficial. Observations of arm movements evoked by vestibular stimulation provide some support for this mechanism. However, the physiological function underlying these artificially evoked movements is unclear from previous research. For such a mechanism to be functional, it should operate only when the arm is being controlled in an earth-fixed rather than a body-fixed reference frame. In the latter case, compensation would be unnecessary and even deleterious. To test this hypothesis, subjects were gently rotated in a chair while being asked to maintain their outstretched arm pointing towards either earth-fixed or body-fixed memorized targets. Galvanic vestibular stimulation was applied concurrently during rotation to isolate the influence of vestibular input, uncontaminated by inertial factors. During the earth-fixed task, galvanic vestibular stimulation produced large polarity-dependent corrections in arm position. These corrections mimicked those evoked when chair velocity was altered without any galvanic vestibular stimulation, indicating a compensatory arm response to a sensation of altered body motion. In stark contrast, corrections were completely absent during the body-fixed task, despite the same chair movement profile and arm posture. These effects persisted when we controlled for differences in limb kinematics between the two tasks. Our results demonstrate that vestibular control of the upper limb maintains reaching accuracy during unpredictable body motion. The observation that such responses occurred only when reaching within an earth-fixed reference frame confirms the functional nature of vestibular-evoked arm movement.},
  annotation = {\_eprint: https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/JP273125},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NB3PUACJ\\Smith and Reynolds - 2017 - Vestibular feedback maintains reaching accuracy du.pdf;C\:\\Users\\u668173\\Zotero\\storage\\F64Z2CUF\\JP273125.html},
  keywords = {galvanic vestibular stimulation,upper-limb control,vestibular system},
  langid = {english},
  number = {4}
}

@report{sorrentinoTimeScaleSeparation2020,
  title = {Time Scale Separation of Information Processing between Sensory and Associative Regions},
  author = {Sorrentino, P and Rabuffo, G and Rucco, R and Baselice, F and Troisi Lopez, E and Liparoti, M and Quarantelli, M and Sorrentino, G and Bernard, C and Jirsa, V},
  date = {2020-10-24},
  institution = {{Neuroscience}},
  doi = {10.1101/2020.10.23.350322},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.10.23.350322},
  urldate = {2020-11-24},
  abstract = {Stimulus perception is assumed to involve the (fast) detection of sensory inputs and their (slower) integration. The capacity of the brain to quickly adapt, at all times, to unexpected stimuli suggests that the interplay between the slow and fast processes happens at short timescales. We hypothesised that, even during resting-state, the flow of information across the brain regions should evolve quickly, but not homogeneously in time. Here we used high temporal-resolution Magnetoencephalography (MEG) signals to estimate the persistence of the information in functional links across the brain. We show that short- and long-lasting retention of the information, entailing different speeds in the update rate, naturally split the brain into two anatomically distinct subnetworks. The “fast updating network” (FUN) is localized in the regions that typically belong to the dorsal and ventral streams during perceptive tasks, while the “slow updating network” (SUN) hinges classically associative areas. Finally, we show that only a subset of the brain regions, which we name the multi-storage core (MSC), belongs to both subnetworks. The MSC is hypothesized to play a role in the communication between the (otherwise) segregated subnetworks.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5WD8EXHQ\\Sorrentino et al. - 2020 - Time scale separation of information processing be.pdf},
  langid = {english},
  type = {preprint}
}

@article{sparrowGestureCommunicationAdult2020,
  title = {Gesture, {{Communication}}, and {{Adult Acquired Hearing Loss}}},
  author = {Sparrow, Karen and Lind, Christopher and van Steenbrugge, Willem},
  date = {2020-07-08},
  journaltitle = {Journal of Communication Disorders},
  shortjournal = {Journal of Communication Disorders},
  pages = {106030},
  issn = {0021-9924},
  doi = {10.1016/j.jcomdis.2020.106030},
  url = {http://www.sciencedirect.com/science/article/pii/S0021992420300988},
  urldate = {2020-07-14},
  abstract = {Nonverbal communication, specifically hand and arm movements (commonly known as gesture), has long been recognized and explored as a significant element in human interaction as well as potential compensatory behavior for individuals with communication difficulties. The use of gesture as a compensatory communication method in expressive and receptive human communication disorders has been the subject of much investigation. Yet within the context of adult acquired hearing loss, gesture has received limited research attention and much remains unknown about patterns of nonverbal behaviors in conversations in which hearing loss is a factor. This paper presents key elements of the background of gesture studies and the theories of gesture function and production followed by a review of research focused on adults with hearing loss and the role of gesture and gaze in rehabilitation. The current examination of the visual resource of co-speech gesture in the context of everyday interactions involving adults with acquired hearing loss suggests the need for the development of an evidence base to affect enhancements and changes in the way in which rehabilitation services are conducted.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BJX9WQMF\\Sparrow et al. - 2020 - Gesture, Communication, and Adult Acquired Hearing.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZQSI9P4U\\S0021992420300988.html},
  keywords = {adult hearing loss,aural rehabilitation,conversation,gaze,gesture},
  langid = {english},
  options = {useprefix=true}
}

@article{SpeechRatesConverge,
  title = {Speech Rates Converge in Scripted Turn-Taking Conversations}
}

@article{spenceCrossmodalCorrespondencesTutorial2011,
  title = {Crossmodal Correspondences: {{A}} Tutorial Review},
  shorttitle = {Crossmodal Correspondences},
  author = {Spence, Charles},
  date = {2011-05-01},
  journaltitle = {Attention, Perception, \& Psychophysics},
  shortjournal = {Atten Percept Psychophys},
  volume = {73},
  pages = {971--995},
  issn = {1943-393X},
  doi = {10.3758/s13414-010-0073-7},
  url = {https://doi.org/10.3758/s13414-010-0073-7},
  urldate = {2020-09-18},
  abstract = {In many everyday situations, our senses are bombarded by many different unisensory signals at any given time. To gain the most veridical, and least variable, estimate of environmental stimuli/properties, we need to combine the individual noisy unisensory perceptual estimates that refer to the same object, while keeping those estimates belonging to different objects or events separate. How, though, does the brain “know” which stimuli to combine? Traditionally, researchers interested in the crossmodal binding problem have focused on the roles that spatial and temporal factors play in modulating multisensory integration. However, crossmodal correspondences between various unisensory features (such as between auditory pitch and visual size) may provide yet another important means of constraining the crossmodal binding problem. A large body of research now shows that people exhibit consistent crossmodal correspondences between many stimulus features in different sensory modalities. For example, people consistently match high-pitched sounds with small, bright objects that are located high up in space. The literature reviewed here supports the view that crossmodal correspondences need to be considered alongside semantic and spatiotemporal congruency, among the key constraints that help our brains solve the crossmodal binding problem.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YEWF3Z63\\Spence - 2011 - Crossmodal correspondences A tutorial review.pdf},
  langid = {english},
  number = {4}
}

@article{steelsEvolvingGroundedCommunication2003,
  title = {Evolving Grounded Communication for Robots},
  author = {Steels, Luc},
  date = {2003-07-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {7},
  pages = {308--312},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(03)00129-3},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661303001293},
  urldate = {2021-02-27},
  abstract = {The computational and robotic synthesis of language evolution is emerging as a new exciting field of research. The objective is to come up with precise operational models of how communities of agents, equipped with a cognitive apparatus, a sensori-motor system, and a body, can arrive at shared grounded communication systems. Such systems may have similar characteristics to animal communication or human language. Apart from its technological interest in building novel applications in the domain of human–robot or robot–robot interaction, this research is of interest to the many disciplines concerned with the origins and evolution of language and communication.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QRFD6YHM\\Steels - 2003 - Evolving grounded communication for robots.pdf;C\:\\Users\\u668173\\Zotero\\storage\\L3T5YR5M\\S1364661303001293.html},
  langid = {english},
  number = {7}
}

@article{steinBehavioralIndicesMultisensory1989,
  title = {Behavioral Indices of Multisensory Integration: {{Orientation}} to Visual Cues Is Affected by Auditory Stimuli},
  shorttitle = {Behavioral {{Indices}} of {{Multisensory Integration}}},
  author = {Stein, Barry E. and Meredith, M. Alex and Huneycutt, W. Scott and McDade, Lawrence},
  date = {1989-01-01},
  journaltitle = {Journal of Cognitive Neuroscience},
  volume = {1},
  pages = {12--24},
  publisher = {{MIT Press}},
  issn = {0898-929X},
  doi = {10.1162/jocn.1989.1.1.12},
  url = {https://doi.org/10.1162/jocn.1989.1.1.12},
  urldate = {2020-12-05},
  abstract = {Physiological studies have demonstrated that inputs from different sensory modalities converge on, and are integrated by, individual superior colliculus neurons and that this integration is governed by specific spatial rules. The present experiments were an attempt to relate these neural processes to overt behavior by determining if behaviors believed to involve the circuitry of the superior colliculus would show similar multisensory dependencies and be subject to the same rules of integration. The neurophysiological-behavioral parallels proved to be striking. The effectiveness of a stimulus of one modality in eliciting attentive and orientation behaviors was dramatically affected by the presence of a stimulus from another modality in each of the three behavioral paradigms used here. Animals trained to approach a low intensity visual cue had their performance significantly enhanced when a brief, low intensity auditory stimulus was presented at the same location as the visual cue, but their performance was significantly depressed when the auditory stimulus was disparate to it. These effects were independent of the animals' experience with the modifying (i.e. auditory) stimulus and exceeded what might have been predicted statistically based on the animals' performance with each single-modality cue. The multiplicative nature of these multisensory interactions and their dependence on the relative positions and intensities of the two stimuli were all very similar to those observed physiologically for single cells. The few differences that were observed appeared to reflect the fact that understanding integration at the level of the single cell requires reference to the individual cell's multisensory receptive field properties, while at the behavioral level populations of receptive fields must be evaluated. These data illustrate that the rules governing multisensory integration at the level of the single cell also predict responses to these stimuli in the intact behaving organism.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PL3YFMHJ\\jocn.1989.1.1.html},
  number = {1}
}

@book{steinMergingSenses1993,
  title = {The Merging of the Senses},
  author = {Stein, B. E. and Meredith, M. A.},
  date = {1993},
  publisher = {{MIT Press}},
  location = {{Massachusetts}}
}

@article{stennekenSelfinducedReactiveTriggering2002,
  title = {Self-Induced versus Reactive Triggering of Synchronous Movements in a Deafferented Patient and Control Subjects},
  author = {Stenneken, P. and Aschersleben, G. and Cole, J. and Prinz, W.},
  date = {2002-02},
  journaltitle = {Psychological Research},
  volume = {66},
  pages = {40--49},
  issn = {0340-0727, 1430-2772},
  doi = {10.1007/s004260100072},
  url = {http://link.springer.com/10.1007/s004260100072},
  urldate = {2019-04-19},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EAPIWRAU\\Stenneken et al. - 2002 - Self-induced versus reactive triggering of synchro.pdf},
  langid = {english},
  number = {1}
}

@article{stennekenSelfinducedReactiveTriggering2002a,
  title = {Self-Induced versus Reactive Triggering of Synchronous Movements in a Deafferented Patient and Control Subjects},
  author = {Stenneken, P. and Aschersleben, G. and Cole, J. D. and Prinz, W.},
  date = {2002-02-01},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  volume = {66},
  pages = {40--49},
  issn = {1430-2772},
  doi = {10.1007/s004260100072},
  url = {https://doi.org/10.1007/s004260100072},
  urldate = {2019-04-02},
  abstract = {. The present study investigates the contribution of tactile-kinesthetic information to the timing of movements. The relative timing of simultaneous tapping movements of finger and foot (hand-foot asynchrony) was examined in a simple reaction time task and in discrete self-initiated taps (Experiment 1), and in externally triggered synchronization tapping (Experiment 2). We compared the performance of a deafferented participant (IW) to the performance of two control groups of different ages. The pattern of results in control groups replicates previous findings: Whereas positive hand-foot asynchronies (hand precedes foot) are observed in a simultaneous reaction to an auditory stimulus, hand-foot asynchronies are negative with discrete self-initiated as well as auditorily paced sequences of synchronized finger and foot taps. In the first case, results are explained by a simultaneous triggering of motor commands. In contrast, self-initiated and auditorily paced movements are assumed to be controlled in terms of their afferent consequences, as provided by tactile-kinesthetic information. The performance of the deafferented participant differed from that of healthy participants in some aspects. As expected on the basis of unaffected motor functions, the participant was able to generate finger and foot movements in reaction to an external signal. In spite of the lack of movement-contingent sensory feedback, the deafferented participant showed comparable timing errors in self-initiated and regularly paced tapping as observed in control participants. However, in discrete self-initiated taps IW's hand-foot asynchronies were considerably larger than in control participants, while performance did not differ from that of controls in continuous movement generation. These findings are discussed in terms of an internal generation of the movement's sensory consequences (forward-modeling).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MNW3S2WF\\Stenneken et al. - 2002 - Self-induced versus reactive triggering of synchro.pdf},
  keywords = {Control Participant,Motor Command,Reactive Trigger,Simple Reaction Time Task,Synchronization Task},
  langid = {english},
  number = {1}
}

@article{sterelnyLanguageGestureSkill2012,
  title = {Language, Gesture, Skill: The Co-Evolutionary Foundations of Language},
  shorttitle = {Language, Gesture, Skill},
  author = {Sterelny, Kim},
  date = {2012-08-05},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {367},
  pages = {2141--2151},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2012.0116},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0116},
  urldate = {2020-02-24},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\56LVW5VW\\Sterelny - 2012 - Language, gesture, skill the co-evolutionary foun.pdf},
  langid = {english},
  number = {1599}
}

@article{sternadDynamicsCoordinationGeneralizing1999,
  title = {Dynamics of 1:2 {{Coordination}}: {{Generalizing Relative Phase}} to {\emph{n:M}} {{Rhythms}}},
  shorttitle = {Dynamics of 1},
  author = {Sternad, Dagmar and Turvey, M. T. and Saltzman, Elliot L.},
  date = {1999-09},
  journaltitle = {Journal of Motor Behavior},
  volume = {31},
  pages = {207--223},
  issn = {0022-2895, 1940-1027},
  doi = {10.1080/00222899909600989},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00222899909600989},
  urldate = {2020-07-13},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BX2WN3TV\\Sternad et al. - 1999 - Dynamics of 12 Coordination Generalizing Relativ.pdf},
  langid = {english},
  number = {3}
}

@book{stetsonMotorPhoneticsStudy1928,
  title = {Motor {{Phonetics}}: {{A Study}} of {{Speech Movements}} in {{Action}}},
  shorttitle = {Motor {{Phonetics}}},
  author = {Stetson, R. H.},
  date = {1928},
  publisher = {{Springer Netherlands}},
  url = {https://www.springer.com/gp/book/9789401521475},
  urldate = {2019-08-08},
  abstract = {Motor Phonetics...},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZEVAZGJY\\9789401521475.html},
  isbn = {978-94-015-2147-5},
  langid = {english}
}

@book{stetsonMotorPhoneticsStudy1928a,
  title = {Motor {{Phonetics}}: {{A Study}} of {{Speech Movements}} in {{Action}}},
  shorttitle = {Motor {{Phonetics}}},
  author = {Stetson, R. H.},
  date = {1928},
  publisher = {{Springer Netherlands}},
  doi = {10.1007/978-94-015-3356-0},
  url = {https://www.springer.com/gp/book/9789401521475},
  urldate = {2021-02-27},
  abstract = {Motor Phonetics...},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\C366VBPG\\9789401521475.html},
  isbn = {978-94-015-2147-5},
  langid = {english}
}

@article{steyversLargeScaleStructureSemantic2005,
  title = {The {{Large}}-{{Scale Structure}} of {{Semantic Networks}}: {{Statistical Analyses}} and a {{Model}} of {{Semantic Growth}}},
  shorttitle = {The {{Large}}-{{Scale Structure}} of {{Semantic Networks}}},
  author = {Steyvers, Mark and Tenenbaum, Joshua B.},
  date = {2005},
  journaltitle = {Cognitive Science},
  volume = {29},
  pages = {41--78},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2901_3},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2901_3},
  urldate = {2021-01-23},
  abstract = {We present statistical analyses of the large-scale structure of 3 types of semantic networks: word associations, WordNet, and Roget's Thesaurus. We show that they have a small-world structure, characterized by sparse connectivity, short average path lengths between words, and strong local clustering. In addition, the distributions of the number of connections follow power laws that indicate a scale-free pattern of connectivity, with most nodes having relatively few connections joined together through a small number of hubs with many connections. These regularities have also been found in certain other complex natural networks, such as the World Wide Web, but they are not consistent with many conventional models of semantic organization, based on inheritance hierarchies, arbitrarily structured networks, or high-dimensional vector spaces. We propose that these structures reflect the mechanisms by which semantic networks grow. We describe a simple model for semantic growth, in which each new word or concept is connected to an existing network by differentiating the connectivity pattern of an existing node. This model generates appropriate small-world statistics and power-law connectivity distributions, and it also suggests one possible mechanistic basis for the effects of learning history variables (age of acquisition, usage frequency) on behavioral performance in semantic processing tasks.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog2901\_3},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CXQ4FGBM\\Steyvers and Tenenbaum - 2005 - The Large-Scale Structure of Semantic Networks St.pdf;C\:\\Users\\u668173\\Zotero\\storage\\3NA7IYDD\\s15516709cog2901_3.html},
  keywords = {Growing network models,Semantic networks,Semantic representation,Small worlds},
  langid = {english},
  number = {1}
}

@article{stickfordVentilationLocomotionHumans2014,
  title = {Ventilation and {{Locomotion}} in {{Humans}}: {{Mechanisms}}, {{Implications}}, and {{Perturbations}} to the {{Coupling}} of {{These Two Rhythms}}},
  shorttitle = {Ventilation and {{Locomotion}} in {{Humans}}},
  author = {Stickford, Abigail S. L. and Stickford, Jonathon L.},
  date = {2014-12-01},
  journaltitle = {Springer Science Reviews},
  shortjournal = {Springer Science Reviews},
  volume = {2},
  pages = {95--118},
  issn = {2213-7793},
  doi = {10.1007/s40362-014-0020-4},
  url = {https://doi.org/10.1007/s40362-014-0020-4},
  urldate = {2020-09-19},
  abstract = {To best sustain endurance activity, two systems must be effectively coordinated: ventilation and locomotion. Evidence has long suggested that these two mammalian rhythms are linked, yet determinants and implications of locomotor–respiratory coupling (LRC) continue to be investigated. Two general areas explaining the potential mechanisms underlying LRC are (1) neural interactions between central and peripheral controllers of locomotion and respiration, and (2) mechanical interactions between locomotor dynamics and respiratory mechanics. Additional suggested explanations for/consequence of the existence of LRC in mammals include an improved energetic cost of locomotion and a reduced sensation of breathlessness. As such, any perturbation to LRC, via alterations in breathing or kinematic patterns, could have negative performance implications to both athlete and patient populations.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6J5PEXTI\\Stickford and Stickford - 2014 - Ventilation and Locomotion in Humans Mechanisms, .pdf},
  langid = {english},
  number = {1}
}

@article{stiversUniversalsCulturalVariation2009,
  title = {Universals and Cultural Variation in Turn-Taking in Conversation},
  author = {Stivers, Tanya and Enfield, N. J. and Brown, Penelope and Englert, Christina and Hayashi, Makoto and Heinemann, Trine and Hoymann, Gertie and Rossano, Federico and de Ruiter, Jan Peter and Yoon, Kyung-Eun and Levinson, Stephen C.},
  date = {2009-06-30},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {106},
  pages = {10587--10592},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0903616106},
  url = {https://www.pnas.org/content/106/26/10587},
  urldate = {2020-05-28},
  abstract = {Informal verbal interaction is the core matrix for human social life. A mechanism for coordinating this basic mode of interaction is a system of turn-taking that regulates who is to speak and when. Yet relatively little is known about how this system varies across cultures. The anthropological literature reports significant cultural differences in the timing of turn-taking in ordinary conversation. We test these claims and show that in fact there are striking universals in the underlying pattern of response latency in conversation. Using a worldwide sample of 10 languages drawn from traditional indigenous communities to major world languages, we show that all of the languages tested provide clear evidence for a general avoidance of overlapping talk and a minimization of silence between conversational turns. In addition, all of the languages show the same factors explaining within-language variation in speed of response. We do, however, find differences across the languages in the average gap between turns, within a range of 250 ms from the cross-language mean. We believe that a natural sensitivity to these tempo differences leads to a subjective perception of dramatic or even fundamental differences as offered in ethnographic reports of conversational style. Our empirical evidence suggests robust human universals in this domain, where local variations are quantitative only, pointing to a single shared infrastructure for language use with likely ethological foundations.},
  eprint = {19553212},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BV6JI9E2\\Stivers et al. - 2009 - Universals and cultural variation in turn-taking i.pdf;C\:\\Users\\u668173\\Zotero\\storage\\LJZJKGA6\\10587.html},
  isbn = {9780903616102},
  keywords = {cooperation,response speed,social interaction},
  langid = {english},
  number = {26}
}

@article{stoffregenSpecificationSenses2001,
  title = {On Specification and the Senses},
  author = {Stoffregen, Thomas A. and Bardy, Benoît G.},
  date = {2001-04},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {24},
  pages = {195--213},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X01003946},
  url = {https://www.cambridge.org/core/product/identifier/S0140525X01003946/type/journal_article},
  urldate = {2020-09-02},
  abstract = {In this target article we question the assumption that perception is divided into separate domains of vision, hearing, touch, taste, and smell. We review implications of this assumption for theories of perception and for our understanding of ambient energy arrays (e.g., the optic and acoustic arrays) that are available to perceptual systems. We analyze three hypotheses about relations between ambient arrays and physical reality: (1) that there is an ambiguous relation between ambient energy arrays and physical reality, (2) that there is a unique relation between individual energy arrays and physical reality, and (3) that there is a redundant but unambiguous relation, within or across arrays, between energy arrays and physical reality. This is followed by a review of the physics of motion, focusing on the existence and status of referents for physical motion. Our review indicates that it is not possible, in principle, for there to be a unique relation between physical motion and the structure of individual energy arrays. We argue that physical motion relative to different referents is specified only in the global array, which consists of higher-order relations across different forms of energy. The existence of specificity in the global array is consistent with the idea of direct perception, and so poses a challenge to traditional, inference-based theories of perception and cognition. However, it also presents a challenge to much of the ecological approach to perception and action, which has accepted the assumption of separate senses.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QSHJQM45\\Stoffregen and Bardy - 2001 - On specification and the senses.pdf},
  langid = {english},
  number = {2}
}

@article{stoltmannSyllablepointingGestureCoordination2017,
  title = {Syllable-Pointing Gesture Coordination in {{Polish}} Counting out Rhymes: {{The}} Effect of Speech Rate},
  author = {Stoltmann, K. and Fuchs, S.},
  date = {2017},
  journaltitle = {Journal of Multimodal Communication Studies},
  volume = {4},
  pages = {63--68},
  number = {1-2}
}

@article{strausPrimatesComparativeAnatomy1956,
  title = {Primates (Comparative Anatomy and Taxonomy). {{I}}. {{Strepsirhini}}. {{By W}}. {{C}}. {{Osman Hill}}, Pp. Xxiv + 798. 5 Pounds 5 Shillings. {{University Press}}, {{Edinburgh}}. 1953; {{II}}. {{Haplorhini}}: {{Tarsioidea}}. {{By W}}. {{C}}. {{Osman Hill}}, Pp. Xx + 347. 63 Shillings, {{University Press}}, {{Edinburgh}}. 1955},
  shorttitle = {Primates (Comparative Anatomy and Taxonomy). {{I}}. {{Strepsirhini}}. {{By W}}. {{C}}. {{Osman Hill}}, Pp. Xxiv + 798. 5 Pounds 5 Shillings. {{University Press}}, {{Edinburgh}}. 1953; {{II}}. {{Haplorhini}}},
  author = {Straus, William L.},
  date = {1956},
  journaltitle = {American Journal of Physical Anthropology},
  volume = {14},
  pages = {668--673},
  issn = {1096-8644},
  doi = {10.1002/ajpa.1330140411},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ajpa.1330140411},
  urldate = {2020-07-07},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ajpa.1330140411},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\W5CFE2JJ\\ajpa.html},
  langid = {english},
  number = {4}
}

@article{streeckDepictingGesture2008,
  title = {Depicting by Gesture},
  author = {Streeck, Jürgen},
  date = {2008},
  journaltitle = {Gesture},
  volume = {8},
  pages = {285--301},
  issn = {1569-9773(Electronic),1568-1475(Print)},
  doi = {10.1075/gest.8.3.02str},
  abstract = {This paper deals with ways in which gestural "pictures" are made, i.e., manual depictions of phenomena in the world. The view that "iconic" gestures uniformly function by way of some resemblance between signifier and signified is rejected, giving way to an understanding of depiction by gesture as the achievement of a heterogeneous set of practices, some of which rely on relations of contiguity or indexicality to evoke commonly known objects or scenes. Others seem to be derivative of other representation methods (e.g., drawing on surfaces). The paper reviews some existing work on gestural depiction methods, offers a working heuristics, and illustrates some of its categories. It is suggested that some of the basic ways in which actions of the hands evoke the world in gesture correspond to fundamental modes of existence and activity of human hands in the world: hands depict by enacting their familiar, "real-world" capacities as users, transporters, experiencers, assemblers, molders, and shapers of things. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BVVVHGT5\\2009-01670-001.html},
  keywords = {Gestures,Hand (Anatomy),Heuristics,Iconic Memory},
  number = {3}
}

@incollection{studdert-kennedyLaunchingLanguageGestural2003,
  title = {Launching {{Language}}: {{The Gestural Origin}} of {{Discrete Infinity}}},
  shorttitle = {Launching {{Language}}},
  booktitle = {Language {{Evolution}}},
  author = {Studdert-Kennedy, Michael and Goldstein, Louis},
  editor = {Christiansen, Morten H. and Kirby, Simon},
  date = {2003-07-24},
  pages = {235--254},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199244843.003.0013},
  url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199244843.001.0001/acprof-9780199244843-chapter-13},
  urldate = {2020-03-16},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Y4DEGYFR\\Studdert-Kennedy and Goldstein - 2003 - Launching Language The Gestural Origin of Discret.pdf},
  isbn = {978-0-19-924484-3},
  langid = {english}
}

@article{sugiharaDetectingCausalityComplex2012,
  title = {Detecting {{Causality}} in {{Complex Ecosystems}}},
  author = {Sugihara, George and May, Robert and Ye, Hao and Hsieh, Chih-hao and Deyle, Ethan and Fogarty, Michael and Munch, Stephan},
  date = {2012-10-26},
  journaltitle = {Science},
  volume = {338},
  pages = {496--500},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1227079},
  url = {https://science.sciencemag.org/content/338/6106/496},
  urldate = {2020-01-16},
  abstract = {Identifying causal networks is important for effective policy and management recommendations on climate, epidemiology, financial regulation, and much else. We introduce a method, based on nonlinear state space reconstruction, that can distinguish causality from correlation. It extends to nonseparable weakly connected dynamic systems (cases not covered by the current Granger causality paradigm). The approach is illustrated both by simple models (where, in contrast to the real world, we know the underlying equations/relations and so can check the validity of our method) and by application to real ecological systems, including the controversial sardine-anchovy-temperature problem. A new method, based on nonlinear state space reconstruction, can distinguish causality from correlation. A new method, based on nonlinear state space reconstruction, can distinguish causality from correlation.},
  eprint = {22997134},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XMNY2UJX\\Sugihara et al. - 2012 - Detecting Causality in Complex Ecosystems.pdf;C\:\\Users\\u668173\\Zotero\\storage\\CFDTXH86\\496.html},
  langid = {english},
  number = {6106}
}

@article{sundbergInfluenceBodyPosture1991,
  title = {Influence of Body Posture and Lung Volume on Subglottal Pressure Control during Singing},
  author = {Sundberg, J. and Leanderson, R. and von Euler, C. and Knutsson, E.},
  date = {1991-01-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {5},
  pages = {283--291},
  issn = {0892-1997},
  doi = {10.1016/S0892-1997(05)80057-8},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199705800578},
  urldate = {2019-10-17},
  abstract = {The role of different breathing muscles during singing was investigated by synchronously recording EMG, pressure, and sound signals, using lung volume and gravity as experimental parameters. Surface EMG signals from the external and internal intercostals, the diaphragm, and the abdominal oblique muscles were recorded, while two singer subjects performed various singing tasks associated with rapid and precise changes of subglottal pressure. Esophageal and gastric pressures were measured by pressure transducers, and lung volume by means of impedance plethysmography. The results show that the breathing system efficiently compensates for drastic differences in the mechanics of the breathing apparatus, caused by differences in lung volume and gravity induced by changes of body posture.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\WAMA79AT\\S0892199705800578.html},
  keywords = {Abdominal oblique muscle,Body posture,Breathing,Diaphragm muscle,EMG,Intercostal muscles},
  langid = {english},
  number = {4},
  options = {useprefix=true}
}

@article{sundbergPhonatoryControlMale1993,
  title = {Phonatory Control in Male Singing: {{A}} Study of the Effects of Subglottal Pressure, Fundamental Frequency, and Mode of Phonation on the Voice Source},
  shorttitle = {Phonatory Control in Male Singing},
  author = {Sundberg, J. and Titze, I. and Scherer, R.},
  date = {1993-03-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {7},
  pages = {15--29},
  issn = {0892-1997},
  doi = {10.1016/S0892-1997(05)80108-0},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199705801080},
  urldate = {2020-02-25},
  abstract = {This article describes experiments carried out in order to gain a deeper understanding of the mechanisms underlying variation of vocal loudness in singers. Ten singers, two of whom are famous professional opera tenor soloists, phonated at different pitches and different loudnesses. Their voice source characteristics were analyzed by inverse filtering the oral airflow signal. It was found that the main physiological variable underlying loudness variation is subglottal pressure (Ps). The voice source property determining most of the loudness variation is the amplitude of the negative peak of the differentiated flow signal, as predicted by previous research. Increases in this amplitude are achieved by (a) increasing the pulse amplitude of the flow waveform; (b) moving the moment of vocal fold contact earlier in time, closer to the center of the pulse; and (c) skewing the pulses. The last mentioned alternative seems dependent on both Ps and the ratio between the fundamental frequency and the first formant. On the average, the singers doubled Ps when they increased fundamental frequency by one octave, and a doubling of the excess Ps over threshold caused the sound pressure level (SPL) to increase by 8–9 dB for neutral phonation, less if mode of phonation was changed to pressed. A shift of mode of phonation from flow over neutral to pressed was associated with a reduction of the peak glottal permittance i.e., the ratio between peak transglottal airflow to Ps. Flow phonation had the most favorable relationship between Ps and SPL.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5DRF82RP\\Sundberg et al. - 1993 - Phonatory control in male singing A study of the .pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZQKV67K9\\S0892199705801080.html},
  keywords = {Flow glottogram,Glottal permittance,Mode of phonation-Fundamental frequency,Subglottal pressure,Vocal loudness,Voice source},
  langid = {english},
  number = {1},
  series = {The {{Voice Foundation}}'s 22nd {{Annual Symposium}}}
}

@article{suthersRespirationWingBeatUltrasonic1972,
  title = {Respiration, {{Wing}}-{{Beat}} and {{Ultrasonic Pulse Emission}} in an {{Echo}}-{{Locating Bat}}},
  author = {Suthers, Roderick A. and Thomas, Steven P. and Suthers, Barbara J.},
  date = {1972-02-01},
  journaltitle = {Journal of Experimental Biology},
  volume = {56},
  pages = {37--48},
  publisher = {{The Company of Biologists Ltd}},
  issn = {0022-0949, 1477-9145},
  url = {https://jeb.biologists.org/content/56/1/37},
  urldate = {2020-10-07},
  abstract = {Skip to Next Section 1. The relationship between respiration (by telemetry), wing-beat (by cinematography) and ultrasonic pulse emission (by telephony) was studied in the echo-locating bat, Phyllostomus hastatus, at rest and when flying in a large enclosure. 2. In resting bats echo-locative pulses were produced at almost any point in the respiratory cycle, in flying bats more frequently near the beginning and/or end of expiration. 3. During flight the respiratory cycle showed a one-to-one relationship with the wing-beat cycle, both having a frequency of about 10 c/sec. Expiration was associated with the upward recovery stroke of the wings. 4. Both resting and flying bats emitted up to four pulses per respiratory cycle. Grouping of pulses was prominent during flight, the first pulse in each group being normally emitted at the start of expiration, with the last pulse and the silent period between groups being associated with inspiration. 5. Momentary inflexions in the respiratory signal associated with pulse production may represent momentary reversals in the direction of air flow, or ‘mini-breaths’, to permit vocalization. This and other evidence suggests that the production of pulses during the inspiratory phase is probably accompanied by a very brief expiration.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QLSR365M\\Suthers et al. - 1972 - Respiration, Wing-Beat and Ultrasonic Pulse Emissi.pdf;C\:\\Users\\u668173\\Zotero\\storage\\TQQLDNLV\\37.html},
  langid = {english},
  number = {1}
}

@inproceedings{szekelyCASTINGCORPUSSEGMENTING2019,
  title = {{{CASTING TO CORPUS}} : {{SEGMENTING AND SELECTING SPONTANEOUS DIALOGUE FOR TTS WITH A CNN}}-{{LSTM SPEAKER}}-{{DEPENDENT BREATH DETECTOR}}},
  shorttitle = {{{CASTING TO CORPUS}}},
  author = {Székely, Éva and Henter, Gustav Eje and Gustafson, Joakim},
  date = {2019},
  pages = {6925--6929},
  publisher = {{IEEE}},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-261049},
  urldate = {2021-01-27},
  abstract = {This paper considers utilising breaths to create improved spontaneous-speech corpora for conversational text-to-speech from found audio recordings such as dialogue podcasts. Breaths are of interest ...},
  eventtitle = {44th {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}}), {{MAY}} 12-17, 2019, {{Brighton}}, {{ENGLAND}}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I6U6NJSJ\\record.html},
  langid = {english}
}

@article{tabaryIncreasedBreathingResistance2015,
  title = {Increased Breathing Resistance Compromises the Time Course of Rhythmical Forearm Movements—a Pilot Study},
  author = {Tabary, Ariane and Rassler, Beate},
  date = {2015},
  journaltitle = {Journal of Translational Internal Medicine},
  shortjournal = {J Transl Int Med},
  volume = {3},
  pages = {161--166},
  issn = {2450-131X},
  doi = {10.1515/jtim-2015-0022},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4936457/},
  urldate = {2020-09-28},
  abstract = {Background and Objectives Skeletal muscle dysfunction is a major problem among the co-morbidities associated with chronic obstructive pulmonary disease (COPD). However, muscle weakness and increased fatigability are not the only limitations of skeletal muscle function. Motor–respiratory coordination (MRC) may occur even during movements at lowest workloads. MRC modifies the temporal pattern of motor actions, thus probably impairing motor performance and movement precision. Little attention has been paid to the question of whether motor functions may be compromised in COPD patients independent of workload and required muscle strength and endurance. The present pilot study was designed to investigate the effects of a simulated obstruction (SO) in healthy subjects on their breathing pattern and the timing of a rhythmical forearm movement. Methods Twenty-one subjects performed flexion– extension movements with their right forearm at a self-chosen rate within a range between 0.2 and 0.4 Hz. After a control experiment with normal breathing, a plug with a narrow hole was inserted between face mask and pneumotachograph to simulate obstruction. Subjects were required to repeat the rhythmical forearm movement at the same rate as in the control experiment. Results The condition of SO significantly prolonged breath duration but reduced tidal volume and ventilation. In addition, period duration of the forearm movement increased significantly under this condition while the movement-to-breathing frequency ratio remained almost constant. Increased breathing resistance was considered to cause prolonged breath duration accompanied by an increase in movement period duration. The constant near-integer ratio between movement and breathing rates indicates that the change in movement period duration resulted from MRC. Conclusions The findings of this pilot study demonstrate that increased breathing resistance may compromise motor performance even at lower workloads. This means that in COPD patients, not only muscle strength and endurance are reduced but, moreover, fine motor skills may be impaired. This aspect has particular importance for many everyday activities as reduced fine motor performance substantially contributes to a progressive inability of the patients to manage their daily life.},
  eprint = {27847907},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6LBQ7IEU\\Tabary and Rassler - 2015 - Increased breathing resistance compromises the tim.pdf},
  number = {4},
  pmcid = {PMC4936457}
}

@article{taglialatelaMultimodalCommunicationChimpanzees2015,
  title = {Multimodal Communication in Chimpanzees},
  author = {Taglialatela, Jared P. and Russell, Jamie L. and Pope, Sarah M. and Morton, Tamara and Bogart, Stephanie and Reamer, Lisa A. and Schapiro, Steven J. and Hopkins, William D.},
  date = {2015-11},
  journaltitle = {American Journal of Primatology},
  shortjournal = {Am J Primatol},
  volume = {77},
  pages = {1143--1148},
  issn = {1098-2345},
  doi = {10.1002/ajp.22449},
  abstract = {A fundamental characteristic of human language is multimodality. In other words, humans use multiple signaling channels concurrently when communicating with one another. For example, people frequently produce manual gestures while speaking, and the words a person perceives are impacted by visual information. For this study, we hypothesized that similar to the way that humans regularly couple their spoken utterances with gestures and facial expressions, chimpanzees regularly produce vocalizations in conjunction with other communicative signals. To test this hypothesis, data were collected from 101 captive chimpanzees living in mixed-sex social groupings of seven to twelve individuals. A total of 2,869 vocal events were collected. The data indicate that approximately 50\% of the vocal events were produced in conjunction with another communicative modality. In addition, approximately 68\% were directed to a specific individual, and these directed vocalizations were more likely to include a signal from another communicative modality than were vocalizations that were not directed to a specific individual. These results suggest that, like humans, chimpanzees often pair their vocalizations with signals from other communicative modalities. In addition, chimpanzees appear to use their communicative signals strategically to meet specific socio-communicative ends, providing support for the growing literature that indicates that at least some chimpanzee vocal signaling is intentional.},
  eprint = {26212686},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\N3UAN8WD\\Taglialatela et al. - 2015 - Multimodal communication in chimpanzees.pdf},
  keywords = {Animal Communication,Animals,chimpanzees,Facial Expression,Female,Gestures,language origins,Male,multimodal communication,Pan troglodytes,Social Behavior,vocal communication,Vocalization; Animal},
  langid = {english},
  number = {11},
  pmcid = {PMC5038593}
}

@article{takahashiCoupledOscillatorDynamics2013,
  title = {Coupled Oscillator Dynamics of Vocal Turn-Taking in Monkeys},
  author = {Takahashi, Daniel Y. and Narayanan, Darshana Z. and Ghazanfar, Asif A.},
  date = {2013-11-04},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {23},
  pages = {2162--2168},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2013.09.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0960982213011172},
  urldate = {2020-05-28},
  abstract = {Cooperation is central to human communication [1, 2, 3]. The foundation of cooperative verbal communication is taking turns to speak. Given the universality of turn-taking [4], it is natural to ask how it evolved. We used marmoset monkeys to explore whether another primate species exhibits cooperative vocal communication by taking turns. Marmosets share with humans a cooperative breeding strategy and volubility. Cooperative care behaviors are thought to scaffold prosocial cognitive processes [5, 6]. Moreover, marmosets and other callitrichid primates are very vocal and readily exchange vocalizations with conspecifics [7, 8, 9, 10, 11]. By measuring the natural statistics of marmoset vocal exchanges, we observed that they take turns in extended sequences and show that this vocal turn-taking has as its foundation dynamics characteristic of coupled oscillators—one that is similar to the dynamics proposed for human conversational turn-taking [12]. As marmoset monkeys are on a different branch of the evolutionary tree that led to humans, our data demonstrate convergent evolution of vocal cooperation. Perhaps more importantly, our data offer a plausible alternative scenario to “gestural origin” hypotheses for how human cooperative vocal communication could have evolved.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IE5LZLMJ\\Takahashi et al. - 2013 - Coupled Oscillator Dynamics of Vocal Turn-Taking i.pdf;C\:\\Users\\u668173\\Zotero\\storage\\W7TENNMA\\S0960982213011172.html},
  langid = {english},
  number = {21}
}

@article{tchernichovskiVocalDevelopmentHow2016,
  title = {Vocal {{Development}}: {{How Marmoset Infants Express Their Feelings}}},
  shorttitle = {Vocal {{Development}}},
  author = {Tchernichovski, Ofer and Oller, D. Kimbrough},
  date = {2016-05-23},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {26},
  pages = {R422-R424},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2016.03.063},
  url = {http://www.sciencedirect.com/science/article/pii/S0960982216303086},
  urldate = {2020-12-03},
  abstract = {A new study shows that vocal sequences produced by newborn marmoset monkeys are driven by slow fluctuations in physiological state; the results shed light on the evolution of vocal communication between newborns and parents.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QF6I936L\\Tchernichovski and Oller - 2016 - Vocal Development How Marmoset Infants Express Th.pdf;C\:\\Users\\u668173\\Zotero\\storage\\UUZS6H3C\\S0960982216303086.html},
  langid = {english},
  number = {10}
}

@article{teasdaleDeterminingMovementOnsets1993,
  title = {Determining {{Movement Onsets}} from {{Temporal Series}}},
  author = {Teasdale, Normand and Bard, Chantal and Fleury, Michelle and Young, Douglas E. and Proteau, Luc},
  date = {1993-06-01},
  journaltitle = {Journal of Motor Behavior},
  volume = {25},
  pages = {97--106},
  publisher = {{Routledge}},
  issn = {0022-2895},
  doi = {10.1080/00222895.1993.9941644},
  url = {https://doi.org/10.1080/00222895.1993.9941644},
  urldate = {2020-11-12},
  abstract = {With the advent of recent measurement techniques, kinematic and kinetic measures commonly are used to describe events over time. Often, the central and peripheral nature of the control processes involved are derived from these temporal series. For example, movement onset often arbitrarily defines the end of the central and the beginning of the peripheral processes. Because of its critical temporal location, we examined whether response dynamics (average movement velocity) affects the determination of movement onset. Interactive graphics and numerical methods of determining movement onsets from temporal series were evaluated on various kinematic signals. Variations in the initial rate of change in a given signal significantly affected the determination of movement onset. Consequently, measurements of component latency must be regarded with caution. A cursory description of related problems elucidated in previous research is discussed, and procedures that can minimize these artifacts are suggested.},
  annotation = {\_eprint: https://doi.org/10.1080/00222895.1993.9941644},
  eprint = {15064201},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MGW82N9U\\Teasdale et al. - 1993 - Determining Movement Onsets from Temporal Series.pdf;C\:\\Users\\u668173\\Zotero\\storage\\VBYZ7XBB\\00222895.1993.html},
  keywords = {kinematics,movement onsets,reaction time},
  number = {2}
}

@article{tempradoAttentionalLoadAssociated1999,
  title = {Attentional Load Associated with Performing and Stabilizing Preferred Bimanual Patterns},
  author = {Temprado, Jean-Jacques and Zanone, Pier-Giorgio and Monno, Audrey and Laurent, Michel},
  date = {1999},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {25},
  pages = {1579--1594},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.25.6.1579},
  abstract = {This study aimed to determine whether the stability of preferred coordination patterns could be modified intentionally and whether such stabilization involved an additional attentional load. Eight participants performed in-phase and anti-phase bimanual coordination patterns, a reaction time (RT) task, and several dual tasks (coordination + RT) that manipulated attentional priority by requiring either shared attention, priority to the coordination task, or priority to the RT task. Results showed that RT was smaller for in-phase than anti-phase. Moreover, attentional manipulations led to a trade-off between pattern stability and RT performance. This suggests that performing and intentionally stabilizing a coordination pattern incur a central cost that depends on the coordination pattern's dynamic properties. Thus, this study opens a conceptual and methodological bridge between information processing and dynamic approaches to coordination. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NQHP53MU\\2000-15288-007.html},
  keywords = {Attention,Human Channel Capacity,Perceptual Motor Coordination,Task Complexity},
  number = {6}
}

@article{tempradoDynamicPatternAnalysis2002,
  title = {A Dynamic Pattern Analysis of Coordination between Breathing and Rhythmic Arm Movements in Humans},
  author = {Temprado, J. J. and Milliex, L. and Grélot, L. and Coyle, T. and Calvin, S. and Laurent, M.},
  date = {2002-09-06},
  journaltitle = {Neuroscience Letters},
  shortjournal = {Neuroscience Letters},
  volume = {329},
  pages = {314--318},
  issn = {0304-3940},
  doi = {10.1016/S0304-3940(02)00455-X},
  url = {http://www.sciencedirect.com/science/article/pii/S030439400200455X},
  urldate = {2020-09-01},
  abstract = {We investigated the behavioral dynamics of human breathing–wrist movement coordination in a 1:1 frequency locking task. A pronation–supination wrist movement and a short trial duration were chosen to limit both mechanical and metabolic constraints on the respiratory system. Subjects voluntarily controlled their breathing rhythm to follow the metronome. We found that pronation–expiration and pronation–inspiration patterns coexisted as the (sole) stable fixed-point attractors of the coordination system. The pronation–expiration pattern was more stable than the pronation–inspiration pattern. Depending on the oscillation frequency, this differential stability gave rise to both absolute and relative coordination. These results show that simple behavioral laws of coordination encapsulate neural coupling dynamics evidenced from experimental research in human beings and animals. They challenge the classical view that such a coupling is not present for all imposed movement frequencies. Rather, relative coordination emerges as a result of the modification of coupling strength with frequency. These results can be accommodated by the asymmetric version of the HKB model of coordination dynamics. Thus, our data suggest that the principles and models of coordination dynamics may be taken as a reference to study the coupling of the motor and physiological subsystems involved in breathing–movement coordination.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PLF3YN3A\\S030439400200455X.html},
  keywords = {Breathing,Coordination,Coupled oscillators,Dynamical systems,Intermittent dynamics},
  langid = {english},
  number = {3}
}

@article{tenazaSongsChorusesCountersinging1976,
  title = {Songs, Choruses and Countersinging of {{Kloss}}' Gibbons ({{Hylobates}} Klossii) in {{Siberut Island}}, {{Indonesia}}},
  author = {Tenaza, R. R.},
  date = {1976-01},
  journaltitle = {Zeitschrift Fur Tierpsychologie},
  shortjournal = {Z Tierpsychol},
  volume = {40},
  pages = {37--52},
  issn = {0044-3573},
  doi = {10.1111/j.1439-0310.1976.tb00924.x},
  abstract = {This study evaluates the social spacing mechanism of song as it occurs in Kloss' gibbons. The study population included individuals in 13 family groups whose composition and territories were known (TENZA 1975) plus a number of others. Sonagrams illustrate individual and sexual differences in singing. Sex differences in chorusing, countersinging and other behavior related to song are described. Variations in singing or chorusing or both are related to season, time of day, sex, age, spatial factors and social factors. The adaptive functions of singing, countersinging and chorusing are discussed. It is concluded that: (1) Song is mainly for interterritorial communication between members of the same sex, (2) male-song probably also functions in mate attraction and (3) chorusing is primarily an adaptation reducing predation risk to singing gibbons.},
  eprint = {1274481},
  eprinttype = {pmid},
  keywords = {Animals,Behavior; Animal,Female,Hominidae,Hylobates,Indonesia,Male,Seasons,Sex Factors,Social Behavior,Species Specificity,Territoriality,Time Factors,Vocalization; Animal},
  langid = {english},
  number = {1}
}

@inproceedings{tenboschDurationalAspectsTurnTaking2004,
  title = {Durational {{Aspects}} of {{Turn}}-{{Taking}} in {{Spontaneous Face}}-to-{{Face}} and {{Telephone Dialogues}}},
  booktitle = {Text, {{Speech}} and {{Dialogue}}},
  author = {ten Bosch, Louis and Oostdijk, Nelleke and de Ruiter, Jan Peter},
  editor = {Sojka, Petr and Kopeček, Ivan and Pala, Karel},
  date = {2004},
  pages = {563--570},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-30120-2_71},
  abstract = {On the basis of two-speaker spontaneous conversations, it is shown that the distributions of both pauses and speech-overlaps of telephone and face-to-face dialogues have different statistical properties. Pauses in a face-to-face dialogue last up to 4 times longer than pauses in telephone conversations in functionally comparable conditions. There is a high correlation (0.88 or larger) between the average pause duration for the two speakers across face-to-face dialogues and telephone dialogues. The data provided form a first quantitative analysis of the complex turn-taking mechanism evidenced in the dialogues available in the 9-million-word Spoken Dutch Corpus.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\K4B6W3EC\\ten Bosch et al. - 2004 - Durational Aspects of Turn-Taking in Spontaneous F.pdf},
  isbn = {978-3-540-30120-2},
  langid = {english},
  options = {useprefix=true},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{teshAbdominalMusclesVertebral1987,
  title = {The Abdominal Muscles and Vertebral Stability},
  author = {Tesh, K. M. and Dunn, J. S. and Evans, J. H.},
  date = {1987-06},
  journaltitle = {Spine},
  shortjournal = {Spine},
  volume = {12},
  pages = {501--508},
  issn = {0362-2436},
  doi = {10.1097/00007632-198706000-00014},
  abstract = {It has been suggested that the muscles of the anterolateral abdominal wall increase the stability of the lumbar region of the vertebral column by tensing the thoracolumbar fascia and by raising intra-abdominal pressure. In this report these new mechanisms are reviewed and their contribution to vertebral stability assessed. The thoracolumbar fascia consists of two principal layers of dense fibrous tissue that attach the abdominal muscles to the vertebral column. Each of these layers was dissected in fresh and fixed material and samples chosen for light and scanning electron microscopy to study the arrangement of the component fibers. Computed axial tomography in volunteers showed the changes in spatial organization that occur during flexion of the back and during the Valsalva maneuver. The fascia was then tensed experimentally in isolated unfixed motion segments. The results suggested that the stabilizing action of the thoracolumbar fascia is less than had been thought previously but was consistent with calculations based on the more accurate structural and mechanical information that had been derived from the current study. Abdominal muscle contraction was simulated in whole cadavers in both the flexed and lateral bending positions to compare the stabilizing effect of the thoracolumbar fascia and intra-abdominal pressure mechanisms. These definitive experiments showed that the resistance to bending in the sagittal plane offered by the abdominal muscles acting through fascial tension was of a similar magnitude to that offered by a raised intra-abdominal pressure, both being relatively small in the fully flexed position. The stabilizing influence of the middle layer of the thoracolumbar fascia in lateral bending was clearly demonstrated and warrants further study in vivo.},
  eprint = {2957802},
  eprinttype = {pmid},
  keywords = {Abdominal Muscles,Biomechanical Phenomena,Fascia,Humans,Lumbar Vertebrae,Pressure},
  langid = {english},
  number = {5}
}

@article{thelenRhythmicalStereotypiesNormal1979,
  title = {Rhythmical Stereotypies in Normal Human Infants},
  author = {Thelen, Esther},
  date = {1979-08-01},
  journaltitle = {Animal Behaviour},
  shortjournal = {Animal Behaviour},
  volume = {27},
  pages = {699--715},
  issn = {0003-3472},
  doi = {10.1016/0003-3472(79)90006-X},
  url = {http://www.sciencedirect.com/science/article/pii/000334727990006X},
  urldate = {2020-12-02},
  abstract = {Naturalistic, longitudinal observations of 20 normal infants biweekly during their first year showed that they performed a great quantity and variety of rhythmical and highly stereotyped behaviours. Forty-seven movement patterns are described involving the legs and feet; the head and face; the arms, hands, and fingers; and the whole torso in various postures. These behaviours showed developmental regularities as well as constancy of form and distribution. Groups of stereotypies involving particular parts of the body or postures had characteristic ages of onset, peak performance, and decline. The onset of particular stereotypy groups was highly correlated with motor development. It is proposed that rhythmical stereotypies are manifestations of incomplete cortical control of endogenous patterning in maturing neuromuscular pathways.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\A4LAXDAJ\\000334727990006X.html},
  langid = {english}
}

@article{thompsonGreatApeThorax2018,
  title = {Great Ape Thorax and Shoulder Configuration-{{An}} Adaptation for Arboreality or Knuckle-Walking?},
  author = {Thompson, Nathan E},
  date = {2018},
  journaltitle = {Journal of Human Evolution},
  pages = {12},
  abstract = {Great apes exhibit a suite of morphological traits of the shoulder and upper thorax that have traditionally been linked to orthograde arborealism. Recently it has been proposed that these traits are instead adaptations for knuckle-walking, and more broadly, that knuckle-walking itself is an adaptation for shock absorption during terrestriality. Here we test several tenets of these hypotheses using kinematic and kinetic data from chimpanzees and macaques, and electromyographic data of shoulder muscle activity in chimpanzees. We collected 3D kinematic data to quantify motion of the acromion and trunk during quadrupedalism and vertical climbing in chimpanzees as well as ground reaction forces to investigate the presence and magnitude of impact transient forces during terrestrial locomotion in chimpanzees and macaques. We also investigated patterns of recruitment of select forelimb musculature (triceps brachii and serratus anterior) using previously collected data in chimpanzees to determine whether these muscles may function to absorb impact transient forces. We found that the acromion is significantly more elevated in vertical climbing than during knuckle-walking, while dorsoventral ranges and magnitudes of motion were similar between gaits. Ground reaction forces indicate that only a minority of strides in either chimpanzees or macaques have transient forces and, when present, these transient forces as well as loading rates are small. Electromyographic results show that activity of the triceps brachii may facilitate energy absorption while serratus anterior likely functions to support the trunk, as in other primates. Our data suggest there is little to no evidence supporting recent hypotheses that the African ape upper thorax and shoulder configuration is an adaptation for knuckle-walking, or more broadly, that knuckle-walking exists as an adaptation to absorb impact shock during terrestriality. We do however find some evidence that shoulder configuration allows greater scapular elevation in chimpanzees during arboreal behaviors (e.g., vertical climbing).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZR47PXLP\\Thompson - 2018 - Great ape thorax and shoulder configuration-An ada.pdf},
  langid = {english}
}

@article{thompsonGreatApeThorax2018a,
  title = {Great Ape Thorax and Shoulder Configuration—{{An}} Adaptation for Arboreality or Knuckle-Walking?},
  author = {Thompson, Nathan E. and Rubinstein, Danielle and Larson, Susan G.},
  date = {2018-12-01},
  journaltitle = {Journal of Human Evolution},
  shortjournal = {Journal of Human Evolution},
  volume = {125},
  pages = {15--26},
  issn = {0047-2484},
  doi = {10.1016/j.jhevol.2018.09.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0047248418301192},
  urldate = {2020-11-20},
  abstract = {Great apes exhibit a suite of morphological traits of the shoulder and upper thorax that have traditionally been linked to orthograde arborealism. Recently it has been proposed that these traits are instead adaptations for knuckle-walking, and more broadly, that knuckle-walking itself is an adaptation for shock absorption during terrestriality. Here we test several tenets of these hypotheses using kinematic and kinetic data from chimpanzees and macaques, and electromyographic data of shoulder muscle activity in chimpanzees. We collected 3D kinematic data to quantify motion of the acromion and trunk during quadrupedalism and vertical climbing in chimpanzees as well as ground reaction forces to investigate the presence and magnitude of impact transient forces during terrestrial locomotion in chimpanzees and macaques. We also investigated patterns of recruitment of select forelimb musculature (triceps brachii and serratus anterior) using previously collected data in chimpanzees to determine whether these muscles may function to absorb impact transient forces. We found that the acromion is significantly more elevated in vertical climbing than during knuckle-walking, while dorsoventral ranges and magnitudes of motion were similar between gaits. Ground reaction forces indicate that only a minority of strides in either chimpanzees or macaques have transient forces and, when present, these transient forces as well as loading rates are small. Electromyographic results show that activity of the triceps brachii may facilitate energy absorption while serratus anterior likely functions to support the trunk, as in other primates. Our data suggest there is little to no evidence supporting recent hypotheses that the African ape upper thorax and shoulder configuration is an adaptation for knuckle-walking, or more broadly, that knuckle-walking exists as an adaptation to absorb impact shock during terrestriality. We do however find some evidence that shoulder configuration allows greater scapular elevation in chimpanzees during arboreal behaviors (e.g., vertical climbing).},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2QLKP4HE\\S0047248418301192.html},
  keywords = {Chimpanzee,Electromyography,Kinematic,Kinetic,Locomotion,Vertical climbing},
  langid = {english}
}

@article{thornhillSpectralPrincipalComponent2002,
  title = {Spectral Principal Component Analysis of Dynamic Process Data},
  author = {Thornhill, N.F. and Shah, S.L. and Huang, B. and Vishnubhotla, A.},
  date = {2002-08},
  journaltitle = {Control Engineering Practice},
  volume = {10},
  pages = {833--846},
  issn = {09670661},
  doi = {10.1016/S0967-0661(02)00035-7},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0967066102000357},
  urldate = {2020-07-21},
  abstract = {This article describes principal component analysis (PCA) of the power spectra of data from chemical processes. Spectral PCA can be applied to the measurements from a whole unit or plant because spectra are invariant to the phase lags caused by time delays and process dynamics. The same comment applies to PCA using autocovariance functions, which was also studied. Two case studies are presented. One was derived from simulation of a pulp process. The second was from a refinery involving 37 tags. In both cases, PCA clusters were observed which were characterised by distinct spectral features. Spectral PCA was compared with PCA using autocovariance functions. The performance was similar, and both offered an improvement over PCA using the time domain signals even when time shifting was used to align the phases. r 2002 Elsevier Science Ltd. All rights reserved.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DVCK6MU4\\Thornhill et al. - 2002 - Spectral principal component analysis of dynamic p.pdf},
  langid = {english},
  number = {8}
}

@article{thurlbeckPostmortemLungVolumes1979,
  title = {Post-Mortem Lung Volumes.},
  author = {Thurlbeck, W M},
  date = {1979-12-01},
  journaltitle = {Thorax},
  volume = {34},
  pages = {735--739},
  issn = {0040-6376},
  doi = {10.1136/thx.34.6.735},
  url = {http://thorax.bmj.com/cgi/doi/10.1136/thx.34.6.735},
  urldate = {2020-06-10},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\56TP29BW\\Thurlbeck - 1979 - Post-mortem lung volumes..pdf},
  langid = {english},
  number = {6}
}

@article{tilsenSpeechRhythmAnalysis2013,
  title = {Speech Rhythm Analysis with Decomposition of the Amplitude Envelope: {{Characterizing}} Rhythmic Patterns within and across Languages},
  shorttitle = {Speech Rhythm Analysis with Decomposition of the Amplitude Envelope},
  author = {Tilsen, Sam and Arvaniti, Amalia},
  date = {2013-07-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {134},
  pages = {628--639},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.4807565},
  url = {https://asa.scitation.org/doi/full/10.1121/1.4807565},
  urldate = {2020-05-12},
  abstract = {This study presents a method for analyzing speech rhythm using empirical mode decomposition of the speech amplitude envelope, which allows for extraction and quantification of syllabic- and supra-syllabic time-scale components of the envelope. The method of empirical mode decomposition of a vocalic energy amplitude envelope is illustrated in detail, and several types of rhythm metrics derived from this method are presented. Spontaneous speech extracted from the Buckeye Corpus is used to assess the effect of utterance length on metrics, and it is shown how metrics representing variability in the supra-syllabic time-scale components of the envelope can be used to identify stretches of speech with targeted rhythmic characteristics. Furthermore, the envelope-based metrics are used to characterize cross-linguistic differences in speech rhythm in the UC San Diego Speech Lab corpus of English, German, Greek, Italian, Korean, and Spanish speech elicited in read sentences, read passages, and spontaneous speech. The envelope-based metrics exhibit significant effects of language and elicitation method that argue for a nuanced view of cross-linguistic rhythm patterns.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QERBU8FV\\Tilsen and Arvaniti - 2013 - Speech rhythm analysis with decomposition of the a.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XL9X9I5D\\1.html},
  number = {1}
}

@article{toddDynamicsDynamicsModel1992,
  title = {The Dynamics of Dynamics: {{A}} Model of Musical Expression},
  shorttitle = {The Dynamics of Dynamics},
  author = {Todd, N. P. M.},
  date = {1992-06},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {91},
  pages = {3540--3550},
  issn = {0001-4966},
  doi = {10.1121/1.402843},
  url = {http://asa.scitation.org/doi/10.1121/1.402843},
  urldate = {2020-09-18},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NPP9S9UF\\McAngus Todd - 1992 - The dynamics of dynamics A model of musical expre.pdf},
  langid = {english},
  number = {6}
}

@article{toddMotionMusicNeurobiological1999,
  title = {Motion in {{Music}}: {{A Neurobiological Perspective}}},
  shorttitle = {Motion in {{Music}}},
  author = {Todd, N. P. M.},
  date = {1999},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  volume = {17},
  pages = {115--126},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.2307/40285814},
  abstract = {The topic of musical motion has generated a considerable amount of controversy in the past few years (P. Desain, H. Honing, H. van Thienen, \& L. Windsor, 1998). In this essay it is argued that motion is central to our understanding of many aspects of music, particularly to our understanding of rhythm, and that an adequate account of motion in music requires a neurobiological perspective. Two possible mechanisms are discussed that may form a neurobiological basis for the association of motion in music: a vestibulomotor mechanism and an audio-visuo-motor mechanism. These two mechanisms in turn may mediate two distinct kinds of musical motion: gesture and locomotion.},
  eprint = {40285814},
  eprinttype = {jstor},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5DHIUZS6\\Todd - 1999 - Motion in Music A Neurobiological Perspective.pdf},
  number = {1}
}

@article{tognoliCoordinationDynamicsFoundation2020,
  title = {Coordination {{Dynamics}}: {{A Foundation}} for {{Understanding Social Behavior}}},
  shorttitle = {Coordination {{Dynamics}}},
  author = {Tognoli, Emmanuelle and Zhang, Mengsen and Fuchs, Armin and Beetle, Christopher and Kelso, J. A. S.},
  date = {2020},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {14},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2020.00317},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2020.00317/full},
  urldate = {2020-09-01},
  abstract = {Humans’ interactions with each other or with socially competent machines exhibit lawful coordination patterns at multiple levels of description. According to Coordination Dynamics, such laws specify the flow of coordination states produced by functional synergies of elements (e.g. cells, body parts, brain areas, people…) that are temporarily organized as single, coherent units. These coordinative structures or synergies may be mathematically characterized as informationally coupled self-organizing dynamical systems. In this paper, we start from a simple foundation, an elemental model system for social interactions, whose behavior has been captured in the Haken-Kelso-Bunz (HKB) model. We follow a tried and tested scientific method that tightly interweaves experimental neurobehavioral studies and mathematical models. We use this method to further develop a body of empirical research that advances the theory toward more generalized forms. In concordance with this interdisciplinary spirit, the present paper is written both as an overview of relevant advances and as an introduction to its mathematical underpinnings. We demonstrate HKB’s evolution in the context of social coordination along several directions, its applicability growing to increasingly complex scenarios. In particular, we show that accommodating for symmetry breaking in intrinsic dynamics and coupling, multiscale generalization and adaptation are principal evolutions. We conclude that a general framework for social coordination dynamics is on the horizon, when models support experiments with hypothesis generation and mechanistic insights.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KUPJUPUM\\Tognoli et al. - 2020 - Coordination Dynamics A Foundation for Understand.pdf},
  keywords = {Complex System,Coordination Dynamics,dynamic clamp,HKB,HMI,HRI,Metastability,Multiscale,social interaction},
  langid = {english}
}

@article{toiviainenEmbodiedMeterHierarchical2010,
  title = {Embodied Meter: {{Hierarchical}} Eigenmodes in Music-Induced Movement},
  shorttitle = {Embodied {{Meter}}},
  author = {Toiviainen, Petri and Luck, Geoff and Thompson, Marc R.},
  date = {2010-09-01},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  volume = {28},
  pages = {59--70},
  publisher = {{University of California Press}},
  issn = {0730-7829},
  doi = {10.1525/mp.2010.28.1.59},
  url = {/mp/article/28/1/59/62474/Embodied-Meter-Hierarchical-Eigenmodes-in-Music},
  urldate = {2020-12-05},
  abstract = {Listening to music often is associated with spontaneous body movements frequently synchronized with its periodic structure. The notion of embodied cognition assumes that intelligent behavior does not emerge from mere passive perception, but requires goal-directed interactions between the organism and its environment. According to this view, one could postulate that we may use our bodily movements to help parse the metric structure of music. The aim of this study was to investigate how pulsations on different metrical levels manifest in music-induced movement. Musicians were presented with a piece of instrumental music in 4/4 time, played at four different tempi ranging from 92 to 138 bpm. Participants were instructed to move to the music, and their movements were recorded with a high quality optical motion capture system. Subsequently, signal processing methods and principal components analysis were applied to extract movement primitives synchronized with different metrical levels. We found differences between metric levels in terms of the prevalence of synchronized eigenmovements. For instance, mediolateral movements of arms were found to be frequently synchronized with the tactus level pulse, while rotation and lateral flexion of the upper torso were commonly found to exhibit periods of two and four beats, respectively. The results imply that periodicities on several metric levels are simultaneously present in music-induced movement. This could suggest that the metric structure of music is encoded in these movements.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BCPR3QKS\\Toiviainen et al. - 2010 - Embodied Meter Hierarchical Eigenmodes in Music-I.pdf;C\:\\Users\\u668173\\Zotero\\storage\\93DIJ7VL\\Embodied-Meter-Hierarchical-Eigenmodes-in-Music.html;C\:\\Users\\u668173\\Zotero\\storage\\RVKSAZBI\\Embodied-Meter-Hierarchical-Eigenmodes-in-Music.html},
  langid = {english},
  number = {1}
}

@book{tomaselloOriginsHumanCommunication2008,
  title = {The Origins of Human Communication},
  author = {Tomasello, M.},
  date = {2008},
  publisher = {{MIT press}},
  location = {{Cambdride, MA}}
}

@article{tomaselloThirtyYearsGreat2019,
  title = {Thirty Years of Great Ape Gestures},
  author = {Tomasello, Michael and Call, Josep},
  date = {2019-07},
  journaltitle = {Animal Cognition},
  volume = {22},
  pages = {461--469},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-018-1167-1},
  url = {http://link.springer.com/10.1007/s10071-018-1167-1},
  urldate = {2020-03-06},
  abstract = {We and our colleagues have been doing studies of great ape gestural communication for more than 30 years. Here we attempt to spell out what we have learned. Some aspects of the process have been reliably established by multiple researchers, for example, its intentional structure and its sensitivity to the attentional state of the recipient. Other aspects are more controversial. We argue here that it is a mistake to assimilate great ape gestures to the species-typical displays of other mammals by claiming that they are fixed action patterns, as there are many differences, including the use of attention-getters. It is also a mistake, we argue, to assimilate great ape gestures to human gestures by claiming that they are used referentially and declaratively in a human-like manner, as apes’ “pointing” gesture has many limitations and they do not gesture iconically. Great ape gestures constitute a unique form of primate communication with their own unique qualities.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QBB3YSIV\\Tomasello and Call - 2019 - Thirty years of great ape gestures.pdf},
  langid = {english},
  number = {4}
}

@article{tormeneMatchingIncompleteTime2009,
  title = {Matching Incomplete Time Series with Dynamic Time Warping: An Algorithm and an Application to Post-Stroke Rehabilitation},
  shorttitle = {Matching Incomplete Time Series with Dynamic Time Warping},
  author = {Tormene, Paolo and Giorgino, Toni and Quaglini, Silvana and Stefanelli, Mario},
  date = {2009-01},
  journaltitle = {Artificial Intelligence in Medicine},
  volume = {45},
  pages = {11--34},
  issn = {09333657},
  doi = {10.1016/j.artmed.2008.11.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365708001772},
  urldate = {2019-05-15},
  abstract = {Objective: The purpose of this study was to assess the performance of a real-time (‘‘open-end’’) version of the dynamic time warping (DTW) algorithm for the recognition of motor exercises. Given a possibly incomplete input stream of data and a reference time series, the open-end DTW algorithm computes both the size of the prefix of reference which is best matched by the input, and the dissimilarity between the matched portions. The algorithm was used to provide real-time feedback to neurological patients undergoing motor rehabilitation. Methods and materials: We acquired a dataset of multivariate time series from a sensorized long-sleeve shirt which contains 29 strain sensors distributed on the upper limb. Seven typical rehabilitation exercises were recorded in several variations, both correctly and incorrectly executed, and at various speeds, totaling a data set of 840 time series. Nearest-neighbour classifiers were built according to the outputs of openend DTW alignments and their global counterparts on exercise pairs. The classifiers were also tested on well-known public datasets from heterogeneous domains. Results: Nonparametric tests show that (1) on full time series the two algorithms achieve the same classification accuracy ( p-value ¼ 0:32); (2) on partial time series, classifiers based on open-end DTW have a far higher accuracy (k ¼ 0:898 versus k ¼ 0:447; p {$<$} 10À5); and (3) the prediction of the matched fraction follows closely the ground truth (root mean square {$<$} 10\%). The results hold for the motor rehabilitation and the other datasets tested, as well. Conclusions: The open-end variant of the DTW algorithm is suitable for the classification of truncated quantitative time series, even in the presence of noise. Early recognition and accurate class prediction can be achieved, provided that enough},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LNSWNC7R\\Tormene et al. - 2009 - Matching incomplete time series with dynamic time .pdf},
  langid = {english},
  number = {1}
}

@article{townsendExorcisingGriceGhost2017,
  title = {Exorcising {{Grice}}'s Ghost: An Empirical Approach to Studying Intentional Communication in Animals},
  shorttitle = {Exorcising {{Grice}}'s Ghost},
  author = {Townsend, Simon W. and Koski, Sonja E. and Byrne, Richard W. and Slocombe, Katie E. and Bickel, Balthasar and Boeckle, Markus and Goncalves, Ines Braga and Burkart, Judith M. and Flower, Tom and Gaunet, Florence and Glock, Hans Johann and Gruber, Thibaud and Jansen, David A. W. A. M. and Liebal, Katja and Linke, Angelika and Miklósi, Ádám and Moore, Richard and van Schaik, Carel P. and Stoll, Sabine and Vail, Alex and Waller, Bridget M. and Wild, Markus and Zuberbühler, Klaus and Manser, Marta B.},
  date = {2017},
  journaltitle = {Biological Reviews},
  volume = {92},
  pages = {1427--1433},
  issn = {1469-185X},
  doi = {10.1111/brv.12289},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12289},
  urldate = {2020-09-02},
  abstract = {Language's intentional nature has been highlighted as a crucial feature distinguishing it from other communication systems. Specifically, language is often thought to depend on highly structured intentional action and mutual mindreading by a communicator and recipient. Whilst similar abilities in animals can shed light on the evolution of intentionality, they remain challenging to detect unambiguously. We revisit animal intentional communication and suggest that progress in identifying analogous capacities has been complicated by (i) the assumption that intentional (that is, voluntary) production of communicative acts requires mental-state attribution, and (ii) variation in approaches investigating communication across sensory modalities. To move forward, we argue that a framework fusing research across modalities and species is required. We structure intentional communication into a series of requirements, each of which can be operationalised, investigated empirically, and must be met for purposive, intentionally communicative acts to be demonstrated. Our unified approach helps elucidate the distribution of animal intentional communication and subsequently serves to clarify what is meant by attributions of intentional communication in animals and humans.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/brv.12289},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8MYYARPF\\Townsend et al. - 2017 - Exorcising Grice's ghost an empirical approach to.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ZLVY4IH7\\brv.html},
  keywords = {communication,gesture,intentionality,language evolution,vocalisation},
  langid = {english},
  number = {3}
}

@article{traserRespiratoryKinematicsRegulation2020,
  title = {Respiratory Kinematics and the Regulation of Subglottic Pressure for Phonation of Pitch Jumps - a Dynamic {{MRI}} Study},
  author = {Traser, Louisa and Burk, Fabian and Özen, Ali Caglar and Burdumy, Michael and Bock, Michael and Blaser, Daniela and Richter, Bernhard and Echternach, Matthias},
  date = {2020},
  journaltitle = {PloS One},
  shortjournal = {PLoS One},
  volume = {15},
  pages = {e0244539},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0244539},
  abstract = {The respiratory system is a central part of voice production as it contributes to the generation of subglottic pressure, which has an impact on voice parameters including fundamental frequency and sound pressure level. Both parameters need to be adjusted precisely during complex phonation tasks such as singing. In particular, the underlying functions of the diaphragm and rib cage in relation to the phonation of pitch jumps are not yet understood in detail. This study aims to analyse respiratory movements during phonation of pitch jumps using dynamic MRI of the lungs. Dynamic images of the breathing apparatus of 7 professional singers were acquired in the supine position during phonation of upwards and downwards pitch jumps in a high, medium, and low range of the singer's tessitura. Distances between characteristic anatomical landmarks in the lung were measured from the series of images obtained. During sustained phonation, the diaphragm elevates, and the rib cage is lowered in a monotonic manner. During downward pitch jumps the diaphragm suddenly changed its movement direction and presented with a short inspiratory activation which was predominant in the posterior part and was associated with a shift of the cupola in an anterior direction. The magnitude of this inspiratory movement was greater for jumps that started at higher compared to lower fundamental frequency. In contrast, expiratory movement of the rib cage and anterior diaphragm were simultaneous and continued constantly during the jump. The data underline the theory of a regulation of subglottic pressure via a sudden diaphragm contraction during phonation of pitch jumps downwards, while the rib cage is not involved in short term adaptations. This strengthens the idea of a differentiated control of rib cage and diaphragm as different functional units during singing phonation.},
  eprint = {33382744},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RZ4THK2C\\Traser et al. - 2020 - Respiratory kinematics and the regulation of subgl.pdf},
  langid = {english},
  number = {12},
  pmcid = {PMC7775092}
}

@article{treffnerGesturesPhasesDynamics2008,
  title = {Gestures and {{Phases}}: {{The Dynamics}} of {{Speech}}-{{Hand Communication}}},
  shorttitle = {Gestures and {{Phases}}},
  author = {Treffner, Paul and Peter, Mira and Kleidon, Mark},
  date = {2008-01-28},
  journaltitle = {Ecological Psychology},
  volume = {20},
  pages = {32--64},
  publisher = {{Routledge}},
  issn = {1040-7413},
  doi = {10.1080/10407410701766643},
  url = {https://doi.org/10.1080/10407410701766643},
  urldate = {2020-11-14},
  abstract = {We investigated how a listener's perceived meaning of a spoken sentence is influenced by the relative timing between a speaker's speech and accompanying hand gestures. Participants viewed a computer-animated character who uttered the phrase, “Put the book there now.” while executing a simple right-handed beat gesture whose location relative to the utterance was precisely controlled in a frame-by-frame fashion. The participant's task consisted of making a judgment about two related aspects of the actor's perceived speech: (a) Which word was emphasized? and (b) How clear was the emphasis? That is, did it make sense? The results revealed that the perceived emphasis was determined by the timing (phasing) of the speaker's hand gesture. Furthermore, the clarity of the perceived emphasis (i.e., meaningfulness) was influenced by the affordances in the immediate environment of the speaker. Discussion addresses the primacy of ostensive specification and gesture in communicative events, the dynamics of speech-hand coordination during both actual and virtual dialogue, and the role of environmental affordances in grounding informative communicative acts in the ecology of organism-environment dynamics.},
  annotation = {\_eprint: https://doi.org/10.1080/10407410701766643},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LT64PT3X\\Treffner et al. - 2008 - Gestures and Phases The Dynamics of Speech-Hand C.pdf;C\:\\Users\\u668173\\Zotero\\storage\\XFKRICGJ\\10407410701766643.html},
  number = {1}
}

@article{treffnerIntentionalAttentionalDynamics2002,
  title = {Intentional and Attentional Dynamics of Speech–Hand Coordination},
  author = {Treffner, P. J. and Peter, M.},
  date = {2002},
  journaltitle = {Human Movement Science},
  volume = {21},
  pages = {641--697},
  doi = {10.1016/S0167-9457(02)00178-1},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0167945702001781},
  urldate = {2019-08-08},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\M6EBIXEJ\\S0167945702001781.html},
  number = {5-6}
}

@article{tremblaySomatosensoryBasisSpeech2003,
  title = {Somatosensory Basis of Speech Production},
  author = {Tremblay, Stéphanie and Shiller, Douglas M. and Ostry, David J.},
  date = {2003-06-19},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {423},
  pages = {866--869},
  issn = {0028-0836},
  doi = {10.1038/nature01710},
  abstract = {The hypothesis that speech goals are defined acoustically and maintained by auditory feedback is a central idea in speech production research. An alternative proposal is that speech production is organized in terms of control signals that subserve movements and associated vocal-tract configurations. Indeed, the capacity for intelligible speech by deaf speakers suggests that somatosensory inputs related to movement play a role in speech production-but studies that might have documented a somatosensory component have been equivocal. For example, mechanical perturbations that have altered somatosensory feedback have simultaneously altered acoustics. Hence, any adaptation observed under these conditions may have been a consequence of acoustic change. Here we show that somatosensory information on its own is fundamental to the achievement of speech movements. This demonstration involves a dissociation of somatosensory and auditory feedback during speech production. Over time, subjects correct for the effects of a complex mechanical load that alters jaw movements (and hence somatosensory feedback), but which has no measurable or perceptible effect on acoustic output. The findings indicate that the positions of speech articulators and associated somatosensory inputs constitute a goal of speech movements that is wholly separate from the sounds produced.},
  eprint = {12815431},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Adaptation; Physiological,Feedback,Hearing,Humans,Jaw,Movement,Robotics,Somatosensory Cortex,Speech,Speech Acoustics,Speech Intelligibility},
  langid = {english},
  number = {6942}
}

@article{trettenbreinControllingVideoStimuli2021,
  title = {Controlling {{Video Stimuli}} in {{Sign Language}} and {{Gesture Research}}: {{The OpenPoseR Package}} for {{Analyzing OpenPose Motion}}-{{Tracking Data}} in {{R}}},
  shorttitle = {Controlling {{Video Stimuli}} in {{Sign Language}} and {{Gesture Research}}},
  author = {Trettenbrein, Patrick C. and Zaccarella, Emiliano},
  date = {2021},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {12},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.628728},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.628728/full},
  urldate = {2021-02-19},
  abstract = {Researchers in the fields of sign language and gesture studies frequently present their participants with video stimuli showing actors performing linguistic signs or co-speech gestures. Up to now, such video stimuli have been mostly controlled only for some of the technical aspects of the video material (e.g., duration of clips, encoding, framerate, etc.), leaving open the possibility that systematic differences in video stimulus materials may be concealed in the actual motion properties of the actor’s movements. Computer vision methods such as OpenPose enable the fitting of body-pose models to the consecutive frames of a video clip and thereby make it possible to recover the movements performed by the actor in a particular video clip without the use of a point-based or markerless motion-tracking system during recording. The OpenPoseR package provides a straightforward and reproducible way of working with these body-pose model data extracted from video clips using OpenPose, allowing researchers in the fields of sign language and gesture studies to quantify the amount of motion (velocity and acceleration) pertaining only to the movements performed by the actor in a video clip. These quantitative measures can be used for controlling differences in the movements of an actor in stimulus video clips or, for example, between different conditions of an experiment. In addition, the package also provides a set of functions for generating plots for data visualization, as well as an easy-to-use way of automatically extracting metadata (e.g., duration, framerate, etc.) from large sets of video files.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\N4BRLTDW\\Trettenbrein and Zaccarella - 2021 - Controlling Video Stimuli in Sign Language and Ges.pdf},
  keywords = {Gesture,Linguistics,Neuroscience,Psychology,r,sign language,video stimuli},
  langid = {english}
}

@article{trujilloCommunicativeAdvantageHow2020,
  title = {The Communicative Advantage: How Kinematic Signaling Supports Semantic Comprehension},
  shorttitle = {The Communicative Advantage},
  author = {Trujillo, James P. and Simanova, Irina and Bekkering, Harold and Özyürek, Asli},
  date = {2020-10-01},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  volume = {84},
  pages = {1897--1911},
  issn = {1430-2772},
  doi = {10.1007/s00426-019-01198-y},
  url = {https://doi.org/10.1007/s00426-019-01198-y},
  urldate = {2021-03-03},
  abstract = {Humans are unique in their ability to communicate information through representational gestures which visually simulate an action (eg.~moving hands as if opening a jar). Previous research indicates that the intention to communicate modulates the kinematics (e.g., velocity, size) of such gestures. If and how this modulation influences addressees’ comprehension of gestures have not been investigated. Here we ask whether communicative kinematic modulation enhances semantic comprehension (i.e., identification) of gestures. We additionally investigate whether any comprehension advantage is due to enhanced early identification or late identification. Participants (n\,=\,20) watched videos of representational gestures produced in a more- (n\,=\,60) or less-communicative (n\,=\,60) context and performed a forced-choice recognition task. We tested the isolated role of kinematics by removing visibility of actor’s faces in Experiment I, and by reducing the stimuli to stick-light figures in Experiment II. Three video lengths were used to disentangle early identification from late identification. Accuracy and response time quantified main effects. Kinematic modulation was tested for correlations with task performance. We found higher gesture identification performance in more- compared to less-communicative gestures. However, early identification was only enhanced within a full visual context, while late identification occurred even when viewing isolated kinematics. Additionally, temporally segmented acts with more post-stroke holds were associated with higher accuracy. Our results demonstrate that communicative signaling, interacting with other visual cues, generally supports gesture identification, while kinematic modulation specifically enhances late identification in the absence of other cues. Results provide insights into mutual understanding processes as well as creating artificial communicative agents.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IV7GHQ4K\\Trujillo et al. - 2020 - The communicative advantage how kinematic signali.pdf},
  langid = {english},
  number = {7}
}

@report{trujilloEvidenceMultimodalLombard2020,
  title = {Evidence for a {{Multimodal Lombard Effect}}: {{Speakers}} Modulate Not Only Speech but Also Gesture to Overcome Noise},
  shorttitle = {Evidence for a {{Multimodal Lombard Effect}}},
  author = {Trujillo, James and Özyürek, Asli and Holler, Judith and Drijvers, Linda},
  date = {2020-05-01T14:11:06},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/3jdmq},
  url = {https://psyarxiv.com/3jdmq/},
  urldate = {2020-10-20},
  abstract = {In everyday conversation, we are often challenged with communicating in non-ideal settings, such as in noise. Increased speech intensity and larger mouth movements are used to overcome noise in constrained settings (the Lombard effect). How we adapt to noise in face-to-face interaction, the natural environment of human language use, where manual gestures are ubiquitous, is currently unknown.  We asked Dutch adults to wear headphones with varying levels of multi-talker babble while attempting to communicate action verbs to one another. Using quantitative motion capture and acoustic analyses, we found that 1) noise is associated with increased speech intensity and enhanced gesture kinematics, and 2) acoustic modulation of the speech signal only occurs when gestures are not present, while gesture kinematic modulation occurs regardless of co-occurring speech.  Thus, in face-to-face encounters the Lombard effect is not constrained to speech but is a multimodal phenomenon where gestures carry most of the communicative burden.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\AZJ4JM9M\\Trujillo et al. - 2020 - Evidence for a Multimodal Lombard Effect Speakers.pdf},
  keywords = {acoustics,gesture,kinematics,multimodality,noise,Nonverbal Behavior,other,Psychology,Social and Behavioral Sciences,Social and Personality Psychology}
}

@article{trujilloMarkerlessAutomaticAnalysis2019,
  title = {Toward the Markerless and Automatic Analysis of Kinematic Features: {{A}} Toolkit for Gesture and Movement Research},
  shorttitle = {Toward the Markerless and Automatic Analysis of Kinematic Features},
  author = {Trujillo, James P. and Vaitonyte, Julija and Simanova, Irina and Özyürek, Asli},
  date = {2019-04-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {51},
  pages = {769--777},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-1086-8},
  url = {https://doi.org/10.3758/s13428-018-1086-8},
  urldate = {2020-03-19},
  abstract = {Action, gesture, and sign represent unique aspects of human communication that use form and movement to convey meaning. Researchers typically use manual coding of video data to characterize naturalistic, meaningful movements at various levels of description, but the availability of markerless motion-tracking technology allows for quantification of the kinematic features of gestures or any meaningful human movement. We present a novel protocol for extracting a set of kinematic features from movements recorded with Microsoft Kinect. Our protocol captures spatial and temporal features, such as height, velocity, submovements/strokes, and holds. This approach is based on studies of communicative actions and gestures and attempts to capture features that are consistently implicated as important kinematic aspects of communication. We provide open-source code for the protocol, a description of how the features are calculated, a validation of these features as quantified by our protocol versus manual coders, and a discussion of how the protocol can be applied. The protocol effectively quantifies kinematic features that are important in the production (e.g., characterizing different contexts) as well as the comprehension (e.g., used by addressees to understand intent and semantics) of manual acts. The protocol can also be integrated with qualitative analysis, allowing fast and objective demarcation of movement units, providing accurate coding even of complex movements. This can be useful to clinicians, as well as to researchers studying multimodal communication or human–robot interactions. By making this protocol available, we hope to provide a tool that can be applied to understanding meaningful movement characteristics in human communication.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GVMYESRI\\Trujillo et al. - 2019 - Toward the markerless and automatic analysis of ki.pdf},
  langid = {english},
  number = {2}
}

@article{turpinHowImproveMuscle2021,
  title = {How to Improve the Muscle Synergy Analysis Methodology?},
  author = {Turpin, Nicolas A. and Uriac, Stéphane and Dalleau, Georges},
  date = {2021-01-26},
  journaltitle = {European Journal of Applied Physiology},
  shortjournal = {Eur J Appl Physiol},
  issn = {1439-6327},
  doi = {10.1007/s00421-021-04604-9},
  url = {https://doi.org/10.1007/s00421-021-04604-9},
  urldate = {2021-03-03},
  abstract = {Muscle synergy analysis is increasingly used in domains such as neurosciences, robotics, rehabilitation or sport sciences to analyze and better understand motor coordination. The analysis uses dimensionality reduction techniques to identify regularities in spatial, temporal or spatio-temporal patterns of multiple muscle activation. Recent studies have pointed out variability in outcomes associated with the different methodological options available and there was a need to clarify several aspects of the analysis methodology. While synergy analysis appears to be a robust technique, it remain a statistical tool and is, therefore, sensitive to the amount and quality of input data (EMGs). In particular, attention should be paid to EMG amplitude normalization, baseline noise removal or EMG filtering which may diminish or increase the signal-to-noise ratio of the EMG signal and could have major effects on synergy estimates. In order to robustly identify synergies, experiments should be performed so that the groups of muscles that would potentially form a synergy are activated with a sufficient level of activity, ensuring that the synergy subspace is fully explored. The concurrent use of various synergy formulations-spatial, temporal and spatio-temporal synergies- should be encouraged. The number of synergies represents either the dimension of the spatial structure or the number of independent temporal patterns, and we observed that these two aspects are often mixed in the analysis. To select a number, criteria based on noise estimates, reliability of analysis results, or functional outcomes of the synergies provide interesting substitutes to criteria solely based on variance thresholds.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YL8SIAPH\\Turpin et al. - 2021 - How to improve the muscle synergy analysis methodo.pdf},
  langid = {english}
}

@incollection{turveyBernsteinPerspectiveProblems1982,
  title = {The {{Bernstein Perspective}}: {{I}}. {{The Problems}} of {{Degrees}} of {{Freedom}} and {{Context}}-{{Conditioned Variability}}},
  shorttitle = {The {{Bernstein Perspective}}},
  booktitle = {Human {{Motor Behavior}}: {{An Introduction}}},
  author = {Turvey, M. T.},
  editor = {Kelso, J. A. Scott},
  date = {1982},
  pages = {251--264},
  publisher = {{Psychology Press}},
  doi = {10.4324/9781315802794-20},
  url = {https://www.taylorfrancis.com/},
  urldate = {2020-09-10},
  abstract = {The perspective to be developed in this chapter and the two that follow might be termed the Bernstein perspective after the Soviet physiologist Nicolai},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5IY6T3C3\\9781315802794-20.html},
  langid = {english}
}

@article{turveyCoordination1990,
  title = {Coordination},
  author = {Turvey, M. T.},
  date = {1990},
  journaltitle = {American Psychologist},
  volume = {45},
  pages = {938--953},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.45.8.938},
  abstract = {The Russian physiologist N. Bernstein (1967) defined coordination as a problem of mastering the very many degrees of freedom involved in a particular movement—of reducing the number of independent variables to be controlled. The initial theorizing and experimentation on "Bernstein's problem" was conducted largely in terms of how a device of very many independent variables might be regulated without ascribing excessive responsibility to an executive subsystem. A second round of theory and research on Bernstein's problem is now under way. This second round is motivated by similarities between coordination and physical processes in which multiple components become collectively self-organized; it is directed at an explanation of coordination in terms of very general laws and principles. The major achievements of the first round of efforts to address Bernstein's problem are summarized, and six examples of the theory and research typifying the second round are presented. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PXDZCKSF\\1990-30330-001.html},
  keywords = {Experimentation,Motor Coordination,Theories},
  number = {8}
}

@incollection{turveyImpredicativityDynamicsPerceptionAction2004,
  title = {Impredicativity, {{Dynamics}}, and the {{Perception}}-{{Action Divide}}},
  booktitle = {Coordination {{Dynamics}}: {{Issues}} and {{Trends}}},
  author = {Turvey, M. T.},
  editor = {Jirsa, Viktor K. and Kelso, J. A. S.},
  date = {2004},
  pages = {1--20},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-39676-5_1},
  url = {https://doi.org/10.1007/978-3-540-39676-5_1},
  urldate = {2020-10-30},
  abstract = {In this brief and largely pictorial essay I address the divide between perception and action. I review theoretical perspectives on ways in which the divide might be crossed and on ways in which the divide might be dissolved. Some of the ways to either cross or dissolve are traditional, others are recent, and some, importantly for our present purposes, are fundamentally dynamical.},
  isbn = {978-3-540-39676-5},
  keywords = {Anatomical Unit,Collective Variable,Pictorial Essay,Tensor Network,Turing Machine},
  langid = {english},
  series = {Understanding {{Complex Systems}}}
}

@book{turveyLecturesPerceptionEcological2018,
  title = {Lectures on {{Perception}}: {{An Ecological Perspective}}},
  shorttitle = {Lectures on {{Perception}}},
  author = {Turvey, M. T.},
  date = {2018-11-07},
  edition = {1 edition},
  publisher = {{Routledge}},
  abstract = {Lectures on Perception: An Ecological Perspective addresses the generic principles by which each and every kind of life form―from single celled organisms (e.g., difflugia) to multi-celled organisms (e.g., primates)―perceives the circumstances of their living so that they can behave adaptively. It focuses on the fundamental ability that relates each and every organism to its surroundings, namely, the ability to perceive things in the sense of how to get about among them and what to do, or not to do, with them. The book’s core thesis breaks from the conventional interpretation of perception as a form of abduction based on innate hypotheses and acquired knowledge, and from the historical scientific focus on the perceptual abilities of animals, most especially those abilities ascribed to humankind. Specifically, it advances the thesis of perception as a matter of laws and principles at nature’s ecological scale, and gives equal theoretical consideration to the perceptual achievements of all of the classically defined ‘kingdoms’ of organisms―Archaea, Bacteria, Protoctista, Fungi, Plantae, and Animalia.},
  isbn = {978-1-138-33526-4},
  langid = {english},
  pagetotal = {446}
}

@article{turveyMediumHapticPerception2014,
  title = {The {{Medium}} of {{Haptic Perception}}: {{A Tensegrity Hypothesis}}},
  shorttitle = {The {{Medium}} of {{Haptic Perception}}},
  author = {Turvey, M. T. and Fonseca, S. T.},
  date = {2014-05},
  journaltitle = {Journal of Motor Behavior},
  volume = {46},
  pages = {143--187},
  issn = {0022-2895, 1940-1027},
  doi = {10.1080/00222895.2013.798252},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00222895.2013.798252},
  urldate = {2019-04-18},
  abstract = {For any given animal, the sources of mechanical disturbances inducing tissue deformation define environment from the perspective of the animal’s haptic perceptual system. The system’s achievements include perceiving the body, attachments to the body, and the surfaces and substances adjacent to the body. Among the perceptual systems, it stands alone in having no defined medium. There is no articulated functional equivalent to air and water, the media that make possible the energy transmissions and diffusions underpinning the other perceptual systems. To identify the haptic system’s medium the authors focus on connective tissue and the conjunction of muscular, connective tissue net, and skeletal (MCS) as the body’s proper characterization. The challenge is a biophysical formulation of MCS as a continuum that, similar to air and water, is homogeneous and isotropic. The authors hypothesized a multifractal tensegrity (MFT) with the shape and stability of the constituents of each scale, from individual cell to whole body, derivative of continuous tension and discontinuous compression. Each component tensegrity of MFT is an adjustive-receptive unit, and the array of tensions in MFT is information about MCS. The authors extend the MFT hypothesis to body-brain linkages, and to limb perception phenomena attendant to amputation, vibration, anesthesia, neuropathy, and microgravity.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QSVVYB7C\\Turvey and Fonseca - 2014 - The Medium of Haptic Perception A Tensegrity Hypo.pdf},
  langid = {english},
  number = {3}
}

@article{tyronePhoneticsHeadBody2016,
  title = {The {{Phonetics}} of {{Head}} and {{Body Movement}} in the {{Realization}} of {{American Sign Language Signs}}},
  author = {Tyrone, Martha E. and Mauk, Claude E.},
  date = {2016},
  journaltitle = {Phonetica},
  shortjournal = {PHO},
  volume = {73},
  pages = {120--140},
  publisher = {{Karger Publishers}},
  issn = {0031-8388, 1423-0321},
  doi = {10.1159/000443836},
  url = {https://www.karger.com/Article/FullText/443836},
  urldate = {2020-03-09},
  abstract = {Background/Aims: Because the primary articulators for sign languages are the hands, sign phonology and phonetics have focused mainly on them and treated other articulators as passive targets. However, there is abundant research on the role of nonmanual articulators in sign language grammar and prosody. The current study examines how hand and head/body movements are coordinated to realize phonetic targets. Methods: Kinematic data were collected from 5 deaf American Sign Language (ASL) signers to allow the analysis of movements of the hands, head and body during signing. In particular, we examine how the chin, forehead and torso move during the production of ASL signs at those three phonological locations. Results: Our findings suggest that for signs with a lexical movement toward the head, the forehead and chin move to facilitate convergence with the hand. By comparison, the torso does not move to facilitate convergence with the hand for signs located at the torso. Conclusion: These results imply that the nonmanual articulators serve a phonetic as well as a grammatical or prosodic role in sign languages. Future models of sign phonetics and phonology should take into consideration the movements of the nonmanual articulators in the realization of signs.},
  eprint = {27225639},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KNI39GPU\\Tyrone and Mauk - 2016 - The Phonetics of Head and Body Movement in the Rea.pdf;C\:\\Users\\u668173\\Zotero\\storage\\INQJT5XX\\443836.html},
  langid = {english},
  number = {2}
}

@article{uetzMultisensoryCuesMultimodal2002,
  title = {Multisensory {{Cues}} and {{Multimodal Communication}} in {{Spiders}}: {{Insights}} from {{Video}}/{{Audio Playback Studies}}},
  shorttitle = {Multisensory {{Cues}} and {{Multimodal Communication}} in {{Spiders}}},
  author = {Uetz, George W. and Roberts, J. Andrew},
  date = {2002},
  journaltitle = {Brain, Behavior and Evolution},
  volume = {59},
  pages = {222--230},
  issn = {0006-8977, 1421-9743},
  doi = {10.1159/000064909},
  url = {https://www.karger.com/Article/FullText/64909},
  urldate = {2020-10-13},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\5H2N2Q8L\\Uetz and Roberts - 2002 - Multisensory Cues and Multimodal Communication in .pdf},
  langid = {english},
  number = {4}
}

@online{UnravellingGeneticArchitecture,
  title = {Unravelling the Genetic Architecture of Musical Rhythm | {{bioRxiv}}},
  url = {https://www.biorxiv.org/content/10.1101/836197v1},
  urldate = {2020-09-03},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2WGDHNDN\\836197v1.html}
}

@article{valenteAdultsVisualRecognition2019,
  title = {Adults’ Visual Recognition of Actions Simulations by Finger Gestures ({{ASFGs}}) Produced by Sighted and Blind Individuals},
  author = {Valente, Dannyelle and Palama, Amaya and Malsert, Jennifer and Bolens, Guillemette and Gentaz, Edouard},
  date = {2019-03-28},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  pages = {e0214371},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0214371},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0214371},
  urldate = {2019-11-30},
  abstract = {The present study examines the visual recognition of action simulations by finger gestures (ASFGs) produced by sighted and blind individuals. In ASFGs, fingers simulate legs to represent actions such as jumping, spinning, climbing, etc. The question is to determine whether the common motor experience of one’s own body is sufficient to produce adequate ASFGs or whether the possibility to see gestures from others are also necessary to do it. Three experiments were carried out to address this question. Experiment 1 examined in 74 sighted adults the recognition of 18 types of ASFGs produced by 20 blindfolded sighted adults. Results showed that rates of correct recognition were globally very high, but varied with the type of ASFG. Experiment 2 studied in 91 other sighted adults the recognition of ASFGs produced by 10 early blind and 7 late blind adults. Results also showed a high level of recognition with a similar order of recognizability by type of ASFG. However, ASFGs produced by early blind individuals were more poorly recognized than those produced by late blind individuals. In order to match data of recognition obtained with the form that gestures are produced by individuals, two independant judges evaluated prototypical and atypical attributes of ASFG produced by blindfolded sighted, early blind and late blind individuals in Experiment 3. Results revealed the occurrence of more atypical attributes in ASFG produced by blind individuals: their ASFGs transpose more body movements from a character-viewpoint in less agreement with visual rules. The practical interest of the study relates to the relevance of including ASFGs as a new exploratory procedure in tactile devices which are more apt to convey action concepts to blind users/readers.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\W8QFSG9D\\Valente et al. - 2019 - Adults’ visual recognition of actions simulations .pdf;C\:\\Users\\u668173\\Zotero\\storage\\Q4LQTI9U\\article.html},
  keywords = {Adults,Blindness,Climbing,Congenital disorders,Fingers,Language,Surveys,Vision},
  langid = {english},
  number = {3}
}

@article{vanarkelSimpleRepairMechanism,
  title = {A Simple Repair Mechanism Can Alleviate Computational Demands of Pragmatic Reasoning: Simulations and Complexity Analysis},
  author = {van Arkel, Jacqueline and Woensdregt, Marieke and Dingemanse, Mark and Blokpoel, Mark},
  pages = {18},
  abstract = {How can people communicate successfully while keeping resource costs low in the face of ambiguity? We present a principled theoretical analysis comparing two strategies for disambiguation in communication: (i) pragmatic reasoning, where communicators reason about each other, and (ii) other-initiated repair, where communicators signal and resolve trouble interactively. Using agent-based simulations and computational complexity analyses, we compare the efficiency of these strategies in terms of communicative success, computation cost and interaction cost. We show that agents with a simple repair mechanism can increase efficiency, compared to pragmatic agents, by reducing their computational burden at the cost of longer interactions. We also find that efficiency is highly contingent on the mechanism, highlighting the importance of explicit formalisation and computational rigour.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7EA9LEYW\\van Arkel et al. - A simple repair mechanism can alleviate computatio.pdf},
  langid = {english},
  options = {useprefix=true}
}

@article{vanberkel-vanhoofBenefitsAugmentativeSigns2016,
  title = {Benefits of Augmentative Signs in Word Learning: {{Evidence}} from Children Who Are Deaf/Hard of Hearing and Children with Specific Language Impairment},
  shorttitle = {Benefits of Augmentative Signs in Word Learning},
  author = {van Berkel-van Hoof, Lian and Hermans, Daan and Knoors, Harry and Verhoeven, Ludo},
  date = {2016-12-01},
  journaltitle = {Research in Developmental Disabilities},
  shortjournal = {Research in Developmental Disabilities},
  volume = {59},
  pages = {338--350},
  issn = {0891-4222},
  doi = {10.1016/j.ridd.2016.09.015},
  url = {http://www.sciencedirect.com/science/article/pii/S0891422216302128},
  urldate = {2020-07-14},
  abstract = {Background Augmentative signs may facilitate word learning in children with vocabulary difficulties, for example, children who are Deaf/Hard of Hearing (DHH) and children with Specific Language Impairment (SLI). Despite the fact that augmentative signs may aid second language learning in populations with a typical language development, empirical evidence in favor of this claim is lacking. Aims We aim to investigate whether augmentative signs facilitate word learning for DHH children, children with SLI, and typically developing (TD) children. Methods and procedures Whereas previous studies taught children new labels for familiar objects, the present study taught new labels for new objects. In our word learning experiment children were presented with pictures of imaginary creatures and pseudo words. Half of the words were accompanied by an augmentative pseudo sign. The children were tested for their receptive word knowledge. Outcomes and results The DHH children benefitted significantly from augmentative signs, but the children with SLI and TD age-matched peers did not score significantly different on words from either the sign or no-sign condition. Conclusions and implications These results suggest that using Sign-Supported speech in classrooms of bimodal bilingual DHH children may support their spoken language development. The difference between earlier research findings and the present results may be caused by a difference in methodology.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3CHU79KK\\van Berkel-van Hoof et al. - 2016 - Benefits of augmentative signs in word learning E.pdf;C\:\\Users\\u668173\\Zotero\\storage\\BA5NGTKD\\S0891422216302128.html},
  keywords = {Augmentative signs,Children,Deafness,Hearing loss,Specific language impairment,Vocabulary learning},
  langid = {english},
  options = {useprefix=true}
}

@article{vandersteenADaptationAnticipationModel2013,
  title = {The {{ADaptation}} and {{Anticipation Model}} ({{ADAM}}) of Sensorimotor Synchronization},
  author = {van der Steen, M. C. (Marieke) and Keller, Peter E.},
  date = {2013},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {7},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00253},
  url = {http://journal.frontiersin.org/article/10.3389/fnhum.2013.00253/abstract},
  urldate = {2020-12-21},
  abstract = {A constantly changing environment requires precise yet flexible timing of movements. Sensorimotor synchronization (SMS)—the temporal coordination of an action with events in a predictable external rhythm—is a fundamental human skill that contributes to optimal sensory-motor control in daily life. A large body of research related to SMS has focused on adaptive error correction mechanisms that support the synchronization of periodic movements (e.g., finger taps) with events in regular pacing sequences. The results of recent studies additionally highlight the importance of anticipatory mechanisms that support temporal prediction in the context of SMS with sequences that contain tempo changes. To investigate the role of adaptation and anticipatory mechanisms in SMS we introduce ADAM: an ADaptation and Anticipation Model. ADAM combines reactive error correction processes (adaptation) with predictive temporal extrapolation processes (anticipation) inspired by the computational neuroscience concept of internal models. The combination of simulations and experimental manipulations based on ADAM creates a novel and promising approach for exploring adaptation and anticipation in SMS. The current paper describes the conceptual basis and architecture of ADAM.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RWA75K9Q\\van der Steen and Keller - 2013 - The ADaptation and Anticipation Model (ADAM) of se.pdf},
  langid = {english},
  options = {useprefix=true}
}

@article{vandeventer4DCardiffConversation2015,
  title = {{{4D Cardiff Conversation Database}} ({{4D CCDb}}): {{A 4D Database}} of {{Natural}}, {{Dyadic Conversations}}},
  author = {Vandeventer, Jason and Aubrey, Andrew J and Rosin, Paul L and Marshall, David},
  date = {2015},
  pages = {6},
  abstract = {The 4D Cardiff Conversation Database (4D CCDb) is the first 4D (3D Video) audio-visual database containing natural conversations between pairs of people. This publicly available database contains 17 conversations which have been fully annotated for speaker and listener activity: conversational facial expressions, head motion, and verbal/non-verbal utterances. It can be accessed at http://www.cs.cf.ac.uk/CCDb.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PNL3Y4GN\\Vandeventer et al. - 2015 - 4D Cardiff Conversation Database (4D CCDb) A 4D D.pdf},
  langid = {english}
}

@article{vanhouttePathophysiologyTreatmentMuscle2011,
  title = {Pathophysiology and Treatment of Muscle Tension Dysphonia: A Review of the Current Knowledge},
  shorttitle = {Pathophysiology and Treatment of Muscle Tension Dysphonia},
  author = {Van Houtte, Evelyne and Van Lierde, Kristiane and Claeys, Sofie},
  date = {2011-03},
  journaltitle = {Journal of Voice: Official Journal of the Voice Foundation},
  volume = {25},
  pages = {202--207},
  issn = {1873-4588},
  doi = {10.1016/j.jvoice.2009.10.009},
  abstract = {OBJECTIVE: Muscle tension dysphonia (MTD) is a clinical and diagnostic term describing a spectrum of disturbed vocal fold behavior caused by increased tension of the (para)laryngeal musculature. Recent knowledge introduced MTD as a bridge between functional and organic disorders. This review addresses the causal and contributing factors of MTD and evaluates the different treatment options. METHODS: We searched MEDLINE (Pubmed, 1950-2009) and CENTRAL (The Cochrane Library, Issue 2, 2009). Studies were included if they reviewed the classification of functional dysphonia or the pathophysiology of MTD. Etiology and pathophysiology of MTD and circumlaryngeal manual therapy (CMT) were obligatory based on reviews and prospective cohort studies because randomized controlled trials (RCTs) are nonexisting. Concerning the treatment options of voice therapy and vocal hygiene, selection was based on RCTs and systematic reviews. RESULTS: Etiological factors can be categorized into three new subgroups: (1) psychological and/or personality factors, (2) vocal misuse and abuse, and (3) compensation for underlying disease. The effective treatment options for MTD are (1) indirect therapy: vocal hygiene and patient education; (2) direct therapy: voice therapy and CMT; (3) medical treatment; and (4) surgery for secondary organic lesions. CONCLUSIONS: MTD is the pathological condition in which an excessive tension of the (para)laryngeal musculature, caused by a diverse number of etiological factors, leads to a disturbed voice. Etiological factors range from psychological/personality disorders and vocal misuse/abuse to compensatory vocal habits in case of laryngopharyngeal reflux, upper airway infections, and organic lesions. MTD needs to be approached in a multidisciplinary setting where close cooperation between a laryngologist and a speech language pathologist is possible.},
  eprint = {20400263},
  eprinttype = {pmid},
  keywords = {Dysphonia,Humans,Laryngeal Muscles,Muscle Tonus,Phonation,Predictive Value of Tests,Risk Factors,Treatment Outcome,Vocal Cords,Voice Quality},
  langid = {english},
  number = {2}
}

@article{vanrossumPitchAccentAlaryngeal2002,
  title = {"{{Pitch}}" Accent in Alaryngeal Speech},
  author = {van Rossum, M. A. and de Krom, G. and Nooteboom, S. G. and Quené, H.},
  date = {2002-12},
  journaltitle = {Journal of speech, language, and hearing research: JSLHR},
  volume = {45},
  pages = {1106--1118},
  issn = {1092-4388},
  doi = {10.1044/1092-4388(2002/089)},
  abstract = {Highly proficient alaryngeal speakers are known to convey prosody successfully. The present study investigated whether alaryngeal speakers not selected on grounds of proficiency were able to convey pitch accent (a pitch accent is realized on the word that is in focus, cf. Bolinger, 1958). The participating speakers (10 tracheoesophageal, 9 esophageal, and 10 laryngeal [control] speakers) produced sentences in which accent was cued by the preceding context. For each utterance, a group of listeners identified which word conveyed accent. All speakers were able to convey accent. Acoustic analyses showed that some alaryngeal speakers had little or no control over fundamental frequency. Contrary to expectation, these speakers did not compensate by using nonmelodic cues, whereas speakers using F0 did use nonmelodic cues. Thus, temporal and intensity cues are concomitant with the use of F0; if F0 is affected, these nonmelodic cues will be as well. A pitch perception experiment confirmed that alaryngeal speakers who had no control over F0 and who did not use nonmelodic cues were nevertheless able to produce pitch movements. Speakers with no control over F0 apparently relied on an alternative pitch system to convey accents and other pitch movements.},
  eprint = {12546481},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GQAU9M6Q\\van Rossum et al. - 2002 - Pitch accent in alaryngeal speech.pdf},
  keywords = {Adult,Aged,Cues,Female,Humans,Male,Middle Aged,Pitch Perception,Sound Spectrography,Speech Production Measurement,Speech; Alaryngeal,Time Factors,Vocabulary},
  langid = {english},
  number = {6},
  options = {useprefix=true}
}

@article{vanvugtRoleAuditoryFeedback2016,
  title = {The Role of Auditory Feedback in Music-Supported Stroke Rehabilitation: {{A}} Single-Blinded Randomised Controlled Intervention},
  shorttitle = {The Role of Auditory Feedback in Music-Supported Stroke Rehabilitation},
  author = {van Vugt, F. T. and Kafczyk, T. and Kuhn, W. and Rollnik, J. D. and Tillmann, B. and Altenmüller, E.},
  date = {2016},
  journaltitle = {Restorative Neurology and Neuroscience},
  shortjournal = {Restor Neurol Neurosci},
  volume = {34},
  pages = {297--311},
  issn = {1878-3627},
  doi = {10.3233/RNN-150588},
  abstract = {PURPOSE: Learning to play musical instruments such as piano was previously shown to benefit post-stroke motor rehabilitation. Previous work hypothesised that the mechanism of this rehabilitation is that patients use auditory feedback to correct their movements and therefore show motor learning. We tested this hypothesis by manipulating the auditory feedback timing in a way that should disrupt such error-based learning. METHODS: We contrasted a patient group undergoing music-supported therapy on a piano that emits sounds immediately (as in previous studies) with a group whose sounds are presented after a jittered delay. The delay was not noticeable to patients. Thirty-four patients in early stroke rehabilitation with moderate motor impairment and no previous musical background learned to play the piano using simple finger exercises and familiar children's songs. RESULTS: Rehabilitation outcome was not impaired in the jitter group relative to the normal group. Conversely, some clinical tests suggests the jitter group outperformed the normal group. CONCLUSIONS: Auditory feedback-based motor learning is not the beneficial mechanism of music-supported therapy. Immediate auditory feedback therapy may be suboptimal. Jittered delay may increase efficacy of the proposed therapy and allow patients to fully benefit from motivational factors of music training. Our study shows a novel way to test hypotheses concerning music training in a single-blinded way, which is an important improvement over existing unblinded tests of music interventions.},
  eprint = {26923616},
  eprinttype = {pmid},
  keywords = {Adult,Aged,Analysis of Variance,auditory feedback,Auditory Perception,Feedback; Sensory,Female,Humans,Male,Middle Aged,Mood Disorders,motor learning,Motor Skills,music intervention,Psychiatric Status Rating Scales,sensorimotor integration,Single-Blind Method,Stroke,Stroke rehabilitation,Stroke Rehabilitation,timing},
  langid = {english},
  number = {2},
  options = {useprefix=true}
}

@article{veneziaPerceptionDrivesProduction2016,
  title = {Perception Drives Production across Sensory Modalities: {{A}} Network for Sensorimotor Integration of Visual Speech},
  shorttitle = {Perception Drives Production across Sensory Modalities},
  author = {Venezia, Jonathan H. and Fillmore, Paul and Matchin, William and Lisette Isenberg, A. and Hickok, Gregory and Fridriksson, Julius},
  date = {2016-02-01},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {126},
  pages = {196--207},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2015.11.038},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811915010605},
  urldate = {2020-12-01},
  abstract = {Sensory information is critical for movement control, both for defining the targets of actions and providing feedback during planning or ongoing movements. This holds for speech motor control as well, where both auditory and somatosensory information have been shown to play a key role. Recent clinical research demonstrates that individuals with severe speech production deficits can show a dramatic improvement in fluency during online mimicking of an audiovisual speech signal suggesting the existence of a visuomotor pathway for speech motor control. Here we used fMRI in healthy individuals to identify this new visuomotor circuit for speech production. Participants were asked to perceive and covertly rehearse nonsense syllable sequences presented auditorily, visually, or audiovisually. The motor act of rehearsal, which is prima facie the same whether or not it is cued with a visible talker, produced different patterns of sensorimotor activation when cued by visual or audiovisual speech (relative to auditory speech). In particular, a network of brain regions including the left posterior middle temporal gyrus and several frontoparietal sensorimotor areas activated more strongly during rehearsal cued by a visible talker versus rehearsal cued by auditory speech alone. Some of these brain regions responded exclusively to rehearsal cued by visual or audiovisual speech. This result has significant implications for models of speech motor control, for the treatment of speech output disorders, and for models of the role of speech gesture imitation in development.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GQQNM42G\\Venezia et al. - 2016 - Perception drives production across sensory modali.pdf;C\:\\Users\\u668173\\Zotero\\storage\\4I4Q4A8K\\S1053811915010605.html},
  keywords = {Audiovisual speech,fMRI,Multisensory integration,Sensorimotor integration,Speech production},
  langid = {english}
}

@report{venkateshCapturingBrainDynamics2020,
  title = {Capturing Brain Dynamics: Latent Spatiotemporal Patterns Predict Stimuli and Individual Differences},
  shorttitle = {Capturing Brain Dynamics},
  author = {Venkatesh, Manasij and JaJa, Joseph and Pessoa, Luiz},
  date = {2020-06-12},
  institution = {{Neuroscience}},
  doi = {10.1101/2020.06.11.146969},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.11.146969},
  urldate = {2020-06-17},
  abstract = {Insights from functional Magnetic Resonance Imaging (fMRI), and more recently from recordings of large numbers of neurons through calcium imaging, reveal that many cognitive, emotional, and motor functions depend on the multivariate interactions of neuronal populations. To capture and characterize spatiotemporal properties of brain events, we propose an architecture based on long short-term memory (LSTM) networks to uncover distributed spatiotemporal signatures during dynamic experimental conditions1. We demonstrate the potential of the approach using naturalistic movie-watching fMRI data. We show that movie clips result in complex but distinct spatiotemporal patterns in brain data that can be classified using LSTMs (≈ 90\% for 15-way classification), demonstrating that learned representations generalized to unseen participants. LSTMs were also superior to existing methods in predicting behavior and personality traits of individuals. We propose a dimensionality reduction approach that uncovers low-dimensional trajectories and captures essential informational properties of brain dynamics. Finally, we employed saliency maps to characterize spatiotemporally-varying brain-region importance. The spatiotemporal saliency maps revealed dynamic but consistent changes in fMRI activation data. We believe our approach provides a powerful framework for visualizing, analyzing, and discovering dynamic spatially distributed brain representations during naturalistic conditions.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IYZLP5ZI\\Venkatesh et al. - 2020 - Capturing brain dynamics latent spatiotemporal pa.pdf},
  langid = {english},
  type = {preprint}
}

@article{verhoefEmergenceCombinatorialStructure2014,
  title = {Emergence of Combinatorial Structure and Economy through Iterated Learning with Continuous Acoustic Signals},
  author = {Verhoef, Tessa and Kirby, Simon and de Boer, Bart},
  date = {2014-03-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {43},
  pages = {57--68},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2014.02.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0095447014000205},
  urldate = {2021-03-23},
  abstract = {Human speech has combinatorial structure, but it is still unclear how this type of organization emerged in the course of language evolution. There are two positions in the debate about the evolution of combinatorial structure: one stresses the importance of distinctiveness, while the other stresses economy and efficient reuse of building blocks. Different sources of evidence can be used to investigate the origins of combinatorial structure, such as emerging sign languages, animal communication systems, analysis of modern language and computer simulations but each source has its problems. In this article it is demonstrated that a novel empirical method from the field of language evolution can help to gain insight into the emergence of phonological combinatorial organization. This method, experimental iterated learning, allows investigating cultural evolution and the development of structure over time with human participants. We present data from an experiment in which combinatorial structure emerges in artificial whistled languages. We show that our experiment can give insight into the role of distinctiveness and reuse of building blocks and how they interact. We argue that experimental iterated learning offers a valuable new tool for investigating questions on evolutionary phonology and phonetics.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\H8SQ956K\\S0095447014000205.html},
  langid = {english},
  options = {useprefix=true}
}

@article{verhoefIconicityEmergenceCombinatorial2016,
  title = {Iconicity and the {{Emergence}} of {{Combinatorial Structure}} in {{Language}}},
  author = {Verhoef, T. and Kirby, S. and de Boer, B.},
  date = {2016-11},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  volume = {40},
  pages = {1969--1994},
  issn = {1551-6709},
  doi = {10.1111/cogs.12326},
  abstract = {In language, recombination of a discrete set of meaningless building blocks forms an unlimited set of possible utterances. How such combinatorial structure emerged in the evolution of human language is increasingly being studied. It has been shown that it can emerge when languages culturally evolve and adapt to human cognitive biases. How the emergence of combinatorial structure interacts with the existence of holistic iconic form-meaning mappings in a language is still unknown. The experiment presented in this paper studies the role of iconicity and human cognitive learning biases in the emergence of combinatorial structure in artificial whistled languages. Participants learned and reproduced whistled words for novel objects with the use of a slide whistle. Their reproductions were used as input for the next participant, to create transmission chains and simulate cultural transmission. Two conditions were studied: one in which the persistence of iconic form-meaning mappings was possible and one in which this was experimentally made impossible. In both conditions, cultural transmission caused the whistled languages to become more learnable and more structured, but this process was slightly delayed in the first condition. Our findings help to gain insight into when and how words may lose their iconic origins when they become part of an organized linguistic system.},
  eprint = {26706244},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MK2NES8M\\Verhoef et al. - 2016 - Iconicity and the Emergence of Combinatorial Struc.pdf},
  keywords = {Adult,Cognition,Cognitive biases,Combinatorial structure,Concept Formation,Cultural evolution,Cultural Evolution,Culture,Female,Humans,Iconicity,Iterated learning,Language,Language Development,Language evolution,Male,Young Adult},
  langid = {english},
  number = {8},
  options = {useprefix=true}
}

@article{vervloedTeachingMeaningWords2014,
  title = {Teaching the {{Meaning}} of {{Words}} to {{Children}} with {{Visual Impairments}}},
  author = {Vervloed, Mathijs P. J. and Loijens, Nancy E. A. and Waller, Sarah E.},
  date = {2014-09-01},
  journaltitle = {Journal of Visual Impairment \& Blindness},
  shortjournal = {Journal of Visual Impairment \& Blindness},
  volume = {108},
  pages = {433--438},
  issn = {0145-482X},
  doi = {10.1177/0145482X1410800508},
  url = {https://doi.org/10.1177/0145482X1410800508},
  urldate = {2019-11-30},
  langid = {english},
  number = {5}
}

@article{vicaryJointActionAesthetics2017,
  title = {Joint Action Aesthetics},
  author = {Vicary, Staci and Sperling, Matthias and von Zimmermann, Jorina and Richardson, Daniel C. and Orgs, Guido},
  date = {2017-07-25},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {12},
  pages = {e0180101},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0180101},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180101},
  urldate = {2020-12-05},
  abstract = {Synchronized movement is a ubiquitous feature of dance and music performance. Much research into the evolutionary origins of these cultural practices has focused on why humans perform rather than watch or listen to dance and music. In this study, we show that movement synchrony among a group of performers predicts the aesthetic appreciation of live dance performances. We developed a choreography that continuously manipulated group synchronization using a defined movement vocabulary based on arm swinging, walking and running. The choreography was performed live to four audiences, as we continuously tracked the performers’ movements, and the spectators’ affective responses. We computed dynamic synchrony among performers using cross recurrence analysis of data from wrist accelerometers, and implicit measures of arousal from spectators’ heart rates. Additionally, a subset of spectators provided continuous ratings of enjoyment and perceived synchrony using tablet computers. Granger causality analyses demonstrate predictive relationships between synchrony, enjoyment ratings and spectator arousal, if audiences form a collectively consistent positive or negative aesthetic evaluation. Controlling for the influence of overall movement acceleration and visual change, we show that dance communicates group coordination via coupled movement dynamics among a group of performers. Our findings are in line with an evolutionary function of dance–and perhaps all performing arts–in transmitting social signals between groups of people. Human movement is the common denominator of dance, music and theatre. Acknowledging the time-sensitive and immediate nature of the performer-spectator relationship, our study makes a significant step towards an aesthetics of joint actions in the performing arts.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GP9TXEUN\\Vicary et al. - 2017 - Joint action aesthetics.pdf;C\:\\Users\\u668173\\Zotero\\storage\\R4AIN45W\\article.html;C\:\\Users\\u668173\\Zotero\\storage\\S2XEXMYD\\article.html},
  keywords = {Evolutionary linguistics,Heart rate,Motion,Musculoskeletal mechanics,Music perception,Psychological attitudes,Vision,Wrist},
  langid = {english},
  number = {7}
}

@article{viherStructuralOrganizationPraxis2020,
  title = {Structural Organization of the Praxis Network Predicts Gesture Production: {{Evidence}} from Healthy Subjects and Patients with Schizophrenia},
  shorttitle = {Structural Organization of the Praxis Network Predicts Gesture Production},
  author = {Viher, P. V. and Abdulkadir, A. and Savadijev, P. and Stegmayer, K. and Kubicki, M. and Makris, N. and Karmacharya, S. and Federspiel, A. and Bohlhalter, S. and Vanbellingen, T. and Müri, R. and Wiest, R. and Strik, W. and Walther, S.},
  date = {2020-08-27},
  journaltitle = {Cortex},
  shortjournal = {Cortex},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2020.05.023},
  url = {http://www.sciencedirect.com/science/article/pii/S0010945220303087},
  urldate = {2020-09-18},
  abstract = {Hand gestures are an integral part of social interactions and communication. Several imaging studies in healthy subjects and lesion studies in patients with apraxia suggest the praxis network for gesture production, involving mainly left inferior frontal, posterior parietal and temporal regions. However, little is known about the structural connectivity underlying gesture production. We recruited 41 healthy participants and 39 patients with schizophrenia. All participants performed a gesture production test, the Test of Upper Limb Apraxia, and underwent diffusion tensor imaging. We hypothesized that gesture production is associated with structural network connectivity as well as with tract integrity. We defined the praxis network as an undirected graph comprised of 13 bilateral regions of interest and derived measures of local and global structural connectivity and tract integrity from Finsler geometry. We found an association of gesture deficit with reduced global and local efficiency of the praxis network. Furthermore, reduced tract integrity, for example in the superior longitudinal fascicle, arcuate fascicle or corpus callosum were related to gesture deficits. Our findings contribute to the understanding of structural correlates of gesture production as they first present diffusion tensor imaging data in a combined sample of healthy subjects and a patient cohort with gestural deficits.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BXEWWXYN\\S0010945220303087.html},
  keywords = {Diffusion weighted MRI,Gesture,Nonverbal communication,Structural connectivity},
  langid = {english}
}

@incollection{vihmanDynamicSystemsApproach2009,
  title = {A Dynamic Systems Approach to Babbling and Words},
  booktitle = {The {{Cambridge Handbook}} of {{Child Language}}},
  author = {Vihman, Marilyn M. and DePaolis, Rory A. and Keren-Portnoy, Tamar},
  editor = {Bavin, Edith L.},
  date = {2009},
  pages = {163--182},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511576164.010},
  url = {http://ebooks.cambridge.org/ref/id/CBO9780511576164A021},
  urldate = {2020-12-05},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Y2P5W3GZ\\Vihman et al. - 2009 - A dynamic systems approach to babbling and words.pdf},
  isbn = {978-0-511-57616-4},
  langid = {english}
}

@article{vila-gimenezObservingStorytellersWho2019,
  title = {Observing Storytellers Who Use Rhythmic Beat Gestures Improves Children’s Narrative Discourse Performance},
  author = {Vilà-Giménez, Ingrid and Igualada, Alfonso and Prieto, Pilar},
  date = {2019},
  journaltitle = {Developmental Psychology},
  volume = {55},
  pages = {250--262},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-0599(Electronic),0012-1649(Print)},
  doi = {10.1037/dev0000604},
  abstract = {Iconic and pointing gestures are important precursors of children’s early language and cognitive development. While beat gestures seem to have positive effects on the recall of information by preschoolers, little is known about the potential beneficial effects of observing beat gestures on the development of children’s narrative performance. We tested 44 5- and 6-year-old children in a between-subject study with a pretest–posttest design. After a pretest in which they were asked to retell the story of an animated cartoon they had watched, the children were exposed to a training session in which they observed an adult telling a total of 6 1-min stories under 2 between-subject experimental conditions: (a) a no-beat condition, where focal elements in the narratives were not highlighted by means of beat gestures; and (b) a beat condition, in which focal elements were highlighted by beat gestures. After the training session, a posttest was administered following the same procedure as the pretest. Narrative structure scores were independently coded from recordings of the pretest and posttest and subjected to statistical comparisons. The results revealed that children who were exposed to the beat condition showed a higher gain in narrative structure scores. This study thus shows for the first time that a brief training session with beat gestures has immediate benefits for children’s narrative discourse performance. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PUZXV74K\\Vilà-Giménez et al. - 2019 - Observing storytellers who use rhythmic beat gestu.pdf;C\:\\Users\\u668173\\Zotero\\storage\\D2MHNP45\\2018-58538-001.html},
  keywords = {Childhood Development,Cognitive Development,Gestures,Narratives,Posttesting,Pretesting,Storytelling,Training},
  number = {2}
}

@article{vilkmanExternalLaryngealFrame1996,
  title = {External Laryngeal Frame Function in Voice Production Revisited: A Review},
  shorttitle = {External Laryngeal Frame Function in Voice Production Revisited},
  author = {Vilkman, E. and Sonninen, A. and Hurme, P. and Körkkö, P.},
  date = {1996-03},
  journaltitle = {Journal of Voice: Official Journal of the Voice Foundation},
  volume = {10},
  pages = {78--92},
  issn = {0892-1997},
  doi = {10.1016/s0892-1997(96)80021-x},
  abstract = {Research indicates significant contribution of extrinsic laryngeal mechanisms to voice production. This article reviews the major theories of the role of the external laryngeal factors in voice production and relevant experimental data. The review suggests that partly neglected external factors and possibly even misinterpretation of some of the recently documented individual variation in physiological data may have unnecessarily complicated the issues pertaining to the interplay between the physiological mechanisms of the larynx. The implications of contemporary findings and documentation in the modeling of the extrinsic factors are discussed and a synthesis of empirical data into two simple models of the extrinsic forces of pitch control is presented. Also suggested by the review, a basic principle, probably underlying the laryngeal control of phonation, is put forward.},
  eprint = {8653181},
  eprinttype = {pmid},
  keywords = {Animals,Dogs,Electric Stimulation,Female,Humans,Laryngeal Muscles,Larynx,Male,Phonation,Speech,Vocal Cords},
  langid = {english},
  number = {1}
}

@article{vivianiTrajectoryDeterminesMovement1982,
  title = {Trajectory Determines Movement Dynamics},
  author = {Viviani, P. and Terzuolo, C.},
  date = {1982-02},
  journaltitle = {Neuroscience},
  shortjournal = {Neuroscience},
  volume = {7},
  pages = {431--437},
  issn = {0306-4522},
  doi = {10.1016/0306-4522(82)90277-9},
  abstract = {The relation between figural and kimematic aspects of movement was studied in handwriting and drawing. It was found that, throughout the movement, the tangential velocity. V is proportional to the radius of curvature r of the trajectory: V= kr, or, equivalently, that the angular velocity is constant: dalpha(t)/dt = K. However, the constant k generally takes several distinct values during the movement, the changes being abrupt. These changes suggest a clear segmentation of the movement into units of action which overlap but do not coincide with the figural units as defined by the discontinuities of the movement (cuspids, points of inflection). This organisational principle holds even when movements are mechanically constrained or are executed under strict visuo-motor guidance. Moreover, the segmentation of a given trajectory is invariant with respect to the total duration of the movement. A tentative interpretation of the principle is proposed which results from the assumption that the actual movement is produced as a continuous approximation to an intended movement, and that the well known relationship between movement speed and extent in rectilinear trajectories (Fitts' law) also applies to such continuous approximation.},
  eprint = {7078732},
  eprinttype = {pmid},
  keywords = {Biomechanical Phenomena,Handwriting,Humans,Mathematics,Models; Biological,Movement},
  langid = {english},
  number = {2}
}

@book{vogelLifeDevicesPhysical1988,
  title = {Life's {{Devices}}: {{The Physical World}} of {{Animals}} and {{Plants}}},
  author = {Vogel, S.},
  year = {Wed, 12/21/1988 - 12:00},
  url = {https://press.princeton.edu/books/paperback/9780691024189/lifes-devices},
  urldate = {2020-09-09},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\L8JIEVMJ\\lifes-devices.html},
  isbn = {978-0-691-02418-9},
  langid = {english}
}

@article{voisinClavicleNeglectedBone2006,
  title = {Clavicle, a Neglected Bone: {{Morphology}} and Relation to Arm Movements and Shoulder Architecture in Primates},
  shorttitle = {Clavicle, a Neglected Bone},
  author = {Voisin, Jean-Luc},
  date = {2006},
  journaltitle = {The Anatomical Record Part A: Discoveries in Molecular, Cellular, and Evolutionary Biology},
  volume = {288A},
  pages = {944--953},
  issn = {1552-4892},
  doi = {10.1002/ar.a.20354},
  url = {https://anatomypubs.onlinelibrary.wiley.com/doi/abs/10.1002/ar.a.20354},
  urldate = {2020-04-27},
  abstract = {In spite of its importance for movements of the upper limbs, the clavicle is an infrequently studied shoulder bone. The present study compares clavicular morphology among different extant primates. Methods have included the assessment of clavicular curvatures projected on two perpendicular planes that can be assessed overall as cranial and dorsal primary curvatures. Results showed that in cranial view, three morphologies can be defined. One group exhibited an external curvature considerably more pronounced than the internal one (Gorilla, Papio); a second group was characterized by an internal curvature much more pronounced than the external one (Hylobates, Ateles); and a third group contained those with the two curvatures equally pronounced (Pan, Homo, Pongo, Procolobus, Colobus). Clavicle curvatures projected on the dorsal plane could be placed into four groups. The first group is characterized by two curvatures, an inferior and a superior (Apes, Spider monkeys). The second included monkeys, whose clavicles have an inferior curvature much more pronounced than the superior one. The third group includes only Hylobates, whose clavicles possess only the superior curvature. The last group includes only modern humans, whose clavicles show only the inferior curvature, which is less pronounced than that which exists in monkeys. Curvatures in cranial view relate information regarding the parameters of arm elevation while those in dorsal view offer insights into the position of the scapula related to the thorax. The use of clavicular curvature analysis offers a new dimension in assessment of the functional morphology of the clavicle and its relationship to the shoulder complex. Anat Rec Part A, 288A:944–953, 2006. © 2006 Wiley-Liss, Inc.},
  annotation = {\_eprint: https://anatomypubs.onlinelibrary.wiley.com/doi/pdf/10.1002/ar.a.20354},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\YCFW8D39\\Voisin - 2006 - Clavicle, a neglected bone Morphology and relatio.pdf;C\:\\Users\\u668173\\Zotero\\storage\\PANQMDV9\\ar.a.html},
  keywords = {brachiation,catyrrhine,clavicle,hominoid,locomotion,platyrrhine,shoulder},
  langid = {english},
  number = {9}
}

@article{vonstadComparisonDeepLearningBased2020,
  title = {Comparison of a {{Deep Learning}}-{{Based Pose Estimation System}} to {{Marker}}-{{Based}} and {{Kinect Systems}} in {{Exergaming}} for {{Balance Training}}},
  author = {Vonstad, Elise Klæbo and Su, Xiaomeng and Vereijken, Beatrix and Bach, Kerstin and Nilsen, Jan Harald},
  date = {2020-01},
  journaltitle = {Sensors},
  volume = {20},
  pages = {6940},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/s20236940},
  url = {https://www.mdpi.com/1424-8220/20/23/6940},
  urldate = {2021-01-12},
  abstract = {Using standard digital cameras in combination with deep learning (DL) for pose estimation is promising for the in-home and independent use of exercise games (exergames). We need to investigate to what extent such DL-based systems can provide satisfying accuracy on exergame relevant measures. Our study assesses temporal variation (i.e., variability) in body segment lengths, while using a Deep Learning image processing tool (DeepLabCut, DLC) on two-dimensional (2D) video. This variability is then compared with a gold-standard, marker-based three-dimensional Motion Capturing system (3DMoCap, Qualisys AB), and a 3D RGB-depth camera system (Kinect V2, Microsoft Inc). Simultaneous data were collected from all three systems, while participants (N = 12) played a custom balance training exergame. The pose estimation DLC-model is pre-trained on a large-scale dataset (ImageNet) and optimized with context-specific pose annotated images. Wilcoxon\&rsquo;s signed-rank test was performed in order to assess the statistical significance of the differences in variability between systems. The results showed that the DLC method performs comparably to the Kinect and, in some segments, even to the 3DMoCap gold standard system with regard to variability. These results are promising for making exergames more accessible and easier to use, thereby increasing their availability for in-home exercise.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FL4BI8UX\\Vonstad et al. - 2020 - Comparison of a Deep Learning-Based Pose Estimatio.pdf;C\:\\Users\\u668173\\Zotero\\storage\\8XDSWLYC\\6940.html},
  issue = {23},
  keywords = {deep learning,exergaming,human movement,image analysis,kinect,markerless motion capture,motion capture,segment lengths},
  langid = {english},
  number = {23}
}

@software{wadhwaTDAstatsPipelineTopological2019,
  title = {{{TDAstats}}: {{Pipeline}} for {{Topological Data Analysis}}},
  shorttitle = {{{TDAstats}}},
  author = {Wadhwa, Raoul and Dhawan, Andrew and Williamson, Drew and Scott, Jacob and Brunson, Jason Cory and Ochi, Shota},
  date = {2019-12-12},
  url = {https://CRAN.R-project.org/package=TDAstats},
  urldate = {2020-03-11},
  abstract = {A comprehensive toolset for any useR conducting topological data analysis, specifically via the calculation of persistent homology in a Vietoris-Rips complex. The tools this package currently provides can be conveniently split into three main sections: (1) calculating persistent homology; (2) conducting statistical inference on persistent homology calculations; (3) visualizing persistent homology and statistical inference. The published form of TDAstats can be found in Wadhwa et al. (2018) {$<$}doi:10.21105/joss.00860{$>$}. For a general background on computing persistent homology for topological data analysis, see Otter et al. (2017) {$<$}doi:10.1140/epjds/s13688-017-0109-5{$>$}. To learn more about how the permutation test is used for nonparametric statistical inference in topological data analysis, read Robinson \& Turner (2017) {$<$}doi:10.1007/s41468-017-0008-7{$>$}. To learn more about how TDAstats calculates persistent homology, you can visit the GitHub repository for Ripser, the software that works behind the scenes at {$<$}https://github.com/Ripser/ripser{$>$}. This package has been published as Wadhwa et al. (2018) {$<$}doi:10.21105/joss.00860{$>$}.},
  version = {0.4.1}
}

@article{wagnerExploitingSpeechgestureLink2019,
  title = {Exploiting the Speech-Gesture Link to Capture Fine-Grained Prosodic Prominence Impressions and Listening Strategies},
  author = {Wagner, Petra and Ćwiek, Aleksandra and Samlowski, Barbara},
  date = {2019-09-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {76},
  pages = {100911},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2019.07.001},
  url = {http://www.sciencedirect.com/science/article/pii/S009544701830038X},
  urldate = {2020-07-13},
  abstract = {In this paper, we explore the possibility to gather perceptual impressions of prosodic prominence by exploiting the strong prosody-gesture link, i.e., by having listeners transform a perceptual impression into a motor movement, namely drumming, for two domains of prominence: word-level and syllable-level. A feasibility study reveals that such a procedure is indeed easily and speedily mastered by naïve listeners, but more difficult for word-level prominences. We furthermore examine whether “drummed” annotations are comparable to those gathered with more established annotation protocols based on cumulative naïve impressions and fine-grained expert ratings. These comparisons reveal high correspondences across all prominence annotation protocols, thus corroborating the general usefulness of the gestural approach. The analyses also reveal that all annotation protocols are strongly driven by structural linguistic considerations. We then use Random Forest Models to investigate the relative impact of signal and structural cues to prominence annotations. We find that expert ratings of prosodic prominence are guided comparatively more by structural concerns than those of naïve annotators, that word-level annotations are influenced more by structural linguistic cues than syllable-level ones, and that “drummed” annotations are driven least by structural cues. Lastly, we isolate two main listener strategies among our group of “drummers”, namely those integrating structural and signal cues to prominence, and those being guided predominantly by signal cues.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\PREA9LVB\\Wagner et al. - 2019 - Exploiting the speech-gesture link to capture fine.pdf;C\:\\Users\\u668173\\Zotero\\storage\\YEXXBHYC\\S009544701830038X.html},
  keywords = {Annotation,Gestures,Prominence,Prominence domains,Prosody,Signal correlates,Structural correlates},
  langid = {english}
}

@article{wagnerGestureSpeechInteraction2014,
  title = {Gesture and Speech in Interaction: {{An}} Overview},
  shorttitle = {Gesture and Speech in Interaction},
  author = {Wagner, Petra and Malisz, Zofia and Kopp, Stefan},
  date = {2014-02-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {57},
  pages = {209--232},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2013.09.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639313001295},
  urldate = {2019-04-16},
  abstract = {Gestures and speech interact. They are linked in language production and perception, with their interaction contributing to felicitous communication. The multifaceted nature of these interactions has attracted considerable attention from the speech and gesture community. This article provides an overview of our current understanding of manual and head gesture form and function, of the principle functional interactions between gesture and speech aiding communication, transporting meaning and producing speech. Furthermore, we present an overview of research on temporal speech-gesture synchrony, including the special role of prosody in speech-gesture alignment. In addition, we provide a summary of tools and data available for gesture analysis, and describe speech-gesture interaction models and simulations in technical systems. This overview also serves as an introduction to a Special Issue covering a wide range of articles on these topics. We provide links to the Special Issue throughout this paper.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I8QZH4YS\\S0167639313001295.html}
}

@article{wallotGroundingLanguagePerformance2011,
  title = {Grounding {{Language Performance}} in the {{Anticipatory Dynamics}} of the {{Body}}},
  author = {Wallot, Sebastian and Orden, Guy Van},
  date = {2011-07-25},
  journaltitle = {Ecological Psychology},
  volume = {23},
  pages = {157--184},
  publisher = {{Routledge}},
  issn = {1040-7413},
  doi = {10.1080/10407413.2011.591262},
  url = {https://doi.org/10.1080/10407413.2011.591262},
  urldate = {2020-10-15},
  abstract = {Speech acts, conversations, and other language activities emerge from anticipatory dynamics that situate minds and bodies near critical states. Critical states entail a kind of symmetry in which possible actions exist simultaneously as propensities to act. To speak or understand is to break the symmetry of these possibilities and realize the utterance that is expressed. This hypothesis is derived from complexity theory and agrees with findings that concern action generally and linguistic performance in particular.},
  annotation = {\_eprint: https://doi.org/10.1080/10407413.2011.591262},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LPCGMN8D\\Wallot and Orden - 2011 - Grounding Language Performance in the Anticipatory.pdf;C\:\\Users\\u668173\\Zotero\\storage\\723ZMTH7\\10407413.2011.html},
  number = {3}
}

@article{wallotInteractionDominantCausationMind2018,
  title = {Interaction-{{Dominant Causation}} in {{Mind}} and {{Brain}}, and {{Its Implication}} for {{Questions}} of {{Generalization}} and {{Replication}}},
  author = {Wallot, Sebastian and Kelty-Stephen, Damian G.},
  date = {2018-06-01},
  journaltitle = {Minds and Machines},
  shortjournal = {Minds \& Machines},
  volume = {28},
  pages = {353--374},
  issn = {1572-8641},
  doi = {10.1007/s11023-017-9455-0},
  url = {https://doi.org/10.1007/s11023-017-9455-0},
  urldate = {2020-02-12},
  abstract = {The dominant assumption about the causal architecture of the mind is, that it is composed of a stable set of components that contribute independently to relevant observables that are employed to measure cognitive activity. This view has been called component-dominant dynamics. An alternative has been proposed, according to which the different components are not independent, but fundamentally interdependent, and are not stable basic properties of the mind, but rather an emergent feature of the mind given a particular task context. This view has been called interaction-dominant dynamics. In this paper, we review evidence for interaction-dominant dynamics as the causal architecture of the mind. We point out, that such an architecture is consistent with problems of convergence in research on the level of results and theorizing. Moreover, we point out that if interaction-dominant dynamics as the causal architecture of the mind were to be true, this would naturally lead to (some degree of) problems with generalization and replicability in sciences of the mind and brain, and would probably warrant changes in the scientific practice with regard to study-design and data analysis.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DYD768VE\\Wallot and Kelty-Stephen - 2018 - Interaction-Dominant Causation in Mind and Brain, .pdf},
  langid = {english},
  number = {2}
}

@article{wallotRecurrenceQuantificationAnalysis2017,
  title = {Recurrence {{Quantification Analysis}} of {{Processes}} and {{Products}} of {{Discourse}}: {{A Tutorial}} in {{R}}},
  shorttitle = {Recurrence {{Quantification Analysis}} of {{Processes}} and {{Products}} of {{Discourse}}},
  author = {Wallot, Sebastian},
  date = {2017-07-04},
  journaltitle = {Discourse Processes},
  volume = {54},
  pages = {382--405},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2017.1297921},
  url = {https://doi.org/10.1080/0163853X.2017.1297921},
  urldate = {2019-09-20},
  abstract = {Processes of naturalistic reading and writing are based on complex linguistic input, stretch-out over time, and rely on an integrated performance of multiple perceptual, cognitive, and motor processes. Hence, naturalistic reading and writing performance is nonstationary and exhibits fluctuations and transitions. However, instead of being just complications for the analysis of such data, they are also informative about cognitive change, fluency, and reading or writing skill. To use and quantify such dynamics, one needs appropriate statistics that capture these aspects. In this article I introduce Recurrence Quantification Analysis (RQA) as a tool to capture such dynamic structure. After a conceptual introduction of the analysis, I present a step-by-step tutorial on how to run RQA using R. Guidance is given with regard to common issues and best practices using this time-series analysis technique. Finally, I review previous results from studies applying RQA to reading and writing and summarize current hypotheses and interpretations of RQA measures in the context of reading and writing.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\BBK6PIN3\\Wallot - 2017 - Recurrence Quantification Analysis of Processes an.pdf;C\:\\Users\\u668173\\Zotero\\storage\\ALHILH8H\\0163853X.2017.html},
  number = {5-6}
}

@article{waltonCreatingTimeSocial2018,
  title = {Creating Time: {{Social}} Collaboration in Music Improvisation},
  shorttitle = {Creating Time},
  author = {Walton, Ashley E. and Washburn, Auriel and Langland‐Hassan, Peter and Chemero, Anthony and Kloos, Heidi and Richardson, Michael J.},
  date = {2018},
  journaltitle = {Topics in Cognitive Science},
  volume = {10},
  pages = {95--119},
  publisher = {{Wiley-Blackwell Publishing Ltd.}},
  location = {{United Kingdom}},
  issn = {1756-8765(Electronic),1756-8757(Print)},
  doi = {10.1111/tops.12306},
  abstract = {Musical collaboration emerges from the complex interaction of environmental and informational constraints, including those of the instruments and the performance context. Music improvisation in particular is more like everyday interaction in that dynamics emerge spontaneously without a rehearsed score or script. We examined how the structure of the musical context affords and shapes interactions between improvising musicians. Six pairs of professional piano players improvised with two different backing tracks while we recorded both the music produced and the movements of their heads, left arms, and right arms. The backing tracks varied in rhythmic and harmonic information, from a chord progression to a continuous drone. Differences in movement coordination and playing behavior were evaluated using the mathematical tools of complex dynamical systems, with the aim of uncovering the multiscale dynamics that characterize musical collaboration. Collectively, the findings indicated that each backing track afforded the emergence of different patterns of coordination with respect to how the musicians played together, how they moved together, as well as their experience collaborating with each other. Additionally, listeners’ experiences of the music when rating audio recordings of the improvised performances were related to the way the musicians coordinated both their playing behavior and their bodily movements. Accordingly, the study revealed how complex dynamical systems methods (namely recurrence analysis) can capture the turn‐taking dynamics that characterized both the social exchange of the music improvisation and the sounds of collaboration more generally. The study also demonstrated how musical improvisation provides a way of understanding how social interaction emerges from the structure of the behavioral task context. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XQY4XCCP\\Walton et al. - 2018 - Creating time Social collaboration in music impro.pdf;C\:\\Users\\u668173\\Zotero\\storage\\RD8DVCSH\\2017-52209-001.html},
  keywords = {Collaboration,Improvisation,Music,Musical Instruments,Musicians,Social Interaction},
  number = {1}
}

@article{wangRoleBeatGesture2013,
  title = {The Role of Beat Gesture and Pitch Accent in Semantic Processing: {{An ERP}} Study},
  shorttitle = {The Role of Beat Gesture and Pitch Accent in Semantic Processing},
  author = {Wang, Lin and Chu, Mingyuan},
  date = {2013-09},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  volume = {51},
  pages = {2847--2855},
  publisher = {{Elsevier Limited}},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2013.09.027},
  url = {https://abdn.pure.elsevier.com/en/publications/the-role-of-beat-gesture-and-pitch-accent-in-semantic-processing-},
  urldate = {2020-10-13},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KJVLFUBX\\Wang and Chu - 2013 - The role of beat gesture and pitch accent in seman.pdf;C\:\\Users\\u668173\\Zotero\\storage\\MUSU2PYH\\the-role-of-beat-gesture-and-pitch-accent-in-semantic-processing-.html},
  langid = {english},
  number = {13}
}

@article{wangSpeakingRhythmicallyImproves2018,
  title = {Speaking Rhythmically Improves Speech Recognition under "Cocktail-Party" Conditions},
  author = {Wang, Mengyuan and Kong, Lingzhi and Zhang, Changxin and Wu, Xihong and Li, Liang},
  date = {2018-04},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {143},
  pages = {EL255},
  issn = {1520-8524},
  doi = {10.1121/1.5030518},
  abstract = {This study examines whether speech rhythm affects speech recognition under "cocktail-party" conditions. Against a two-talker masker, but not a speech-spectrum noise masker, recognition of the last (third) keyword in a normal rhythmic sentence was significantly better than that of the first keyword. However, this word-position-related speech-recognition improvement disappeared for rhythmically hybrid target sentences that were constructed by grouping parts from different sentences with different artificially modulated rhythms (rates) (fast, normal, or slow). Thus, the normal rhythm with a constant rate plays a role in improving speech recognition against informational speech masking, probably through a build-up of temporal prediction for target words.},
  eprint = {29716270},
  eprinttype = {pmid},
  langid = {english},
  number = {4}
}

@video{WantBeBetter2016,
  title = {Want to Be a Better Singer? {{Just}} Clench Your Butt!},
  shorttitle = {Want to Be a Better Singer?},
  date = {2016-11-14},
  url = {https://www.youtube.com/watch?v=bYKdW32-rxM&ab_channel=TheSkillfulVoice},
  urldate = {2020-11-06},
  abstract = {Try this technique out and let me know what you think in the comments below. www.theskillfulvoice.com www.instagram.com/theskillfulvoice www.facebook.com/theskillfulvoice}
}

@article{washburnFeedbackDelaysCan2019,
  title = {Feedback Delays Can Enhance Anticipatory Synchronization in Human-Machine Interaction},
  author = {Washburn, Auriel and Kallen, Rachel W. and Lamb, Maurice and Stepp, Nigel and Shockley, Kevin and Richardson, Michael J.},
  editor = {Ma, Jun},
  date = {2019-08-22},
  journaltitle = {PLOS ONE},
  volume = {14},
  pages = {e0221275},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0221275},
  url = {https://dx.plos.org/10.1371/journal.pone.0221275},
  urldate = {2020-09-25},
  abstract = {Research investigating the dynamics of coupled physical systems has demonstrated that small feedback delays can allow a dynamic response system to anticipate chaotic behavior. This counterintuitive phenomenon, termed anticipatory synchronization, has been observed in coupled electrical circuits, laser semi-conductors, and artificial neurons. Recent research indicates that the same process might also support the ability of humans to anticipate the occurrence of chaotic behavior in other individuals. Motivated by this latter work, the current study examined whether the process of feedback delay induced anticipatory synchronization could be employed to develop an interactive artificial agent capable of anticipating chaotic human movement. Results revealed that incorporating such delays within the movement-control dynamics of an artificial agent not only enhances an artificial agent’s ability to anticipate chaotic human behavior, but to synchronize with such behavior in a manner similar to natural human-human anticipatory synchronization. The implication of these findings for the development of human-machine interaction systems is discussed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IIGCDEJM\\Washburn et al. - 2019 - Feedback delays can enhance anticipatory synchroni.pdf},
  langid = {english},
  number = {8}
}

@book{webberRecurrenceQuantificationAnalysis2015,
  title = {Recurrence {{Quantification Analysis}}: {{Theory}} and {{Best Practices}}},
  author = {Webber, C. L. and Marwan, N.},
  date = {2015},
  publisher = {{Springer}},
  location = {{Cham}},
  url = {https://doi.org/10.1007/978-3-319-07155-8}
}

@article{wehlingArgumentGestureWar2009,
  title = {Argument Is {{Gesture War}}: {{Function}}, {{Form}}, and {{Prosody}} of {{Discourse Structuring Gestures}} in {{Political Argument}}},
  shorttitle = {Argument Is {{Gesture War}}},
  author = {Wehling, Elisabeth},
  date = {2009-12-05},
  journaltitle = {Annual Meeting of the Berkeley Linguistics Society},
  volume = {35},
  pages = {54--65},
  issn = {2377-1666},
  doi = {10.3765/bls.v35i2.3511},
  url = {https://journals.linguisticsociety.org/proceedings/index.php/BLS/article/view/3511},
  urldate = {2021-03-03},
  abstract = {n/a},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NSIE75P9\\Wehling - 2009 - Argument is Gesture War Function, Form, and Proso.pdf;C\:\\Users\\u668173\\Zotero\\storage\\CB3S3T26\\3511.html},
  issue = {2},
  langid = {american},
  number = {2}
}

@online{WeightingVowelQuality,
  title = {The Weighting of Vowel Quality in Native and Non-Native Listeners’ Perception of {{English}} Lexical Stress | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.wocn.2009.11.002},
  url = {https://reader.elsevier.com/reader/sd/pii/S0095447009000680?token=9069786BEDF546667AA9352AFE73427135E7CB36A504C079FE2B9B4C7E9DBB8492725F7F11E58DC192ED384B0EFEB028},
  urldate = {2020-06-25},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MGWWEIKJ\\S0095447009000680.html},
  langid = {english}
}

@article{weissSingingVoiceSpecial2020,
  title = {The Singing Voice Is Special: {{Persistence}} of Superior Memory for Vocal Melodies despite Vocal-Motor Distractions},
  shorttitle = {The Singing Voice Is Special},
  author = {Weiss, Michael W. and Bissonnette, Anne-Marie and Peretz, Isabelle},
  date = {2020-11-24},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  pages = {104514},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104514},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027720303334},
  urldate = {2020-11-27},
  abstract = {Vocal melodies sung without lyrics (la la) are remembered better than instrumental melodies. What causes the advantage? One possibility is that vocal music elicits subvocal imitation, which could promote enhanced motor representations of a melody. If this motor interpretation is correct, distracting the motor system during encoding should reduce the memory advantage for vocal over piano melodies. In Experiment 1, participants carried out movements of the mouth (i.e., chew gum) or hand (i.e., squeeze a beanbag) while listening to 24 unfamiliar folk melodies (half vocal, half piano). In a subsequent memory test, they rated the same melodies and 24 timbre-matched foils from ‘1–Definitely New’ to ‘7–Definitely Old’. There was a memory advantage for vocal over piano melodies with no effect of group and no interaction. In Experiment 2, participants carried out motor activities during encoding more closely related to singing, either silently articulating (la la) or vocalizing without articulating (humming continuously). Once again, there was a significant advantage for vocal melodies with no effect or interaction of group. In Experiment 3, participants audibly whispered (la la) repeatedly during encoding. Again, the voice advantage was present and did not differ appreciably from prior research with no motor task during encoding. However, we observed that the spontaneous phase-locking of whisper rate and musical beat tended to predict enhanced memory for vocal melodies. Altogether the results challenge the notion that subvocal rehearsal of the melody drives enhanced memory for vocal melodies. Instead, the voice may enhance engagement.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UBEQQ8CE\\S0010027720303334.html},
  keywords = {Articulatory suppression,Memory,Music,Singing,Timbre},
  langid = {english}
}

@article{welMovingArmDifferent2009,
  title = {Moving the {{Arm}} at {{Different Rates}}: {{Slow Movements}} Are {{Avoided}}},
  shorttitle = {Moving the {{Arm}} at {{Different Rates}}},
  author = {van der Wel, Robrecht P. R. D. and Sternad, Dagmar and Rosenbaum, David A.},
  date = {2009-12-23},
  journaltitle = {Journal of Motor Behavior},
  volume = {42},
  pages = {29--36},
  publisher = {{Routledge}},
  issn = {0022-2895},
  doi = {10.1080/00222890903267116},
  url = {https://doi.org/10.1080/00222890903267116},
  urldate = {2020-11-06},
  abstract = {Qualitative and quantitative changes characterize locomotion and rhythmic interlimb coordination at different speeds. Legs and hands do not move more or less quickly; they also adopt different relative coordination patterns. In the present article, the authors asked whether similar transitions occur for unimanual hand movements when speed is slowed below the preferred speed. Participants moved a handheld dowel back and forth between 2 large circular targets in time with a metronome at periods between 370 ms and 1667 ms. The authors analyzed the kinematics of participants’ movements at each period and found that proportional dwell time and number of peaks in the velocity profile increased as driving periods increased. Path lengths and peak velocities remained relatively constant for driving periods exceeding 800 ms. Participants made only gradual changes to their movement parameters, so that they went from a continuous mode to a more discrete mode of behavior for longer driving periods. Thus, unlike for rhythmic bimanual movements or locomotory patterns, there are quantitative but no clear qualitative changes for unimanual movements. The results suggest that participants tried to move close to their preferred tempo at different rates, and that they avoided moving slowly.},
  annotation = {\_eprint: https://doi.org/10.1080/00222890903267116},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8872H5FG\\Wel et al. - 2009 - Moving the Arm at Different Rates Slow Movements .pdf;C\:\\Users\\u668173\\Zotero\\storage\\45DNXWYX\\00222890903267116.html;C\:\\Users\\u668173\\Zotero\\storage\\D5AZVURQ\\00222890903267116.html},
  keywords = {aiming,coordination patterns,dwell time,preferred speed movement,rate control},
  number = {1}
}

@software{wickhamGgplot2CreateElegant2019,
  title = {Ggplot2: {{Create Elegant Data Visualisations Using}} the {{Grammar}} of {{Graphics}}},
  shorttitle = {Ggplot2},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and RStudio},
  date = {2019-04-07},
  url = {https://CRAN.R-project.org/package=ggplot2},
  urldate = {2019-04-23},
  abstract = {A system for 'declaratively' creating graphics, based on "The Grammar of Graphics". You provide the data, tell 'ggplot2' how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.},
  keywords = {Graphics,Phylogenetics,TeachingStatistics},
  version = {3.1.1}
}

@article{widdessMusickingBodiesGesture2013,
  title = {Musicking {{Bodies}}: {{Gesture}} and {{Voice}} in {{Hindustani Music}}},
  shorttitle = {Musicking {{Bodies}}},
  author = {Widdess, Richard},
  date = {2013-12-01},
  journaltitle = {Ethnomusicology Forum},
  volume = {22},
  pages = {377--379},
  publisher = {{Routledge}},
  issn = {1741-1912},
  doi = {10.1080/17411912.2013.837778},
  url = {https://doi.org/10.1080/17411912.2013.837778},
  urldate = {2020-10-23},
  annotation = {\_eprint: https://doi.org/10.1080/17411912.2013.837778},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\N4CXTFZJ\\Widdess - 2013 - Musicking Bodies Gesture and Voice in Hindustani .pdf},
  number = {3}
}

@article{wielingAnalyzingDynamicPhonetic2018,
  title = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling: {{A}} Tutorial Focusing on Articulatory Differences between {{L1}} and {{L2}} Speakers of {{English}}},
  shorttitle = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling},
  author = {Wieling, Martijn},
  date = {2018-09-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {70},
  pages = {86--116},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2018.03.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447017301377},
  urldate = {2019-10-12},
  abstract = {In phonetics, many datasets are encountered which deal with dynamic data collected over time. Examples include diphthongal formant trajectories and articulator trajectories observed using electromagnetic articulography. Traditional approaches for analyzing this type of data generally aggregate data over a certain timespan, or only include measurements at a fixed time point (e.g., formant measurements at the midpoint of a vowel). This paper discusses generalized additive modeling, a non-linear regression method which does not require aggregation or the pre-selection of a fixed time point. Instead, the method is able to identify general patterns over dynamically varying data, while simultaneously accounting for subject and item-related variability. An advantage of this approach is that patterns may be discovered which are hidden when data is aggregated or when a single time point is selected. A corresponding disadvantage is that these analyses are generally more time consuming and complex. This tutorial aims to overcome this disadvantage by providing a hands-on introduction to generalized additive modeling using articulatory trajectories from L1 and L2 speakers of English within the freely available R environment. All data and R code is made available to reproduce the analysis presented in this paper.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\6ILUPIEI\\Wieling - 2018 - Analyzing dynamic phonetic data using generalized .pdf;C\:\\Users\\u668173\\Zotero\\storage\\N67F9T36\\S0095447017301377.html},
  keywords = {Dynamic data,Electromagnetic articulography,Generalized additive modeling,Tutorial}
}

@article{wielingAnalyzingDynamicPhonetic2018a,
  title = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling: {{A}} Tutorial Focusing on Articulatory Differences between {{L1}} and {{L2}} Speakers of {{English}}},
  shorttitle = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling},
  author = {Wieling, Martijn},
  date = {2018-09},
  journaltitle = {Journal of Phonetics},
  volume = {70},
  pages = {86--116},
  issn = {00954470},
  doi = {10.1016/j.wocn.2018.03.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447017301377},
  urldate = {2019-10-12},
  abstract = {In phonetics, many datasets are encountered which deal with dynamic data collected over time. Examples include diphthongal formant trajectories and articulator trajectories observed using electromagnetic articulography. Traditional approaches for analyzing this type of data generally aggregate data over a certain timespan, or only include measurements at a fixed time point (e.g., formant measurements at the midpoint of a vowel). In this paper, I discuss generalized additive modeling, a non-linear regression method which does not require aggregation or the pre-selection of a fixed time point. Instead, the method is able to identify general patterns over dynamically varying data, while simultaneously accounting for subject and item-related variability. An advantage of this approach is that patterns may be discovered which are hidden when data is aggregated or when a single time point is selected. A corresponding disadvantage is that these analyses are generally more time consuming and complex. This tutorial aims to overcome this disadvantage by providing a hands-on introduction to generalized additive modeling using articulatory trajectories from L1 and L2 speakers of English within the freely available R environment. All data and R code is made available to reproduce the analysis presented in this paper.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3HBTJ6T5\\Wieling - 2018 - Analyzing dynamic phonetic data using generalized .pdf},
  langid = {english}
}

@article{wigginsCreativityInformationConsciousness2020,
  title = {Creativity, Information, and Consciousness: {{The}} Information Dynamics of Thinking},
  shorttitle = {Creativity, Information, and Consciousness},
  author = {Wiggins, Geraint A.},
  date = {2020-12-01},
  journaltitle = {Physics of Life Reviews},
  shortjournal = {Physics of Life Reviews},
  volume = {34-35},
  pages = {1--39},
  issn = {1571-0645},
  doi = {10.1016/j.plrev.2018.05.001},
  url = {http://www.sciencedirect.com/science/article/pii/S1571064518300599},
  urldate = {2020-12-03},
  abstract = {This paper presents a theory of the basic operation of mind, Information Dynamics of Thinking, which is intended for computational implementation and thence empirical testing. It is based on the information theory of Shannon, and treats the mind/brain as an information processing organ that aims to be information-efficient, in that it predicts its world, so as to use information efficiently, and regularly re-represents it, so as to store information efficiently. The theory is presented in context of a background review of various research areas that impinge upon its development. Consequences of the theory and testable hypotheses arising from it are discussed.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Q35WRHVR\\Wiggins - 2020 - Creativity, information, and consciousness The in.pdf;C\:\\Users\\u668173\\Zotero\\storage\\GZSYTIU5\\S1571064518300599.html},
  keywords = {Cognitive architecture,Cognitive representation,Computational creativity,Information theory,Machine consciousness,Music cognition},
  langid = {english}
}

@article{wigginsMusicMindMathematics2012,
  title = {Music, Mind and Mathematics: Theory, Reality and Formality},
  shorttitle = {Music, Mind and Mathematics},
  author = {Wiggins, Geraint A.},
  date = {2012-07-01},
  journaltitle = {Journal of Mathematics and Music},
  volume = {6},
  pages = {111--123},
  publisher = {{Taylor \& Francis}},
  issn = {1745-9737},
  doi = {10.1080/17459737.2012.694710},
  url = {https://doi.org/10.1080/17459737.2012.694710},
  urldate = {2020-12-04},
  abstract = {I consider the nature of music as a cognitive and cultural construct and discuss the relationship between mathematical systems and musical ones. I propose that mathematical modelling of certain kinds may be appropriate to model certain musical relationships, but this is so because of underlying cognitive principles. The conclusion is that to model music mathematically is essentially to model (parts of) cognition mathematically, which means that to model music in the abstract, as though it were itself a mathematical construct, divorced from its source in the human mind, is misleading. For a true understanding, the power of mathematics should be applied to the process of musical behaviour, not merely to its product. That process lies in the embodied human mind.},
  annotation = {\_eprint: https://doi.org/10.1080/17459737.2012.694710},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IY6W8E74\\17459737.2012.html},
  keywords = {cognition,mathematics,modelling,music},
  number = {2}
}

@article{wilburExperimentalInvestigationStressed1990,
  title = {An Experimental Investigation of Stressed Sign Production},
  author = {Wilbur, R. B.},
  date = {1990},
  journaltitle = {International Journal of Sign Linguistics},
  volume = {1},
  number = {1}
}

@article{willardThoracolumbarFasciaAnatomy2012,
  title = {The Thoracolumbar Fascia: Anatomy, Function and Clinical Considerations},
  shorttitle = {The Thoracolumbar Fascia},
  author = {Willard, F H and Vleeming, A and Schuenke, M D and Danneels, L and Schleip, R},
  date = {2012-12},
  journaltitle = {Journal of Anatomy},
  shortjournal = {J Anat},
  volume = {221},
  pages = {507--536},
  issn = {0021-8782},
  doi = {10.1111/j.1469-7580.2012.01511.x},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3512278/},
  urldate = {2020-04-03},
  abstract = {In this overview, new and existent material on the organization and composition of the thoracolumbar fascia (TLF) will be evaluated in respect to its anatomy, innervation biomechanics and clinical relevance. The integration of the passive connective tissues of the TLF and active muscular structures surrounding this structure are discussed, and the relevance of their mutual interactions in relation to low back and pelvic pain reviewed. The TLF is a girdling structure consisting of several aponeurotic and fascial layers that separates the paraspinal muscles from the muscles of the posterior abdominal wall. The superficial lamina of the posterior layer of the TLF (PLF) is dominated by the aponeuroses of the latissimus dorsi and the serratus posterior inferior. The deeper lamina of the PLF forms an encapsulating retinacular sheath around the paraspinal muscles. The middle layer of the TLF (MLF) appears to derive from an intermuscular septum that developmentally separates the epaxial from the hypaxial musculature. This septum forms during the fifth and sixth weeks of gestation. The paraspinal retinacular sheath (PRS) is in a key position to act as a ‘hydraulic amplifier’, assisting the paraspinal muscles in supporting the lumbosacral spine. This sheath forms a lumbar interfascial triangle (LIFT) with the MLF and PLF. Along the lateral border of the PRS, a raphe forms where the sheath meets the aponeurosis of the transversus abdominis. This lateral raphe is a thickened complex of dense connective tissue marked by the presence of the LIFT, and represents the junction of the hypaxial myofascial compartment (the abdominal muscles) with the paraspinal sheath of the epaxial muscles. The lateral raphe is in a position to distribute tension from the surrounding hypaxial and extremity muscles into the layers of the TLF. At the base of the lumbar spine all of the layers of the TLF fuse together into a thick composite that attaches firmly to the posterior superior iliac spine and the sacrotuberous ligament. This thoracolumbar composite (TLC) is in a position to assist in maintaining the integrity of the lower lumbar spine and the sacroiliac joint. The three-dimensional structure of the TLF and its caudally positioned composite will be analyzed in light of recent studies concerning the cellular organization of fascia, as well as its innervation. Finally, the concept of a TLC will be used to reassess biomechanical models of lumbopelvic stability, static posture and movement.},
  eprint = {22630613},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9XJCZZ4H\\Willard et al. - 2012 - The thoracolumbar fascia anatomy, function and cl.pdf},
  number = {6},
  pmcid = {PMC3512278}
}

@article{willburExperimentalInvestigationStressed,
  title = {An Experimental Investigation of Stressed Sign Production},
  author = {Willbur, R. B.},
  journaltitle = {International Journal of Sign Linguistics},
  volume = {1},
  pages = {41--60},
  number = {1}
}

@article{wilsonEmbodiedCognitionNot2013,
  title = {Embodied {{Cognition}} Is {{Not What}} You {{Think}} It Is},
  author = {Wilson, Andrew D. and Golonka, Sabrina},
  date = {2013},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {4},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00058},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00058/full},
  urldate = {2020-03-08},
  abstract = {The most exciting hypothesis in cognitive science right now is the theory that cognition is embodied. Like all good ideas in cognitive science, however, embodiment immediately came to mean six different things. The most common definitions involve the straightforward claim that ‘states of the body modify states of the mind’. However, the implications of embodiment are actually much more radical than this. If cognition can span the brain, body and the environment, then the ‘states of mind’ of disembodied cognitive science won’t exist to be modified. Cognition will instead be an extended system assembled from a broad array of resources. Taking embodiment seriously therefore requires both new methods and theory. Here we outline four key steps that research programmes should follow in order to fully engage with the implications of embodiment. The first step is to conduct a task analysis, which characterises from a first person perspective the specific task that a perceiving-acting cognitive agent is faced with. The second step is to identify the task-relevant resources the agent has access to in order to solve the task. These resources can span brain, body and environment. The third step is to identify how the agent can assemble these resources into a system capable of solving the problem at hand. The last step is to test the agent’s performance to confirm that agent is actually using the solution identified in step 3. We explore these steps in more detail with reference to two useful examples (the outfielder problem and the A-not-B error), and introduce how to apply this analysis to the thorny question of language use. Embodied cognition is more than we think it is, and we have the tools we need to realise its full potential.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\CPVFCASV\\Wilson and Golonka - 2013 - Embodied Cognition is Not What you Think it is.pdf},
  keywords = {A-not-B error,dynamical systems,Embodied Cognition,Language,outfielder problem,replacement hypothesis,Robotics},
  langid = {english}
}

@article{wilsonOscillatorModelTiming2005,
  title = {An Oscillator Model of the Timing of Turn-Taking},
  author = {Wilson, Margaret and Wilson, Thomas P.},
  date = {2005-12-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {12},
  pages = {957--968},
  issn = {1531-5320},
  doi = {10.3758/BF03206432},
  url = {https://doi.org/10.3758/BF03206432},
  urldate = {2020-05-19},
  abstract = {When humans talk without conventionalized arrangements, they engage in conversation—that is, a continuous and largely nonsimultaneous exchange in which speakers take turns. Turn-taking is ubiquitous in conversation and is the normal case against which alternatives, such as interruptions, are treated as violations that warrant repair. Furthermore, turn-taking involves highly coordinated timing, including a cyclic rise and fall in the probability of initiating speech during brief silences, and involves the notable rarity, especially in two-party conversations, of two speakers’ breaking a silence at once. These phenomena, reported by conversation analysts, have been neglected by cognitive psychologists, and to date there has been no adequate cognitive explanation. Here, we propose that, during conversation, endogenous oscillators in the brains of the speaker and the listeners become mutually entrained, on the basis of the speaker’s rate of syllable production. This entrained cyclic pattern governs the potential for initiating speech at any given instant for the speaker and also for the listeners (as potential next speakers). Furthermore, the readiness functions of the listeners are counterphased with that of the speaker, minimizing the likelihood of simultaneous starts by a listener and the previous speaker. This mutual entrainment continues for a brief period when the speech stream ceases, accounting for the cyclic property of silences. This model not only captures the timing phenomena observed in the literature on conversation analysis, but also converges with findings from the literatures on phoneme timing, syllable organization, and interpersonal coordination.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\QMH69846\\Wilson and Wilson - 2005 - An oscillator model of the timing of turn-taking.pdf},
  langid = {english},
  number = {6}
}

@article{wilsonRhythmicEntrainmentWhy2016,
  title = {Rhythmic Entrainment: {{Why}} Humans Want to, Fireflies Can't Help It, Pet Birds Try, and Sea Lions Have to Be Bribed},
  shorttitle = {Rhythmic Entrainment},
  author = {Wilson, Margaret and Cook, Peter F.},
  date = {2016-12},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {23},
  pages = {1647--1659},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1013-x},
  abstract = {Until recently, the literature on rhythmic ability took for granted that only humans are able to synchronize body movements to an external beat-to entrain. This assumption has been undercut by findings of beat-matching in various species of parrots and, more recently, in a sea lion, several species of primates, and possibly horses. This throws open the question of how widespread beat-matching ability is in the animal kingdom. Here we reassess the arguments and evidence for an absence of beat-matching in animals, and conclude that in fact no convincing case against beat-matching in animals has been made. Instead, such evidence as there is suggests that this capacity could be quite widespread. Furthermore, mutual entrainment of oscillations is a general principle of physical systems, both biological and nonbiological, suggesting that entrainment of motor systems by sensory systems may be a default rather than an oddity. The question then becomes, not why a few privileged species are able to beat-match, but why species do not always do so-why they vary in both spontaneous and learned beat-matching. We propose that when entrainment is not driven by fixed, mandatory connections between input and output (as in the case of, e.g., fireflies entraining to each others' flashes), it depends on voluntary control over, and voluntary or learned coupling of, sensory and motor systems, which can paradoxically lead to apparent failures of entrainment. Among the factors that affect whether an animal will entrain are sufficient control over the motor behavior to be entrained, sufficient perceptual sophistication to extract the entraining beat from the overall sensory environment, and the current cognitive state of the animal, including attention and motivation. The extent of entrainment in the animal kingdom potentially has widespread implications, not only for understanding the roots of human dance, but also for understanding the neural and cognitive architectures of animals.},
  eprint = {26920589},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\DDMH7YR4\\Wilson and Cook - 2016 - Rhythmic entrainment Why humans want to, fireflie.pdf},
  keywords = {Animal cognition,Animals,Behavior; Animal,Entrainment,Humans,Motor Activity,Time Perception},
  langid = {english},
  number = {6}
}

@article{wilsonStructureSilenceTurns1986,
  title = {The Structure of Silence between Turns in Two‐party Conversation},
  author = {Wilson, Thomas P. and Zimmerman, Don H.},
  date = {1986-10},
  journaltitle = {Discourse Processes},
  volume = {9},
  pages = {375--390},
  issn = {0163-853X, 1532-6950},
  doi = {10.1080/01638538609544649},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01638538609544649},
  urldate = {2020-05-19},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GW8KUN2S\\Wilson and Zimmerman - 1986 - The structure of silence between turns in two‐part.pdf},
  langid = {english},
  number = {4}
}

@article{wiltermuthSynchronyCooperation2009,
  title = {Synchrony and Cooperation},
  author = {Wiltermuth, Scott S. and Heath, Chip},
  date = {2009-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {20},
  pages = {1--5},
  issn = {1467-9280},
  doi = {10.1111/j.1467-9280.2008.02253.x},
  abstract = {Armies, churches, organizations, and communities often engage in activities-for example, marching, singing, and dancing-that lead group members to act in synchrony with each other. Anthropologists and sociologists have speculated that rituals involving synchronous activity may produce positive emotions that weaken the psychological boundaries between the self and the group. This article explores whether synchronous activity may serve as a partial solution to the free-rider problem facing groups that need to motivate their members to contribute toward the collective good. Across three experiments, people acting in synchrony with others cooperated more in subsequent group economic exercises, even in situations requiring personal sacrifice. Our results also showed that positive emotions need not be generated for synchrony to foster cooperation. In total, the results suggest that acting in synchrony with others can increase cooperation by strengthening social attachment among group members.},
  eprint = {19152536},
  eprinttype = {pmid},
  keywords = {Adolescent,Altruism,Ceremonial Behavior,Choice Behavior,Cooperative Behavior,Emotions,Female,Humans,Male,Models; Psychological,Motivation,Self Efficacy,Social Conformity,Social Identification,Young Adult},
  langid = {english},
  number = {1}
}

@software{winkelmannWrasspInterfaceASSP2018,
  title = {Wrassp: {{Interface}} to the '{{ASSP}}' {{Library}}},
  shorttitle = {Wrassp},
  author = {Winkelmann, Raphael and Bombien, Lasse and Scheffers, Michel},
  date = {2018-08-31},
  url = {https://CRAN.R-project.org/package=wrassp},
  urldate = {2019-09-24},
  abstract = {A wrapper around Michel Scheffers's 'libassp' ({$<$}http://libassp.sourceforge.net/{$>$}). The 'libassp' (Advanced Speech Signal Processor) library aims at providing functionality for handling speech signal files in most common audio formats and for performing analyses common in phonetic science/speech science. This includes the calculation of formants, fundamental frequency, root mean square, auto correlation, a variety of spectral analyses, zero crossing rate, filtering etc. This wrapper provides R with a large subset of 'libassp's signal processing functions and provides them to the user in a (hopefully) user-friendly manner.},
  version = {0.1.8}
}

@article{winnerRecipientDesignCommunicative2019,
  title = {Recipient {{Design}} in {{Communicative Pointing}}},
  author = {Winner, Tobias and Selen, Luc and Oosterwijk, Anke Murillo and Verhagen, Lennart and Medendorp, W. Pieter and van Rooij, Iris and Toni, Ivan},
  date = {2019},
  journaltitle = {Cognitive Science},
  volume = {43},
  pages = {e12733},
  issn = {1551-6709},
  doi = {10.1111/cogs.12733},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12733},
  urldate = {2021-03-03},
  abstract = {A long-standing debate in the study of human communication centers on the degree to which communicators tune their communicative signals (e.g., speech, gestures) for specific addressees, as opposed to taking a neutral or egocentric perspective. This tuning, called recipient design, is known to occur under special conditions (e.g., when errors in communication need to be corrected), but several researchers have argued that it is not an intrinsic feature of human communication, because that would be computationally too demanding. In this study, we contribute to this debate by studying a simple communicative behavior, communicative pointing, under conditions of successful (error-free) communication. Using an information-theoretic measure, called legibility, we present evidence of recipient design in communicative pointing. The legibility effect is present early in the movement, suggesting that it is an intrinsic part of the communicative plan. Moreover, it is reliable only from the viewpoint of the addressee, suggesting that the motor plan is tuned to the addressee. These findings suggest that recipient design is an intrinsic feature of human communication.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12733},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EKDPMVI2\\Winner et al. - 2019 - Recipient Design in Communicative Pointing.pdf},
  keywords = {Communication,Legibility,Perspective taking,Pointing,Recipient design},
  langid = {english},
  number = {5}
}

@article{wittenburgELANProfessionalFramework2006,
  title = {{{ELAN}}: A {{Professional Framework}} for {{Multimodality Research}}},
  author = {Wittenburg, Peter and Brugman, Hennie and Russel, Albert and Klassmann, Alex and Sloetjes, Han},
  date = {2006},
  pages = {4},
  abstract = {Utilization of computer tools in linguistic research has gained importance with the maturation of media frameworks for the handling of digital audio and video. The increased use of these tools in gesture, sign language and multimodal interaction studies has led to stronger requirements on the flexibility, the efficiency and in particular the time accuracy of annotation tools. This paper describes the efforts made to make ELAN a tool that meets these requirements, with special attention to the developments in the area of time accuracy. In subsequent sections an overview will be given of other enhancements in the latest versions of ELAN, that make it a useful tool in multimodality research.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\Q84I5QW9\\Wittenburg et al. - ELAN a Professional Framework for Multimodality R.pdf},
  langid = {english}
}

@online{wolfertReviewEvaluationPractices2021,
  title = {A {{Review}} of {{Evaluation Practices}} of {{Gesture Generation}} in {{Embodied Conversational Agents}}},
  author = {Wolfert, Pieter and Robinson, Nicole and Belpaeme, Tony},
  date = {2021-01-11},
  url = {http://arxiv.org/abs/2101.03769},
  urldate = {2021-02-27},
  abstract = {Embodied Conversational Agents (ECA) take on different forms, including virtual avatars or physical agents, such as a humanoid robot. ECAs are often designed to produce nonverbal behaviour to complement or enhance its verbal communication. One form of nonverbal behaviour is co-speech gesturing, which involves movements that the agent makes with its arms and hands that is paired with verbal communication. Co-speech gestures for ECAs can be created using different generation methods, such as rule-based and data-driven processes. However, reports on gesture generation methods use a variety of evaluation measures, which hinders comparison. To address this, we conducted a systematic review on co-speech gesture generation methods for iconic, metaphoric, deictic or beat gestures, including their evaluation methods. We reviewed 22 studies that had an ECA with a human-like upper body that used co-speech gesturing in a social human-agent interaction, including a user study to evaluate its performance. We found most studies used a within-subject design and relied on a form of subjective evaluation, but lacked a systematic approach. Overall, methodological quality was low-to-moderate and few systematic conclusions could be drawn. We argue that the field requires rigorous and uniform tools for the evaluation of co-speech gesture systems. We have proposed recommendations for future empirical evaluation, including standardised phrases and test scenarios to test generative models. We have proposed a research checklist that can be used to report relevant information for the evaluation of generative models as well as to evaluate co-speech gesture use.},
  archiveprefix = {arXiv},
  eprint = {2101.03769},
  eprinttype = {arxiv},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7D2W46CR\\Wolfert et al. - 2021 - A Review of Evaluation Practices of Gesture Genera.pdf;C\:\\Users\\u668173\\Zotero\\storage\\5BFZ35T7\\2101.html},
  keywords = {Computer Science - Human-Computer Interaction},
  primaryclass = {cs}
}

@book{woodGeneralizedAdditiveModels2017,
  title = {Generalized {{Additive Models}}: {{An Introduction}} with {{R}}, {{Second Edition}}},
  shorttitle = {Generalized {{Additive Models}}},
  author = {Wood, S. N.},
  date = {2017},
  publisher = {{Chapman and Hall/CRC}},
  url = {https://www.crcpress.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331},
  urldate = {2019-10-12},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NZSPGFBD\\9781498728331.html},
  langid = {english}
}

@online{WordFamiliarityPredicts,
  title = {Word Familiarity Predicts Temporal Asynchrony of Hand Gestures and Speech. - {{PsycNET}}},
  url = {/doiLanding?doi=10.1037%2F0278-7393.18.3.615},
  urldate = {2020-12-08},
  abstract = {APA PsycNet DoiLanding page},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\2BR98PBQ\\doiLanding.html},
  langid = {english}
}

@article{wuModelingConditionalDistribution2021,
  title = {Modeling the {{Conditional Distribution}} of {{Co}}-{{Speech Upper Body Gesture Jointly Using Conditional}}-{{GAN}} and {{Unrolled}}-{{GAN}}},
  author = {Wu, Bowen and Liu, Chaoran and Ishi, Carlos Toshinori and Ishiguro, Hiroshi},
  date = {2021-01},
  journaltitle = {Electronics},
  volume = {10},
  pages = {228},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/electronics10030228},
  url = {https://www.mdpi.com/2079-9292/10/3/228},
  urldate = {2021-01-27},
  abstract = {Co-speech gestures are a crucial, non-verbal modality for humans to communicate. Social agents also need this capability to be more human-like and comprehensive. This study aims to model the distribution of gestures conditioned on human speech features. Unlike previous studies that try to find injective functions that map speech to gestures, we propose a novel, conditional GAN-based generative model to not only convert speech into gestures but also to approximate the distribution of gestures conditioned on speech through parameterization. An objective evaluation and user study show that the proposed model outperformed the existing deterministic model, indicating that generative models can approximate real patterns of co-speech gestures better than the existing deterministic model. Our results suggest that it is critical to consider the nature of randomness when modeling co-speech gestures.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\49XZ79E5\\Wu et al. - 2021 - Modeling the Conditional Distribution of Co-Speech.pdf;C\:\\Users\\u668173\\Zotero\\storage\\JYG3ZEQG\\228.html},
  issue = {3},
  keywords = {deep learning,generative model,gesture generation,neural network,social robots},
  langid = {english},
  number = {3}
}

@article{xuIncorporatingCrossModalStatistics2012,
  title = {Incorporating {{Cross}}-{{Modal Statistics}} in the {{Development}} and {{Maintenance}} of {{Multisensory Integration}}},
  author = {Xu, Jinghong and Yu, Liping and Rowland, Benjamin A. and Stanford, Terrence R. and Stein, Barry E.},
  date = {2012-02-15},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {32},
  pages = {2287--2298},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4304-11.2012},
  url = {https://www.jneurosci.org/content/32/7/2287},
  urldate = {2020-12-16},
  abstract = {Development of multisensory integration capabilities in superior colliculus (SC) neurons was examined in cats whose visual–auditory experience was restricted to a circumscribed period during early life (postnatal day 30–8 months). Animals were periodically exposed to visual and auditory stimuli appearing either randomly in space and time, or always in spatiotemporal concordance. At all other times animals were maintained in darkness. Physiological testing was initiated at ∼2 years of age. Exposure to random visual and auditory stimuli proved insufficient to spur maturation of the ability to integrate cross-modal stimuli, but exposure to spatiotemporally concordant cross-modal stimuli was highly effective. The multisensory integration capabilities of neurons in the latter group resembled those of normal animals and were retained for {$>$}16 months in the absence of subsequent visual–auditory experience. Furthermore, the neurons were capable of integrating stimuli having physical properties differing significantly from those in the exposure set. These observations suggest that acquiring the rudiments of multisensory integration requires little more than exposure to consistent relationships between the modality-specific components of a cross-modal event, and that continued experience with such events is not necessary for their maintenance. Apparently, the statistics of cross-modal experience early in life define the spatial and temporal filters that determine whether the components of cross-modal stimuli are to be integrated or treated as independent events, a crucial developmental process that determines the spatial and temporal rules by which cross-modal stimuli are integrated to enhance both sensory salience and the likelihood of eliciting an SC-mediated motor response.},
  eprint = {22396404},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XVSXWFL7\\Xu et al. - 2012 - Incorporating Cross-Modal Statistics in the Develo.pdf;C\:\\Users\\u668173\\Zotero\\storage\\H5BNHPYJ\\2287.html;C\:\\Users\\u668173\\Zotero\\storage\\LCVG4HQT\\2287.html},
  langid = {english},
  number = {7}
}

@article{xuNoiserearingDisruptsMaturation2014,
  title = {Noise-Rearing Disrupts the Maturation of Multisensory Integration},
  author = {Xu, Jinghong and Yu, Liping and Rowland, Benjamin A. and Stanford, Terrence R. and Stein, Barry E.},
  date = {2014-02},
  journaltitle = {The European Journal of Neuroscience},
  shortjournal = {Eur J Neurosci},
  volume = {39},
  pages = {602--613},
  issn = {1460-9568},
  doi = {10.1111/ejn.12423},
  abstract = {It is commonly believed that the ability to integrate information from different senses develops according to associative learning principles as neurons acquire experience with co-active cross-modal inputs. However, previous studies have not distinguished between requirements for co-activation versus co-variation. To determine whether cross-modal co-activation is sufficient for this purpose in visual-auditory superior colliculus (SC) neurons, animals were reared in constant omnidirectional noise. By masking most spatiotemporally discrete auditory experiences, the noise created a sensory landscape that decoupled stimulus co-activation and co-variance. Although a near-normal complement of visual-auditory SC neurons developed, the vast majority could not engage in multisensory integration, revealing that visual-auditory co-activation was insufficient for this purpose. That experience with co-varying stimuli is required for multisensory maturation is consistent with the role of the SC in detecting and locating biologically significant events, but it also seems likely that this is a general requirement for multisensory maturation throughout the brain.},
  eprint = {24251451},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\XBJ39AC6\\Xu et al. - 2014 - Noise-rearing disrupts the maturation of multisens.pdf},
  keywords = {Acoustic Stimulation,Animals,Auditory Perception,cat,Cats,cross-modal,hearing,Neurons,Noise,Photic Stimulation,Superior Colliculi,vision,Visual Perception},
  langid = {english},
  number = {4},
  pmcid = {PMC3944832}
}

@article{xuWhatDoesNeuron2015,
  title = {What Does a Neuron Learn from Multisensory Experience?},
  author = {Xu, Jinghong and Yu, Liping and Stanford, Terrence R. and Rowland, Benjamin A. and Stein, Barry E.},
  date = {2015-02-01},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {J Neurophysiol},
  volume = {113},
  pages = {883--889},
  issn = {1522-1598},
  doi = {10.1152/jn.00284.2014},
  abstract = {The brain's ability to integrate information from different senses is acquired only after extensive sensory experience. However, whether early life experience instantiates a general integrative capacity in multisensory neurons or one limited to the particular cross-modal stimulus combinations to which one has been exposed is not known. By selectively restricting either visual-nonvisual or auditory-nonauditory experience during the first few months of life, the present study found that trisensory neurons in cat superior colliculus (as well as their bisensory counterparts) became adapted to the cross-modal stimulus combinations specific to each rearing environment. Thus, even at maturity, trisensory neurons did not integrate all cross-modal stimulus combinations to which they were capable of responding, but only those that had been linked via experience to constitute a coherent spatiotemporal event. This selective maturational process determines which environmental events will become the most effective targets for superior colliculus-mediated shifts of attention and orientation.},
  eprint = {25392160},
  eprinttype = {pmid},
  keywords = {Animals,Attention,auditory,Auditory Perception,cat,Cats,Female,Learning,Male,Neurons,Orientation,somatosensory,Superior Colliculi,superior colliculus,visual,Visual Perception},
  langid = {english},
  number = {3},
  pmcid = {PMC4312864}
}

@inproceedings{yadavClusteringLungCancer2013,
  title = {Clustering of Lung Cancer Data Using {{Foggy K}}-Means},
  booktitle = {2013 {{International Conference}} on {{Recent Trends}} in {{Information Technology}} ({{ICRTIT}})},
  author = {Yadav, A. K. and Tomar, D. and Agarwal, S.},
  date = {2013-07},
  pages = {13--18},
  doi = {10.1109/ICRTIT.2013.6844173},
  abstract = {In the medical field, huge data is available, which leads to the need of a powerful data analysis tool for extraction of useful information. Several studies have been carried out in data mining field to improve the capability of data analysis on huge datasets. Cancer is one of the most fatal diseases in the world. Lung Cancer with high rate of accurance is one of the serious problems and biggest killing disease in India. Prediction of occurance of the lung cancer is very difficult because it depends upon multiple attributes which could not be analyzedeasily. In this paper a real time lung cancer dataset is taken from SGPGI (Sanjay Gandhi Post Graduate Institute of Medical Sciences) Lucknow. A realtime dataset is always associated with its obvious challenges such as missing values, highly dimensional, noise, and outlier, which is not suitable for efficient classification. A clustering approach is an alternative solution to analyze the data in an unsupervised manner. In this current research work main focus is to develop a novel approach to create accurate clusters of desired real time datasets called Foggy K-means clustering. The result of the experiment indicates that foggy k-means clustering algorithm gives better result on real datasets as compared to simple k-means clustering algorithm and provides a better solution to the real world problem.},
  eventtitle = {2013 {{International Conference}} on {{Recent Trends}} in {{Information Technology}} ({{ICRTIT}})},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8KQCMJAW\\Yadav et al. - 2013 - Clustering of lung cancer data using Foggy K-means.pdf},
  keywords = {cancer,Cancer,Clustering,Clustering algorithms,data analysis,data mining,Data mining,fatal diseases,Foggy k-means clustering,foggy k-means clustering algorithm,Indexes,India,Information technology,lung,Lung Cancer,lung cancer data clustering approach,Lungs,medical computing,pattern clustering,real time lung cancer dataset,SGPGI,Tumors,unsupervised learning,unsupervised manner}
}

@article{yadegariFrameAphasiaDue2015,
  title = {Frame Aphasia Due to {{Broca}}’s Area Impairment: A {{Persian}} Case Report},
  shorttitle = {Frame Aphasia Due to {{Broca}}’s Area Impairment},
  author = {Yadegari, Fariba and Razavi, Mohammadreza and Azimian, Mojtaba},
  date = {2015-04-03},
  journaltitle = {Aphasiology},
  volume = {29},
  pages = {457--465},
  publisher = {{Routledge}},
  issn = {0268-7038},
  doi = {10.1080/02687038.2014.971221},
  url = {https://doi.org/10.1080/02687038.2014.971221},
  urldate = {2020-06-18},
  abstract = {Background: Frame/content theory considers speech as a phenomenon evolved from mandibular cyclicities for the physiologic primitive functions of feeding.Aims: This case report emphasises two aspects: first, the speech clinical characteristics of a left hemisphere damaged patient, which may be regarded as evidence of frame/content theory because there appears to be a regression to the most primitive stages of canonical babbling, and second, regarding its localised brain impairment limited to Broca’s area boundaries.Methods \& Procedures: We have provided a phonetically transcribed sample of speech and then analysed the articulated forms of syllables, mainly in words. A total of 403 syllables of the patient’s productions were analysed by articulatory phonetics method.Outcomes \& Results: Many patterns of articulation of this case, especially the labial–central and fronted dorsal–front pattern, may reveal a “frame without content” pattern that we believe to be a case of frame aphasia.Conclusions: This patient may be considered as evidence of frame/content dissociation due to brain damage. Broca’s area’s role in subserving such a stage of content representation and loading in speech production is hypothesised.},
  annotation = {\_eprint: https://doi.org/10.1080/02687038.2014.971221},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\JXMTSF3D\\02687038.2014.html},
  keywords = {aphasia,Broca’s region,content,frame,Persian},
  number = {4}
}

@article{yamamotoInfluenceAttractorStability2020,
  title = {The Influence of Attractor Stability of Intrinsic Coordination Patterns on the Adaptation to New Constraints},
  author = {Yamamoto, Kota and Shinya, Masahiro and Kudo, Kazutoshi},
  date = {2020-12},
  journaltitle = {Scientific Reports},
  volume = {10},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-60066-7},
  url = {http://www.nature.com/articles/s41598-020-60066-7},
  urldate = {2020-03-26},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\HJNIN4VL\\Yamamoto et al. - 2020 - The influence of attractor stability of intrinsic .pdf},
  langid = {english},
  number = {1}
}

@article{yamawakiForwardMovementPosterior2003,
  title = {Forward Movement of Posterior Pharyngeal Wall on Phonation},
  author = {Yamawaki, Yoshio},
  year = {2003 Nov-Dec},
  journaltitle = {American Journal of Otolaryngology},
  volume = {24},
  pages = {400--404},
  issn = {0196-0709},
  doi = {10.1016/s0196-0709(03)00089-9},
  abstract = {Forward movement of the posterior pharyngeal wall on phonation is divided into 2 types. One is movement of Passavant's ridge, which is a crescent-shaped or shelf-like posterior pharyngeal structure, and the other is generalized forward movement, which is not sharply outlined, of the posterior pharyngeal wall. Although nasopharyngoscopic and multiview videofluoroscopic findings on their movement show that they are markedly different, there are few reports about differentiation between them. In this study, multiple cross-sectional observations with rapid magnetic resonance imaging were performed to elucidate the forward movement of the posterior pharyngeal wall on phonation. I used rapid MRI to examine 4 subjects (3 females and 1 male) who had forward movement of the posterior pharyngeal wall with or without Passavant's ridge. Judging from the findings and local morphology, Passavant's ridge consisted of pharyngeal mucous and/or pharyngeal superficial muscles. On the other hand, it was revealed that generalized forward movement of the posterior pharyngeal wall was caused by contraction of the longus capitis muscle. The longus capitis muscle is well known as a flexor muscle of the head. The findings of the present study are significant and useful in speech rehabilitation for patients with velopharyngeal insufficiency.},
  eprint = {14608573},
  eprinttype = {pmid},
  keywords = {Adult,Child,Cleft Palate,Female,Humans,Magnetic Resonance Imaging,Male,Pharyngeal Muscles,Pharynx,Phonation},
  langid = {english},
  number = {6}
}

@article{yaMonkeysShareNeurophysiological2017,
  title = {Monkeys Share the Neurophysiological Basis for Encoding Sound Periodicities Captured by the Frequency-Following Response with Humans.},
  author = {Ya, Ayala and A, Lehmann and H, Merchant},
  date = {2017-11-30},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {7},
  pages = {16687--16687},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-16774-8},
  url = {https://europepmc.org/article/pmc/5709359},
  urldate = {2020-12-16},
  abstract = {Europe PMC is an archive of life sciences journal literature., Monkeys share the neurophysiological basis for encoding sound periodicities captured by the frequency-following response with humans.},
  eprint = {29192170},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\MJEX9PVY\\Ya et al. - 2017 - Monkeys share the neurophysiological basis for enc.pdf},
  langid = {english},
  number = {1}
}

@article{yanUnexpectedComplexityEveryday2020,
  title = {Unexpected Complexity of Everyday Manual Behaviors},
  author = {Yan, Yuke and Goodman, James M. and Moore, Dalton D. and Solla, Sara A. and Bensmaia, Sliman J.},
  date = {2020-07-16},
  journaltitle = {Nature Communications},
  volume = {11},
  pages = {3564},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17404-0},
  url = {https://www.nature.com/articles/s41467-020-17404-0},
  urldate = {2021-03-03},
  abstract = {How does the brain control an effector as complex and versatile as the hand? One possibility is that neural control is simplified by limiting the space of hand movements. Indeed, hand kinematics can be largely described within 8 to 10 dimensions. This oft replicated finding has been construed as evidence that hand postures are confined to this subspace. A prediction from this hypothesis is that dimensions outside of this subspace reflect noise. To address this question, we track the hand of human participants as they perform two tasks—grasping and signing in American Sign Language. We apply multiple dimension reduction techniques and replicate the finding that most postural variance falls within a reduced subspace. However, we show that dimensions outside of this subspace are highly structured and task dependent, suggesting they too are under volitional control. We propose that hand control occupies a higher dimensional space than previously considered.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\G986WN29\\Yan et al. - 2020 - Unexpected complexity of everyday manual behaviors.pdf;C\:\\Users\\u668173\\Zotero\\storage\\UY96CJCU\\s41467-020-17404-0.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@incollection{ybarraComparativeApproachNonHuman1995,
  title = {A {{Comparative Approach}} to the {{Non}}-{{Human Primate Vocal Tract}}: {{Implications}} for {{Sound Production}}},
  shorttitle = {A {{Comparative Approach}} to the {{Non}}-{{Human Primate Vocal Tract}}},
  booktitle = {Current {{Topics}} in {{Primate Vocal Communication}}},
  author = {Ybarra, Miguel A. Schön},
  editor = {Zimmermann, Elke and Newman, John D. and Jürgens, Uwe},
  date = {1995},
  pages = {185--198},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4757-9930-9_9},
  url = {https://doi.org/10.1007/978-1-4757-9930-9_9},
  urldate = {2020-09-18},
  abstract = {Despite the fact that it is the vocal tract morphology what ultimately shapes the acoustic structure of the sounds used in vocal communication (see Lieberman, 1984), recent state-of-the-art publications on the subject of non-human primate communication have not been concerned with that morphology (e.g., Snowdon, Brown, and Petersen, 1982; Todt, Goedeking, and Symmes, 1988). To help fill the gap thus left in our comprehension of the biological bases of vocal communication in non-human primates, this chapter reviews current knowledge pertaining to the comparative and functional morphology of non-human primate vocal tract features that, because of their shape, structure, and location, can influence sound structure. Conceptually, the chapter is based on (1) known relationships among the human vocal tract anatomy, the movements of its walls, air flow patterns, and the production of speech sounds (see Lieberman and Blumstein, 1988; Pickett, 1980); (2) Hirano’s structural model for vocal fold vibrator capabilities (Hirano, 1974, 1991; Gray et al., 1993); (3) the source-filter theory of speech production as summarized by Lieberman and Blumstein (1988).},
  isbn = {978-1-4757-9930-9},
  keywords = {Hyoid Bone,Nonhuman Primate,Vocal Communication,Vocal Fold,Vocal Tract},
  langid = {english}
}

@article{yeungPosturalControlVocal2020,
  title = {Postural Control of the Vocal Tract Affects Auditory Speech Perception},
  author = {Yeung, H. Henny and Scott, Mark},
  date = {2020},
  journaltitle = {Journal of Experimental Psychology: General},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/xge0000990},
  abstract = {Many researchers have proposed that sensorimotor information about the dynamic production of speech gestures can supplement the auditory perception of speech. Here we show that information about postural, nonspeech control of the vocal tract—such as breathing through the nose or mouth—also affects speech perception. Experimental participants breathed either through the nose or the mouth while identifying categories of speech sounds differing in nasal versus oral airflow. Participants showed an increased tendency to hear speech sounds as having nasal articulation when breathing through the nose, relative to when breathing through the mouth. These results suggest that postural information about the state of the vocal tract, like the motor configuration of the speech articulators while breathing, can modulate the perceptual processing of speech sounds. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\S665AI2H\\2020-80844-001.html},
  keywords = {Auditory Perception,Auditory Stimulation,Oral Communication,Respiration,Speech Perception,Vocal Cords}
}

@article{yuCrossmodalCompetitionDefault2019,
  title = {Cross-Modal Competition: {{The}} Default Computation for Multisensory Processing},
  shorttitle = {Cross-{{Modal Competition}}},
  author = {Yu, Liping and Cuppini, Cristiano and Xu, Jinghong and Rowland, Benjamin A. and Stein, Barry E.},
  date = {2019-02-20},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {39},
  pages = {1374--1385},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1806-18.2018},
  url = {https://www.jneurosci.org/content/39/8/1374},
  urldate = {2020-12-05},
  abstract = {Mature multisensory superior colliculus (SC) neurons integrate information across the senses to enhance their responses to spatiotemporally congruent cross-modal stimuli. The development of this neurotypic feature of SC neurons requires experience with cross-modal cues. In the absence of such experience the response of an SC neuron to congruent cross-modal cues is no more robust than its response to the most effective component cue. This “default” or “naive” state is believed to be one in which cross-modal signals do not interact. The present results challenge this characterization by identifying interactions between visual-auditory signals in male and female cats reared without visual-auditory experience. By manipulating the relative effectiveness of the visual and auditory cross-modal cues that were presented to each of these naive neurons, an active competition between cross-modal signals was revealed. Although contrary to current expectations, this result is explained by a neuro-computational model in which the default interaction is mutual inhibition. These findings suggest that multisensory neurons at all maturational stages are capable of some form of multisensory integration, and use experience with cross-modal stimuli to transition from their initial state of competition to their mature state of cooperation. By doing so, they develop the ability to enhance the physiological salience of cross-modal events thereby increasing their impact on the sensorimotor circuitry of the SC, and the likelihood that biologically significant events will elicit SC-mediated overt behaviors. SIGNIFICANCE STATEMENT The present results demonstrate that the default mode of multisensory processing in the superior colliculus is competition, not non-integration as previously characterized. A neuro-computational model explains how these competitive dynamics can be implemented via mutual inhibition, and how this default mode is superseded by the emergence of cooperative interactions during development.},
  eprint = {30573648},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\EMXKW85H\\Yu et al. - 2019 - Cross-Modal Competition The Default Computation f.pdf},
  keywords = {computational modeling,enhancement,inhibition,integration,plasticity,superior colliculus},
  langid = {english},
  number = {8}
}

@article{yunInterpersonalBodyNeural2012,
  title = {Interpersonal Body and Neural Synchronization as a Marker of Implicit Social Interaction},
  author = {Yun, Kyongsik and Watanabe, Katsumi and Shimojo, Shinsuke},
  date = {2012-12-11},
  journaltitle = {Scientific Reports},
  volume = {2},
  pages = {959},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep00959},
  url = {https://www.nature.com/articles/srep00959},
  urldate = {2020-12-05},
  abstract = {One may have experienced his or her footsteps unconsciously synchronize with the footsteps of a friend while walking together, or heard an audience's clapping hands naturally synchronize into a steady rhythm. However, the mechanisms of body movement synchrony and the role of this phenomenon in implicit interpersonal interactions remain unclear. We aimed to evaluate unconscious body movement synchrony changes as an index of implicit interpersonal interaction between the participants and also to assess the underlying neural correlates and functional connectivity among and within the brain regions. We found that synchrony of both fingertip movement and neural activity between the two participants increased after cooperative interaction. These results suggest that the increase of interpersonal body movement synchrony via interpersonal interaction can be a measurable basis of implicit social interaction. The paradigm provides a tool for identifying the behavioral and the neural correlates of implicit social interaction.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\UIN94PU5\\Yun et al. - 2012 - Interpersonal body and neural synchronization as a.pdf;C\:\\Users\\u668173\\Zotero\\storage\\NYZ4JL4T\\srep00959.html},
  issue = {1},
  langid = {english},
  number = {1}
}

@online{yunusSequencetoSequencePredictiveModel2020,
  title = {Sequence-to-{{Sequence Predictive Model}}: {{From Prosody To Communicative Gestures}}},
  shorttitle = {Sequence-to-{{Sequence Predictive Model}}},
  author = {Yunus, Fajrian and Clavel, Chloé and Pelachaud, Catherine},
  date = {2020-08-17},
  url = {http://arxiv.org/abs/2008.07643},
  urldate = {2020-08-24},
  abstract = {Communicative gestures and speech prosody are tightly linked. Our objective is to predict the timing of gestures according to the prosody. That is, we want to predict when a certain gesture occurs. We develop a model based on a recurrent neural network with attention mechanism. The model is trained on a corpus of natural dyadic interaction where the speech prosody and the gesture phases and types have been annotated. The input of the model is a sequence of speech prosody and the output is a sequence of gesture classes. The classes we are using for the model output is based on a combination of gesture phases and gesture types. We use a sequence comparison technique to evaluate the model performance. We find that the model can predict better certain gesture classes than others. We also perform ablation studies which reveal that fundamental frequency is a pertinent feature. We also find that a model trained on the data of one speaker only also works for the other speaker of the same conversation. Lastly, we also find that including eyebrow movements as a form of beat gesture improves the performance.},
  archiveprefix = {arXiv},
  eprint = {2008.07643},
  eprinttype = {arxiv},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9WCDH4WF\\Yunus et al. - 2020 - Sequence-to-Sequence Predictive Model From Prosod.pdf;C\:\\Users\\u668173\\Zotero\\storage\\DR6TG2DW\\2008.html},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryclass = {cs, eess}
}

@incollection{zafarCoordinatedHumanJaw1995,
  title = {Coordinated {{Human Jaw}} and {{Head}}-{{Neck Movements During Natural Jaw Opening}}-{{Closing}}: {{Reproducible Movement Patterns Indicate Linked Motor Control}}},
  shorttitle = {Coordinated {{Human Jaw}} and {{Head}}-{{Neck Movements During Natural Jaw Opening}}-{{Closing}}},
  booktitle = {Alpha and {{Gamma Motor Systems}}},
  author = {Zafar, H. and Eriksson, P.-O. and Nordh, E. and Al-Falahe, N.},
  editor = {Taylor, Anthony and Gladden, Margaret H. and Durbaba, Rade},
  date = {1995},
  pages = {502--504},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4615-1935-5_106},
  url = {https://doi.org/10.1007/978-1-4615-1935-5_106},
  urldate = {2020-06-12},
  abstract = {The complexity of the movements and muscle attachments between head, neck and mandible in man implies the existence of close functional relationships between the corresponding motor systems (see Figure 1A). Recent kinesiographic studies by our group have revealed concomitant head-neck movements during different voluntary jaw movement tasks (Nordh, Eriksson, Zafar \& Al-Falahe, 1993). In the present report, we analyse the short and long term reproducibility of the temporal and spatial patterns of associated head-neck movements during voluntary jaw opening-closing tasks in human subjects.},
  isbn = {978-1-4615-1935-5},
  langid = {english}
}

@article{zafarIntegratedJawNeck2000,
  title = {Integrated Jaw and Neck Function in Man. {{Studies}} of Mandibular and Head-Neck Movements during Jaw Opening-Closing Tasks},
  author = {Zafar, H.},
  date = {2000},
  journaltitle = {Swedish Dental Journal. Supplement},
  shortjournal = {Swed Dent J Suppl},
  pages = {1--41},
  issn = {0348-6672},
  abstract = {This investigation was undertaken to test the hypothesis of a functional relationship between the human temporomandibular and craniocervical regions. Mandibular and head-neck movements were simultaneously recorded in healthy young adults using a wireless optoelectronic system for three dimensional movement recording. The subjects were seated in an upright position without head support and were instructed to perform maximal jaw opening-closing movements at fast and slow speed. As a basis, a study was undertaken to develop a method for recording and analysis of mandibular and head-neck movements during natural jaw function. A consistent finding was parallel and coordinated head-neck movements during both fast and slow jaw opening-closing movements. The head in general started to move simultaneously with or before the mandible at the initiation of jaw opening. Most often, the head attained maximum velocity after the mandible. A high degree of spatiotemporal consistency of mandibular and head-neck movement trajectories was found in successive recording sessions. The head movement amplitude and the temporal coordination between mandibular and head-neck movements were speed related but not the movement trajectory patterns. Examination of individuals suffering from temporomandibular disorders and whiplash associated disorders (WAD) showed, compared with healthy subjects, smaller amplitudes, a diverse pattern of temporal coordination but a similar high degree of spatiotemporal consistency for mandibular and head-neck movements. In conclusion, the results suggest the following: A functional linkage exists between the human temporomandibular and craniocervical regions. Head movements are an integral part of natural jaw opening-closing. "Functional jaw movements" comprise concomitant mandibular and head-neck movements which involve the temporomandibular, the atlanto-occipital and the cervical spine joints, caused by jointly activated jaw and neck muscles. Jaw and neck muscle actions are elicited and synchronised by neural commands in common for both the jaw and the neck motor systems. These commands are preprogrammed, particularly at fast speed. In the light of previous observations of concurrent jaw and head movements during foetal yawning, it is suggested that these motor programs are innate. Neural processes underlying integrated jaw and neck function are invariant both in short- and long-term perspectives. Integrated jaw and neck function seems to be crucial for maintaining optimal orientation of the gape in natural jaw function. Injury to the head-neck, leading to WAD may derange integrated jaw-neck motor control and compromise natural jaw function.},
  eprint = {11234611},
  eprinttype = {pmid},
  keywords = {Adult,Atlanto-Occipital Joint,Cervical Vertebrae,Confidence Intervals,Electronics; Medical,Female,Head Movements,Humans,Male,Mandible,Masticatory Muscles,Middle Aged,Motor Neurons,Movement,Neck,Neck Muscles,Neuromuscular Junction,Optics and Photonics,Range of Motion; Articular,Reproducibility of Results,Statistics; Nonparametric,Temporomandibular Joint,Temporomandibular Joint Disorders,Time Factors,Whiplash Injuries},
  langid = {english},
  number = {143}
}

@article{zahaviPatternVocalSignals1982,
  title = {The Pattern of Vocal Signals and the Information They Convey},
  author = {Zahavi, Amotz},
  date = {1982},
  journaltitle = {Behaviour},
  volume = {80},
  pages = {1--8},
  publisher = {{Brill Academic Publishers}},
  location = {{United Kingdom}},
  issn = {1568-539X(Electronic),0005-7959(Print)},
  doi = {10.1163/156853982X00409},
  abstract = {Suggests that the vocal signal is an indicator of the posture and movements (PM) of the signaler at the time of vocalization; it reveals, through an additional modality, information about the PM of the signaler. The vocal signal is a reliable indicator of motivation because cheating would incur the cost of changing the PM away from the optimal PM for the real motivation. Certain vocal patterns discriminate better than others the small differences in the motivation of individuals. It is concluded that the vocal signal that conveys a certain motivation is dependent on (1) the strategy used to solve a particular conflict, (2) the kinds of reliable information that may resolve the conflict, (3) the PM that best convey such information, and (4) the vocal pattern that enables the listener to distinguish between signalers that differ slightly in their motivation. (20 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\GL2IEKXA\\1983-00422-001.html},
  keywords = {Animal Vocalizations,Motivation,Motor Processes},
  number = {1-2}
}

@article{zammPathwaysSeeingMusic2013,
  title = {Pathways to Seeing Music: Enhanced Structural Connectivity in Colored-Music Synesthesia},
  shorttitle = {Pathways to Seeing Music},
  author = {Zamm, Anna and Schlaug, Gottfried and Eagleman, David M. and Loui, Psyche},
  date = {2013-07-01},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  volume = {74},
  pages = {359--366},
  issn = {1095-9572},
  doi = {10.1016/j.neuroimage.2013.02.024},
  abstract = {Synesthesia, a condition in which a stimulus in one sensory modality consistently and automatically triggers concurrent percepts in another modality, provides a window into the neural correlates of cross-modal associations. While research on grapheme-color synesthesia has provided evidence for both hyperconnectivity-hyperbinding and disinhibited feedback as potential underlying mechanisms, less research has explored the neuroanatomical basis of other forms of synesthesia. In the current study we investigated the white matter correlates of colored-music synesthesia. As these synesthetes report seeing colors upon hearing musical sounds, we hypothesized that they might show unique patterns of connectivity between visual and auditory association areas. We used diffusion tensor imaging to trace the white matter tracts in temporal and occipital lobe regions in 10 synesthetes and 10 matched non-synesthete controls. Results showed that synesthetes possessed hemispheric patterns of fractional anisotropy, an index of white matter integrity, in the inferior fronto-occipital fasciculus (IFOF), a major white matter pathway that connects visual and auditory association areas to frontal regions. Specifically, white matter integrity within the right IFOF was significantly greater in synesthetes than controls. Furthermore, white matter integrity in synesthetes was correlated with scores on audiovisual tests of the Synesthesia Battery, especially in white matter underlying the right fusiform gyrus. Our findings provide the first evidence of a white matter substrate of colored-music synesthesia, and suggest that enhanced white matter connectivity is involved in enhanced cross-modal associations.},
  eprint = {23454047},
  eprinttype = {pmid},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\I7KSVMF6\\Zamm et al. - 2013 - Pathways to seeing music enhanced structural conn.pdf},
  keywords = {Auditory Perception,Brain,Color Perception,Diffusion Tensor Imaging,Female,Humans,Image Interpretation; Computer-Assisted,Male,Music,Neural Pathways,Perceptual Disorders,Synesthesia,Young Adult},
  langid = {english},
  pmcid = {PMC3643691}
}

@article{zbikowskiMetaphorMusicTheory1998,
  title = {Metaphor and {{Music Theory}}: {{Reflections}} from {{Cognitive Science}}},
  author = {Zbikowski, L. M.},
  date = {1998},
  journaltitle = {Music Theory Online},
  url = {http://societymusictheory.org/mto/issues/mto.98.4.1/mto.98. 4.1.zbikowski.html},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\LY7U5XQ2\\266316608_Metaphor_and_Music_Theory_Reflections_from_Cognitive_Science.html},
  langid = {english}
}

@article{zdrazilovaCommunicatingAbstractMeaning2018,
  title = {Communicating Abstract Meaning: Concepts Revealed in Words and Gestures},
  shorttitle = {Communicating Abstract Meaning},
  author = {Zdrazilova, Lenka and Sidhu, David M. and Pexman, Penny M.},
  date = {2018-08-05},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {373},
  pages = {20170138},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2017.0138},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rstb.2017.0138},
  urldate = {2020-03-26},
  abstract = {words refer to concepts that cannot be directly experienced through our senses (e.g. truth, morality). How we ground the meanings of abstract words is one of the deepest problems in cognitive science today. We investigated this question in an experiment in which 62 participants were asked to communicate the meanings of words (20 abstract nouns, e.g. impulse; 10 concrete nouns, e.g. insect) to a partner without using the words themselves (the taboo task). We analysed the speech and associated gestures that participants used to communicate the meaning of each word in the taboo task. Analysis of verbal and gestural data yielded a number of insights. When communicating about the meanings of abstract words, participants' speech referenced more people and introspections. In contrast, the meanings of concrete words were communicated by referencing more objects and entities. Gesture results showed that when participants spoke about abstract word meanings their speech was accompanied by more metaphorical and beat gestures, and speech about concrete word meanings was accompanied by more iconic gestures. Taken together, the results suggest that abstract meanings are best captured by a model that allows dynamic access to multiple representation systems.This article is part of the theme issue ‘Varieties of abstract concepts: development, use and representation in the brain’.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\RV686JIY\\Zdrazilova et al. - 2018 - Communicating abstract meaning concepts revealed .pdf;C\:\\Users\\u668173\\Zotero\\storage\\AV2A493J\\rstb.2017.html},
  number = {1752}
}

@article{zdrazilovaCommunicatingAbstractMeaning2018a,
  title = {Communicating Abstract Meaning: Concepts Revealed in Words and Gestures},
  shorttitle = {Communicating Abstract Meaning},
  author = {Zdrazilova, Lenka and Sidhu, David M. and Pexman, Penny M.},
  date = {2018-08-05},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {373},
  pages = {20170138},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2017.0138},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2017.0138},
  urldate = {2020-03-26},
  abstract = {words refer to concepts that cannot be directly experienced through our senses (e.g. truth, morality). How we ground the meanings of abstract words is one of the deepest problems in cognitive science today. We investigated this question in an experiment in which 62 participants were asked to communicate the meanings of words (20 abstract nouns, e.g. impulse; 10 concrete nouns, e.g. insect) to a partner without using the words themselves (the taboo task). We analysed the speech and associated gestures that participants used to communicate the meaning of each word in the taboo task. Analysis of verbal and gestural data yielded a number of insights. When communicating about the meanings of abstract words, participants' speech referenced more people and introspections. In contrast, the meanings of concrete words were communicated by referencing more objects and entities. Gesture results showed that when participants spoke about abstract word meanings their speech was accompanied by more metaphorical and beat gestures, and speech about concrete word meanings was accompanied by more iconic gestures. Taken together, the results suggest that abstract meanings are best captured by a model that allows dynamic access to multiple representation systems.This article is part of the theme issue ‘Varieties of abstract concepts: development, use and representation in the brain’.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\ZJU45ZL3\\Zdrazilova et al. - 2018 - Communicating abstract meaning concepts revealed .pdf;C\:\\Users\\u668173\\Zotero\\storage\\43ZD5MMS\\rstb.2017.html},
  number = {1752}
}

@article{zelicArticulatoryConstraintsSpontaneous2015,
  title = {Articulatory Constraints on Spontaneous Entrainment between Speech and Manual Gesture},
  author = {Zelic, Gregory and Kim, Jeesun and Davis, Chris},
  date = {2015-08-01},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {42},
  pages = {232--245},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2015.05.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0167945715000937},
  urldate = {2019-05-03},
  abstract = {The present study examined the extent to which speech and manual gestures spontaneously entrain in a non-communicative task. Participants had to repeatedly utter nonsense /CV/ syllables while continuously moving the right index finger in flexion/extension. No instructions to coordinate were given. We manipulated the type of syllable uttered (/ba/ vs. /sa/), and vocalization (phonated vs. silent speech). Assuming principles of coordination dynamics, a stronger entrainment between the fingers oscillations and the jaw motion was predicted (1) for /ba/, due to expected larger amplitude of jaw motion and (2) in phonated speech, due to the auditory feedback. Fifteen out of twenty participants showed simple ratios of speech to finger cycles (1:1, 1:2 or 2:1). In contrast with our predictions, speech–gesture entrainment was stronger when vocalizing /sa/ than /ba/, also more widely distributed on an in-phase mode. Furthermore, results revealed a spatial anchoring and an increased temporal variability in jaw motion when producing /sa/. We suggested that this indicates a greater control of the speech articulators for /sa/, making the speech performance more receptive to environmental forces, resulting in the greater entrainment observed to gesture oscillations. The speech–gesture coordination was maintained in silent speech, suggesting a somatosensory basis for their endogenous coupling.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\KVGKEBKU\\S0167945715000937.html},
  keywords = {Rhythmic coordination dynamics,Speech articulatory constraints,Spontaneous entrainment processes}
}

@inproceedings{zemanCoNLL2017Shared2017,
  title = {{{CoNLL}} 2017 {{Shared Task}}: {{Multilingual Parsing}} from {{Raw Text}} to {{Universal Dependencies}}},
  shorttitle = {{{CoNLL}} 2017 {{Shared Task}}},
  booktitle = {Proceedings of the {{CoNLL}} 2017 {{Shared Task}}: {{Multilingual Parsing}} from {{Raw Text}} to {{Universal Dependencies}}},
  author = {Zeman, Daniel and Popel, Martin and Straka, Milan and Hajič, Jan and Nivre, Joakim and Ginter, Filip and Luotolahti, Juhani and Pyysalo, Sampo and Petrov, Slav and Potthast, Martin and Tyers, Francis and Badmaeva, Elena and Gokirmak, Memduh and Nedoluzhko, Anna and Cinková, Silvie and Hajič jr., Jan and Hlaváčová, Jaroslava and Kettnerová, Václava and Urešová, Zdeňka and Kanerva, Jenna and Ojala, Stina and Missilä, Anna and Manning, Christopher D. and Schuster, Sebastian and Reddy, Siva and Taji, Dima and Habash, Nizar and Leung, Herman and de Marneffe, Marie-Catherine and Sanguinetti, Manuela and Simi, Maria and Kanayama, Hiroshi and de Paiva, Valeria and Droganova, Kira and Martínez Alonso, Héctor and Çöltekin, Çağrı and Sulubacak, Umut and Uszkoreit, Hans and Macketanz, Vivien and Burchardt, Aljoscha and Harris, Kim and Marheinecke, Katrin and Rehm, Georg and Kayadelen, Tolga and Attia, Mohammed and Elkahky, Ali and Yu, Zhuoran and Pitler, Emily and Lertpradit, Saran and Mandl, Michael and Kirchner, Jesse and Alcalde, Hector Fernandez and Strnadová, Jana and Banerjee, Esha and Manurung, Ruli and Stella, Antonio and Shimada, Atsuko and Kwak, Sookyoung and Mendonça, Gustavo and Lando, Tatiana and Nitisaroj, Rattima and Li, Josie},
  date = {2017-08},
  pages = {1--19},
  publisher = {{Association for Computational Linguistics}},
  location = {{Vancouver, Canada}},
  doi = {10.18653/v1/K17-3001},
  url = {https://www.aclweb.org/anthology/K17-3001},
  urldate = {2021-01-23},
  abstract = {The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, the task was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe how the data sets were prepared, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.},
  eventtitle = {{{CoNLL}} 2017},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VIDYXVYB\\Zeman et al. - 2017 - CoNLL 2017 Shared Task Multilingual Parsing from .pdf},
  options = {useprefix=true}
}

@article{zhangConnectingEmpiricalPhenomena2019,
  title = {Connecting Empirical Phenomena and Theoretical Models of Biological Coordination across Scales},
  author = {Zhang, Mengsen and Beetle, Christopher and Kelso, J. A. S. and Tognoli, Emmanuelle},
  date = {2019-08-30},
  journaltitle = {Journal of The Royal Society Interface},
  shortjournal = {Journal of The Royal Society Interface},
  volume = {16},
  pages = {20190360},
  publisher = {{Royal Society}},
  doi = {10.1098/rsif.2019.0360},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2019.0360},
  urldate = {2020-10-07},
  abstract = {Coordination in living systems—from cells to people—must be understood at multiple levels of description. Analyses and modelling of empirically observed patterns of biological coordination often focus either on ensemble-level statistics in large-scale systems with many components, or on detailed dynamics in small-scale systems with few components. The two approaches have proceeded largely independent of each other. To bridge this gap between levels and scales, we have recently conducted a human experiment of mid-scale social coordination specifically designed to reveal coordination at multiple levels (ensemble, subgroups and dyads) simultaneously. Based on this experiment, the present work shows that, surprisingly, a single system of equations captures key observations at all relevant levels. It also connects empirically validated models of large- and small-scale biological coordination—the Kuramoto and extended Haken–Kelso–Bunz (HKB) models—and the hallmark phenomena that each is known to capture. For example, it exhibits both multistability and metastability observed in small-scale empirical research (via the second-order coupling and symmetry breaking in extended HKB) and the growth of biological complexity as a function of scale (via the scalability of the Kuramoto model). Only by incorporating both of these features simultaneously can we reproduce the essential coordination behaviour observed in our experiment.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\VXZI7BHW\\Zhang et al. - 2019 - Connecting empirical phenomena and theoretical mod.pdf;C\:\\Users\\u668173\\Zotero\\storage\\WW48FGQF\\rsif.2019.html},
  number = {157}
}

@article{zhangMoreWordsOnline2020,
  title = {More than Words: {{The}} Online Orchestration of Word Predictability, Prosody, Gesture, and Mouth Movements during Natural Language Comprehension},
  shorttitle = {More than Words},
  author = {Zhang, Ye and Frassinelli, Diego and Tuomainen, Jyrki and Skipper, Jeremy I and Vigliocco, Gabriella},
  date = {2020-01-09},
  journaltitle = {bioRxiv},
  doi = {10.1101/2020.01.08.896712},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.01.08.896712},
  urldate = {2020-01-13},
  abstract = {Communication naturally occurs in dynamic face-to-face environments where spoken words are embedded in linguistic discourse and accompanied by multimodal cues. Existing research supports predictive brain models where prior discourse but also prosody, mouth movements, and hand-gestures individually contribute to comprehension. In electroencephalography (EEG) studies, more predictable words show reduced negativity, peaking at approximately 400ms after onset of the word (N400). Meaningful gestures and prosody also reduce the N400, while the effects of rhythmic gestures and mouth movements remain unclear. However, these studies have only focused on individual cues while in the real world, communicative cues co-occur and potentially interact. We measured EEG elicited by words while participants watched videos of a speaker producing short naturalistic passages. For each word, we quantified the information carried by prior linguistic discourse (surprisal), prosody (mean pitch), mouth informativeness (lip-reading), and presence of meaningful and/or rhythmic gestures. Discourse predictability reduced the N400 amplitude and multimodal cues impacted and interacted with this effect. Specifically, higher pitch and meaningful gestures reduced N400 amplitude, beyond the effect of discourse, while rhythmic gestures increased the N400 amplitude. Moreover, higher pitch overall reduced N400 amplitude while informative mouth movements only showed effects when no gestures were present. These results can constrain existing predictive brain models in that they demonstrate that the brain uses cues selectively in a dynamic and contextually determined manner in real-world language comprehension.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\C44EXJ56\\Zhang et al. - 2020 - More than words The online orchestration of word .pdf},
  langid = {english}
}

@report{zhangMoreWordsOnline2020a,
  title = {More than Words: {{The}} Online Orchestration of Word Predictability, Prosody, Gesture, and Mouth Movements during Natural Language Comprehension},
  shorttitle = {More than Words},
  author = {Zhang, Ye and Frassinelli, Diego and Tuomainen, Jyrki and Skipper, Jeremy I and Vigliocco, Gabriella},
  date = {2020-01-09},
  institution = {{Neuroscience}},
  doi = {10.1101/2020.01.08.896712},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.01.08.896712},
  urldate = {2020-03-06},
  abstract = {Communication naturally occurs in dynamic face-to-face environments where spoken words are embedded in linguistic discourse and accompanied by multimodal cues. Existing research supports predictive brain models where prior discourse but also prosody, mouth movements, and hand-gestures individually contribute to comprehension. In electroencephalography (EEG) studies, more predictable words show reduced negativity, peaking at approximately 400ms after onset of the word (N400). Meaningful gestures and prosody also reduce the N400, while the effects of rhythmic gestures and mouth movements remain unclear. However, these studies have only focused on individual cues while in the real world, communicative cues co-occur and potentially interact. We measured EEG elicited by words while participants watched videos of a speaker producing short naturalistic passages. For each word, we quantified the information carried by prior linguistic discourse (surprisal), prosody (mean pitch), mouth informativeness (lip-reading), and presence of meaningful and/or rhythmic gestures. Discourse predictability reduced the N400 amplitude and multimodal cues impacted and interacted with this effect. Specifically, higher pitch and meaningful gestures reduced N400 amplitude, beyond the effect of discourse, while rhythmic gestures increased the N400 amplitude. Moreover, higher pitch overall reduced N400 amplitude while informative mouth movements only showed effects when no gestures were present. These results can constrain existing predictive brain models in that they demonstrate that the brain uses cues selectively in a dynamic and contextually determined manner in real-world language comprehension.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\3BESFLPY\\Zhang et al. - 2020 - More than words The online orchestration of word .pdf},
  langid = {english},
  type = {preprint}
}

@article{zhangMoreWordsWord2020,
  title = {More than Words: {{Word}} Predictability, Prosody, Gesture and Mouth Movements in Natural Language Comprehension},
  shorttitle = {More than Words},
  author = {Zhang, Ye and Frassinelli, Diego and Tuomainen, Jyrki and Skipper, Jeremy I. and Vigliocco, Gabriella},
  date = {2020-03-23},
  journaltitle = {bioRxiv},
  pages = {2020.01.08.896712},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.01.08.896712},
  url = {https://www.biorxiv.org/content/10.1101/2020.01.08.896712v2},
  urldate = {2020-10-07},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The natural ecology of human language is face-to-face interaction, comprising cues, like co-speech gestures, mouth movements and prosody, tightly synchronized with speech. Yet, this rich multimodal context is usually stripped away in experimental studies as the dominant paradigm focuses on speech alone. We ask how these audio-visual cues impact brain activity during naturalistic language comprehension, how they are dynamically orchestrated and whether they are organized hierarchically. We quantify each cue in video-clips of a speaker and we used a well-established electroencephalographic marker of comprehension difficulties, an event-related potential, peaking around 400ms after word-onset. We found that multimodal cues always modulated brain activity in interaction with speech, that their impact dynamically changes with their informativeness and that there is a hierarchy: prosody shows the strongest effect followed by gestures and mouth movements. Thus, this study provides a first snapshot into how the brain dynamically weights audiovisual cues in real-world language comprehension.{$<$}/p{$>$}},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\NEG54EXJ\\Zhang et al. - 2020 - More than words Word predictability, prosody, ges.pdf;C\:\\Users\\u668173\\Zotero\\storage\\M73Z6YN2\\2020.01.08.896712v2.html},
  langid = {english}
}

@article{zhangMultimodalDeepLearning,
  title = {Multimodal {{Deep Learning Framework}} for {{Mental Disorder Recognition}}},
  author = {Zhang, Ziheng and Lin, Weizhe and Liu, Mingyu and Mahmoud, Marwa},
  pages = {7},
  abstract = {Current methods for mental disorder recognition mostly depend on clinical interviews and self-reported scores that can be highly subjective. Building an automatic recognition system can help in early detection of symptoms and providing insights into the biological markers for diagnosis. It is, however, a challenging task as it requires taking into account indicators from different modalities, such as facial expressions, gestures, acoustic features and verbal content. To address this issue, we propose a general-purpose multimodal deep learning framework, in which multiple modalities - including acoustic, visual and textual features - are processed individually with the cross-modality correlation considered. Specifically, a Multimodal Deep Denoising Autoencoder (multiDDAE) is designed to obtain multimodal representations of audio-visual features followed by the Fisher Vector encoding which produces session-level descriptors. For textual modality, a Paragraph Vector (PV) is proposed to embed the transcripts of interview sessions into document representations capturing cues related to mental disorders. Following an early fusion strategy, both audio-visual and textual features are then fused prior to feeding them to a Multitask Deep Neural Network (DNN) as the final classifier. Our framework is evaluated on the automatic detection of two mental disorders: bipolar disorder (BD) and depression, using two datasets: Bipolar Disorder Corpus (BDC) and the Extended Distress Analysis Interview Corpus (E-DAIC), respectively. Our experimental evaluation results showed comparable performance to the state-of-theart in BD and depression detection, thus demonstrating the effective multimodal representation learning and the capability to generalise across different mental disorders.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\8KURKRDY\\Zhang et al. - Multimodal Deep Learning Framework for Mental Diso.pdf},
  langid = {english}
}

@article{zhangPerinatallyInfluencedAutonomic2016,
  title = {Perinatally {{Influenced Autonomic System Fluctuations Drive Infant Vocal Sequences}}},
  author = {Zhang, Yisi S. and Ghazanfar, Asif A.},
  date = {2016-05},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {26},
  pages = {1249--1260},
  issn = {09609822},
  doi = {10.1016/j.cub.2016.03.023},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982216301993},
  urldate = {2020-12-03},
  abstract = {The variable vocal behavior of human infants is the scaffolding upon which speech and social interactions develop. It is important to know what factors drive this developmentally critical behavioral output. Using marmoset monkeys as a model system, we first addressed whether the initial conditions for vocal output and its sequential structure are perinatally influenced. Using dizygotic twins and Markov analyses of their vocal sequences, we found that in the first postnatal week, twins had more similar vocal sequences to each other than to their non-twin siblings. Moreover, both twins and their siblings had more vocal sequence similarity with each other than with non-sibling infants. Using electromyography, we then investigated the physiological basis of vocal sequence structure by measuring respiration and arousal levels (via changes in heart rate). We tested the hypothesis that early-life influences on vocal output are via fluctuations of the autonomic nervous system (ANS) mediated by vocal biomechanics. We found that arousal levels fluctuate at \$0.1 Hz (the Mayer wave) and that this slow oscillation modulates the amplitude of the faster, \$1.0 Hz respiratory rhythm. The systematic changes in respiratory amplitude result in the different vocalizations that comprise infant vocal sequences. Among twins, the temporal structure of arousal level changes was similar and therefore indicates why their vocal sequences were similar. Our study shows that vocal sequences are tightly linked to respiratory patterns that are modulated by ANS fluctuations and that the temporal structure of ANS fluctuations is perinatally influenced.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\9K2E3IGA\\Zhang and Ghazanfar - 2016 - Perinatally Influenced Autonomic System Fluctuatio.pdf},
  langid = {english},
  number = {10}
}

@article{zhangTopologicalPortraitsMultiscale2020,
  title = {Topological Portraits of Multiscale Coordination Dynamics},
  author = {Zhang, Mengsen and Kalies, William D. and Kelso, J. A. Scott and Tognoli, Emmanuelle},
  date = {2020-06-01},
  journaltitle = {Journal of Neuroscience Methods},
  shortjournal = {Journal of Neuroscience Methods},
  volume = {339},
  pages = {108672},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2020.108672},
  url = {http://www.sciencedirect.com/science/article/pii/S0165027020300947},
  urldate = {2020-05-26},
  abstract = {Living systems exhibit complex yet organized behavior on multiple spatiotemporal scales. To investigate the nature of multiscale coordination in living systems, one needs a meaningful and systematic way to quantify the complex dynamics, a challenge in both theoretical and empirical realms. The present work shows how integrating approaches from computational algebraic topology and dynamical systems may help us meet this challenge. In particular, we focus on the application of multiscale topological analysis to coordinated rhythmic processes. First, theoretical arguments are introduced as to why certain topological features and their scale-dependency are highly relevant to understanding complex collective dynamics. Second, we propose a method to capture such dynamically relevant topological information using persistent homology, which allows us to effectively construct a multiscale topological portrait of rhythmic coordination. Finally, the method is put to test in detecting transitions in real data from an experiment of rhythmic coordination in ensembles of interacting humans. The recurrence plots of topological portraits highlight collective transitions in coordination patterns that were elusive to more traditional methods. This sensitivity to collective transitions would be lost if the behavioral dynamics of individuals were treated as separate degrees of freedom instead of constituents of the topology that they collectively forge. Such multiscale topological portraits highlight collective aspects of coordination patterns that are irreducible to properties of individual parts. The present work demonstrates how the analysis of multiscale coordination dynamics can benefit from topological methods, thereby paving the way for further systematic quantification of complex, high-dimensional dynamics in living systems.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\IVYLD3NF\\Zhang et al. - 2020 - Topological portraits of multiscale coordination d.pdf;C\:\\Users\\u668173\\Zotero\\storage\\LXCDVLSV\\S0165027020300947.html},
  keywords = {Complex systems,Coordination Dynamics,Metastability,Oscillators,Topological data analysis},
  langid = {english}
}

@article{zhangVocalDevelopmentMorphological2018,
  title = {Vocal Development through Morphological Computation},
  author = {Zhang, Yisi S. and Ghazanfar, Asif A.},
  date = {2018-02-20},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {16},
  pages = {e2003933},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2003933},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2003933},
  urldate = {2019-10-17},
  abstract = {The vocal behavior of infants changes dramatically during early life. Whether or not such a change results from the growth of the body during development—as opposed to solely neural changes—has rarely been investigated. In this study of vocal development in marmoset monkeys, we tested the putative causal relationship between bodily growth and vocal development. During the first two months of life, the spontaneous vocalizations of marmosets undergo (1) a gradual disappearance of context-inappropriate call types and (2) an elongation in the duration of context-appropriate contact calls. We hypothesized that both changes are the natural consequences of lung growth and do not require any changes at the neural level. To test this idea, we first present a central pattern generator model of marmoset vocal production to demonstrate that lung growth can affect the temporal and oscillatory dynamics of neural circuits via sensory feedback from the lungs. Lung growth qualitatively shifted vocal behavior in the direction observed in real marmoset monkey vocal development. We then empirically tested this hypothesis by placing the marmoset infants in a helium–oxygen (heliox) environment in which air is much lighter. This simulated a reversal in development by decreasing the effort required to respire, thus increasing the respiration rate (as though the lungs were smaller). The heliox manipulation increased the proportions of inappropriate call types and decreased the duration of contact calls, consistent with a brief reversal of vocal development. These results suggest that bodily growth alone can play a major role in shaping the development of vocal behavior.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\4DFQJQJC\\Zhang and Ghazanfar - 2018 - Vocal development through morphological computatio.pdf;C\:\\Users\\u668173\\Zotero\\storage\\X2PTYECE\\article.html},
  keywords = {Behavior,Lungs,Marmosets,Monkeys,Nervous system,Respiratory physiology,Syllables,Vocalization},
  langid = {english},
  number = {2}
}

@article{zhangVocalStateChange2019,
  title = {Vocal State Change through Laryngeal Development},
  author = {Zhang, Yisi S. and Takahashi, Daniel Y. and Liao, Diana A. and Ghazanfar, Asif A. and Elemans, Coen P. H.},
  date = {2019-12},
  journaltitle = {Nature Communications},
  volume = {10},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-12588-6},
  url = {http://www.nature.com/articles/s41467-019-12588-6},
  urldate = {2019-10-15},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\7X5E7J3J\\Zhang et al. - 2019 - Vocal state change through laryngeal development.pdf},
  langid = {english},
  number = {1}
}

@article{zhigalovProbingCorticalExcitability2019,
  title = {Probing Cortical Excitability Using Rapid Frequency Tagging},
  author = {Zhigalov, A. and Herring, J. D. and Herpers, J. and Bergmann, T. O. and Jensen, O.},
  date = {2019-07-15},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {195},
  pages = {59--66},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2019.03.056},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811919302563},
  urldate = {2020-12-05},
  abstract = {Frequency tagging has been widely used to study the role of visual selective attention. Presenting a visual stimulus flickering at a specific frequency generates so-called steady-state visually evoked responses. However, frequency tagging is mostly done at lower frequencies ({$<$}30 Hz). This produces a visible flicker, potentially interfering with both perception and neuronal oscillations in the theta, alpha and beta band. To overcome these problems, we used a newly developed projector with a 1440 Hz refresh rate allowing for frequency tagging at higher frequencies. We asked participants to perform a cued spatial attention task in which imperative pictorial stimuli were presented at 63 Hz or 78 Hz while measuring whole-head magnetoencephalography (MEG). We found posterior sensors to show a strong response at the tagged frequency. Importantly, this response was enhanced by spatial attention. Furthermore, we reproduced the typical modulations of alpha band oscillations, i.e., decrease in the alpha power contralateral to the attentional cue. The decrease in alpha power and increase in frequency tagged signal with attention correlated over subjects. We hereby provide proof-of-principle for the use of high-frequency tagging to study sensory processing and neuronal excitability associated with attention.},
  file = {C\:\\Users\\u668173\\Zotero\\storage\\FUMBXEGA\\Zhigalov et al. - 2019 - Probing cortical excitability using rapid frequenc.pdf;C\:\\Users\\u668173\\Zotero\\storage\\3DHSY5V2\\S1053811919302563.html},
  langid = {english}
}


@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2020},
  url = {https://www.R-project.org/},
}
@Manual{R-cluster,
  title = {cluster: Cluster Analysis Basics and Extensions},
  author = {Martin Maechler and Peter Rousseeuw and Anja Struyf and Mia Hubert and Kurt Hornik},
  year = {2019},
  note = {R package version 2.1.0 --- For new features, see the 'Changelog' file (in the package source)},
}
@Article{R-clValid,
  title = {{clValid}: An {R} Package for Cluster Validation},
  author = {Guy Brock and Vasyl Pihur and Susmita Datta and Somnath Datta},
  journal = {Journal of Statistical Software},
  year = {2008},
  volume = {25},
  number = {4},
  pages = {1--22},
  url = {http://www.jstatsoft.org/v25/i04/},
}
@Manual{R-cowplot,
  title = {cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2020},
  note = {R package version 1.1.1},
  url = {https://CRAN.R-project.org/package=cowplot},
}
@Manual{R-DescTools,
  title = {{DescTools}: Tools for Descriptive Statistics},
  author = {Signorell {Andri et mult. al.}},
  year = {2020},
  note = {R package version 0.99.39},
  url = {https://cran.r-project.org/package=DescTools},
}
@Article{R-dtw_a,
  title = {Computing and Visualizing Dynamic Time Warping Alignments in {R}: The {dtw} Package},
  author = {Toni Giorgino},
  journal = {Journal of Statistical Software},
  year = {2009},
  volume = {31},
  number = {7},
  pages = {1--24},
  doi = {10.18637/jss.v031.i07},
}
@Article{R-dtw_b,
  title = {Matching Incomplete Time Series with Dynamic Time Warping: An Algorithm and an Application to Post-Stroke Rehabilitation},
  author = {Paolo Tormene and Toni Giorgino and Silvana Quaglini and Mario Stefanelli},
  journal = {Artificial Intelligence in Medicine},
  year = {2008},
  volume = {45},
  number = {1},
  pages = {11--34},
  doi = {10.1016/j.artmed.2008.11.007},
}
@Manual{R-effsize,
  title = {effsize: Efficient Effect Size Computation},
  author = {Marco Torchiano},
  year = {2020},
  note = {R package version 0.8.1},
  doi = {10.5281/zenodo.1480624},
  url = {https://CRAN.R-project.org/package=effsize},
}
@Manual{R-EMAtools,
  title = {EMAtools: Data Management Tools for Real-Time Monitoring/Ecological
Momentary Assessment Data},
  author = {Evan Kleiman},
  year = {2017},
  note = {R package version 0.1.3},
  url = {https://CRAN.R-project.org/package=EMAtools},
}
@Manual{R-entropy,
  title = {entropy: Estimation of Entropy, Mutual Information and Related Quantities},
  author = {Jean Hausser and Korbinian Strimmer},
  year = {2014},
  note = {R package version 1.2.1},
  url = {https://CRAN.R-project.org/package=entropy},
}
@Manual{R-ggbeeswarm,
  title = {ggbeeswarm: Categorical Scatter (Violin Point) Plots},
  author = {Erik Clarke and Scott Sherrill-Mix},
  year = {2017},
  note = {R package version 0.6.0},
  url = {https://CRAN.R-project.org/package=ggbeeswarm},
}
@Manual{R-ggExtra,
  title = {ggExtra: Add Marginal Histograms to 'ggplot2', and More 'ggplot2'
Enhancements},
  author = {Dean Attali and Christopher Baker},
  year = {2019},
  note = {R package version 0.9},
  url = {https://CRAN.R-project.org/package=ggExtra},
}
@Manual{R-ggnetwork,
  title = {ggnetwork: Geometries to Plot Networks with 'ggplot2'},
  author = {François Briatte},
  year = {2020},
  note = {R package version 0.5.8},
  url = {https://CRAN.R-project.org/package=ggnetwork},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}
@Manual{R-gridExtra,
  title = {gridExtra: Miscellaneous Functions for "Grid" Graphics},
  author = {Baptiste Auguie},
  year = {2017},
  note = {R package version 2.3},
  url = {https://CRAN.R-project.org/package=gridExtra},
}
@Article{R-igraph,
  title = {The igraph software package for complex network research},
  author = {Gabor Csardi and Tamas Nepusz},
  journal = {InterJournal},
  volume = {Complex Systems},
  pages = {1695},
  year = {2006},
  url = {https://igraph.org},
}
@Book{R-MASS,
  title = {Modern Applied Statistics with S},
  author = {W. N. Venables and B. D. Ripley},
  publisher = {Springer},
  edition = {Fourth},
  address = {New York},
  year = {2002},
  note = {ISBN 0-387-95457-0},
  url = {http://www.stats.ox.ac.uk/pub/MASS4/},
}
@Article{R-network,
  title = {network: a Package for Managing Relational Data in R.},
  author = {Carter T. Butts},
  journal = {Journal of Statistical Software},
  year = {2008},
  volume = {24},
  number = {2},
  url = {https://www.jstatsoft.org/v24/i02/paper},
}
@Manual{R-nlme,
  title = {{nlme}: Linear and Nonlinear Mixed Effects Models},
  author = {Jose Pinheiro and Douglas Bates and Saikat DebRoy and Deepayan Sarkar and {R Core Team}},
  year = {2020},
  note = {R package version 3.1-150},
  url = {https://CRAN.R-project.org/package=nlme},
}
@Manual{R-papaja,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2020},
  note = {R package version 0.1.0.9997},
  url = {https://github.com/crsh/papaja},
}
@Manual{R-parallelsugar,
  title = {parallelsugar: mclapply() syntax for Windows machines},
  author = {Nathan VanHoudnos},
  year = {2021},
  note = {R package version 0.0.0.2},
}
@Book{R-plotly,
  author = {Carson Sievert},
  title = {Interactive Web-Based Data Visualization with R, plotly, and shiny},
  publisher = {Chapman and Hall/CRC},
  year = {2020},
  isbn = {9781138331457},
  url = {https://plotly-r.com},
}
@Manual{R-pracma,
  title = {pracma: Practical Numerical Math Functions},
  author = {Hans W. Borchers},
  year = {2021},
  note = {R package version 2.3.3},
  url = {https://CRAN.R-project.org/package=pracma},
}
@Manual{R-proxy,
  title = {proxy: Distance and Similarity Measures},
  author = {David Meyer and Christian Buchta},
  year = {2020},
  note = {R package version 0.4-24},
  url = {https://CRAN.R-project.org/package=proxy},
}
@Manual{R-r2glmm,
  title = {r2glmm: Computes R Squared for Mixed (Multilevel) Models},
  author = {Byron Jaeger},
  year = {2017},
  note = {R package version 0.1.2},
  url = {https://CRAN.R-project.org/package=r2glmm},
}
@Manual{R-raster,
  title = {raster: Geographic Data Analysis and Modeling},
  author = {Robert J. Hijmans},
  year = {2020},
  note = {R package version 3.4-5},
  url = {https://CRAN.R-project.org/package=raster},
}
@Manual{R-RColorBrewer,
  title = {RColorBrewer: ColorBrewer Palettes},
  author = {Erich Neuwirth},
  year = {2014},
  note = {R package version 1.1-2},
  url = {https://CRAN.R-project.org/package=RColorBrewer},
}
@Manual{R-scales,
  title = {scales: Scale Functions for Visualization},
  author = {Hadley Wickham and Dana Seidel},
  year = {2020},
  note = {R package version 1.1.1},
  url = {https://CRAN.R-project.org/package=scales},
}
@Manual{R-signal,
  title = {{signal}: Signal processing},
  author = {{signal developers}},
  year = {2014},
  url = {http://r-forge.r-project.org/projects/signal/},
}
@Manual{R-sna,
  title = {sna: Tools for Social Network Analysis},
  author = {Carter T. Butts},
  year = {2020},
  note = {R package version 2.6},
  url = {https://CRAN.R-project.org/package=sna},
}
@Article{R-sp,
  author = {Edzer J. Pebesma and Roger S. Bivand},
  title = {Classes and methods for spatial data in {R}},
  journal = {R News},
  year = {2005},
  volume = {5},
  number = {2},
  pages = {9--13},
  month = {November},
  url = {https://CRAN.R-project.org/doc/Rnews/},
}
@Manual{R-statnet.common,
  author = {Pavel N. Krivitsky},
  title = {statnet.common: Common R Scripts and Utilities Used by the Statnet Project Software},
  organization = {The Statnet Project (\url{https://statnet.org})},
  year = {2020},
  note = {R package version 4.4.1},
  url = {https://CRAN.R-project.org/package=statnet.common},
}
@Manual{R-stringr,
  title = {stringr: Simple, Consistent Wrappers for Common String Operations},
  author = {Hadley Wickham},
  year = {2019},
  note = {R package version 1.4.0},
  url = {https://CRAN.R-project.org/package=stringr},
}
@Article{R-TDAstats,
  title = {{T}{D}{A}stats: {R} pipeline for computing persistent homology in topological data analysis},
  author = {Raoul R. Wadhwa and Drew F. K. Williamson and Andrew Dhawan and Jacob G. Scott},
  journal = {Journal of Open Source Software},
  year = {2018},
  volume = {3},
  number = {28},
  pages = {860},
  url = {https://doi.org/10.21105/joss.00860},
  doi = {10.21105/joss.00860},
}
@Manual{R-tsne,
  title = {tsne: T-Distributed Stochastic Neighbor Embedding for R (t-SNE)},
  author = {Justin Donaldson},
  year = {2016},
  note = {R package version 0.1-3},
  url = {https://CRAN.R-project.org/package=tsne},
}
